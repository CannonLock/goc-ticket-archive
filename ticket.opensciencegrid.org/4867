<!DOCTYPE html>
<html lang="en">
  <head>
  <base href="">
    <title>[4867] Fatal gridmanager error killing jobs</title>    <meta charset="utf-8" />
    <meta name="verify-v1" content="na5IcAJsZVOfEkboRxuIiZ1zpZgnZiWra+nKcS7nA/o=" />
    <meta name="google-site-verification" content="DLrk3ft4s8b-S2TloLCL2LD_t6wcTjgSluf5pmiu2kA" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <link href="https://ticket.opensciencegrid.org/rss" rel="alternate" type="application/rss+xml" title="GOC Ticket Update feed" />

    <style type="text/css">
      body {
        padding-top: 50px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
     #search {
            width: 300px;
     }

    </style>

<script src="https://code.jquery.com/jquery-3.0.0.js"></script>
<script src="https://code.jquery.com/jquery-migrate-3.0.1.js"></script>

   <link href="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet"/>
    <script src="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"/>
    <link href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.min.css" rel="stylesheet"/>
 <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>


    <link href="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/css/select2.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/js/select2.min.js"></script>

    <link href="css/ticket.css" rel="stylesheet" />
    <script src="lib/jquery.cookie.js"></script>

    <link href="images/tag_orange.png" rel="icon" type="image/png"/>
    <script src='https://www.google.com/recaptcha/api.js'></script>
  </head>

  <body>
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
            <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </a>

            <a class="brand" style="padding: 6px 0px 0px 6px;" href="http://opensciencegrid.org"><img src="images/osglogo.40x30.png"/></a>
            <ul class="nav">
                <li class="dropdown"><a href="https://ticket.opensciencegrid.org/#" class="dropdown-toggle" data-toggle="dropdown">GOC Ticket <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    <li><a href="https://my.opensciencegrid.org">MyOSG</a></li>
                    <li><a href="https://oim.opensciencegrid.org">OIM</a></li>
                    <li class="active"><a href="https://ticket.opensciencegrid.org/index">Ticket</a></li>
	<li class="divider"></li>
	<li><a href="http://repo.grid.iu.edu">Repo</a></li>
	<li class="divider"></li>
	<li><a href="http://blogs.grid.iu.edu">Blog</a></li>
                    <li><a href="http://display.grid.iu.edu">Display</a></li>
                    <li><a href="http://osggoc.blogspot.com/">News</a></li>
                    </ul>
                </li>
            </ul>
            <ul class="nav pull-right">
                <li><a href="https://ticket.opensciencegrid.org/sso/">Login</a></li>            </ul>

            <div class="nav-collapse">
                <ul class="nav">
			 <li id="menu_submit"><a href="https://ticket.opensciencegrid.org/submit">Submit</a></li><li id="menu_view" class="dropdown"><a href="https://ticket.opensciencegrid.org/\#" class="dropdown-toggle" data-toggle="dropdown">View <b class="caret"></b></a><ul class="dropdown-menu"><li id="submenu_listopen"><a href="https://ticket.opensciencegrid.org/list/open">Open Tickets</a></li><li id="submenu_listrecentclose"><a href="https://ticket.opensciencegrid.org/list/recentclose">Recently Closed Tickets</a></li><li class="divider"></li><li id="submenu_alltickets"><a href="https://ticket.opensciencegrid.org/search?q=&amp;sort=id">All Tickets</a></li></ul></li>                </ul>

                <form class="navbar-search pull-right" action="https://ticket.opensciencegrid.org/viewer">
                    <input id="search" type="text" name="id" class="search-query span2" placeholder="Search Ticket" value=""/>
                </form>
            </div>
        </div>
      </div>
    </div>

<script type='text/javascript' src='lib/jquery.timeago.js'></script>
<script type='text/javascript' src='lib/byte2size.js'></script>
<style>
#updates .toolbar {
position: relative;
margin-top: 0px;
top: -10px;
font-weight: normal;
}
#updates a.anchor {
position: relative;
top: -50px;
}
#updates .selected pre {
animation:selected 2s;
animation-iteration-count: 2;
animation-direction: alternate;
-webkit-animation:selected 2s; 
-webkit-animation-iteration-count: 2;
-webkit-animation-direction: alternate;
box-shadow: inset 1px 1px 20px #9ad;
border: 1px solid #9ab;
margin: 5px 0px;
padding-left: 10px;
}
@keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ab;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
@-webkit-keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ad;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
#updates pre {
background-color: inherit;
line-height: 15px;
padding: 5px;
}
#updates .header {
color: #999;
}
#updates .update_history pre {
background-color: #eee;
color: #666;
font-size: 85%;
}
#updates .clickable {
cursor: pointer;
}
#updates .clickable:hover {
color: #D98719;
}
#updates .meta_information pre {
background-color: #fed;
}
#similar_tickets {
max-height: 300px;
overflow-y: auto;
pointer-events: none;
padding: 5px;
background-color: #f4f4f4;
}
.btn-toolbar {
margin-bottom: 0;
height: 30px;
}
#peers {
position: fixed;
bottom: 0px;
right: 0px;
z-index: 100;
list-style: none;
padding: 5px 0px 0px 5px;
margin: 0px;
background-color: white;
box-shadow: 0px 0px 10px white;
}
#peers li {
background-color: #ccc;
color: #000;
display: inline-block;
padding: 5px 10px;
margin-right: 5px;
position: relative;
}
/*
#peers li:hover {
background-color: #999;
cursor: pointer;
}
*/
#peers span.ip {
padding-left: 5px;
color: #666;
}
#peers .new {
bottom: -30px;
}
/*
#peers .me {
background-color: red;
}
*/
</style>

<div class="container-fluid">
<ul id="peers"></ul>
<div class="alert alert-danger"><a class="close" href="https://ticket.opensciencegrid.org/#" data-dismiss="alert">&times;</a>By the end of May 2018, the ticketing system at https://ticket.opensciencegrid.org will be retired and support will be provided at https://support.opensciencegrid.org. Throughout this transition the support email (help@opensciencegrid.org) will be available as a point of contact.<br><br>                                                   
                                                                                                                                                                                   
Please see the service migration page for details: https://opensciencegrid.github.io/technology/policy/service-migrations-spring-2018/#ticket</div><div id="presence" class="pull-right"></div><div class="ticketgui"><script type="text/javascript" src="lib/checktab.js"></script>

<script>
var expanded = false;
function expand_description() {
    var desc = $(".description");
    if(!expanded) {
        expanded = true;
        //expand to minheight
        var min = 250;
        if(desc.height() < min) {
            desc.animate({height: min}, 200);
        }
    }
}

$(document).ready(function() {
    $("input[name='nad']").datepicker({
        dateFormat: 'yy-mm-dd'
    });
});

</script>



<style>
.form-horizontal .control-label {
padding-top: inherit;
font-size:90%;
color:#666;
}
label {
margin-bottom: 0px;
}
.controls {
line-height: 18px;
}
</style>
<form class="form-horizontal" action="https://ticket.opensciencegrid.org/viewer/updatebasic?id=4867" method="post">
<div class="page-header">
    <h3><span class="muted">4867</span> / Fatal gridmanager error killing jobs</h3>
</div>

<div class="row-fluid">
<div class="span5">
    <legend>Contact</legend>
    <div class="control-group">
        <label class="control-label">Full Name</label>
        <div class="controls">Elisabeth Atems</div>
    </div>
    <div class="control-group">
        <label class="control-label">Email</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">Phone</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">CC</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>

    <legend>Details</legend>
    
    <div class="control-group">
        <label class="control-label">Ticket Type</label>
        <div class="controls">Problem/Request</div>
    </div>
    <div class="control-group">
        <label class="control-label">Priority</label>
        <div class="controls">Normal</div>
    </div>
    <div class="control-group">
        <label class="control-label">Status</label>
        <div class="controls">
Closed</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action</label>
        <div class="controls">Eng Review</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action Deadline</label>
        <div class="controls flag_red">2008-07-31</div>
    </div>

</div><!--span-->
<div class="span7">
    <legend>Assignees</legend>
    <div class="assignee" style="width: 60%">OSG-GOC <span class="muted"> / OSG Support Centers</span></div><div class="assignee" style="width: 60%">STAR <span class="muted"> / OSG Support Centers</span></div><div class="assignee" style="width: 60%">Rob Quick <span class="muted"> / OSG GOC Management</span></div>    <br>

    <legend>Assignees</legend>
    TODO
    <br>

    <style>
legend.noborder {
border-bottom: none;
}
</style>

<div id="attachment-list"/>
<script>
$(function () {
    var first = true;
    $.getJSON("https://ticket.opensciencegrid.org/attachment/list?id=4867", function (files) {
        //console.dir(files);
        var html = "<table class=\"table table-condensed\">";
        $(files).each(function() {
            if(first) {
                first = false;
                html += "<legend class=\"noborder\">Attachmenets</legend>";
            }
            html += "<tr class=\"attachment\">";
            html += "<td><img src="https://ticket.opensciencegrid.org/\&quot;&quot;+this.thumbnail_url+&quot;\&quot;/"></td>";
            html += "<td><a href="https://ticket.opensciencegrid.org/\&quot;&quot;+this.url+&quot;\&quot;" target=\"_blank\">"+this.name+"</a></td>";
            html += "<td>"+bytesToSize(this.size, 1)+"</td>";
            html += "</tr>";
        });
        html += "</table>";
        $("#attachment-list").html(html);
    });
});

function download(url) {
    window.open(url, "_blank");
}
</script>


</div><!--span-->
</div><!--row-fluid-->


</form>

</div>
<div id="updates" style="clear: both;">
    <legend>Past Updates
    <div class="btn-toolbar pull-right toolbar">
        <div class="btn-group">
                <a class="btn btn-small" href="https://ticket.opensciencegrid.org/4867?sort=up&amp;"><i class="icon-arrow-up"></i> Sort</a>

        
        <a class="btn btn-small" href="https://ticket.opensciencegrid.org/4867?expandall=true&amp;">Expand Descriptions</a>        <a class="btn btn-small" target="_blank" href="mailto:osg@tick.globalnoc.iu.edu?subject=Open%20Science%20Grid%3A%20Fatal%20gridmanager%20error%20killing%20jobs%20ISSUE%3D4867%20PROJ%3D71"><i class="icon-envelope"></i> Update w/Email</a>
        </div>
    </div><!--btn-toolbar-->
    </legend>

    <div class='update_description'><i onclick="document.location='4867#1216924986'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-24T18:43:06+00:00">Jul 24, 2008 06:43 PM UTC</time> by <b>echism</b><a class="anchor" name="1216924986">&nbsp;</a></div><pre>Closing as requested.

Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1216924749'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-24T18:39:09+00:00">Jul 24, 2008 06:39 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1216924749">&nbsp;</a></div><pre>Please see previous message and close this ticket.

Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1216924569'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-24T18:36:09+00:00">Jul 24, 2008 06:36 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1216924569">&nbsp;</a></div><pre>We believe this issue is now resolved. Please close this ticket.

Thanks,
Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1216924248'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-24T18:30:48+00:00">Jul 24, 2008 06:30 PM UTC</time> by <b>echism</b><a class="anchor" name="1216924248">&nbsp;</a></div><pre>Are there any updates on this issue? This ticket remains open.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1216303211'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-17T14:00:11+00:00">Jul 17, 2008 02:00 PM UTC</time> by <b>didenko@....</b><a class="anchor" name="1216303211">&nbsp;</a></div><pre>Liz, acually I see this message in 1 job from 10 submitted&#58;

Wire Plane is empty
/data/r23b/sge-root/wsurhic/spool/rhic26/job_scripts/15804&#58; line 40&#58;  9387
File size limit
exceeded/home/osg-star/.globus/.gass_cache/local/md5/3b/06393ef7b3b51d3044d084d255c782/md5/a1/faf8de5aee5bd0a711b05c2a835bb5/data
&#60;/dev/null

log file for this job is too big, over 2GB and there are 23909870 lines
like
&#39;StDetectorDbTpcRDOMasks&#58;&#58;getSectorMask &#58; return default mask for
sector= 22 mNumEntries=0&#39;

Perhaps again DB entries are red properly. I see problem connecting
to DB server in each file&#58;

StInfo&#58; MysqlDb&#58;&#58;reConnect() line=241  Connection Failed with MySQL on
dbx.star.bnl.gov&#58;3316
Returned error =Can&#39;t connect to MySQL server on &#39;dbx.star.bnl.gov&#39; (4).
Will re-try with timeout set at
==&#62; 2 seconds &#60;==

Lidia.

On Wed, 16 Jul 2008, Elisabeth Atems wrote&#58;

<font color='#7F7E6F'>&#62; I am still not sure. Lidia is not seeing the signature error messages</font>
<font color='#7F7E6F'>&#62; of the problem but there were still job failures with a message that</font>
<font color='#7F7E6F'>&#62; sometimes accompanied the earlier failures&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;    /data/r23b/sge-root/wsurhic/spool/rhic24/job_scripts/15762&#58; line 40&#58;</font>
<font color='#7F7E6F'>&#62;    7968 File size limit exceeded/home/osg-star/.globus/.gass_cache/</font>
<font color='#7F7E6F'>&#62;   local/md5/70/b64c58fdfff3ef8a9899e2d79a8bda/md5/f2/362cfcd0eaf9bc7a088f0a5dccb9c2/data</font>
<font color='#7F7E6F'>&#62;   &#60;/dev/null</font>
<font color='#7F7E6F'>&#62;</font>
<div id='show_425868309' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_425868309'><font color='#7F7E6F'>&#62; which led to a runaway load average problem on the gatekeeper last</font>
<font color='#7F7E6F'>&#62; week. The gatekeeper was rebooted and the large files were deleted,</font>
<font color='#7F7E6F'>&#62; the load avg problem has not recurred so far this week, but we don&#39;t</font>
<font color='#7F7E6F'>&#62; understand why it occurred, thus can&#39;t consider it resolved, and don&#39;t</font>
<font color='#7F7E6F'>&#62; know whether it was related to the killing jobs problem.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Sorry I didn&#39;t update the ticket till now, I was distracted by the</font>
<font color='#7F7E6F'>&#62; load average problem which seriously affected local services on the</font>
<font color='#7F7E6F'>&#62; gatekeeper node.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Liz</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Open Science Grid FootPrints writes&#58;</font>
<font color='#7F7E6F'>&#62;&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_425868309').click(function() {
            $('#detail_425868309').slideDown("normal");
            $('#show_425868309').hide();
            $('#hide_425868309').show();
        });
        $('#hide_425868309').click(function() {
            $('#detail_425868309').slideUp();
            $('#hide_425868309').hide();
            $('#show_425868309').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='4867#1216263609'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-17T03:00:09+00:00">Jul 17, 2008 03:00 AM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1216263609">&nbsp;</a></div><pre>I am still not sure. Lidia is not seeing the signature error messages
of the problem but there were still job failures with a message that
sometimes accompanied the earlier failures&#58;

/data/r23b/sge-root/wsurhic/spool/rhic24/job_scripts/15762&#58; line 40&#58;
7968 File size limit exceeded/home/osg-star/.globus/.gass_cache/
local/md5/70/b64c58fdfff3ef8a9899e2d79a8bda/md5/f2/362cfcd0eaf9bc7a088f0a5dccb9c2/data
&#60;/dev/null

which led to a runaway load average problem on the gatekeeper last
week. The gatekeeper was rebooted and the large files were deleted,
the load avg problem has not recurred so far this week, but we don&#39;t
understand why it occurred, thus can&#39;t consider it resolved, and don&#39;t
know whether it was related to the killing jobs problem.

Sorry I didn&#39;t update the ticket till now, I was distracted by the
load average problem which seriously affected local services on the
gatekeeper node.

Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1216248392'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-16T22:46:32+00:00">Jul 16, 2008 10:46 PM UTC</time> by <b>echism</b><a class="anchor" name="1216248392">&nbsp;</a></div><pre>Hello,

We are still waiting for an update. Are the jobs getting through? If there is no
response in a week, I will consider this ticket abandoned.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1215719759'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-10T19:55:59+00:00">Jul 10, 2008 07:55 PM UTC</time> by <b>echism</b><a class="anchor" name="1215719759">&nbsp;</a></div><pre>Are the jobs getting through now? We never received an update.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1215109389'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-03T18:23:09+00:00">Jul 3, 2008 06:23 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1215109389">&nbsp;</a></div><pre>That would be in the Condor config file on the submitting node.
Counter-intuitive, I know.

-- Jaime

On Jul 3, 2008, at 12&#58;38 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1215107769'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-03T17:56:09+00:00">Jul 3, 2008 05:56 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1215107769">&nbsp;</a></div><pre>Yes, actually, Jaime found that the WSU firewall was blocking most of
the port range set in GLOBUS_TCP_PORT_RANGE and we have taken steps to
correct that. There have been some messages about this today on
stargrid-l. I hope to hear soon from Lidia on whether her jobs are
now getting through and running to completion.

Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1215106692'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-03T17:38:12+00:00">Jul 3, 2008 05:38 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1215106692">&nbsp;</a></div><pre>That would be the config file on the gridmanager side, on the
submitting node? I&#39;m confused since it seems that the number of
jobmanagers to run on the gatekeeper should be something under the CE
admin&#39;s control not remote users. But I don&#39;t find that parameter in
my Condor config file at all.

Our network liaison passed along the port range request yesterday but
I am not sure whether they have acted on it yet.

Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1215105917'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-03T17:25:17+00:00">Jul 3, 2008 05:25 PM UTC</time> by <b>echism</b><a class="anchor" name="1215105917">&nbsp;</a></div><pre>Are there any updates on this?

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1215027489'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-02T19:38:09+00:00">Jul 2, 2008 07:38 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1215027489">&nbsp;</a></div><pre>Condor-G&#39;s default configuration is to run no more than 10 jobmanagers
at a time on the gatekeeper. Each jobmanager listens on two ports, so
if only one user is submitting jobs at a time, they would just fit
inside a 20-port window. In the latest gridmanager log posted on this
ticket, it appeared Condor was configured to run more than 10
jobmanagers at a time.

The relevant parameter is GRIDMANAGER_MAX_JOBMANAGERS_PER_RESOURCE
in the Condor config file.

-- Jaime

On Jul 2, 2008, at 10&#58;32 AM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1215012729'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-02T15:32:09+00:00">Jul 2, 2008 03:32 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1215012729">&nbsp;</a></div><pre>Thank you for looking into this. I will ask our network liaison to
check the firewall. This would be a very easy typo to make! I looked
back at my emails asking for them to open 20000-20200
(GLOBUS_TCP_PORT_RANGE) and made sure that I really did write 20200.

I hope this turns out to be the problem, but I am skeptical... because
Lidia&#39;s jobs ran fine for several weeks and then started failing
for no apparent reason. They may simply have not started blocking
ports in that range until then however.

Fingers crossed, and thanks again!

Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1215011649'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-07-02T15:14:09+00:00">Jul 2, 2008 03:14 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1215011649">&nbsp;</a></div><pre>I apologize for the delayed response.

I have looked at the log and this appears to be a firewall problem on
rhic23.physics.wayne.edu. The range of open ports on the firewall is
too small. It appears to be about only 20 ports wide (20000-20020).
Gram jobmanagers that get a port higher than 20020 fail.

--  Jaime

On Jul 1, 2008, at 1&#58;55 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1214590631'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-27T18:17:11+00:00">Jun 27, 2008 06:17 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1214590631">&nbsp;</a></div><pre>It has mostly been Jaime and Leve... since the info Jaime needs is in
the gridmanager logs on the node submitting the job. I think Leve sent
a new log from a failed job a couple of days ago and we haven&#39;t heard
back from Jaime. Unless I missed a mail?

Anyway, the short answer is no, we (they) don&#39;t have an answer yet
that I know of, so please keep the ticket open for now.

Thanks,
Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1214590192'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-27T18:09:52+00:00">Jun 27, 2008 06:09 PM UTC</time> by <b>Rob Quick</b><a class="anchor" name="1214590192">&nbsp;</a></div><pre>Elisabeth,

I see Jamie, Wayne and you have been troubleshooting this issue... have you found a
resolution yet?

Rob</pre></div><div class='update_description'><i onclick="document.location='4867#1214317028'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-24T14:17:08+00:00">Jun 24, 2008 02:17 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1214317028">&nbsp;</a></div><pre>I don&#39;t know how upgrades of the OSG software stack typically occur.
I&#39;ll bet someone or something copied the Condor configuration files
from the 0.6.0 directory to the new 0.8.0 one, and didn&#39;t spot the
path in MYPROXY_GET_DELEGATION.

To avoid this sort of problem you can change the parameter to this&#58;
MYPROXY_GET_DELEGATION = $(BIN)/myproxy-get-delegation

I&#39;m not surprised that the myproxy-get-delegation from OSG 0.6.0
worked fine in this case.

-- Jaime

On Jun 19, 2008, at 4&#58;11 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213987271'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-20T18:41:11+00:00">Jun 20, 2008 06:41 PM UTC</time> by <b>lbhajdu@....</b><a class="anchor" name="1213987271">&nbsp;</a></div><pre>Here is the new log (2.14MB). A failed job is 13754.

Leve

Lidia writes&#58;

/star/data05/scratch/didenko/gridwork

JobId is 72059E1F05CB6371C76B55D4A981FA6D

failed job sched72059E1F05CB6371C76B55D4A981FA6D_3.condorg.log

-- 000 (13754.000.000) 06/20 02&#58;02&#58;58 Job submitted from host&#58;
&#60;130.199.6.111&#58;20615&#62;
...
018 (13754.000.000) 06/20 02&#58;13&#58;18 Globus job submission failed!
Reason&#58; 111 the job manager timed out while waiting for a commit signal
...
012 (13754.000.000) 06/20 02&#58;13&#58;23 Job was held.
Unspecified gridmanager error
Code 0 Subcode 0</pre></div><div class='update_description'><i onclick="document.location='4867#1213909871'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-19T21:11:11+00:00">Jun 19, 2008 09:11 PM UTC</time> by <b>Wayne Betts</b><a class="anchor" name="1213909871">&nbsp;</a></div><pre>I see an error in the file that must have been there from the time of
the 0.8.0 client installation, but strikes me as rather peculiar how it
could wind up like this&#58;

MYPROXY_GET_DELEGATION =
/opt/OSG-0.6.0-client/globus/bin/myproxy-get-delegation

should be

MYPROXY_GET_DELEGATION =
/opt/OSG-0.8.0-client/globus/bin/myproxy-get-delegation

I changed this just now and did another condor_reconfig.  (FWIW, the
0.6.0 path is valid -- the 0.6.0 client files are still on the system,
so maybe it would work just fine anyway.  I don&#39;t know.)

-Wayne

---/opt/OSG-0.8.0-client/condor/local.stargrid01/condor_config.local---

##  What machine is your central manager?

CONDOR_HOST = stargrid01.rcf.bnl.gov

##  Pathnames&#58;
##  Where have you installed the bin, sbin and lib condor directories?

RELEASE_DIR = /opt/OSG-0.8.0-client/condor

##  Where is the local condor directory for each host?
##  This is where the local config file(s), logs and
##  spool/execute directories are located

LOCAL_DIR = /opt/OSG-0.8.0-client/condor/local.$(HOSTNAME)

<div id='show_525192247' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_525192247'>##  Mail parameters&#58;
##  When something goes wrong with condor at your site, who should get
##  the email?

CONDOR_ADMIN = root@....

##  Full path to a mail delivery program that understands that &#34;-s&#34;
##  means you want to specify a subject&#58;

MAIL = /bin/mailx

##  Network domain parameters&#58;
##  Internet domain of machines sharing a common UID space.  If your
##  machines don&#39;t share a common UID space, set it to
##  UID_DOMAIN = $(FULL_HOSTNAME)
##  to specify that each machine has its own UID space.

UID_DOMAIN = rcf.bnl.gov

##  Internet domain of machines sharing a common file system.
##  If your machines don&#39;t use a network file system, set it to
##  FILESYSTEM_DOMAIN = $(FULL_HOSTNAME)
##  to specify that each machine has its own file system.

FILESYSTEM_DOMAIN = rcf.bnl.gov

##  This macro is used to specify a short description of your pool.
##  It should be about 20 characters long. For example, the name of
##  the UW-Madison Computer Science Condor Pool is &#96;&#96;UW-Madison CS&#39;&#39;.

COLLECTOR_NAME = Personal Condor at stargrid01.rcf.bnl.gov

##  The user/group ID &#60;uid&#62;.&#60;gid&#62; of the &#34;Condor&#34; user.
##  (this can also be specified in the environment)
##  Note&#58; the CONDOR_IDS setting is ignored on Win32 platforms

CONDOR_IDS = 31020.31016

##  Condor needs to create a few lock files to synchronize access to
##  various log files.  Because of problems we&#39;ve had with network
##  filesystems and file locking over the years, we HIGHLY recommend
##  that you put these lock files on a local partition on each
##  machine.  If you don&#39;t have your LOCAL_DIR on a local partition,
##  be sure to change this entry.  Whatever user (or group) condor is
##  running as needs to have write access to this directory.  If
##  you&#39;re not running as root, this is whatever user you started up
##  the condor_master as.  If you are running as root, and there&#39;s a
##  condor account, it&#39;s probably condor.  Otherwise, it&#39;s whatever
##  you&#39;ve set in the CONDOR_IDS environment variable.  See the Admin
##  manual for details on this.

LOCK = /tmp/condor-lock.$(HOSTNAME)0.119711166554229

##  When is this machine willing to start a job?

START = TRUE

##  When to suspend a job?

SUSPEND = FALSE

##  When to nicely stop a job?
##  (as opposed to killing it instantaneously)

PREEMPT = FALSE

##  When to instantaneously kill a preempting job
##  (e.g. if a job is in the pre-empting stage for too long)

KILL = FALSE

##  condor_master
##  Daemons you want the master to keep running for you&#58;

DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, SCHEDD, STARTD

##  Java parameters&#58;
##  If you would like this machine to be able to run Java jobs,
##  then set JAVA to the path of your JVM binary.  If you are not
##  interested in Java, there is no harm in leaving this entry
##  empty or incorrect.

JAVA = /opt/OSG-0.8.0-client/jdk1.5/bin/java

##  Some JVMs need to be told the maximum amount of heap memory
##  to offer to the process.  If your JVM supports this, give
##  the argument here, and Condor will fill in the memory amount.
##  If left blank, your JVM will choose some default value,
##  typically 64 MB.  The default (-Xmx) works with the Sun JVM.

JAVA_MAXHEAP_ARGUMENT = -Xmx

## Condor-G and CredD can use MyProxy to refresh GSI proxies which are
## about to expire.

MYPROXY_GET_DELEGATION =
/opt/OSG-0.6.0-client/globus/bin/myproxy-get-delegation

# CA certificate directory for Condor-G
GSI_DAEMON_TRUSTED_CA_DIR = /opt/OSG-0.8.0-client/globus/TRUSTED_CA
# This value of HOSTALLOW_WRITE overwrites the invalid value in
condor_config
HOSTALLOW_WRITE = $(FULL_HOSTNAME)
# Security setup to use pool password
SEC_PASSWORD_FILE =
/opt/OSG-0.8.0-client/condor/local.stargrid01/pool_password
SEC_DAEMON_AUTHENTICATION = REQUIRED
SEC_DAEMON_INTEGRITY = REQUIRED
SEC_DAEMON_AUTHENTICATION_METHODS = PASSWORD
SEC_NEGOTIATOR_AUTHENTICATION = REQUIRED
SEC_NEGOTIATOR_INTEGRITY = REQUIRED
SEC_NEGOTIATOR_AUTHENTICATION_METHODS = PASSWORD
SEC_CLIENT_AUTHENTICATION_METHODS = FS, PASSWORD

Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_525192247').click(function() {
            $('#detail_525192247').slideDown("normal");
            $('#show_525192247').hide();
            $('#hide_525192247').show();
        });
        $('#hide_525192247').click(function() {
            $('#detail_525192247').slideUp();
            $('#hide_525192247').hide();
            $('#show_525192247').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='4867#1213908789'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-19T20:53:09+00:00">Jun 19, 2008 08:53 PM UTC</time> by <b>lbhajdu@....</b><a class="anchor" name="1213908789">&nbsp;</a></div><pre>Hi Jaime,
The jobs will be submitted tonight. And I should have the log for you by tomorrow.

Leve</pre></div><div class='update_description'><i onclick="document.location='4867#1213908248'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-19T20:44:08+00:00">Jun 19, 2008 08:44 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1213908248">&nbsp;</a></div><pre>Can you also send the Condor local config file (/opt/OSG-0.8.0-client/
condor/local.stargrid01/condor_config.local)?

-- Jaime

On Jun 19, 2008, at 3&#58;38 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213907889'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-19T20:38:09+00:00">Jun 19, 2008 08:38 PM UTC</time> by <b>Wayne Betts</b><a class="anchor" name="1213907889">&nbsp;</a></div><pre>I made the requested changes to the Condor config file.  I think it is
the stock config file from the OSG-client (0.8) packages, except for the
addition of&#58;

GLOBUS_TCP_PORT_RANGE=20000,22000

The complete config file is shown below.

-Wayne

######################################################################
##
##  condor_config
##
##  This is the global configuration file for condor.
##
##  The file is divided into four main parts&#58;
##  Part 1&#58;  Settings you MUST customize
##  Part 2&#58;  Settings you may want to customize
##  Part 3&#58;  Settings that control the policy of when condor will
##           start and stop jobs on your machines
##  Part 4&#58;  Settings you should probably leave alone (unless you
##  know what you&#39;re doing)
##
##  Please read the INSTALL file (or the Install chapter in the
##  Condor Administrator&#39;s Manual) for detailed explanations of the
##  various settings in here and possible ways to configure your
##  pool.
##
##  If you are installing Condor as root and then handing over the
##  administration of this file to a person you do not trust with
##  root access, please read the Installation chapter paying careful
##  note to the condor_config.root entries.
##
##  Unless otherwise specified, settings that are commented out show
<div id='show_401762058' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_401762058'>##  the defaults that are used if you don&#39;t define a value.  Settings
##  that are defined here MUST BE DEFINED since they have no default
##  value.
##
##  Unless otherwise indicated, all settings which specify a time are
##  defined in seconds.
##
######################################################################

######################################################################
######################################################################
##
##  ######                                     #
##  #     #    ##    #####    #####           ##
##  #     #   #  #   #    #     #            # #
##  ######   #    #  #    #     #              #
##  #        ######  #####      #              #
##  #        #    #  #   #      #              #
##  #        #    #  #    #     #            #####
##
##  Part 1&#58;  Settings you must customize&#58;
######################################################################
######################################################################

##  What machine is your central manager?

##--------------------------------------------------------------------
##  Pathnames&#58;
##--------------------------------------------------------------------
##  Where have you installed the bin, sbin and lib condor directories?
RELEASE_DIR		= /usr/local/condor

##  Where is the local condor directory for each host?
##  This is where the local config file(s), logs and
##  spool/execute directories are located
LOCAL_DIR		= $(TILDE)
#LOCAL_DIR		= $(RELEASE_DIR)/hosts/$(HOSTNAME)

##  Where is the machine-specific local config file for each host?
LOCAL_CONFIG_FILE =
/opt/OSG-0.8.0-client/condor/local.stargrid01/condor_config.local

## If the local config file is not present, is it an error?
## WARNING&#58; This is a potential security issue.
## If not specificed, te default is True
#REQUIRE_LOCAL_CONFIG_FILE = TRUE

##--------------------------------------------------------------------
##  Mail parameters&#58;
##--------------------------------------------------------------------
##  When something goes wrong with condor at your site, who should get
##  the email?
CONDOR_ADMIN		= condor-admin@....in

##  Full path to a mail delivery program that understands that &#34;-s&#34;
##  means you want to specify a subject&#58;
MAIL			= /usr/bin/mail

##--------------------------------------------------------------------
##  Network domain parameters&#58;
##--------------------------------------------------------------------
##  Internet domain of machines sharing a common UID space.  If your
##  machines don&#39;t share a common UID space, set it to
##  UID_DOMAIN = $(FULL_HOSTNAME)
##  to specify that each machine has its own UID space.
UID_DOMAIN		= your.domain

##  Internet domain of machines sharing a common file system.
##  If your machines don&#39;t use a network file system, set it to
##  FILESYSTEM_DOMAIN = $(FULL_HOSTNAME)
##  to specify that each machine has its own file system.
FILESYSTEM_DOMAIN	= your.domain

##  This macro is used to specify a short description of your pool.
##  It should be about 20 characters long. For example, the name of
##  the UW-Madison Computer Science Condor Pool is &#96;&#96;UW-Madison CS&#39;&#39;.
COLLECTOR_NAME 		= My Pool

######################################################################
######################################################################
##
##  ######                                   #####
##  #     #    ##    #####    #####         #     #
##  #     #   #  #   #    #     #                 #
##  ######   #    #  #    #     #            #####
##  #        ######  #####      #           #
##  #        #    #  #   #      #           #
##  #        #    #  #    #     #           #######
##
##  Part 2&#58;  Settings you may want to customize&#58;
##  (it is generally safe to leave these untouched)
######################################################################
######################################################################

##
##  The user/group ID &#60;uid&#62;.&#60;gid&#62; of the &#34;Condor&#34; user.
##  (this can also be specified in the environment)
##  Note&#58; the CONDOR_IDS setting is ignored on Win32 platforms
#CONDOR_IDS=x.x

##--------------------------------------------------------------------
##  Flocking&#58; Submitting jobs to more than one pool
##--------------------------------------------------------------------
##  Flocking allows you to run your jobs in other pools, or lets
##  others run jobs in your pool.
##
##  To let others flock to you, define FLOCK_FROM.
##
##  To flock to others, define FLOCK_TO.

##  FLOCK_FROM defines the machines where you would like to grant
##  people access to your pool via flocking. (i.e. you are granting
##  access to these machines to join your pool).
FLOCK_FROM =
##  An example of this is&#58;
#FLOCK_FROM = somehost.friendly.domain, anotherhost.friendly.domain

##  FLOCK_TO defines the central managers of the pools that you want
##  to flock to. (i.e. you are specifying the machines that you
##  want your jobs to be negotiated at -- thereby specifying the
##  pools they will run in.)
FLOCK_TO =
##  An example of this is&#58;
#FLOCK_TO = central_manager.friendly.domain, condor.cs.wisc.edu

##  FLOCK_COLLECTOR_HOSTS should almost always be the same as
##  FLOCK_NEGOTIATOR_HOSTS (as shown below).  The only reason it would be
##  different is if the collector and negotiator in the pool that you are
##  flocking too are running on different machines (not recommended).
##  The collectors must be specified in the same corresponding order as
##  the FLOCK_NEGOTIATOR_HOSTS list.
FLOCK_NEGOTIATOR_HOSTS = $(FLOCK_TO)
FLOCK_COLLECTOR_HOSTS = $(FLOCK_TO)
## An example of having the negotiator and the collector on different
## machines is&#58;
#FLOCK_NEGOTIATOR_HOSTS = condor.cs.wisc.edu,
condor-negotiator.friendly.domain
#FLOCK_COLLECTOR_HOSTS =  condor.cs.wisc.edu,
condor-collector.friendly.domain

##--------------------------------------------------------------------
##  Host/IP access levels
##--------------------------------------------------------------------
##  Please see the administrator&#39;s manual for details on these
##  settings, what they&#39;re for, and how to use them.

##  What machines have administrative rights for your pool?  This
##  defaults to your central manager.  You should set it to the
##  machine(s) where whoever is the condor administrator(s) works
##  (assuming you trust all the users who log into that/those
##  machine(s), since this is machine-wide access you&#39;re granting).
HOSTALLOW_ADMINISTRATOR = $(CONDOR_HOST)

##  If there are no machines that should have administrative access
##  to your pool (for example, there&#39;s no machine where only trusted
##  users have accounts), you can uncomment this setting.
##  Unfortunately, this will mean that administering your pool will
##  be more difficult.
#HOSTDENY_ADMINISTRATOR = *

##  What machines should have &#34;owner&#34; access to your machines, meaning
##  they can issue commands that a machine owner should be able to
##  issue to their own machine (like condor_vacate).  This defaults to
##  machines with administrator access, and the local machine.  This
##  is probably what you want.
HOSTALLOW_OWNER = $(FULL_HOSTNAME), $(HOSTALLOW_ADMINISTRATOR)

##  Read access.  Machines listed as allow (and/or not listed as deny)
##  can view the status of your pool, but cannot join your pool
##  or run jobs.
##  NOTE&#58; By default, without these entries customized, you
##  are granting read access to the whole world.  You may want to
##  restrict that to hosts in your domain.  If possible, please also
##  grant read access to &#34;*.cs.wisc.edu&#34;, so the Condor developers
##  will be able to view the status of your pool and more easily help
##  you install, configure or debug your Condor installation.
##  It is important to have this defined.
HOSTALLOW_READ = *
#HOSTALLOW_READ = *.your.domain, *.cs.wisc.edu
#HOSTDENY_READ = *.bad.subnet, bad-machine.your.domain, 144.77.88.*

##  Write access.  Machines listed here can join your pool, submit
##  jobs, etc.  Note&#58; Any machine which has WRITE access must
##  also be granted READ access.  Granting WRITE access below does
##  not also automatically grant READ access; you must change
##  HOSTALLOW_READ above as well.
##
##  You must set this to something else before Condor will run.
##  This most simple option is&#58;
##    HOSTALLOW_WRITE = *
##  but note that this will allow anyone to submit jobs or add
##  machines to your pool and is serious security risk.
HOSTALLOW_WRITE = YOU_MUST_CHANGE_THIS_INVALID_CONDOR_CONFIGURATION_VALUE
#HOSTALLOW_WRITE = *.your.domain, your-friend&#39;s-machine.other.domain
#HOSTDENY_WRITE = bad-machine.your.domain

##  Negotiator access.  Machines listed here are trusted central
##  managers.  You should normally not have to change this.
HOSTALLOW_NEGOTIATOR = $(CONDOR_HOST)
##  Now, with flocking we need to let the SCHEDD trust the other
##  negotiators we are flocking with as well.  You should normally
##  not have to change this either.
HOSTALLOW_NEGOTIATOR_SCHEDD = $(CONDOR_HOST), $(FLOCK_NEGOTIATOR_HOSTS)

##  Config access.  Machines listed here can use the condor_config_val
##  tool to modify all daemon configurations except those specified in
##  the condor_config.root file.  This level of host-wide access
##  should only be granted with extreme caution.  By default, config
##  access is denied from all hosts.
#HOSTALLOW_CONFIG = trusted-host.your.domain

##  Flocking Configs.  These are the real things that Condor looks at,
##  but we set them from the FLOCK_FROM/TO macros above.  It is safe
##  to leave these unchanged.
HOSTALLOW_WRITE_COLLECTOR = $(HOSTALLOW_WRITE), $(FLOCK_FROM)
HOSTALLOW_WRITE_STARTD    = $(HOSTALLOW_WRITE), $(FLOCK_FROM)
HOSTALLOW_READ_COLLECTOR  = $(HOSTALLOW_READ), $(FLOCK_FROM)
HOSTALLOW_READ_STARTD     = $(HOSTALLOW_READ), $(FLOCK_FROM)

##--------------------------------------------------------------------
##  Security parameters for setting configuration values remotely&#58;
##--------------------------------------------------------------------
##  These parameters define the list of attributes that can be set
##  remotely with condor_config_val for the security access levels
##  defined above (for example, WRITE, ADMINISTRATOR, CONFIG, etc).
##  Please see the administrator&#39;s manual for futher details on these
##  settings, what they&#39;re for, and how to use them.  There are no
##  default values for any of these settings.  If they are not
##  defined, no attributes can be set with condor_config_val.

## Do you want to allow condor_config_val -rset to work at all?
## This feature is disabled by default, so to enable, you must
## uncomment the following setting and change the value to &#34;True&#34;.
## Note&#58; changing this requires a restart not just a reconfig.
#ENABLE_RUNTIME_CONFIG = False

## Do you want to allow condor_config_val -set to work at all?
## This feature is disabled by default, so to enable, you must
## uncomment the following setting and change the value to &#34;True&#34;.
## Note&#58; changing this requires a restart not just a reconfig.
#ENABLE_PERSISTENT_CONFIG = False

## Directory where daemons should write persistent config files (used
## to support condor_config_val -set).  This directory should *ONLY*
## be writable by root (or the user the Condor daemons are running as
## if non-root).  There is no default, administrators must define this.
## Note&#58; changing this requires a restart not just a reconfig.
#PERSISTENT_CONFIG_DIR = /full/path/to/root-only/local/directory

##  Attributes that can be set by hosts with &#34;CONFIG&#34; permission (as
##  defined with HOSTALLOW_CONFIG and HOSTDENY_CONFIG above).
##  The commented-out value here was the default behavior of Condor
##  prior to version 6.3.3.  If you don&#39;t need this behavior, you
##  should leave this commented out.
#SETTABLE_ATTRS_CONFIG = *

##  Attributes that can be set by hosts with &#34;ADMINISTRATOR&#34;
##  permission (as defined above)
#SETTABLE_ATTRS_ADMINISTRATOR = *_DEBUG, MAX_*_LOG

##  Attributes that can be set by hosts with &#34;OWNER&#34; permission (as
##  defined above) NOTE&#58; any Condor job running on a given host will
##  have OWNER permission on that host by default.  If you grant this
##  kind of access, Condor jobs will be able to modify any attributes
##  you list below on the machine where they are running.  This has
##  obvious security implications, so only grant this kind of
##  permission for custom attributes that you define for your own use
##  at your pool (custom attributes about your machines that are
##  published with the STARTD_ATTRS setting, for example).
#SETTABLE_ATTRS_OWNER = your_custom_attribute, another_custom_attr

##  You can also define daemon-specific versions of each of these
##  settings.  For example, to define settings that can only be
##  changed in the condor_startd&#39;s configuration by hosts with OWNER
##  permission, you would use&#58;
#STARTD_SETTABLE_ATTRS_OWNER = your_custom_attribute_name

##--------------------------------------------------------------------
##  Network filesystem parameters&#58;
##--------------------------------------------------------------------
##  Do you want to use NFS for file access instead of remote system
##  calls?
#USE_NFS		= False

##  Do you want to use AFS for file access instead of remote system
##  calls?
#USE_AFS		= False

##--------------------------------------------------------------------
##  Checkpoint server&#58;
##--------------------------------------------------------------------
##  Do you want to use a checkpoint server if one is available?  If a
##  checkpoint server isn&#39;t available or USE_CKPT_SERVER is set to
##  False, checkpoints will be written to the local SPOOL directory on
##  the submission machine.
#USE_CKPT_SERVER	= True

##  What&#39;s the hostname of this machine&#39;s nearest checkpoint server?
#CKPT_SERVER_HOST	= checkpoint-server-hostname.your.domain

##  Do you want the starter on the execute machine to choose the
##  checkpoint server?  If False, the CKPT_SERVER_HOST set on
##  the submit machine is used.  Otherwise, the CKPT_SERVER_HOST set
##  on the execute machine is used.  The default is true.
#STARTER_CHOOSES_CKPT_SERVER = True

##--------------------------------------------------------------------
##  Miscellaneous&#58;
##--------------------------------------------------------------------
##  Try to save this much swap space by not starting new shadows.
##  Specified in megabytes.
#RESERVED_SWAP		= 5

##  What&#39;s the maximum number of jobs you want a single submit machine
##  to spawn shadows for?
#MAX_JOBS_RUNNING	= 200

##  Condor needs to create a few lock files to synchronize access to
##  various log files.  Because of problems we&#39;ve had with network
##  filesystems and file locking over the years, we HIGHLY recommend
##  that you put these lock files on a local partition on each
##  machine.  If you don&#39;t have your LOCAL_DIR on a local partition,
##  be sure to change this entry.  Whatever user (or group) condor is
##  running as needs to have write access to this directory.  If
##  you&#39;re not running as root, this is whatever user you started up
##  the condor_master as.  If you are running as root, and there&#39;s a
##  condor account, it&#39;s probably condor.  Otherwise, it&#39;s whatever
##  you&#39;ve set in the CONDOR_IDS environment variable.  See the Admin
##  manual for details on this.
LOCK		= $(LOG)

##  If you don&#39;t use a fully qualified name in your /etc/hosts file
##  (or NIS, etc.) for either your official hostname or as an alias,
##  Condor wouldn&#39;t normally be able to use fully qualified names in
##  places that it&#39;d like to.  You can set this parameter to the
##  domain you&#39;d like appended to your hostname, if changing your host
##  information isn&#39;t a good option.  This parameter must be set in
##  the global config file (not the LOCAL_CONFIG_FILE from above).
#DEFAULT_DOMAIN_NAME = your.domain.name

##  If you don&#39;t have DNS set up, Condor will normally fail in many
##  places because it can&#39;t resolve hostnames to IP addresses and
##  vice-versa. If you enable this option, Condor will use
##  pseudo-hostnames constructed from a machine&#39;s IP address and the
##  DEFAULT_DOMAIN_NAME. Both NO_DNS and DEFAULT_DOMAIN must set in
##  your top-level config file for this mode of operation to work
##  properly.
#NO_DNS = True

##  Condor can be told whether or not you want the Condor daemons to
##  create a core file if something really bad happens.  This just
##  sets the resource limit for the size of a core file.  By default,
##  we don&#39;t do anything, and leave in place whatever limit was in
##  effect when you started the Condor daemons.  If this parameter is
##  set and &#34;True&#34;, we increase the limit to as large as it gets.  If
##  it&#39;s set to &#34;False&#34;, we set the limit at 0 (which means that no
##  core files are even created).  Core files greatly help the Condor
##  developers debug any problems you might be having.
#CREATE_CORE_FILES	= True

##  Condor Glidein downloads binaries from a remote server for the
##  machines into which you&#39;re gliding. This saves you from manually
##  downloading and installing binaries for every architecture you
##  might want to glidein to. The default server is one maintained at
##  The University of Wisconsin. If you don&#39;t want to use the UW
##  server, you can set up your own and change the following values to
##  point to it, instead.
GLIDEIN_SERVER_URLS =
<a href='http&#58;//www.cs.wisc.edu/condor/glidein/binaries' target='_blank' rel='nofollow'>http&#58;//www.cs.wisc.edu/condor/glidein/binaries</a>
<a href='gsiftp&#58;//gridftp.cs.wisc.edu/p/condor/public/binaries/glidein' target='_blank' rel='nofollow'>gsiftp&#58;//gridftp.cs.wisc.edu/p/condor/public/binaries/glidein</a>

## List the sites you want to GlideIn to on the GLIDEIN_SITES. For example,
## if you&#39;d like to GlideIn to some Alliance GiB resources,
## uncomment the line below.
## Make sure that $(GLIDEIN_SITES) is included in HOSTALLOW_READ and
## HOSTALLW_WRITE, or else your GlideIns won&#39;t be able to join your pool.
#GLIDEIN_SITES = *.ncsa.uiuc.edu, *.cs.wisc.edu, *.mcs.anl.gov
GLIDEIN_SITES =

##  If your site needs to use UID_DOMAIN settings (defined above) that
##  are not real Internet domains that match the hostnames, you can
##  tell Condor to trust whatever UID_DOMAIN a submit machine gives to
##  the execute machine and just make sure the two strings match.  The
##  default for this setting is False, since it is more secure this
##  way.
#TRUST_UID_DOMAIN = False

## If you would like to be informed in near real-time via condor_q when
## a vanilla/standard/java job is in a suspension state, set this
attribute to
## TRUE. However, this real-time update of the condor_schedd by the shadows
## could cause performance issues if there are thousands of concurrently
## running vanilla/standard/java jobs under a single condor_schedd and
they are
## allowed to suspend and resume.
#REAL_TIME_JOB_SUSPEND_UPDATES = False

## A standard universe job can perform arbitrary shell calls via the
## libc &#39;system()&#39; function. This function call is routed back to the shadow
## which performs the actual system() invocation in the initialdir of the
## running program and as the user who submitted the job. However, since the
## user job can request ARBITRARY shell commands to be run by the
shadow, this
## is a generally unsafe practice. This should only be made available if
it is
## actually needed. If this attribute is not defined, then it is the same as
## it being defined to False. Set it to True to allow the shadow to execute
## arbitrary shell code from the user job.
#SHADOW_ALLOW_UNSAFE_REMOTE_EXEC = False

## KEEP_OUTPUT_SANDBOX is an optional feature to tell Condor-G to not
## remove the job spool when the job leaves the queue.  To use, just
## set to TRUE.  Since you will be operating Condor-G in this manner,
## you may want to put leave_in_queue = false in your job submit
## description files, to tell Condor-G to simply remove the job from
## the queue immediately when the job completes (since the output files
## will stick around no matter what).
#KEEP_OUTPUT_SANDBOX = False

## This setting tells the negotiator to ignore user priorities.  This
## avoids problems where jobs from different users won&#39;t run when using
## condor_advertise instead of a full-blown startd (some of the user
## priority system in Condor relies on information from the startd --
## we will remove this reliance when we support the user priority
## system for grid sites in the negotiator; for now, this setting will
## just disable it).
#NEGOTIATOR_IGNORE_USER_PRIORITIES = False

## These are the directories used to locate classad plug-in functions
#CLASSAD_SCRIPT_DIRECTORY =
#CLASSAD_LIB_PATH =

## This setting tells Condor whether to delegate or copy GSI X509
## credentials when sending them over the wire between daemons.
## Delegation can take up to a second, which is very slow when
## submitting a large number of jobs. Copying exposes the credential
## to third parties if Condor isn&#39;t set to encrypt communications.
## By default, Condor will delegate rather than copy.
DELEGATE_JOB_GSI_CREDENTIALS = True

##--------------------------------------------------------------------
##  Settings that control the daemon&#39;s debugging output&#58;
##--------------------------------------------------------------------

##
## The flags given in ALL_DEBUG are shared between all daemons.
##

ALL_DEBUG               =

MAX_COLLECTOR_LOG	= 1000000
COLLECTOR_DEBUG		=

MAX_KBDD_LOG		= 1000000
KBDD_DEBUG		=

MAX_NEGOTIATOR_LOG	= 1000000
NEGOTIATOR_DEBUG	= D_MATCH
MAX_NEGOTIATOR_MATCH_LOG = 1000000

MAX_SCHEDD_LOG		= 1000000
SCHEDD_DEBUG		= D_COMMAND D_PID

MAX_SHADOW_LOG		= 1000000
SHADOW_DEBUG		=

MAX_STARTD_LOG		= 1000000
STARTD_DEBUG		= D_COMMAND

MAX_STARTER_LOG		= 1000000
STARTER_DEBUG		= D_NODATE

MAX_MASTER_LOG		= 1000000
MASTER_DEBUG		= D_COMMAND
##  When the master starts up, should it truncate it&#39;s log file?
#TRUNC_MASTER_LOG_ON_OPEN        = False

## The daemons touch their log file periodically, even when they have
## nothing to write. When a daemon starts up, it prints the last time
## the log file was modified. This lets you estimate when a previous
## instance of a daemon stopped running. This paramete controls often
## the daemons touch the file (in seconds).
TOUCH_LOG_INTERVAL = 60

######################################################################
######################################################################
##
##  ######                                   #####
##  #     #    ##    #####    #####         #     #
##  #     #   #  #   #    #     #                 #
##  ######   #    #  #    #     #            #####
##  #        ######  #####      #                 #
##  #        #    #  #   #      #           #     #
##  #        #    #  #    #     #            #####
##
##  Part 3&#58;  Settings control the policy for running, stopping, and
##  periodically checkpointing condor jobs&#58;
######################################################################
######################################################################

##  This section contains macros are here to help write legible
##  expressions&#58;
MINUTE		= 60
HOUR		= (60 * $(MINUTE))
ActivationTimer = (CurrentTime - JobStart)
LastCkpt	= (CurrentTime - LastPeriodicCheckpoint)

##  The JobUniverse attribute is just an int.  These macros can be
##  used to specify the universe in a human-readable way&#58;
STANDARD	= 1
PVM		= 4
VANILLA		= 5
MPI		= 8
IsPVM           = (TARGET.JobUniverse == $(PVM))
IsMPI           = (TARGET.JobUniverse == $(MPI))
IsVanilla       = (TARGET.JobUniverse == $(VANILLA))
IsStandard      = (TARGET.JobUniverse == $(STANDARD))

NonCondorLoadAvg	= (LoadAvg - CondorLoadAvg)
BackgroundLoad		= 0.3
HighLoad		= 0.5
StartIdleTime		= 15 * $(MINUTE)
ContinueIdleTime	=  5 * $(MINUTE)
MaxSuspendTime		= 10 * $(MINUTE)
MaxVacateTime		= 10 * $(MINUTE)

KeyboardBusy		= (KeyboardIdle &#60; $(MINUTE))
ConsoleBusy		= (ConsoleIdle  &#60; $(MINUTE))
CPUIdle			= ($(NonCondorLoadAvg) &#60;= $(BackgroundLoad))
CPUBusy			= ($(NonCondorLoadAvg) &#62;= $(HighLoad))
KeyboardNotBusy		= ($(KeyboardBusy) == False)

BigJob		= (TARGET.ImageSize &#62;= (50 * 1024))
MediumJob	= (TARGET.ImageSize &#62;= (15 * 1024) && TARGET.ImageSize &#60; (50 *
1024))
SmallJob	= (TARGET.ImageSize &#60;  (15 * 1024))

JustCPU			= ($(CPUBusy) && ($(KeyboardBusy) == False))
MachineBusy		= ($(CPUBusy) || $(KeyboardBusy))

##  The RANK expression controls which jobs this machine prefers to
##  run over others.  Some examples from the manual include&#58;
##    RANK = TARGET.ImageSize
##    RANK = (Owner == &#34;coltrane&#34;) + (Owner == &#34;tyner&#34;)
##                  + ((Owner == &#34;garrison&#34;) * 10) + (Owner == &#34;jones&#34;)
##  By default, RANK is always 0, meaning that all jobs have an equal
##  ranking.
#RANK			= 0

#####################################################################
##  This where you choose the configuration that you would like to
##  use.  It has no defaults so it must be defined.  We start this
##  file off with the UWCS_* policy.
######################################################################

##  Also here is what is referred to as the TESTINGMODE_*, which is
##  a quick hardwired way to test Condor.
##  Replace UWCS_* with TESTINGMODE_* if you wish to do testing mode.
##  For example&#58;
##  WANT_SUSPEND 		= $(UWCS_WANT_SUSPEND)
##  becomes
##  WANT_SUSPEND 		= $(TESTINGMODE_WANT_SUSPEND)

WANT_SUSPEND 		= $(UWCS_WANT_SUSPEND)
WANT_VACATE		= $(UWCS_WANT_VACATE)

##  When is this machine willing to start a job?
START			= $(UWCS_START)

##  When should a local universe job be allowed to start?
START_LOCAL_UNIVERSE	= True
# Only start a local universe jobs if there are less
# than 100 local jobs currently running
#START_LOCAL_UNIVERSE	= TotalLocalJobsRunning &#60; 100

##  When should a scheduler universe job be allowed to start?
START_SCHEDULER_UNIVERSE	= True
# Only start a scheduler universe jobs if there are less
# than 100 scheduler jobs currently running
#START_SCHEDULER_UNIVERSE	= TotalSchedulerJobsRunning &#60; 100

##  When to suspend a job?
SUSPEND			= $(UWCS_SUSPEND)

##  When to resume a suspended job?
CONTINUE		= $(UWCS_CONTINUE)

##  When to nicely stop a job?
##  (as opposed to killing it instantaneously)
PREEMPT			= $(UWCS_PREEMPT)

##  When to instantaneously kill a preempting job
##  (e.g. if a job is in the pre-empting stage for too long)
KILL			= $(UWCS_KILL)

PERIODIC_CHECKPOINT	= $(UWCS_PERIODIC_CHECKPOINT)
PREEMPTION_REQUIREMENTS	= $(UWCS_PREEMPTION_REQUIREMENTS)
PREEMPTION_RANK		= $(UWCS_PREEMPTION_RANK)
NEGOTIATOR_PRE_JOB_RANK = $(UWCS_NEGOTIATOR_PRE_JOB_RANK)
NEGOTIATOR_POST_JOB_RANK = $(UWCS_NEGOTIATOR_POST_JOB_RANK)
MaxJobRetirementTime    = $(UWCS_MaxJobRetirementTime)

#####################################################################
## This is the UWisc - CS Department Configuration.
##################################################################### UWCS_WANT_SUSPEND	= ( $(SmallJob) || $(KeyboardNotBusy)
|| $(IsPVM) || $(IsVanilla) )
UWCS_WANT_VACATE 	= ( $(ActivationTimer) &#62; 10 * $(MINUTE)
|| $(IsPVM) || $(IsVanilla) )

# Only start jobs if&#58;
# 1) the keyboard has been idle long enough, AND
# 2) the load average is low enough OR the machine is currently
#    running a Condor job
# (NOTE&#58; Condor will only run 1 job at a time on a given resource.
# The reasons Condor might consider running a different job while
# already running one are machine Rank (defined above), and user
# priorities.)
UWCS_START	= ( (KeyboardIdle &#62; $(StartIdleTime))
&& ( $(CPUIdle) ||
(State != &#34;Unclaimed&#34; && State != &#34;Owner&#34;)) )

# Suspend jobs if&#58;
# 1) the keyboard has been touched, OR
# 2a) The cpu has been busy for more than 2 minutes, AND
# 2b) the job has been running for more than 90 seconds
UWCS_SUSPEND = ( $(KeyboardBusy) ||
( (CpuBusyTime &#62; 2 * $(MINUTE))
&& $(ActivationTimer) &#62; 90 ) )

# Continue jobs if&#58;
# 1) the cpu is idle, AND
# 2) we&#39;ve been suspended more than 10 seconds, AND
# 3) the keyboard hasn&#39;t been touched in a while
UWCS_CONTINUE = ( $(CPUIdle) && ($(ActivityTimer) &#62; 10)
&& (KeyboardIdle &#62; $(ContinueIdleTime)) )

# Preempt jobs if&#58;
# 1) The job is suspended and has been suspended longer than we want
# 2) OR, we don&#39;t want to suspend this job, but the conditions to
#    suspend jobs have been met (someone is using the machine)
UWCS_PREEMPT = ( ((Activity == &#34;Suspended&#34;) &&
($(ActivityTimer) &#62; $(MaxSuspendTime)))
|| (SUSPEND && (WANT_SUSPEND == False)) )

# Maximum time (in seconds) to wait for a job to finish before kicking
# it off (due to PREEMPT, a higher priority claim, or the startd
# gracefully shutting down).  This is computed from the time the job
# was started, minus any suspension time.  Once the retirement time runs
# out, the usual preemption process will take place.  The job may
# self-limit the retirement time to _less_ than what is given here.
# By default, nice user jobs and standard universe jobs set their
# MaxJobRetirementTime to 0, so they will usually not wait in retirement.

UWCS_MaxJobRetirementTime = 0

# Kill jobs if they have taken too long to vacate gracefully
UWCS_KILL = $(ActivityTimer) &#62; $(MaxVacateTime)

##  Only define vanilla versions of these if you want to make them
##  different from the above settings.
#SUSPEND_VANILLA  = ( $(KeyboardBusy) ||
#       ((CpuBusyTime &#62; 2 * $(MINUTE)) && $(ActivationTimer) &#62; 90) )
#CONTINUE_VANILLA = ( $(CPUIdle) && ($(ActivityTimer) &#62; 10)
#                     && (KeyboardIdle &#62; $(ContinueIdleTime)) )
#PREEMPT_VANILLA  = ( ((Activity == &#34;Suspended&#34;) &&
#                     ($(ActivityTimer) &#62; $(MaxSuspendTime)))
#                     || (SUSPEND_VANILLA && (WANT_SUSPEND == False)) )
#KILL_VANILLA    = $(ActivityTimer) &#62; $(MaxVacateTime)

##  We use a simple Periodic checkpointing mechanism, but then
##  again we have a very fast network.
UWCS_PERIODIC_CHECKPOINT	= $(LastCkpt) &#62; (3 * $(HOUR))

##  You might want to checkpoint a little less often.  A good
##  example of this is below.  For jobs smaller than 60 megabytes, we
##  periodic checkpoint every 6 hours.  For larger jobs, we only
##  checkpoint every 12 hours.
#UWCS_PERIODIC_CHECKPOINT	= ( (TARGET.ImageSize &#60; 60000) &&
#			    ($(LastCkpt) &#62; (6 * $(HOUR))) ) ||
#			  ( $(LastCkpt) &#62; (12 * $(HOUR)) )

##  The rank expressions used by the negotiator are configured below.
##  This is the order in which ranks are applied by the negotiator&#58;
##    1. NEGOTIATOR_PRE_JOB_RANK
##    2. rank in job ClassAd
##    3. NEGOTIATOR_POST_JOB_RANK
##    4. cause of preemption (0=user priority,1=startd rank,2=no preemption)
##    5. PREEMPTION_RANK

##  The NEGOTIATOR_PRE_JOB_RANK expression overrides all other ranks
##  that are used to pick a match from the set of possibilities.
##  The following expression matches jobs to unclaimed resources
##  whenever possible, regardless of the job-supplied rank.
UWCS_NEGOTIATOR_PRE_JOB_RANK = RemoteOwner =?= UNDEFINED

##  The NEGOTIATOR_POST_JOB_RANK expression chooses between
##  resources that are equally preferred by the job.
##  The following example expression steers jobs toward
##  faster machines and tends to fill a cluster of multi-processors
##  breadth-first instead of depth-first.  In this example,
##  the expression is chosen to have no effect when preemption
##  would take place, allowing control to pass on to
##  PREEMPTION_RANK.
#UWCS_NEGOTIATOR_POST_JOB_RANK =
# (RemoteOwner =?= UNDEFINED) * (KFlops - VirtualMachineID)

##  The negotiator will not preempt a job running on a given machine
##  unless the PREEMPTION_REQUIREMENTS expression evaluates to true
##  and the owner of the idle job has a better priority than the owner
##  of the running job.  This expression defaults to true.
UWCS_PREEMPTION_REQUIREMENTS = ( $(StateTimer) &#62; (1 * $(HOUR)) &&
RemoteUserPrio &#62; SubmittorPrio * 1.2 ) || (MY.NiceUser == True)

##  The PREEMPTION_RANK expression is used in a case where preemption
##  is the only option and all other negotiation ranks are equal.  For
##  example, if the job has no preference, it is usually preferable to
##  preempt a job with a small ImageSize instead of a job with a large
##  ImageSize.  The default is to rank all preemptable matches the
##  same.  However, the negotiator will always prefer to match the job
##  with an idle machine over a preemptable machine, if all other
##  negotiation ranks are equal.
UWCS_PREEMPTION_RANK = (RemoteUserPrio * 1000000) - TARGET.ImageSize

#####################################################################
##  This is a Configuration that will cause your Condor jobs to
##  always run.  This is intended for testing only.
######################################################################

##  This mode will cause your jobs to start on a machine an will let
##  them run to completion.  Condor will ignore all of what is going
##  on in the machine (load average, keyboard activity, etc.)

TESTINGMODE_WANT_SUSPEND	= False
TESTINGMODE_WANT_VACATE		= False
TESTINGMODE_START			= True
TESTINGMODE_SUSPEND			= False
TESTINGMODE_CONTINUE		= True
TESTINGMODE_PREEMPT			= False
TESTINGMODE_KILL			= False
TESTINGMODE_PERIODIC_CHECKPOINT	= False
TESTINGMODE_PREEMPTION_REQUIREMENTS = False
TESTINGMODE_PREEMPTION_RANK = 0

######################################################################
######################################################################
##
##  ######                                  #
##  #     #    ##    #####    #####         #    #
##  #     #   #  #   #    #     #           #    #
##  ######   #    #  #    #     #           #    #
##  #        ######  #####      #           #######
##  #        #    #  #   #      #                #
##  #        #    #  #    #     #                #
##
##  Part 4&#58;  Settings you should probably leave alone&#58;
##  (unless you know what you&#39;re doing)
######################################################################
######################################################################

######################################################################
##  Daemon-wide settings&#58;
######################################################################

##  Pathnames
LOG		= $(LOCAL_DIR)/log
SPOOL		= $(LOCAL_DIR)/spool
EXECUTE		= $(LOCAL_DIR)/execute
BIN		= $(RELEASE_DIR)/bin
LIB		= $(RELEASE_DIR)/lib
INCLUDE		= $(RELEASE_DIR)/include
SBIN		= $(RELEASE_DIR)/sbin
LIBEXEC		= $(RELEASE_DIR)/libexec

## If you leave HISTORY undefined (comment it out), no history file
## will be created.
HISTORY		= $(SPOOL)/history

##  Log files
COLLECTOR_LOG	= $(LOG)/CollectorLog
KBDD_LOG	= $(LOG)/KbdLog
MASTER_LOG	= $(LOG)/MasterLog
NEGOTIATOR_LOG	= $(LOG)/NegotiatorLog
NEGOTIATOR_MATCH_LOG = $(LOG)/MatchLog
SCHEDD_LOG	= $(LOG)/SchedLog
SHADOW_LOG	= $(LOG)/ShadowLog
STARTD_LOG	= $(LOG)/StartLog
STARTER_LOG	= $(LOG)/StarterLog

##  Lock files
SHADOW_LOCK	= $(LOCK)/ShadowLock

##  This setting primarily allows you to change the port that the
##  collector is listening on.  By default, the collector uses port
##  9618, but you can set the port with a &#34;&#58;port&#34;, such as&#58;
##  COLLECTOR_HOST = $(CONDOR_HOST)&#58;1234
COLLECTOR_HOST  = $(CONDOR_HOST)

## The NEGOTIATOR_HOST parameter has been deprecated.  The port where
## the negotiator is listening is now dynamically allocated and the IP
## and port are now obtained from the collector, just like all the
## other daemons.  However, if your pool contains any machines that
## are running version 6.7.3 or earlier, you can uncomment this
## setting to go back to the old fixed-port (9614) for the negotiator.
#NEGOTIATOR_HOST = $(CONDOR_HOST)

##  How long are you willing to let daemons try their graceful
##  shutdown methods before they do a hard shutdown? (30 minutes)
#SHUTDOWN_GRACEFUL_TIMEOUT	= 1800

##  How much disk space would you like reserved from Condor?  In
##  places where Condor is computing the free disk space on various
##  partitions, it subtracts the amount it really finds by this
##  many megabytes.  (If undefined, defaults to 0).
RESERVED_DISK		= 5

##  If your machine is running AFS and the AFS cache lives on the same
##  partition as the other Condor directories, and you want Condor to
##  reserve the space that your AFS cache is configured to use, set
##  this to true.
#RESERVE_AFS_CACHE	= False

##  By default, if a user does not specify &#34;notify_user&#34; in the submit
##  description file, any email Condor sends about that job will go to
##  &#34;username@UID_DOMAIN&#34;.  If your machines all share a common UID
##  domain (so that you would set UID_DOMAIN to be the same across all
##  machines in your pool), *BUT* email to user@UID_DOMAIN is *NOT*
##  the right place for Condor to send email for your site, you can
##  define the default domain to use for email.  A common example
##  would be to set EMAIL_DOMAIN to the fully qualified hostname of
##  each machine in your pool, so users submitting jobs from a
##  specific machine would get email sent to user@....in,
##  instead of user@....in.  In general, you should leave this
##  setting commented out unless two things are true&#58; 1) UID_DOMAIN is
##  set to your domain, not $(FULL_HOSTNAME), and 2) email to
##  user@UID_DOMAIN won&#39;t work.
#EMAIL_DOMAIN = $(FULL_HOSTNAME)

##  If your site needs to use TCP updates to the collector, instead of
##  UDP, you can enable this feature.  HOWEVER, WE DO NOT RECOMMEND
##  THIS FOR MOST SITES!  In general, the only sites that might want
##  this feature are pools made up of machines connected via a
##  wide-area network where UDP packets are frequently or always
##  dropped.  If you enable this feature, you *MUST* turn on the
##  COLLECTOR_SOCKET_CACHE_SIZE setting at your collector, and each
##  entry in the socket cache uses another file descriptor.  If not
##  defined, this feature is disabled by default.
#UPDATE_COLLECTOR_WITH_TCP = True

## HIGHPORT and LOWPORT let you set the range of ports that Condor
## will use. This may be useful if you are behind a firewall. By
## default, Condor uses port 9618 for the collector, 9614 for the
## negotiator, and system-assigned (apparently random) ports for
## everything else. HIGHPORT and LOWPORT only affect these
## system-assigned ports, but will restrict them to the range you
## specify here. If you want to change the well-known ports for the
## collector or negotiator, see COLLECTOR_HOST or NEGOTIATOR_HOST.
## Note that both LOWPORT and HIGHPORT must be at least 1024 if you
## are not starting your daemons as root.  You may also specify
## different port ranges for incoming and outgoing connections by
## using IN_HIGHPORT/IN_LOWPORT and OUT_HIGHPORT/OUT_LOWPORT.
#HIGHPORT = 9700
#LOWPORT = 9600
HIGHPORT = 22000
LOWPORT = 20000

######################################################################
##  Daemon-specific settings&#58;
######################################################################

##--------------------------------------------------------------------
##  condor_master
##--------------------------------------------------------------------
##  Daemons you want the master to keep running for you&#58;
DAEMON_LIST			= MASTER, STARTD, SCHEDD

##  Which daemons use the Condor DaemonCore library (i.e., not the
##  checkpoint server or custom user daemons)?
##  Note&#58; Daemons in this list cannot use a static command port.
#DC_DAEMON_LIST =
#MASTER, STARTD, SCHEDD, KBDD, COLLECTOR, NEGOTIATOR, EVENTD,
#VIEW_SERVER, CONDOR_VIEW, VIEW_COLLECTOR, HAWKEYE, CREDD, HAD,
#QUILL

##  Where are the binaries for these daemons?
MASTER				= $(SBIN)/condor_master
STARTD				= $(SBIN)/condor_startd
SCHEDD				= $(SBIN)/condor_schedd
KBDD				= $(SBIN)/condor_kbdd
NEGOTIATOR			= $(SBIN)/condor_negotiator
COLLECTOR			= $(SBIN)/condor_collector
STARTER_LOCAL			= $(SBIN)/condor_starter

##  When the master starts up, it can place it&#39;s address (IP and port)
##  into a file.  This way, tools running on the local machine don&#39;t
##  need to query the central manager to find the master.  This
##  feature can be turned off by commenting out this setting.
MASTER_ADDRESS_FILE = $(LOG)/.master_address

##  Where should the master find the condor_preen binary? If you don&#39;t
##  want preen to run at all, just comment out this setting.
PREEN				= $(SBIN)/condor_preen

##  How do you want preen to behave?  The &#34;-m&#34; means you want email
##  about files preen finds that it thinks it should remove.  The &#34;-r&#34;
##  means you want preen to actually remove these files.  If you don&#39;t
##  want either of those things to happen, just remove the appropriate
##  one from this setting.
PREEN_ARGS			= -m -r

##  How often should the master start up condor_preen? (once a day)
#PREEN_INTERVAL			= 86400

##  If a daemon dies an unnatural death, do you want email about it?
#PUBLISH_OBITUARIES		= True

##  If you&#39;re getting obituaries, how many lines of the end of that
##  daemon&#39;s log file do you want included in the obituary?
#OBITUARY_LOG_LENGTH		= 20

##  Should the master run?
#START_MASTER			= True

##  Should the master start up the daemons you want it to?
#START_DAEMONS			= True

##  How often do you want the master to send an update to the central
##  manager?
#MASTER_UPDATE_INTERVAL		= 300

##  How often do you want the master to check the timestamps of the
##  daemons it&#39;s running?  If any daemons have been modified, the
##  master restarts them.
#MASTER_CHECK_NEW_EXEC_INTERVAL	= 300

##  Once you notice new binaries, how long should you wait before you
##  try to execute them?
#MASTER_NEW_BINARY_DELAY	= 120

##  What&#39;s the maximum amount of time you&#39;re willing to give the
##  daemons to quickly shutdown before you just kill them outright?
#SHUTDOWN_FAST_TIMEOUT		= 120

######
##  Exponential backoff settings&#58;
######
##  When a daemon keeps crashing, we use &#34;exponential backoff&#34; so we
##  wait longer and longer before restarting it.  This is the base of
##  the exponent used to determine how long to wait before starting
##  the daemon again&#58;
#MASTER_BACKOFF_FACTOR		= 2.0

##  What&#39;s the maximum amount of time you want the master to wait
##  between attempts to start a given daemon?  (With 2.0 as the
##  MASTER_BACKOFF_FACTOR, you&#39;d hit 1 hour in 12 restarts...)
#MASTER_BACKOFF_CEILING		= 3600

##  How long should a daemon run without crashing before we consider
##  it &#34;recovered&#34;.  Once a daemon has recovered, we reset the number
##  of restarts so the exponential backoff stuff goes back to normal.
#MASTER_RECOVER_FACTOR		= 300

##--------------------------------------------------------------------
##  condor_startd
##--------------------------------------------------------------------
##  Where are the various condor_starter binaries installed?
STARTER_LIST = STARTER, STARTER_PVM, STARTER_STANDARD
STARTER			= $(SBIN)/condor_starter
STARTER_PVM		= $(SBIN)/condor_starter.pvm
STARTER_STANDARD	= $(SBIN)/condor_starter.std
STARTER_LOCAL		= $(SBIN)/condor_starter

##  When the startd starts up, it can place it&#39;s address (IP and port)
##  into a file.  This way, tools running on the local machine don&#39;t
##  need to query the central manager to find the startd.  This
##  feature can be turned off by commenting out this setting.
STARTD_ADDRESS_FILE	= $(LOG)/.startd_address

##  When a machine is claimed, how often should we poll the state of
##  the machine to see if we need to evict/suspend the job, etc?
#POLLING_INTERVAL        = 5

##  How often should the startd send updates to the central manager?
#UPDATE_INTERVAL         = 300

##  How long is the startd willing to stay in the &#34;matched&#34; state?
#MATCH_TIMEOUT		= 300

##  How long is the startd willing to stay in the preempting/killing
##  state before it just kills the starter directly?
#KILLING_TIMEOUT	= 30

##  When a machine unclaimed, when should it run benchmarks?
##  LastBenchmark is initialized to 0, so this expression says as soon
##  as we&#39;re unclaimed, run the benchmarks.  Thereafter, if we&#39;re
##  unclaimed and it&#39;s been at least 4 hours since we ran the last
##  benchmarks, run them again.  The startd keeps a weighted average
##  of the benchmark results to provide more accurate values.
##  Note, if you don&#39;t want any benchmarks run at all, either comment
##  RunBenchmarks out, or set it to &#34;False&#34;.
BenchmarkTimer = (CurrentTime - LastBenchmark)
RunBenchmarks &#58; (LastBenchmark == 0 ) || ($(BenchmarkTimer) &#62;= (4 *
$(HOUR)))
#RunBenchmarks &#58; False

##  Normally, when the startd is computing the idle time of all the
##  users of the machine (both local and remote), it checks the utmp
##  file to find all the currently active ttys, and only checks access
##  time of the devices associated with active logins.  Unfortunately,
##  on some systems, utmp is unreliable, and the startd might miss
##  keyboard activity by doing this.  So, if your utmp is unreliable,
##  set this setting to True and the startd will check the access time
##  on all tty and pty devices.
#STARTD_HAS_BAD_UTMP = False

##  This entry allows the startd to monitor console (keyboard and
##  mouse) activity by checking the access times on special files in
##  /dev.  Activity on these files shows up as &#34;ConsoleIdle&#34; time in
##  the startd&#39;s ClassAd.  Just give a comma-separated list of the
##  names of devices you want considered the console, without the
##  &#34;/dev/&#34; portion of the pathname.
CONSOLE_DEVICES	= mouse, console

##  The STARTD_ATTRS (and legacy STARTD_EXPRS) entry allows you to
##  have the startd advertise arbitrary attributes from the config
##  file in its ClassAd.  Give the comma-separated list of entries
##  from the config file you want in the startd ClassAd.
##  NOTE&#58; because of the different syntax of the config file and
##  ClassAds, you might have to do a little extra work to get a given
##  entry into the ClassAd.  In particular, ClassAds require double
##  quotes (&#34;) around your strings.  Numeric values can go in
##  directly, as can boolean expressions.  For example, if you wanted
##  the startd to advertise its list of console devices, when it&#39;s
##  configured to run benchmarks, and how often it sends updates to
##  the central manager, you&#39;d have to define the following helper
##  macro&#58;
#MY_CONSOLE_DEVICES = &#34;$(CONSOLE_DEVICES)&#34;
##  Note&#58; this must come before you define STARTD_ATTRS because macros
##  must be defined before you use them in other macros or
##  expressions.
##  Then, you&#39;d set the STARTD_ATTRS setting to this&#58;
#STARTD_ATTRS = MY_CONSOLE_DEVICES, RunBenchmarks, UPDATE_INTERVAL
##
##  STARTD_ATTRS can also be defined on a per-VM basis.  The startd
##  builds the list of attributes to advertise by combining the lists
##  in this order&#58; STARTD_ATTRS, VMx_STARTD_ATTRS.  In the below
##  example, the startd ad for VM1 will have the value for
##  favorite_color, favorite_season, and favorite_movie, and VM2 will
##  have favorite_color, favorite_season, and favorite_song
##
#STARTD_ATTRS = favorite_color, favorite_season
#VM1_STARTD_ATTRS = favorite_movie
#VM2_STARTD_ATTRS = favorite_song
##
##  Attributes in the STARTD_ATTRS list can also be on a per-VM basis.
##  For example, the following configuration&#58;
##
#favorite_color = &#34;blue&#34;
#favorite_season = &#34;spring&#34;
#VM2_favorite_color = &#34;green&#34;
#VM3_favorite_season = &#34;summer&#34;
#STARTD_ATTRS = favorite_color, favorite_season
##
##  will result in the following attributes in the VM classified
##  ads&#58;
##
## VM1 - favorite_color = &#34;blue&#34;; favorite_season = &#34;spring&#34;
## VM2 - favorite_color = &#34;green&#34;; favorite_season = &#34;spring&#34;
## VM3 - favorite_color = &#34;blue&#34;; favorite_season = &#34;summer&#34;
##
##  Finally, the recommended default value for this setting, is to
##  publish the COLLECTOR_HOST setting as a string.  This can be
##  useful using the &#34;$$(COLLECTOR_HOST)&#34; syntax in the submit file
##  for jobs to know (for example, via their environment) what pool
##  they&#39;re running in.
COLLECTOR_HOST_STRING = &#34;$(COLLECTOR_HOST)&#34;
STARTD_ATTRS = COLLECTOR_HOST_STRING

##  When the startd is claimed by a remote user, it can also advertise
##  arbitrary attributes from the ClassAd of the job its working on.
##  Just list the attribute names you want advertised.
##  Note&#58; since this is already a ClassAd, you don&#39;t have to do
##  anything funny with strings, etc.  This feature can be turned off
##  by commenting out this setting (there is no default).
STARTD_JOB_EXPRS = ImageSize, ExecutableSize, JobUniverse, NiceUser

##  If you want to &#34;lie&#34; to Condor about how many CPUs your machine
##  has, you can use this setting to override Condor&#39;s automatic
##  computation.  If you modify this, you must restart the startd for
##  the change to take effect (a simple condor_reconfig will not do).
##  Please read the section on &#34;condor_startd Configuration File
##  Macros&#34; in the Condor Administrators Manual for a further
##  discussion of this setting.  Its use is not recommended.  This
##  must be an integer (&#34;N&#34; isn&#39;t a valid setting, that&#39;s just used to
##  represent the default).
#NUM_CPUS = N

##  If you never want Condor to detect more the &#34;N&#34; CPUs, uncomment this
##  line out. You must restart the startd for this setting to take
##  effect. If set to 0 or a negative number, it is ignored.
##  By default, it is ignored. Otherwise, it must be a positive
##  integer (&#34;N&#34; isn&#39;t a valid setting, that&#39;s just used to
##  represent the default).
#MAX_NUM_CPUS = N

##  Normally, Condor will automatically detect the amount of physical
##  memory available on your machine.  Define MEMORY to tell Condor
##  how much physical memory (in MB) your machine has, overriding the
##  value Condor computes automatically.  For example&#58;
#MEMORY = 128

##  How much memory would you like reserved from Condor?  By default,
##  Condor considers all the physical memory of your machine as
##  available to be used by Condor jobs.  If RESERVED_MEMORY is
##  defined, Condor subtracts it from the amount of memory it
##  advertises as available.
#RESERVED_MEMORY = 0

######
##  SMP startd settings
##
##  By default, Condor will evenly divide the resources in an SMP
##  machine (such as RAM, swap space and disk space) among all the
##  CPUs, and advertise each CPU as its own &#34;virtual machine&#34; with an
##  even share of the system resources.  If you want something other
##  than this, there are a few options available to you.  Please read
##  the section on &#34;Configuring The Startd for SMP Machines&#34; in the
##  Condor Administrator&#39;s Manual for full details.  The various
##  settings are only briefly listed and described here.
######

##  The maximum number of different virtual machine types.

##  Use this setting to define your own virtual machine types.  This
##  allows you to divide system resources unevenly among your CPUs.
##  You must use a different setting for each different type you
##  define.  The &#34;&#60;N&#62;&#34; in the name of the macro listed below must be
##  an integer from 1 to MAX_VIRTUAL_MACHINE_TYPES (defined above),
##  and you use this number to refer to your type.  There are many
##  different formats these settings can take, so be sure to refer to
##  the section on &#34;Configuring The Startd for SMP Machines&#34; in the
##  Condor Administrator&#39;s Manual for full details.  In particular,
##  read the section titled &#34;Defining Virtual Machine Types&#34; to help
##  understand this setting.  If you modify any of these settings, you
##  must restart the condor_start for the change to take effect.
#  For example&#58;

##  If you define your own virtual machine types, you must specify how
##  many virtual machines of each type you wish to advertise.  You do
##  this with the setting below, replacing the &#34;&#60;N&#62;&#34; with the
##  corresponding integer you used to define the type above.  You can
##  change the number of a given type being advertised at run-time,
##  with a simple condor_reconfig.
#  For example&#58;

##  The number of evenly-divided virtual machines you want Condor to
##  report to your pool (if less than the total number of CPUs).  This
##  setting is only considered if the &#34;type&#34; settings described above
##  are not in use.  By default, all CPUs are reported.  This setting
##  must be an integer (&#34;N&#34; isn&#39;t a valid setting, that&#39;s just used to
##  represent the default).
#NUM_VIRTUAL_MACHINES = N

##  How many of the virtual machines the startd is representing should
##  be &#34;connected&#34; to the console (in other words, notice when there&#39;s
##  console activity)?  This defaults to all virtual machines (N in a
##  machine with N CPUs).  This must be an integer (&#34;N&#34; isn&#39;t a valid
##  setting, that&#39;s just used to represent the default).
#VIRTUAL_MACHINES_CONNECTED_TO_CONSOLE = N

##  How many of the virtual machines the startd is representing should
##  be &#34;connected&#34; to the keyboard (for remote tty activity, as well
##  as console activity).  Defaults to 1.
#VIRTUAL_MACHINES_CONNECTED_TO_KEYBOARD = 1

##  If there are virtual machines that aren&#39;t connected to the
##  keyboard or the console (see the above two settings), the
##  corresponding idle time reported will be the time since the startd
##  was spawned, plus the value of this parameter.  It defaults to 20
##  minutes.  We do this because, if the virtual machine is configured
##  not to care about keyboard activity, we want it to be available to
##  Condor jobs as soon as the startd starts up, instead of having to
##  wait for 15 minutes or more (which is the default time a machine
##  must be idle before Condor will start a job).  If you don&#39;t want
##  this boost, just set the value to 0.  If you change your START
##  expression to require more than 15 minutes before a job starts,
##  but you still want jobs to start right away on some of your SMP
##  nodes, just increase this parameter.
#DISCONNECTED_KEYBOARD_IDLE_BOOST = 1200

######
##  Settings for computing optional resource availability statistics&#58;
######
##  If STARTD_COMPUTE_AVAIL_STATS = True, the startd will compute
##  statistics about resource availability to be included in the
##  classad(s) sent to the collector describing the resource(s) the
##  startd manages.  The following attributes will always be included
##  in the resource classad(s) if STARTD_COMPUTE_AVAIL_STATS = True&#58;
##    AvailTime = What proportion of the time (between 0.0 and 1.0)
##      has this resource been in a state other than &#34;Owner&#34;?
##    LastAvailInterval = What was the duration (in seconds) of the
##      last period between &#34;Owner&#34; states?
##  The following attributes will also be included if the resource is
##  not in the &#34;Owner&#34; state&#58;
##    AvailSince = At what time did the resource last leave the
##      &#34;Owner&#34; state?  Measured in the number of seconds since the
##      epoch (00&#58;00&#58;00 UTC, Jan 1, 1970).
##    AvailTimeEstimate = Based on past history, this is an estimate
##      of how long the current period between &#34;Owner&#34; states will
##      last.
#STARTD_COMPUTE_AVAIL_STATS = False

##  If STARTD_COMPUTE_AVAIL_STATS = True, STARTD_AVAIL_CONFIDENCE sets
##  the confidence level of the AvailTimeEstimate.  By default, the
##  estimate is based on the 80th percentile of past values.
#STARTD_AVAIL_CONFIDENCE = 0.8

##  STARTD_MAX_AVAIL_PERIOD_SAMPLES limits the number of samples of
##  past available intervals stored by the startd to limit memory and
##  disk consumption.  Each sample requires 4 bytes of memory and
##  approximately 10 bytes of disk space.
#STARTD_MAX_AVAIL_PERIOD_SAMPLES = 100

##--------------------------------------------------------------------
##  condor_schedd
##--------------------------------------------------------------------
##  Where are the various shadow binaries installed?
SHADOW_LIST = SHADOW, SHADOW_PVM, SHADOW_STANDARD
SHADOW			= $(SBIN)/condor_shadow
SHADOW_PVM		= $(SBIN)/condor_shadow.pvm
SHADOW_STANDARD		= $(SBIN)/condor_shadow.std

##  When the schedd starts up, it can place it&#39;s address (IP and port)
##  into a file.  This way, tools running on the local machine don&#39;t
##  need to query the central manager to find the schedd.  This
##  feature can be turned off by commenting out this setting.
SCHEDD_ADDRESS_FILE	= $(LOG)/.schedd_address

##  How often should the schedd send an update to the central manager?
#SCHEDD_INTERVAL	= 300

##  How long should the schedd wait between spawning each shadow?
#JOB_START_DELAY	= 2

##  How often should the schedd send a keep alive message to any
##  startds it has claimed?  (5 minutes)
#ALIVE_INTERVAL		= 300

##  This setting controls the maximum number of times that a
##  condor_shadow processes can have a fatal error (exception) before
##  the condor_schedd will simply relinquish the match associated with
##  the dying shadow.
#MAX_SHADOW_EXCEPTIONS	= 5

##  Estimated virtual memory size of each condor_shadow process.
##  Specified in kilobytes.
SHADOW_SIZE_ESTIMATE	= 1800

##  The condor_schedd can renice the condor_shadow processes on your
##  submit machines.  How how &#34;nice&#34; do you want the shadows? (1-19).
##  The higher the number, the lower priority the shadows have.
##  This feature can be disabled entirely by commenting it out.
SHADOW_RENICE_INCREMENT	= 10

##  By default, when the schedd fails to start an idle job, it will
##  not try to start any other idle jobs in the same cluster during
##  that negotiation cycle.  This makes negotiation much more
##  efficient for large job clusters.  However, in some cases other
##  jobs in the cluster can be started even though an earlier job
##  can&#39;t.  For example, the jobs&#39; requirements may differ, because of
##  different disk space, memory, or operating system requirements.
##  Or, machines may be willing to run only some jobs in the cluster,
##  because their requirements reference the jobs&#39; virtual memory size
##  or other attribute.  Setting NEGOTIATE_ALL_JOBS_IN_CLUSTER to True
##  will force the schedd to try to start all idle jobs in each
##  negotiation cycle.  This will make negotiation cycles last longer,
##  but it will ensure that all jobs that can be started will be
##  started.
#NEGOTIATE_ALL_JOBS_IN_CLUSTER = False

## This setting controls how often, in seconds, the schedd considers
## periodic job actions given by the user in the submit file.
## (Currently, these are periodic_hold, periodic_release, and
periodic_remove.)
PERIODIC_EXPR_INTERVAL = 60

######
## Queue management settings&#58;
######
##  How often should the schedd truncate it&#39;s job queue transaction
##  log?  (Specified in seconds, once a day is the default.)
#QUEUE_CLEAN_INTERVAL	= 86400

##  How often should the schedd commit &#34;wall clock&#34; run time for jobs
##  to the queue, so run time statistics remain accurate when the
##  schedd crashes?  (Specified in seconds, once per hour is the
##  default.  Set to 0 to disable.)
#WALL_CLOCK_CKPT_INTERVAL = 3600

##  What users do you want to grant super user access to this job
##  queue?  (These users will be able to remove other user&#39;s jobs).
##  By default, this only includes root.
QUEUE_SUPER_USERS	= root, condor

##--------------------------------------------------------------------
##  condor_shadow
##--------------------------------------------------------------------
##  If the shadow is unable to read a checkpoint file from the
##  checkpoint server, it keeps trying only if the job has accumulated
##  more than MAX_DISCARDED_RUN_TIME seconds of CPU usage.  Otherwise,
##  the job is started from scratch.  Defaults to 1 hour.  This
##  setting is only used if USE_CKPT_SERVER (from above) is True.
#MAX_DISCARDED_RUN_TIME = 3600

##  Should periodic checkpoints be compressed?
#COMPRESS_PERIODIC_CKPT = False

##  Should vacate checkpoints be compressed?
#COMPRESS_VACATE_CKPT = False

##  Should we commit the application&#39;s dirty memory pages to swap
##  space during a periodic checkpoint?
#PERIODIC_MEMORY_SYNC = False

##  Should we write vacate checkpoints slowly?  If nonzero, this
##  parameter specifies the speed at which vacate checkpoints should
##  be written, in kilobytes per second.
#SLOW_CKPT_SPEED = 0

##--------------------------------------------------------------------
##  condor_shadow.pvm
##--------------------------------------------------------------------
##  Where is the condor pvm daemon installed?
PVMD			= $(SBIN)/condor_pvmd

##  Where is the condor pvm group server daemon installed?
PVMGS			= $(SBIN)/condor_pvmgs

##--------------------------------------------------------------------
##  condor_starter
##--------------------------------------------------------------------
##  The condor_starter can renice the processes from remote Condor
##  jobs on your execute machines.  If you want this, uncomment the
##  following entry and set it to how &#34;nice&#34; do you want the user
##  jobs. (1-19)  The larger the number, the lower priority the
##  process gets on your machines.
##  Note on Win32 platforms, this number needs to be greater than
##  zero (i.e. the job must be reniced) or the mechanism that
##  monitors CPU load on Win32 systems will give erratic results.
#JOB_RENICE_INCREMENT	= 10

##  Should the starter do local logging to its own log file, or send
##  debug information back to the condor_shadow where it will end up
##  in the ShadowLog?
#STARTER_LOCAL_LOGGING	= TRUE

##  If the UID_DOMAIN settings match on both the execute and submit
##  machines, but the UID of the user who submitted the job isn&#39;t in
##  the passwd file of the execute machine, the starter will normally
##  exit with an error.  Do you want the starter to just start up the
##  job with the specified UID, even if it&#39;s not in the passwd file?
#SOFT_UID_DOMAIN	= FALSE

##--------------------------------------------------------------------
##  condor_submit
##--------------------------------------------------------------------
##  If you want condor_submit to automatically append an expression to
##  the Requirements expression or Rank expression of jobs at your
##  site, uncomment these entries.
#APPEND_REQUIREMENTS	= (expression to append job requirements)
#APPEND_RANK		= (expression to append job rank)

##  If you want expressions only appended for either standard or
##  vanilla universe jobs, you can uncomment these entries.  If any of
##  them are defined, they are used for the given universe, instead of
##  the generic entries above.
#APPEND_REQ_VANILLA	= (expression to append to vanilla job requirements)
#APPEND_REQ_STANDARD	= (expression to append to standard job requirements)
#APPEND_RANK_STANDARD	= (expression to append to vanilla job rank)
#APPEND_RANK_VANILLA	= (expression to append to standard job rank)

##  This can be used to define a default value for the rank expression
##  if one is not specified in the submit file.
#DEFAULT_RANK	        = (default rank expression for all jobs)

##  If you want universe-specific defaults, you can use the following
##  entries&#58;
#DEFAULT_RANK_VANILLA	= (default rank expression for vanilla jobs)
#DEFAULT_RANK_STANDARD	= (default rank expression for standard jobs)

##  If you want condor_submit to automatically append expressions to
##  the job ClassAds it creates, you can uncomment and define the
##  SUBMIT_EXPRS setting.  It works just like the STARTD_EXPRS
##  described above with respect to ClassAd vs. config file syntax,
##  strings, etc.  One common use would be to have the full hostname
##  of the machine where a job was submitted placed in the job
##  ClassAd.  You would do this by uncommenting the following lines&#58;
#MACHINE = &#34;$(FULL_HOSTNAME)&#34;
#SUBMIT_EXPRS = MACHINE

## Condor keeps a buffer of recently-used data for each file an
## application opens.  This macro specifies the default maximum number
## of bytes to be buffered for each open file at the executing
## machine.
#DEFAULT_IO_BUFFER_SIZE = 524288

## Condor will attempt to consolidate small read and write operations
## into large blocks.  This macro specifies the default block size
## Condor will use.
#DEFAULT_IO_BUFFER_BLOCK_SIZE = 32768

##--------------------------------------------------------------------
##  condor_preen
##--------------------------------------------------------------------
##  Who should condor_preen send email to?
#PREEN_ADMIN		= $(CONDOR_ADMIN)

##  What files should condor_preen leave in the spool directory?
VALID_SPOOL_FILES	= job_queue.log, job_queue.log.tmp, history,
Accountant.log, Accountantnew.log,
local_univ_execute, .quillwritepassword

##  What files should condor_preen remove from the log directory?
INVALID_LOG_FILES	= core

##--------------------------------------------------------------------
##  Java parameters&#58;
##--------------------------------------------------------------------
##  If you would like this machine to be able to run Java jobs,
##  then set JAVA to the path of your JVM binary.  If you are not
##  interested in Java, there is no harm in leaving this entry
##  empty or incorrect.

JAVA = /usr/bin/java

##  Some JVMs need to be told the maximum amount of heap memory
##  to offer to the process.  If your JVM supports this, give
##  the argument here, and Condor will fill in the memory amount.
##  If left blank, your JVM will choose some default value,
##  typically 64 MB.  The default (-Xmx) works with the Sun JVM.

JAVA_MAXHEAP_ARGUMENT = -Xmx

## JAVA_CLASSPATH_DEFAULT gives the default set of paths in which
## Java classes are to be found.  Each path is separated by spaces.
## If your JVM needs to be informed of additional directories, add
## them here.  However, do not remove the existing entries, as Condor
## needs them.

JAVA_CLASSPATH_DEFAULT = $(LIB) $(LIB)/scimark2lib.jar .

##  JAVA_CLASSPATH_ARGUMENT describes the command-line parameter
##  used to introduce a new classpath&#58;

JAVA_CLASSPATH_ARGUMENT = -classpath

##  JAVA_CLASSPATH_SEPARATOR describes the character used to mark
##  one path element from another&#58;

JAVA_CLASSPATH_SEPARATOR = &#58;

##  JAVA_BENCHMARK_TIME describes the number of seconds for which
##  to run Java benchmarks.  A longer time yields a more accurate
##  benchmark, but consumes more otherwise useful CPU time.
##  If this time is zero or undefined, no Java benchmarks will be run.

JAVA_BENCHMARK_TIME = 2

##  If your JVM requires any special arguments not mentioned in
##  the options above, then give them here.

JAVA_EXTRA_ARGUMENTS =

##
##--------------------------------------------------------------------
##  Condor-G settings
##--------------------------------------------------------------------
##  Where is the GridManager binary installed?

GRIDMANAGER			= $(SBIN)/condor_gridmanager
GT2_GAHP			= $(SBIN)/gahp_server
GRID_MONITOR			= $(SBIN)/grid_monitor.sh

GLOBUS_TCP_PORT_RANGE=20000,22000

##--------------------------------------------------------------------
##  Settings that control the daemon&#39;s debugging output&#58;
##--------------------------------------------------------------------
##
## Note that the Gridmanager runs as the User, not a Condor daemon, so
## all users must have write permssion to the directory that the
## Gridmanager will use for it&#39;s logfile. Our suggestion is to create a
## directory called GridLogs in $(LOG) with UNIX permissions 1777
## (just like /tmp )
##  Another option is to use /tmp as the location of the GridManager log.
##

## Changing default Max_gridmanager_log to 50000000 to help with WSU
problems, as per request from Jaime Frey on June 19, 2008&#58;
#MAX_GRIDMANAGER_LOG	= 1000000
MAX_GRIDMANAGER_LOG	= 50000000

## Changing default Gridmanager_debug to D_FULLDEBUG to help with WSU
problems, as per request from Jaime Frey on June 19, 2008&#58;
#GRIDMANAGER_DEBUG	= D_COMMAND
GRIDMANAGER_DEBUG	= D_FULLDEBUG

#GRIDMANAGER_LOG = $(LOG)/GridLogs/GridmanagerLog.$(USERNAME)
GRIDMANAGER_LOG = /tmp/GridmanagerLog.$(USERNAME)

##--------------------------------------------------------------------
##  Various other settings that the Condor-G can use.
##--------------------------------------------------------------------

## For grid-type gt2 jobs (pre-WS GRAM), limit the number of jobmanager
## processes the gridmanager will let run on the headnode. Letting too
## many jobmanagers run causes severe load on the headnode.
GRIDMANAGER_MAX_JOBMANAGERS_PER_RESOURCE = 10

## If we&#39;re talking to a Globus 2.0 resource, Condor-G will use the new
## version of the GRAM protocol. The first option is how often to check the
## proxy on the submit site of things. If the GridManager discovers a new
## proxy, it will restart itself and use the new proxy for all future
## jobs launched. In seconds,  and defaults to 10 minutes
#GRIDMANAGER_CHECKPROXY_INTERVAL = 600

## The GridManager will shut things down 3 minutes before loosing Contact
## because of an expired proxy.
## In seconds, and defaults to 3 minutes
#GRDIMANAGER_MINIMUM_PROXY_TIME  = 180

## Condor requires that each submitted job be designated to run under a
## particular &#34;universe&#34;. Condor-G is active when jobs are as marked as
## &#34;GLOBUS&#34; universe jobs. The universe of a job is set in the submit file
## with the &#39;universe = GLOBUS&#39; line.
##
## If no universe is specificed in the submit file, Condor must pick one
## for the job to use. By default, it chooses the &#34;standard&#34; universe.
## The default can be overridden in the config file with the
DEFAULT_UNIVERSE
## setting, which is a string to insert into a job submit description if the
## job does not try and define it&#39;s own universe
##
#DEFAULT_UNIVERSE = grid

#
# The Cred_min_time_left is the first-pass at making sure that Condor-G
# does not submit your job without it having enough time left for the
# job to finish. For example, if you have a job that runs for 20
minutes, and
# you might spend 40 minutes in the queue, it&#39;s a bad idea to submit
with less
# than an hour left before your proxy expires.
# 2 hours seemed like a reasonable default.
#
CRED_MIN_TIME_LEFT		= 120

##
## The GridMonitor allows you to submit many more jobs to a GT2 GRAM server
## than is normally possible.
ENABLE_GRID_MONITOR = TRUE

##
## The location of the wrapper for invoking
## Condor GAHP server
##
CONDOR_GAHP = $(SBIN)/condor_c-gahp
CONDOR_GAHP_WORKER = $(SBIN)/condor_c-gahp_worker_thread

##
## The Condor GAHP server has it&#39;s own log.  Like the Gridmanager, the
## GAHP server is run as the User, not a Condor daemon, so all users must
## have write permssion to the directory used for the logfile. Our
## suggestion is to create a directory called GridLogs in $(LOG) with
## UNIX permissions 1777 (just like /tmp )
## Another option is to use /tmp as the location of the CGAHP log.
##
MAX_C_GAHP_LOG	= 1000000

#C_GAHP_LOG = $(LOG)/GridLogs/CGAHPLog.$(USERNAME)
C_GAHP_LOG = /tmp/CGAHPLog.$(USERNAME)
C_GAHP_WORKER_THREAD_LOG = /tmp/CGAHPWorkerLog.$(USERNAME)

##
## The location of the wrapper for invoking
## GT3 GAHP server
##
GT3_GAHP = $(SBIN)/gt3_gahp

##
## The location of GT3 files. This should normally be lib/gt3
##
GT3_LOCATION = $(LIB)/gt3

##
## The location of the wrapper for invoking
## GT4 GAHP server
##
GT4_GAHP = $(SBIN)/gt4_gahp

##
## The location of GT4 files. This should normally be lib/gt4
##
GT4_LOCATION = $(LIB)/gt4

##
## gt4 gram requires a gridftp server to perform file transfers.
## If GRIDFTP_URL_BASE is set, then Condor assumes there is a gridftp
## server set up at that URL suitable for its use. Otherwise, Condor
## will start its own gridftp servers as needed, using the binary
## pointed at by GRIDFTP_SERVER. GRIDFTP_SERVER_WRAPPER points to a
## wrapper script needed to properly set the path to the gridmap file.
##
#GRIDFTP_URL_BASE = gsiftp&#58;//$(FULL_HOSTNAME)
GRIDFTP_SERVER = $(LIBEXEC)/globus-gridftp-server
GRIDFTP_SERVER_WRAPPER = $(LIBEXEC)/gridftp_wrapper.sh

##
## Location of the PBS/LSF gahp and its associated binaries
##
GLITE_LOCATION = $(LIB)/glite
PBS_GAHP = $(GLITE_LOCATION)/bin/batch_gahp
LSF_GAHP = $(GLITE_LOCATION)/bin/batch_gahp

##
## The location of the wrapper for invoking the Unicore GAHP server
##
UNICORE_GAHP = $(SBIN)/unicore_gahp

##
## The location of the wrapper for invoking the NorduGrid GAHP server
##
NORDUGRID_GAHP = $(SBIN)/nordugrid_gahp

## Condor-G and CredD can use MyProxy to refresh GSI proxies which are
## about to expire.
#MYPROXY_GET_DELEGATION = /path/to/myproxy-get-delegation

##
##--------------------------------------------------------------------
##  condor_credd credential managment daemon
##--------------------------------------------------------------------
##  Where is the CredD binary installed?
CREDD				= $(SBIN)/condor_credd

##  When the credd starts up, it can place it&#39;s address (IP and port)
##  into a file.  This way, tools running on the local machine don&#39;t
##  need an additional &#34;-n host&#58;port&#34; command line option.  This
##  feature can be turned off by commenting out this setting.
CREDD_ADDRESS_FILE	= $(LOG)/.credd_address

##  Specify a remote credd server here,
#CREDD_HOST  = $(CONDOR_HOST)&#58;$(CREDD_PORT)

## CredD startup arguments
## Start the CredD on a well-known port.  Uncomment to to simplify
## connecting to a remote CredD.  Note&#58; that this interface may change
## in a future release.
CREDD_PORT			= 9620
CREDD_ARGS			= -p $(CREDD_PORT) -f

## CredD daemon debugging log
CREDD_LOG			= $(LOG)/CredLog
CREDD_DEBUG			= D_FULLDEBUG
MAX_CREDD_LOG		= 4000000

## The credential owner submits the credential.  This list specififies
## other user who are also permitted to see all credentials.  Defaults
## to root on Unix systems, and Administrator on Windows systems.
#CRED_SUPER_USERS =

## Credential storage location.  This directory must exist
## prior to starting condor_credd.  It is highly recommended to
## restrict access permissions to _only_ the directory owner.
CRED_STORE_DIR = $(LOCAL_DIR)/cred_dir

## Index file path of saved credentials.
## This file will be automatically created if it does not exist.
#CRED_INDEX_FILE = $(CRED_STORE_DIR/cred-index

## condor_credd  will attempt to refresh credentials when their
## remaining lifespan is less than this value.  Units = seconds.
#DEFAULT_CRED_EXPIRE_THRESHOLD = 3600

## condor-credd periodically checks remaining lifespan of stored
## credentials, at this interval.
#CRED_CHECK_INTERVAL = 60

##
##--------------------------------------------------------------------
##  Stork data placment server
##--------------------------------------------------------------------
##  Where is the Stork binary installed?
STORK				= $(SBIN)/stork_server

##  When Stork starts up, it can place it&#39;s address (IP and port)
##  into a file.  This way, tools running on the local machine don&#39;t
##  need an additional &#34;-n host&#58;port&#34; command line option.  This
##  feature can be turned off by commenting out this setting.
STORK_ADDRESS_FILE = $(LOG)/.stork_address

##  Specify a remote Stork server here,
#STORK_HOST  = $(CONDOR_HOST)&#58;$(STORK_PORT)

## STORK_LOG_BASE specifies the basename for heritage Stork log files.
## Stork uses this macro to create the following output log files&#58;
## $(STORK_LOG_BASE)&#58; Stork server job queue classad collection
## journal file.
## $(STORK_LOG_BASE).history&#58; Used to track completed jobs.
## $(STORK_LOG_BASE).user_log&#58; User level log, also used by DAGMan.
STORK_LOG_BASE		= $(LOG)/Stork

## Modern Condor DaemonCore logging feature.
STORK_LOG = $(LOG)/StorkLog
STORK_DEBUG = D_FULLDEBUG
MAX_STORK_LOG = 4000000

## Stork startup arguments
## Start Stork on a well-known port.  Uncomment to to simplify
## connecting to a remote Stork.  Note&#58; that this interface may change
## in a future release.
#STORK_PORT			= 34048
STORK_PORT			= 9621
STORK_ARGS = -p $(STORK_PORT) -f -Serverlog $(STORK_LOG_BASE)

## Stork environment.  Stork modules may require external programs and
## shared object libraries.  These are located using the PATH and
## LD_LIBRARY_PATH environments.  Further, some modules may require
## further specific environments.  By default, Stork inherits a full
## environment when invoked from condor_master or the shell.  If the
## default environment is not adequate for all Stork modules, specify
## a replacement environment here.  This environment will be set by
## condor_master before starting Stork, but does not apply if Stork is
## started directly from the command line.
#STORK_ENVIRONMENT = TMP=/tmp;CONDOR_CONFIG=/special/config;PATH=/lib

## Limits the number of concurrent data placements handled by Stork.
#STORK_MAX_NUM_JOBS = 5

## Limits the number of retries for a failed data placement.
#STORK_MAX_RETRY = 5

## Limits the run time for a data placement job, after which the
## placement is considered failed.

## Temporary credential storage directory used by Stork.
#STORK_TMP_CRED_DIR = /tmp

## Directory containing Stork modules.
#STORK_MODULE_DIR = $(LIBEXEC)

##
##--------------------------------------------------------------------
##  Quill Job Queue Mirroring Server
##--------------------------------------------------------------------
##  Where is the Quill binary installed and what arguments should be passed?
QUILL = $(SBIN)/condor_quill
#QUILL_ARGS =

# Where is the log file for the quill daemon?
QUILL_LOG = $(LOG)/QuillLog

# The identification and location of the quill daemon for local clients.
QUILL_ADDRESS_FILE = $(LOG)/.quill_address

# If this is set to true, then the rest of the QUILL arguments must be
defined
# for quill to function. If it is Fase or left undefined, then quill
will not
# be consulted by either the scheduler or the tools, but in the case of a
# remote quill query where the local client has quill turned off, but the
# remote client has quill turned on, things will still function normally.
#QUILL_ENABLED = TRUE

# This will be the name of a quill daemon using this config file. This name
# should not conflict with any other quill name--or schedd name.
#QUILL_NAME = quill@....

# The Postgreql server requires usernames that can manipulate tables.
This will
# be the username associated with this instance of the quill daemon
mirroring
# a schedd&#39;s job queue. Each quill daemon must have a unique username
# associated with it otherwise multiple quill daemons will corrupt the data
# held under an indentical user name.
#QUILL_DB_NAME = name_of_db

# The required password for the DB user which quill will use to read
# information from the database about the queue.
#QUILL_DB_QUERY_PASSWORD = foobar

# The machine and port of the postgres server.
#QUILL_DB_IP_ADDR = machine.domain.com&#58;5432

# Polling period, in seconds, for when quill reads transactions out of the
# schedd&#39;s job queue log file and puts them into the database.
#QUILL_POLLING_PERIOD = 10

# Number of days that historical information about previous jobs will be
kept.
# It defaults to 180 days
#QUILL_HISTORY_DURATION  = 180

# Number of hours between scans of QUILL_HISTORY_DURATION.
#QUILL_HISTORY_CLEANING_INTERVAL = 24

# Allows or disallows a remote query to the quill daemon and database
# which is reading this log file. Defaults to true.
#QUILL_IS_REMOTELY_QUERYABLE = TRUE

# Add debugging flags to here if you need to debug quill for some reason.
#QUILL_DEBUG = D_FULLDEBUG

Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_401762058').click(function() {
            $('#detail_401762058').slideDown("normal");
            $('#show_401762058').hide();
            $('#hide_401762058').show();
        });
        $('#hide_401762058').click(function() {
            $('#detail_401762058').slideUp();
            $('#hide_401762058').hide();
            $('#show_401762058').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='4867#1213905909'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-19T20:05:09+00:00">Jun 19, 2008 08:05 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1213905909">&nbsp;</a></div><pre>For the failed jobs in the gridmanager log you attached, the Condor
gridmanager is attempting to connect to the GRAM jobmanager to
complete the job submission, but it&#39;s failing for some reason. The log
doesn&#39;t give enough information to know why.

Can you make the following changes/additions to the Condor config file
on the client machine&#58;
GRIDMANAGER_DEBUG = D_FULLDEBUG
MAX_GRIDMANAGER_LOG = 50000000

Then run condor_reconfig, wait for some new jobs to fail, then send me
the gridmanager log? That should provide more information about what&#39;s
going wrong.

Also, can you post the Condor config files from the client machine?

-- Jaime

On Jun 18, 2008, at 1&#58;05 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213736709'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-17T21:05:09+00:00">Jun 17, 2008 09:05 PM UTC</time> by <b>lbhajdu@....</b><a class="anchor" name="1213736709">&nbsp;</a></div><pre>Attached is the Grid manager log.

Leve

Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213736168'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-17T20:56:08+00:00">Jun 17, 2008 08:56 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1213736168">&nbsp;</a></div><pre>Do you have the gridmanager log?

-- Jaime

On Jun 17, 2008, at 3&#58;44 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213735449'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-17T20:44:09+00:00">Jun 17, 2008 08:44 PM UTC</time> by <b>lbhajdu@....</b><a class="anchor" name="1213735449">&nbsp;</a></div><pre>Hello,

I’m going to give you a newer one, because we no longer have the output
files. This 13580 is a known bad job and 13575 is a known good job.

Leve

-----------------------------------------------------------------------------------

E42B07793004B0CA40FDF094E63E1E7_0.condorg.log
000 (13580.000.000) 06/17 02&#58;02&#58;56 Job submitted from host&#58;
&#60;130.199.6.111&#58;20615&#62;
...
018 (13580.000.000) 06/17 02&#58;13&#58;08 Globus job submission failed!
Reason&#58; 111 the job manager timed out while waiting for a commit signal
...
012 (13580.000.000) 06/17 02&#58;13&#58;12 Job was held.
Unspecified gridmanager error
Code 0 Subcode 0
...

-----------------------------------------------------------------------------------

5.condorg.log
000 (13575.000.000) 06/17 02&#58;02&#58;46 Job submitted from host&#58;
&#60;130.199.6.111&#58;20615&#62;
...
017 (13575.000.000) 06/17 02&#58;02&#58;57 Job submitted to Globus
RM-Contact&#58; rhic23.physics.wayne.edu/jobmanager-sge
JM-Contact&#58; <a href='https&#58;//rhic23.physics.wayne.edu&#58;20014/6866/1213682567/' target='_blank' rel='nofollow'>https&#58;//rhic23.physics.wayne.edu&#58;20014/6866/1213682567/</a>
Can-Restart-JM&#58; 1
...
027 (13575.000.000) 06/17 02&#58;02&#58;57 Job submitted to grid resource
GridResource&#58; gt2 rhic23.physics.wayne.edu/jobmanager-sge
GridJobId&#58; gt2 rhic23.physics.wayne.edu/jobmanager-sge
<div id='show_1465051858' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1465051858'><a href='https&#58;//rhic23.physics.wayne.edu&#58;20014/6866/1213682567/' target='_blank' rel='nofollow'>https&#58;//rhic23.physics.wayne.edu&#58;20014/6866/1213682567/</a>
...
001 (13575.000.000) 06/17 05&#58;36&#58;46 Job executing on host&#58; gt2
rhic23.physics.wayne.edu/jobmanager-sge
...
005 (13575.000.000) 06/17 05&#58;37&#58;38 Job terminated.
(1) Normal termination (return value 0)
Usr 0 00&#58;00&#58;00, Sys 0 00&#58;00&#58;00 - Run Remote Usage
Usr 0 00&#58;00&#58;00, Sys 0 00&#58;00&#58;00 - Run Local Usage
Usr 0 00&#58;00&#58;00, Sys 0 00&#58;00&#58;00 - Total Remote Usage
Usr 0 00&#58;00&#58;00, Sys 0 00&#58;00&#58;00 - Total Local Usage
0 - Run Bytes Sent By Job
0 - Run Bytes Received By Job
0 - Total Bytes Sent By Job
0 - Total Bytes Received By Job
...

Levente
</div><script type='text/javascript'>
        $('#show_1465051858').click(function() {
            $('#detail_1465051858').slideDown("normal");
            $('#show_1465051858').hide();
            $('#hide_1465051858').show();
        });
        $('#hide_1465051858').click(function() {
            $('#detail_1465051858').slideUp();
            $('#hide_1465051858').hide();
            $('#show_1465051858').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='4867#1213729149'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-17T18:59:09+00:00">Jun 17, 2008 06:59 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1213729149">&nbsp;</a></div><pre>If you can give me the Condor job ids of the failed jobs, I can sift
them out of the log you emailed.

-- Jaime

On Jun 12, 2008, at 2&#58;14 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213298050'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-12T19:14:10+00:00">Jun 12, 2008 07:14 PM UTC</time> by <b>lbhajdu@....</b><a class="anchor" name="1213298050">&nbsp;</a></div><pre>I have the file here (attached), but it’s long and mixed in are many good jobs. I can arrange a fresh session with my account if it would be easier to isolate given a few hours.

Leve

Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213296250'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-12T18:44:10+00:00">Jun 12, 2008 06:44 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1213296250">&nbsp;</a></div><pre>Yes, the gridmanager log is on the submitting node.

-- Jaime

On Jun 12, 2008, at 11&#58;35 AM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213288508'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-12T16:35:08+00:00">Jun 12, 2008 04:35 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1213288508">&nbsp;</a></div><pre>How to proceed then, as I can&#39;t find any gridmanager log?
condor_config says that it should be in
/tmp/GridManagerLog.$(USERNAME) but no such file exists on the node
running condor-G.

Or is this on the (remote) submitting host?

Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213282209'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-12T14:50:09+00:00">Jun 12, 2008 02:50 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1213282209">&nbsp;</a></div><pre>I need the gridmanager log from the client side, not the jobmanager
log from the server.

-- Jaime

On Jun 12, 2008, at 8&#58;29 AM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1213277349'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-12T13:29:09+00:00">Jun 12, 2008 01:29 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1213277349">&nbsp;</a></div><pre>Here is a recent log from one of Lidia&#39;s jobs that failed with the two
phase commit error&#58;

Liz

/home/osg-star/gram_job_mgr_15595.log&#58;

6/11 02&#58;02&#58;57 JM&#58; Security context imported
6/11 02&#58;02&#58;57 JM&#58; Adding new callback contact
(url=<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20000/' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20000/</a>, mask=1048575)
6/11 02&#58;02&#58;57 JM&#58; Added successfully
6/11 02&#58;02&#58;57 Pre-parsed RSL string&#58;
&(rsl_substitution=(GRIDMANAGER_GASS_URL
<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004</a>))(executable=$(GRIDMANAGER_GASS_URL)#&#39;/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#39;)(scratchdir=&#39;&#39;)(directory=$(SCRATCH_DIRECTORY))(stdout=$(GLOBUS_CACHED_STDOUT))(stderr=$(GLOBUS_CACHED_STDERR))(file_stage_out=($(GLOBUS_CACHED_STDOUT)
$(GRIDMANAGER_GASS_URL)#&#39;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#39;)($(GLOBUS_CACHED_STDERR)
$(GRIDMANAGER_GASS_URL)#&#39;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#39;))(proxy_timeout=240)(save_state=yes)(two_phase=600)(remote_io_url=$(GRIDMANAGER_GASS_URL))
6/11 02&#58;02&#58;57
&#60;&#60;&#60;&#60;&#60;Job Request RSL
&(&#34;rsl_substitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; ) )(&#34;executable&#34; =
$(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34;
)(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; =
$(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;)
)(&#34;file_stage_out&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;)
# &#34;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34; )
($(&#34;GLOBUS_CACHED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34; )
)(&#34;proxy_timeout&#34; = &#34;240&#34; )(&#34;save_state&#34; = &#34;yes&#34; )(&#34;two_phase&#34; = &#34;600&#34;
)(&#34;remote_io_url&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job Request RSL</font>
6/11 02&#58;02&#58;57
&#60;&#60;&#60;&#60;&#60;Job Request RSL (canonical)
&(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; ) )(&#34;executable&#34; =
<div id='show_1533644860' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1533644860'>$(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34;
)(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; =
$(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;)
)(&#34;filestageout&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34; )
($(&#34;GLOBUS_CACHED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34; )
)(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34;
)(&#34;remoteiourl&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job Request RSL (canonical)</font>
6/11 02&#58;02&#58;57 JM&#58; Evaluating RSL Value6/11 02&#58;02&#58;57 JM&#58; Evaluated RSL
Value to GRIDMANAGER_GASS_URL6/11 02&#58;02&#58;57 JM&#58; Evaluating RSL Value6/11
02&#58;02&#58;57 JM&#58; Evaluated RSL Value to
<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;200046/11' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;200046/11</a> 02&#58;02&#58;57 Appending extra env.var
LD_LIBRARY_PATH=/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib
6/11 02&#58;02&#58;57 Evaluating scratch directory RSL
6/11 02&#58;02&#58;57 Scratch Directory RSL -&#62;
6/11 02&#58;02&#58;57 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
6/11 02&#58;02&#58;57 JMI&#58; completed script validation&#58; job manager type is sge.
6/11 02&#58;02&#58;57 JMI&#58; in globus_gram_job_manager_script_make_scratchdir()
6/11 02&#58;02&#58;57 JMI&#58; cmd = make_scratchdir
6/11 02&#58;02&#58;57 JMI&#58; returning with success
Wed Jun 11 02&#58;03&#58;00 2008 JM_SCRIPT&#58; New Perl JobManager created.
Wed Jun 11 02&#58;03&#58;00 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58;
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;03&#58;00 2008 JM_SCRIPT&#58; Entering Job Manager default
implementation of make_scratchdir
Wed Jun 11 02&#58;03&#58;00 2008 JM_SCRIPT&#58; Trying to create directory named
/home/osg-star//gram_scratch_XUZdmmzUbD
Wed Jun 11 02&#58;03&#58;00 2008 JM_SCRIPT&#58; Sent NFS sync for
/home/osg-star//gram_scratch_XUZdmmzUbD
Wed Jun 11 02&#58;03&#58;00 2008 JM_SCRIPT&#58; I think it was made.... verifying
Wed Jun 11 02&#58;03&#58;00 2008 JM_SCRIPT&#58; Using
/home/osg-star//gram_scratch_XUZdmmzUbD as the scratch directory for this
job.
6/11 02&#58;03&#58;00 JMI&#58; while return_buf = GRAM_SCRIPT_SCRATCH_DIR =
/home/osg-star//gram_scratch_XUZdmmzUbD
6/11 02&#58;03&#58;00 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_MAKE_SCRATCHDIR
6/11 02&#58;03&#58;00 Adding scratch dir to symbol table and env&#58;
/home/osg-star//gram_scratch_XUZdmmzUbD
6/11 02&#58;03&#58;00
&#60;&#60;&#60;&#60;&#60;Job RSL
&(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34;
&#34;/home/osg-star//gram_scratch_XUZdmmzUbD&#34; ) (&#34;LD_LIBRARY_PATH&#34;
&#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34;
) (&#34;HOME&#34; &#34;/home/osg-star&#34; ) (&#34;LOGNAME&#34; &#34;osg-star&#34; ) )(&#34;rslsubstitution&#34; =
(&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; )
)(&#34;executable&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34;
)(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; =
$(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;)
)(&#34;filestageout&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34; )
($(&#34;GLOBUS_CACHED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) #
&#34;/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34; )
)(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34;
)(&#34;remoteiourl&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL</font>
6/11 02&#58;03&#58;00
&#60;&#60;&#60;&#60;&#60;Job RSL (post-eval)
&(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34;
&#34;/home/osg-star//gram_scratch_XUZdmmzUbD&#34; ) (&#34;LD_LIBRARY_PATH&#34;
&#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34;
) (&#34;HOME&#34; &#34;/home/osg-star&#34; ) (&#34;LOGNAME&#34; &#34;osg-star&#34; ) )(&#34;rslsubstitution&#34; =
(&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; )
)(&#34;executable&#34; =
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34</a>;
)(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; =
&#34;/home/osg-star//gram_scratch_XUZdmmzUbD&#34; )(&#34;stdout&#34; =
&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout&#34;
)(&#34;stderr&#34; =
&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr&#34;
)(&#34;filestageout&#34; =
(&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34</a>;
)
(&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34</a>;
) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34;
)(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-eval)</font>
Adding default RSL of dry_run = no
Adding default RSL of gram_my_job = collective
Adding default RSL of job_type = multiple
Adding default RSL of count = 1
Adding default RSL of stdin = /dev/null
6/11 02&#58;03&#58;00
&#60;&#60;&#60;&#60;&#60;Job RSL (post-validation)
&(&#34;stdin&#34; = &#34;/dev/null&#34; )(&#34;count&#34; = &#34;1&#34; )(&#34;job_type&#34; = &#34;multiple&#34;
)(&#34;gram_my_job&#34; = &#34;collective&#34; )(&#34;dry_run&#34; = &#34;no&#34; )(&#34;environment&#34; =
(&#34;SCRATCH_DIRECTORY&#34; &#34;/home/osg-star//gram_scratch_XUZdmmzUbD&#34; )
(&#34;LD_LIBRARY_PATH&#34;
&#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34;
) (&#34;HOME&#34; &#34;/home/osg-star&#34; ) (&#34;LOGNAME&#34; &#34;osg-star&#34; ) )(&#34;rslsubstitution&#34; =
(&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; )
)(&#34;executable&#34; =
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34</a>;
)(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; =
&#34;/home/osg-star//gram_scratch_XUZdmmzUbD&#34; )(&#34;stdout&#34; =
&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout&#34;
)(&#34;stderr&#34; =
&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr&#34;
)(&#34;filestageout&#34; =
(&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34</a>;
)
(&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34</a>;
) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34;
)(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-validation)</font>
6/11 02&#58;03&#58;00
&#60;&#60;&#60;&#60;&#60;Job RSL (post-validation-eval)
&(&#34;stdin&#34; = &#34;/dev/null&#34; )(&#34;count&#34; = &#34;1&#34; )(&#34;job_type&#34; = &#34;multiple&#34;
)(&#34;gram_my_job&#34; = &#34;collective&#34; )(&#34;dry_run&#34; = &#34;no&#34; )(&#34;environment&#34; =
(&#34;SCRATCH_DIRECTORY&#34; &#34;/home/osg-star//gram_scratch_XUZdmmzUbD&#34; )
(&#34;LD_LIBRARY_PATH&#34;
&#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34;
) (&#34;HOME&#34; &#34;/home/osg-star&#34; ) (&#34;LOGNAME&#34; &#34;osg-star&#34; ) )(&#34;rslsubstitution&#34; =
(&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; )
)(&#34;executable&#34; =
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/data05/scratch/didenko/gridwork/schedCB68E288FE840BB4309FF6F984E51130_0.csh&#34</a>;
)(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; =
&#34;/home/osg-star//gram_scratch_XUZdmmzUbD&#34; )(&#34;stdout&#34; =
&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout&#34;
)(&#34;stderr&#34; =
&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr&#34;
)(&#34;filestageout&#34; =
(&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log&#34</a>;
)
(&#34;/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr&#34;
&#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err&#34</a>;
) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34;
)(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-validation-eval)</font>
6/11 02&#58;03&#58;00 JMI&#58; Getting RSL output value
6/11 02&#58;03&#58;00 JMI&#58; Processing output positions
6/11 02&#58;03&#58;00 JMI&#58; Getting RSL output value
6/11 02&#58;03&#58;00 JMI&#58; Processing output positions
6/11 02&#58;03&#58;00 JM&#58; Evaluating RSL Value6/11 02&#58;03&#58;00 JM&#58; Evaluated RSL
Value to
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout6/11
02&#58;03&#58;00 JM&#58; Evaluating RSL Value6/11 02&#58;03&#58;00 JM&#58; Evaluated RSL Value to
<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log6/11' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.log6/11</a>
02&#58;03&#58;00 JM&#58; Evaluating RSL Value6/11 02&#58;03&#58;00 JM&#58; Evaluated RSL Value to
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr6/11
02&#58;03&#58;00 JM&#58; Evaluating RSL Value6/11 02&#58;03&#58;00 JM&#58; Evaluated RSL Value to
<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err6/11' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20004/star/rcf/prodlog/GR_wsu/log/trs/rcf1235_20_3361evts.err6/11</a>
02&#58;03&#58;00 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
6/11 02&#58;03&#58;00 JMI&#58; completed script validation&#58; job manager type is sge.
6/11 02&#58;03&#58;00 JMI&#58; in globus_gram_job_manager_script_remote_io_file_create()
6/11 02&#58;03&#58;00 JMI&#58; cmd = remote_io_file_create
Wed Jun 11 02&#58;03&#58;01 2008 JM_SCRIPT&#58; New Perl JobManager created.
Wed Jun 11 02&#58;03&#58;01 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58;
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;03&#58;01 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58;
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;03&#58;01 2008 JM_SCRIPT&#58; remote_io_file_create(enter)
Wed Jun 11 02&#58;03&#58;01 2008 JM_SCRIPT&#58; Sent NFS sync for
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/remote_io_url
Wed Jun 11 02&#58;03&#58;01 2008 JM_SCRIPT&#58; remote_io_file_create(exit)
6/11 02&#58;03&#58;01 JMI&#58; while return_buf = GRAM_SCRIPT_REMOTE_IO_FILE =
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/remote_io_url
6/11 02&#58;03&#58;01 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_REMOTE_IO_FILE_CREATE
6/11 02&#58;03&#58;01 JM&#58; Opening output destinations
6/11 02&#58;03&#58;01 JM&#58; stdout goes to
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stdout
6/11 02&#58;03&#58;01 JM&#58; stderr goes to
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/stderr
6/11 02&#58;03&#58;02 stdout or stderr is being used, starting to poll
6/11 02&#58;03&#58;02 no opens in progress, registering state machine callback
6/11 02&#58;03&#58;02 JM&#58; Finished opening output destinations
6/11 02&#58;03&#58;02 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_OPEN_OUTPUT
6/11 02&#58;03&#58;02 JM&#58; GSSAPI type is GSI.. relocating proxy
6/11 02&#58;03&#58;02 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
6/11 02&#58;03&#58;02 JMI&#58; completed script validation&#58; job manager type is sge.
6/11 02&#58;03&#58;02 JMI&#58; in globus_gram_job_manager_script_proxy_relocate()
6/11 02&#58;03&#58;02 JMI&#58; cmd = proxy_relocate
Wed Jun 11 02&#58;03&#58;03 2008 JM_SCRIPT&#58; New Perl JobManager created.
Wed Jun 11 02&#58;03&#58;03 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58;
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;03&#58;03 2008 JM_SCRIPT&#58; proxy_relocate(enter)
6/11 02&#58;03&#58;03 JMI&#58; while return_buf = GRAM_SCRIPT_X509_USER_PROXY =
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/x509_up
6/11 02&#58;03&#58;03 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_PROXY_RELOCATE
6/11 02&#58;03&#58;03 JM&#58; Relocated Proxy to
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177/x509_up
6/11 02&#58;03&#58;03 JM&#58; Creating and locking state lock file
6/11 02&#58;03&#58;03 JM&#58; Writing state file
6/11 02&#58;03&#58;03 JM&#58; before sending to client&#58; rc=0 (Success)
6/11 02&#58;03&#58;03 Job Manager State Machine (exiting)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_TWO_PHASE
6/11 02&#58;13&#58;03 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_TWO_PHASE
6/11 02&#58;13&#58;03 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED
6/11 02&#58;13&#58;03 JM&#58; Writing state file
6/11 02&#58;13&#58;03 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_CLOSE_OUTPUT
6/11 02&#58;13&#58;03 JM&#58; in globus_gram_job_manager_history_file_create()
6/11 02&#58;13&#58;03 JM&#58; NOT empty client callback list.
6/11 02&#58;13&#58;03 JM&#58; sending callback of status 4 (failure code 111) to
<a href='https&#58;//stargrid01.rcf.bnl.gov&#58;20000/.' target='_blank' rel='nofollow'>https&#58;//stargrid01.rcf.bnl.gov&#58;20000/.</a>
6/11 02&#58;13&#58;03 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE
6/11 02&#58;13&#58;03 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE_COMMITTED
6/11 02&#58;13&#58;03 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_FILE_CLEAN_UP
6/11 02&#58;13&#58;03 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
6/11 02&#58;13&#58;03 JMI&#58; completed script validation&#58; job manager type is sge.
6/11 02&#58;13&#58;03 JMI&#58; in globus_gram_job_manager_rm_scratchdir()
6/11 02&#58;13&#58;03 JMI&#58; cmd = remove_scratchdir
6/11 02&#58;13&#58;04 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_SCRATCH_CLEAN_UP
6/11 02&#58;13&#58;04 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
6/11 02&#58;13&#58;04 JMI&#58; completed script validation&#58; job manager type is sge.
6/11 02&#58;13&#58;04 JMI&#58; cmd = cache_cleanup
Wed Jun 11 02&#58;13&#58;04 2008 JM_SCRIPT&#58; New Perl JobManager created.
Wed Jun 11 02&#58;13&#58;04 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58;
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;13&#58;04 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58;
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;13&#58;04 2008 JM_SCRIPT&#58; cache_cleanup(enter)
Wed Jun 11 02&#58;13&#58;04 2008 JM_SCRIPT&#58; Cleaning files in job dir
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;13&#58;05 2008 JM_SCRIPT&#58; Removed 5 files from
/home/osg-star/.globus/job/rhic23.physics.wayne.edu/15595.1213164177
Wed Jun 11 02&#58;13&#58;05 2008 JM_SCRIPT&#58; cache_cleanup(exit)
6/11 02&#58;13&#58;05 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_CACHE_CLEAN_UP
6/11 02&#58;13&#58;05 JM&#58; in globus_gram_job_manager_reporting_file_remove()
6/11 02&#58;13&#58;05 JM&#58; exiting globus_gram_job_manager.

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_1533644860').click(function() {
            $('#detail_1533644860').slideDown("normal");
            $('#show_1533644860').hide();
            $('#hide_1533644860').show();
        });
        $('#hide_1533644860').click(function() {
            $('#detail_1533644860').slideUp();
            $('#hide_1533644860').hide();
            $('#show_1533644860').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='4867#1213224983'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-11T22:56:23+00:00">Jun 11, 2008 10:56 PM UTC</time> by <b>echism</b><a class="anchor" name="1213224983">&nbsp;</a></div><pre>Liz,

Could you update us with your current logs?

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1213201929'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-11T16:32:09+00:00">Jun 11, 2008 04:32 PM UTC</time> by <b>Jaime Frey</b><a class="anchor" name="1213201929">&nbsp;</a></div><pre><font color='#7F7E6F'>&#62; Jamie,</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Could you take a look at this issue?</font>

Sorry for not replying sooner.

As was noted earlier in the ticket, the Condor gridmanager starts a
new job request, but doesn&#39;t send the commit signal before the GRAM
jobmanager times out (after 10 minutes).

To debug this further, we&#39;re going to want gridmanager log files from
one of the submitters who are seeing this problem.

+--------------------------------+-----------------------------------+
|           Jaime Frey           | I used to be a heavy gambler.     |
|       jfrey@....        | But now I just make mental bets.  |
| <a href='http&#58;//www.cs.wisc.edu/' target='_blank' rel='nofollow'>http&#58;//www.cs.wisc.edu/</a>~jfrey/ | That&#39;s how I lost my mind.        |
+--------------------------------+-----------------------------------+</pre></div><div class='update_description'><i onclick="document.location='4867#1212601444'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-04T17:44:04+00:00">Jun 4, 2008 05:44 PM UTC</time> by <b>echism</b><a class="anchor" name="1212601444">&nbsp;</a></div><pre>Jamie,

Could you take a look at this issue?

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1212419109'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-02T15:05:09+00:00">Jun 2, 2008 03:05 PM UTC</time> by <b>Rob Quick</b><a class="anchor" name="1212419109">&nbsp;</a></div><pre>Begin forwarded message&#58;

<font color='#7F7E6F'>&#62; From&#58; Charles Bacon &#60;bacon@....&#62;</font>
<font color='#7F7E6F'>&#62; Date&#58; June 2, 2008 10&#58;42&#58;46 AM EDT</font>
<font color='#7F7E6F'>&#62; To&#58; &#34;Quick, Robert E&#34; &#60;rquick@....&#62;</font>
<font color='#7F7E6F'>&#62; Cc&#58; &#34;atems@....&#34; &#60;atems@....&#62;</font>
<font color='#7F7E6F'>&#62; Subject&#58; Re&#58; Fatal gridmanager error killing jobs -- ISSUE=4867</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; When Condor-G submits to GRAM, it uses the two-phase commit protocol.</font>
<font color='#7F7E6F'>&#62; This is designed to ensure that the job starts once and only once.</font>
<font color='#7F7E6F'>&#62; From the output below, the error returned (111) means&#58; &#34;the job</font>
<font color='#7F7E6F'>&#62; manager timed out while waiting for a commit signal&#34;.  In other words,</font>
<font color='#7F7E6F'>&#62; condor-g contacted GRAM to initiate the job.  GRAM got everything</font>
<font color='#7F7E6F'>&#62; ready and set back the first phase response.  It waited there for the</font>
<font color='#7F7E6F'>&#62; second phase &#34;go ahead&#34; signal, but never received it.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; I don&#39;t have a lot of background in the two phase commit protocol, so</font>
<font color='#7F7E6F'>&#62; I&#39;m not sure exactly what would be going wrong here.  The simplest</font>
<font color='#7F7E6F'>&#62; explanation for me to understand would be if condor-g either never got</font>
<font color='#7F7E6F'>&#62; the first phase response or somehow failed to get around to sending</font>
<font color='#7F7E6F'>&#62; the second phase go-ahead in time.  I would suggest bringing Jaime</font>
<font color='#7F7E6F'>&#62; into the debugging loop, especially if the condor-g logs for this job</font>
<font color='#7F7E6F'>&#62; submission are available.  That should let a condor expert know</font>
<font color='#7F7E6F'>&#62; whether condor-g got the first phase ACK at all, as I assume that will</font>
<font color='#7F7E6F'>&#62; show up in the logs.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Charles</font></pre></div><div class='update_description'><i onclick="document.location='4867#1212416768'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-06-02T14:26:08+00:00">Jun 2, 2008 02:26 PM UTC</time> by <b>Rob Quick</b><a class="anchor" name="1212416768">&nbsp;</a></div><pre>Charles,

I saw you replied to this message some time ago. I apologize for the
lengthy time between this and the last mail. Please see Liz&#39; mail
below, any ideas what might be happening here?

Thanks,
Rob

I have a little more information now on this problem. The
~/gram_job_mgr*.log files do show errors. As Charles Bacon suggested,
it does seem to have to do with a two-phase commit failure, however I
do not quite understand how this process works nor how to interpret
the errors in the log file. Please see the file listing below.

Thanks,
Liz

/home/lbhajdu/gram_job_mgr_10212.log&#58;

4/28 19&#58;35&#58;31 JM&#58; Security context imported
4/28 19&#58;35&#58;31 JM&#58; Adding new callback contact (url=<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20000/' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20000/</a>
, mask=1048575)
4/28 19&#58;35&#58;31 JM&#58; Added successfully
4/28 19&#58;35&#58;31 Pre-parsed RSL string&#58;
&(rsl_substitution=(GRIDMANAGER_GASS_URL <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001</a>))(executable=$(GRIDMANAGER_GASS_URL)
#&#39;/star/u/lbhajdu/temp/liz_test/
sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#39;)(scratchdir=&#39;&#39;)
(directory=$(SCRATCH_DIRECTORY))(stdout=$(GLOBUS_CACHED_STDOUT))
(stderr=$(GLOBUS_CACHED_STDERR))(file_stage_out=($
(GLOBUS_CACHED_STDOUT) $(GRIDMANAGER_GASS_URL)#&#39;/direct/star+u/lbhajdu/
temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#39;)($
(GLOBUS_CACHED_STDERR) $(GRIDMANAGER_GASS_URL)#&#39;/direct/star+u/lbhajdu/
temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#39;))
(proxy_timeout=240)(save_state=yes)(two_phase=600)(remote_io_url=$
<div id='show_2097516888' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_2097516888'>(GRIDMANAGER_GASS_URL))
4/28 19&#58;35&#58;31
&#60;&#60;&#60;&#60;&#60;Job Request RSL
&(&#34;rsl_substitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001</a>
&#34; ) )(&#34;executable&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/star/u/lbhajdu/temp/
liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34; )(&#34;scratchdir&#34;
= &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; = $
(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;) )
(&#34;file_stage_out&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $
(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/liz_test/./
shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34; ) ($
(&#34;GLOBUS_CACHED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/
lbhajdu/temp/liz_test/./
shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34; ) )(&#34;proxy_timeout&#34; =
$(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job Request RSL</font>
4/28 19&#58;35&#58;31
&#60;&#60;&#60;&#60;&#60;Job Request RSL (canonical)
&(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001</a>
&#34; ) )(&#34;executable&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/star/u/lbhajdu/temp/
liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34; )(&#34;scratchdir&#34;
= &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; = $
(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;) )
(&#34;filestageout&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;)
# &#34;/direct/star+u/lbhajdu/temp/liz_test/./
shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34; ) ($
(&#34;GLOBUS_CACHED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/
lbhajdu/temp/liz_test/./
shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34; ) )(&#34;proxytimeout&#34; =
(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job Request RSL (canonical)</font>
4/28 19&#58;35&#58;31 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;31 JM&#58; Evaluated RSL
Value to GRIDMANAGER_GASS_URL4/28 19&#58;35&#58;31 JM&#58; Evaluating RSL
Value4/28 19&#58;35&#58;31 JM&#58; Evaluated RSL Value to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;200014/28' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;200014/28</a>
19&#58;35&#58;31 Appending extra env.var LD_LIBRARY_PATH=/r23b/grid/apache/
lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/
lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/
jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/
grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/
r23b/grid/globus/lib
4/28 19&#58;35&#58;31 Evaluating scratch directory RSL
4/28 19&#58;35&#58;31 Scratch Directory RSL -&#62;
4/28 19&#58;35&#58;31 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
4/28 19&#58;35&#58;31 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;35&#58;31 JMI&#58; in globus_gram_job_manager_script_make_scratchdir()
4/28 19&#58;35&#58;31 JMI&#58; cmd = make_scratchdir
4/28 19&#58;35&#58;31 JMI&#58; returning with success
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Entering Job Manager default
implementation of make_scratchdir
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Trying to create directory named /
home/lbhajdu//gram_scratch_JrCc5SI0dY
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Sent NFS sync for /home/lbhajdu//
gram_scratch_JrCc5SI0dY
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; I think it was made.... verifying
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using /home/lbhajdu//
gram_scratch_JrCc5SI0dY as the scratch directory for this job.
4/28 19&#58;35&#58;32 JMI&#58; while return_buf = GRAM_SCRIPT_SCRATCH_DIR = /home/
lbhajdu//gram_scratch_JrCc5SI0dY
4/28 19&#58;35&#58;32 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_MAKE_SCRATCHDIR
4/28 19&#58;35&#58;32 Adding scratch dir to symbol table and env&#58; /home/
lbhajdu//gram_scratch_JrCc5SI0dY
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL
&(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//
gram_scratch_JrCc5SI0dY&#34; ) (&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/
r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/
r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/
jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/
mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/
grid/globus/lib&#34; ) (&#34;HOME&#34; &#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )
(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001</a>
&#34; ) )(&#34;executable&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/star/u/lbhajdu/temp/
liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34; )(&#34;scratchdir&#34;
= &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; = $
(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;) )
(&#34;filestageout&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;)
# &#34;/direct/star+u/lbhajdu/temp/liz_test/./
shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34; ) ($(&#34;GLOBUS_CACH
ED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/
liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34; ) )
(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34; )
(&#34;remoteiourl&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL</font>
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL (post-eval)
&(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//
gram_scratch_JrCc5SI0dY&#34; ) (&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/
r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/
r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/
jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/
mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/
grid/globus/lib&#34; ) (&#34;HOME&#34; &#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )
(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001</a>
&#34; ) )(&#34;executable&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh</a>
&#34; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = &#34;/home/lbhajdu//
gram_scratch_JrCc5SI0dY&#34; )(&#34;stdout&#34; = &#34;/home/lbhajdu/.globus/job/
rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; )(&#34;stderr&#34; = &#34;/home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; )
(&#34;filestageout&#34; = (&#34;/home/lbhajdu/.globus/job/r
hic23.physics.wayne.edu/10212.1209425731/stdout&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out</a>
&#34; ) (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/
10212.1209425731/stderr&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out</a>
&#34; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; =
&#34;600&#34; )(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-eval)</font>
Adding default RSL of dry_run = no
Adding default RSL of gram_my_job = collective
Adding default RSL of count = 1
Adding default RSL of stdin = /dev/null
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL (post-validation)
&(&#34;stdin&#34; = &#34;/dev/null&#34; )(&#34;count&#34; = &#34;1&#34; )(&#34;job_type&#34; = &#34;multiple&#34; )
(&#34;gram_my_job&#34; = &#34;collective&#34; )(&#34;dry_run&#34; = &#34;no&#34; )(&#34;environment&#34; =
(&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; )
(&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/
VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/
jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/
jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/
berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34; ) (&#34;HOME&#34;
&#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )(&#34;rslsubstitution&#34; =
(&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )
(&#34;executable&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh</a>
&#34; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = &#34;/home/lbhajdu//
gram_scratch_JrCc5SI0dY&#34; )(&#34;stdout&#34; = &#34;/home/lbhajdu/.globus/job/
rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; )(&#34;stderr&#34; = &#34;/home/lb
hajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; )
(&#34;filestageout&#34; = (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/
10212.1209425731/stdout&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out</a>
&#34; ) (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/
10212.1209425731/stderr&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out</a>
&#34; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; =
&#34;600&#34; )(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-validation)</font>
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL (post-validation-eval)
&(&#34;stdin&#34; = &#34;/dev/null&#34; )(&#34;count&#34; = &#34;1&#34; )(&#34;job_type&#34; = &#34;multiple&#34; )
(&#34;gram_my_job&#34; = &#34;collective&#34; )(&#34;dry_run&#34; = &#34;no&#34; )(&#34;environment&#34; =
(&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; )
(&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/
VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/
jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/
jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/
berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34; ) (&#34;HOME&#34;
&#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )(&#34;rslsubstitution&#34; =
(&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )
(&#34;executable&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh</a>
&#34; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = &#34;/home/lbhajdu//
gram_scratch_JrCc5SI0dY&#34; )(&#34;stdout&#34; = &#34;/home/lbhajdu/.globus/job/
rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; )(&#34;stderr&#34; = &#34;/home/lb
hajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; )
(&#34;filestageout&#34; = (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/
10212.1209425731/stdout&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out</a>
&#34; ) (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/
10212.1209425731/stderr&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out</a>
&#34; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; =
&#34;600&#34; )(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-validation-eval)</font>
4/28 19&#58;35&#58;32 JMI&#58; Getting RSL output value
4/28 19&#58;35&#58;32 JMI&#58; Processing output positions
4/28 19&#58;35&#58;32 JMI&#58; Getting RSL output value
4/28 19&#58;35&#58;32 JMI&#58; Processing output positions
4/28 19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;32 JM&#58; Evaluated RSL
Value to /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/
10212.1209425731/stdout4/28 19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28
19&#58;35&#58;32 JM&#58; Evaluated RSL Value to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out4/28' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out4/28</a>
19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;32 JM&#58; Evaluated RSL
Value to /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/
10212.1209425731/stderr4/28 19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28
19&#58;35&#58;32 JM&#58; Evaluated RSL Value to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out4/28' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out4/28</a>
19&#58;35&#58;32 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
4/28 19&#58;35&#58;32 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;35&#58;32 JMI&#58; in
globus_gram_job_manager_script_remote_io_file_create()
4/28 19&#58;35&#58;32 JMI&#58; cmd = remote_io_file_create
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; remote_io_file_create(enter)
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Sent NFS sync for /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/
remote_io_url
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; remote_io_file_create(exit)
4/28 19&#58;35&#58;32 JMI&#58; while return_buf = GRAM_SCRIPT_REMOTE_IO_FILE = /
home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/
remote_io_url
4/28 19&#58;35&#58;33 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_REMOTE_IO_FILE_CREATE
4/28 19&#58;35&#58;33 JM&#58; Opening output destinations
4/28 19&#58;35&#58;33 JM&#58; stdout goes to /home/lbhajdu/.globus/job/
rhic23.physics.wayne.edu/10212.1209425731/stdout
4/28 19&#58;35&#58;33 JM&#58; stderr goes to /home/lbhajdu/.globus/job/
rhic23.physics.wayne.edu/10212.1209425731/stderr
4/28 19&#58;35&#58;33 stdout or stderr is being used, starting to poll
4/28 19&#58;35&#58;33 no opens in progress, registering state machine callback
4/28 19&#58;35&#58;33 JM&#58; Finished opening output destinations
4/28 19&#58;35&#58;33 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_OPEN_OUTPUT
4/28 19&#58;35&#58;33 JM&#58; GSSAPI type is GSI.. relocating proxy
4/28 19&#58;35&#58;33 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
4/28 19&#58;35&#58;33 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;35&#58;33 JMI&#58; in globus_gram_job_manager_script_proxy_relocate()
4/28 19&#58;35&#58;33 JMI&#58; cmd = proxy_relocate
Mon Apr 28 19&#58;35&#58;33 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;35&#58;33 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;33 2008 JM_SCRIPT&#58; proxy_relocate(enter)
4/28 19&#58;35&#58;33 JMI&#58; while return_buf = GRAM_SCRIPT_X509_USER_PROXY = /
home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/
x509_up
4/28 19&#58;35&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_PROXY_RELOCATE
4/28 19&#58;35&#58;34 JM&#58; Relocated Proxy to /home/lbhajdu/.globus/job/
rhic23.physics.wayne.edu/10212.1209425731/x509_up
4/28 19&#58;35&#58;34 JM&#58; Creating and locking state lock file
4/28 19&#58;35&#58;34 JM&#58; Writing state file
4/28 19&#58;35&#58;34 JM&#58; before sending to client&#58; rc=0 (Success)
4/28 19&#58;35&#58;34 Job Manager State Machine (exiting)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_TWO_PHASE
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_TWO_PHASE
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED
4/28 19&#58;45&#58;34 JM&#58; Writing state file
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_CLOSE_OUTPUT
4/28 19&#58;45&#58;34 JM&#58; in globus_gram_job_manager_history_file_create()
4/28 19&#58;45&#58;34 JM&#58; NOT empty client callback list.
4/28 19&#58;45&#58;34 JM&#58; sending callback of status 4 (failure code 111) to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20000/' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20000/</a>
.
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE_COMMITTED
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_FILE_CLEAN_UP
4/28 19&#58;45&#58;34 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
4/28 19&#58;45&#58;34 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;45&#58;34 JMI&#58; in globus_gram_job_manager_rm_scratchdir()
4/28 19&#58;45&#58;34 JMI&#58; cmd = remove_scratchdir
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_SCRATCH_CLEAN_UP
4/28 19&#58;45&#58;34 JMI&#58; testing job manager scripts for type sge exist and
permissions are ok.
4/28 19&#58;45&#58;34 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;45&#58;34 JMI&#58; cmd = cache_cleanup
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; cache_cleanup(enter)
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Cleaning files in job dir /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Removed 5 files from /home/
lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; cache_cleanup(exit)
4/28 19&#58;45&#58;36 Job Manager State Machine (entering)&#58;
GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_CACHE_CLEAN_UP
4/28 19&#58;45&#58;36 JM&#58; in globus_gram_job_manager_reporting_file_remove()
4/28 19&#58;45&#58;36 JM&#58; exiting globus_gram_job_manager.
</div><script type='text/javascript'>
        $('#show_2097516888').click(function() {
            $('#detail_2097516888').slideDown("normal");
            $('#show_2097516888').hide();
            $('#hide_2097516888').show();
        });
        $('#hide_2097516888').click(function() {
            $('#detail_2097516888').slideUp();
            $('#hide_2097516888').hide();
            $('#show_2097516888').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='4867#1211573246'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-05-23T20:07:26+00:00">May 23, 2008 08:07 PM UTC</time> by <b>Rob Quick</b><a class="anchor" name="1211573246">&nbsp;</a></div><pre>Elizabeth,

I think OSG-Sites will accept a message from here.

Rob</pre></div><div class='update_description'><i onclick="document.location='4867#1210961177'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-05-16T18:06:17+00:00">May 16, 2008 06:06 PM UTC</time> by <b>echism</b><a class="anchor" name="1210961177">&nbsp;</a></div><pre>Will osg-sites accept an email from this address? I haven&#39;t seen a response and I&#39;m
not on that list.

Thanks,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='4867#1210270028'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-05-08T18:07:08+00:00">May 8, 2008 06:07 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1210270028">&nbsp;</a></div><pre>I have a little more information now on this problem. The
~/gram_job_mgr*.log files do show errors. As Charles Bacon suggested,
it does seem to have to do with a two-phase commit failure, however I
do not quite understand how this process works nor how to interpret
the errors in the log file. Please see the file listing below.

Thanks,
Liz

/home/lbhajdu/gram_job_mgr_10212.log&#58;

4/28 19&#58;35&#58;31 JM&#58; Security context imported
4/28 19&#58;35&#58;31 JM&#58; Adding new callback contact (url=<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20000/' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20000/</a>, mask=1048575)
4/28 19&#58;35&#58;31 JM&#58; Added successfully
4/28 19&#58;35&#58;31 Pre-parsed RSL string&#58; &(rsl_substitution=(GRIDMANAGER_GASS_URL <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001</a>))(executable=$(GRIDMANAGER_GASS_URL)#&#39;/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#39;)(scratchdir=&#39;&#39;)(directory=$(SCRATCH_DIRECTORY))(stdout=$(GLOBUS_CACHED_STDOUT))(stderr=$(GLOBUS_CACHED_STDERR))(file_stage_out=($(GLOBUS_CACHED_STDOUT) $(GRIDMANAGER_GASS_URL)#&#39;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#39;)($(GLOBUS_CACHED_STDERR) $(GRIDMANAGER_GASS_URL)#&#39;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#39;))(proxy_timeout=240)(save_state=yes)(two_phase=600)(remote_io_url=$(GRIDMANAGER_GASS_URL))
4/28 19&#58;35&#58;31
&#60;&#60;&#60;&#60;&#60;Job Request RSL
&(&#34;rsl_substitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )(&#34;executable&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; = $(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;) )(&#34;file_stage_out&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34; ) ($(&#34;GLOBUS_CACHED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34; ) )(&#34;proxy_timeout&#34; = &#34;240&#34; )(&#34;save_state&#34; = &#34;yes&#34; )(&#34;two_phase&#34; = &#34;600&#34; )(&#34;remote_io_url&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job Request RSL</font>
4/28 19&#58;35&#58;31
&#60;&#60;&#60;&#60;&#60;Job Request RSL (canonical)
&(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )(&#34;executable&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; = $(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;) )(&#34;filestageout&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34; ) ($(&#34;GLOBUS_CACHED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34; )(&#34;remoteiourl&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job Request RSL (canonical)</font>
4/28 19&#58;35&#58;31 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;31 JM&#58; Evaluated RSL Value to GRIDMANAGER_GASS_URL4/28 19&#58;35&#58;31 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;31 JM&#58; Evaluated RSL Value to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;200014/28' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;200014/28</a> 19&#58;35&#58;31 Appending extra env.var LD_LIBRARY_PATH=/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib
4/28 19&#58;35&#58;31 Evaluating scratch directory RSL
4/28 19&#58;35&#58;31 Scratch Directory RSL -&#62;
4/28 19&#58;35&#58;31 JMI&#58; testing job manager scripts for type sge exist and permissions are ok.
4/28 19&#58;35&#58;31 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;35&#58;31 JMI&#58; in globus_gram_job_manager_script_make_scratchdir()
4/28 19&#58;35&#58;31 JMI&#58; cmd = make_scratchdir
4/28 19&#58;35&#58;31 JMI&#58; returning with success
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Entering Job Manager default implementation of make_scratchdir
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Trying to create directory named /home/lbhajdu//gram_scratch_JrCc5SI0dY
<div id='show_490212106' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_490212106'>Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Sent NFS sync for /home/lbhajdu//gram_scratch_JrCc5SI0dY
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; I think it was made.... verifying
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using /home/lbhajdu//gram_scratch_JrCc5SI0dY as the scratch directory for this job.
4/28 19&#58;35&#58;32 JMI&#58; while return_buf = GRAM_SCRIPT_SCRATCH_DIR = /home/lbhajdu//gram_scratch_JrCc5SI0dY
4/28 19&#58;35&#58;32 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_MAKE_SCRATCHDIR
4/28 19&#58;35&#58;32 Adding scratch dir to symbol table and env&#58; /home/lbhajdu//gram_scratch_JrCc5SI0dY
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL
&(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; ) (&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34; ) (&#34;HOME&#34; &#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )(&#34;executable&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = $(&#34;SCRATCH_DIRECTORY&#34;) )(&#34;stdout&#34; = $(&#34;GLOBUS_CACHED_STDOUT&#34;) )(&#34;stderr&#34; = $(&#34;GLOBUS_CACHED_STDERR&#34;) )(&#34;filestageout&#34; = ($(&#34;GLOBUS_CACHED_STDOUT&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34; ) ($(&#34;GLOBUS_CACH
ED_STDERR&#34;) $(&#34;GRIDMANAGER_GASS_URL&#34;) # &#34;/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34; )(&#34;remoteiourl&#34; = $(&#34;GRIDMANAGER_GASS_URL&#34;) )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL</font>
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL (post-eval)
&(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; ) (&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34; ) (&#34;HOME&#34; &#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )(&#34;executable&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34</a>; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; )(&#34;stdout&#34; = &#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; )(&#34;stderr&#34; = &#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; )(&#34;filestageout&#34; = (&#34;/home/lbhajdu/.globus/job/r
hic23.physics.wayne.edu/10212.1209425731/stdout&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34</a>; ) (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34</a>; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34; )(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-eval)</font>
Adding default RSL of dry_run = no
Adding default RSL of gram_my_job = collective
Adding default RSL of job_type = multiple
Adding default RSL of count = 1
Adding default RSL of stdin = /dev/null
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL (post-validation)
&(&#34;stdin&#34; = &#34;/dev/null&#34; )(&#34;count&#34; = &#34;1&#34; )(&#34;job_type&#34; = &#34;multiple&#34; )(&#34;gram_my_job&#34; = &#34;collective&#34; )(&#34;dry_run&#34; = &#34;no&#34; )(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; ) (&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34; ) (&#34;HOME&#34; &#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )(&#34;executable&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34</a>; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; )(&#34;stdout&#34; = &#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; )(&#34;stderr&#34; = &#34;/home/lb
hajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; )(&#34;filestageout&#34; = (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34</a>; ) (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34</a>; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34; )(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-validation)</font>
4/28 19&#58;35&#58;32
&#60;&#60;&#60;&#60;&#60;Job RSL (post-validation-eval)
&(&#34;stdin&#34; = &#34;/dev/null&#34; )(&#34;count&#34; = &#34;1&#34; )(&#34;job_type&#34; = &#34;multiple&#34; )(&#34;gram_my_job&#34; = &#34;collective&#34; )(&#34;dry_run&#34; = &#34;no&#34; )(&#34;environment&#34; = (&#34;SCRATCH_DIRECTORY&#34; &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; ) (&#34;LD_LIBRARY_PATH&#34; &#34;/r23b/grid/apache/lib&#58;/r23b/grid/MonaLisa/Service/VDTFarm/pgsql/lib&#58;/r23b/grid/glite/lib&#58;/r23b/grid/prima/lib&#58;/r23b/grid/jdk1.4/jre/lib/i386&#58;/r23b/grid/jdk1.4/jre/lib/i386/server&#58;/r23b/grid/jdk1.4/jre/lib/i386/client&#58;/r23b/grid/mysql/lib/mysql&#58;/r23b/grid/berkeley-db/lib&#58;/r23b/grid/expat/lib&#58;/r23b/grid/globus/lib&#34; ) (&#34;HOME&#34; &#34;/home/lbhajdu&#34; ) (&#34;LOGNAME&#34; &#34;lbhajdu&#34; ) )(&#34;rslsubstitution&#34; = (&#34;GRIDMANAGER_GASS_URL&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; ) )(&#34;executable&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/star/u/lbhajdu/temp/liz_test/sched87F04DDFB6326B99C77AE6954D03982D_42.csh&#34</a>; )(&#34;scratchdir&#34; = &#34;&#34; )(&#34;directory&#34; = &#34;/home/lbhajdu//gram_scratch_JrCc5SI0dY&#34; )(&#34;stdout&#34; = &#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; )(&#34;stderr&#34; = &#34;/home/lb
hajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; )(&#34;filestageout&#34; = (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stdout&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out&#34</a>; ) (&#34;/home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr&#34; &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out&#34</a>; ) )(&#34;proxytimeout&#34; = &#34;240&#34; )(&#34;savestate&#34; = &#34;yes&#34; )(&#34;twophase&#34; = &#34;600&#34; )(&#34;remoteiourl&#34; = &#34;<a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001&#34</a>; )
<font color='#7F7E6F'>&#62;&#62;&#62;&#62;&#62;Job RSL (post-validation-eval)</font>
4/28 19&#58;35&#58;32 JMI&#58; Getting RSL output value
4/28 19&#58;35&#58;32 JMI&#58; Processing output positions
4/28 19&#58;35&#58;32 JMI&#58; Getting RSL output value
4/28 19&#58;35&#58;32 JMI&#58; Processing output positions
4/28 19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;32 JM&#58; Evaluated RSL Value to /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stdout4/28 19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;32 JM&#58; Evaluated RSL Value to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out4/28' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.out4/28</a> 19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;32 JM&#58; Evaluated RSL Value to /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr4/28 19&#58;35&#58;32 JM&#58; Evaluating RSL Value4/28 19&#58;35&#58;32 JM&#58; Evaluated RSL Value to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out4/28' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20001/direct/star+u/lbhajdu/temp/liz_test/./shed87F04DDFB6326B99C77AE6954D03982D_42.err.out4/28</a> 19&#58;35&#58;32 JMI&#58; testing job manager scripts for type sge exist and permissions are ok.
4/28 19&#58;35&#58;32 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;35&#58;32 JMI&#58; in globus_gram_job_manager_script_remote_io_file_create()
4/28 19&#58;35&#58;32 JMI&#58; cmd = remote_io_file_create
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; remote_io_file_create(enter)
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; Sent NFS sync for /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/remote_io_url
Mon Apr 28 19&#58;35&#58;32 2008 JM_SCRIPT&#58; remote_io_file_create(exit)
4/28 19&#58;35&#58;32 JMI&#58; while return_buf = GRAM_SCRIPT_REMOTE_IO_FILE = /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/remote_io_url
4/28 19&#58;35&#58;33 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_REMOTE_IO_FILE_CREATE
4/28 19&#58;35&#58;33 JM&#58; Opening output destinations
4/28 19&#58;35&#58;33 JM&#58; stdout goes to /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stdout
4/28 19&#58;35&#58;33 JM&#58; stderr goes to /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/stderr
4/28 19&#58;35&#58;33 stdout or stderr is being used, starting to poll
4/28 19&#58;35&#58;33 no opens in progress, registering state machine callback
4/28 19&#58;35&#58;33 JM&#58; Finished opening output destinations
4/28 19&#58;35&#58;33 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_OPEN_OUTPUT
4/28 19&#58;35&#58;33 JM&#58; GSSAPI type is GSI.. relocating proxy
4/28 19&#58;35&#58;33 JMI&#58; testing job manager scripts for type sge exist and permissions are ok.
4/28 19&#58;35&#58;33 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;35&#58;33 JMI&#58; in globus_gram_job_manager_script_proxy_relocate()
4/28 19&#58;35&#58;33 JMI&#58; cmd = proxy_relocate
Mon Apr 28 19&#58;35&#58;33 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;35&#58;33 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;35&#58;33 2008 JM_SCRIPT&#58; proxy_relocate(enter)
4/28 19&#58;35&#58;33 JMI&#58; while return_buf = GRAM_SCRIPT_X509_USER_PROXY = /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/x509_up
4/28 19&#58;35&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_PROXY_RELOCATE
4/28 19&#58;35&#58;34 JM&#58; Relocated Proxy to /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731/x509_up
4/28 19&#58;35&#58;34 JM&#58; Creating and locking state lock file
4/28 19&#58;35&#58;34 JM&#58; Writing state file
4/28 19&#58;35&#58;34 JM&#58; before sending to client&#58; rc=0 (Success)
4/28 19&#58;35&#58;34 Job Manager State Machine (exiting)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_TWO_PHASE
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_TWO_PHASE
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED
4/28 19&#58;45&#58;34 JM&#58; Writing state file
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_CLOSE_OUTPUT
4/28 19&#58;45&#58;34 JM&#58; in globus_gram_job_manager_history_file_create()
4/28 19&#58;45&#58;34 JM&#58; NOT empty client callback list.
4/28 19&#58;45&#58;34 JM&#58; sending callback of status 4 (failure code 111) to <a href='https&#58;//stargrid03.rcf.bnl.gov&#58;20000/.' target='_blank' rel='nofollow'>https&#58;//stargrid03.rcf.bnl.gov&#58;20000/.</a>
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE_COMMITTED
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_FILE_CLEAN_UP
4/28 19&#58;45&#58;34 JMI&#58; testing job manager scripts for type sge exist and permissions are ok.
4/28 19&#58;45&#58;34 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;45&#58;34 JMI&#58; in globus_gram_job_manager_rm_scratchdir()
4/28 19&#58;45&#58;34 JMI&#58; cmd = remove_scratchdir
4/28 19&#58;45&#58;34 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_SCRATCH_CLEAN_UP
4/28 19&#58;45&#58;34 JMI&#58; testing job manager scripts for type sge exist and permissions are ok.
4/28 19&#58;45&#58;34 JMI&#58; completed script validation&#58; job manager type is sge.
4/28 19&#58;45&#58;34 JMI&#58; cmd = cache_cleanup
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; New Perl JobManager created.
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Using jm supplied job dir&#58; /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; cache_cleanup(enter)
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Cleaning files in job dir /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; Removed 5 files from /home/lbhajdu/.globus/job/rhic23.physics.wayne.edu/10212.1209425731
Mon Apr 28 19&#58;45&#58;36 2008 JM_SCRIPT&#58; cache_cleanup(exit)
4/28 19&#58;45&#58;36 Job Manager State Machine (entering)&#58; GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_CACHE_CLEAN_UP
4/28 19&#58;45&#58;36 JM&#58; in globus_gram_job_manager_reporting_file_remove()
4/28 19&#58;45&#58;36 JM&#58; exiting globus_gram_job_manager.

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_490212106').click(function() {
            $('#detail_490212106').slideDown("normal");
            $('#show_490212106').hide();
            $('#hide_490212106').show();
        });
        $('#hide_490212106').click(function() {
            $('#detail_490212106').slideUp();
            $('#hide_490212106').hide();
            $('#show_490212106').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='4867#1209029828'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-04-24T09:37:08+00:00">Apr 24, 2008 09:37 AM UTC</time> by <b>Rob Quick</b><a class="anchor" name="1209029828">&nbsp;</a></div><pre>OSG Admins,

Elisabeth at Wayne state is having problems with SGE job submitted to
Wayne State. The description of her problem is below. If anyone
familiar with SGE can offer any advise, we would greatly appreciate it.

Thanks,
Rob

Description of Problem&#58;
A user reports that a significant number of jobs submitted to our SGE
job manager through our gatekeeper receive a fatal gridmanager error.
No record of the submission is logged anywhere on our gatekeeper, nor
do these jobs ever show up in the SGE queue as either pending or
running. The occurrences seem to be more frequent when the SGE queue
is full, but some submitted jobs are held in the queue and run later
on. The user reports the following error&#58;

018 (2713.000.000) 03/30 02&#58;13&#58;21 Globus job submission failed!
Reason&#58; 111 the job manager timed out while waiting for a commit
signal
...
012 (2713.000.000) 03/30 02&#58;13&#58;26 Job was held.
Unspecified gridmanager error
Code 0 Subcode 0

Because there is no logged info here pertaining to these events, I&#39;m
at a loss as to which software component is actually generating this
error. I would greatly appreciate any help in tracking this problem
down.</pre></div><div class='update_description'><i onclick="document.location='4867#1208959269'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-04-23T14:01:09+00:00">Apr 23, 2008 02:01 PM UTC</time> by <b>Elisabeth Atems</b><a class="anchor" name="1208959269">&nbsp;</a></div><pre>Yes, I&#39;ve checked gatekeeper logs, condor logs, and SGE logs. There
are no errors logged anywhere. Apparently SGE never sees the jobs, as
it assigns job numbers in sequence and nearly all jobs assigned a
number finish normally. In other words there are no &#34;gaps&#34; in SGE job
numbers. I wouldn&#39;t know about the problem except user tells me about it.

Thanks,
Liz

Open Science Grid FootPrints writes&#58;
<font color='#7F7E6F'>&#62;[Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='4867#1208947102'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-04-23T10:38:22+00:00">Apr 23, 2008 10:38 AM UTC</time> by <b>Rob Quick</b><a class="anchor" name="1208947102">&nbsp;</a></div><pre>Elisabeth,

Sorry for my late response to this ticket. I have been traveling. SGE is not widely
used in OSG and I am completely unfamiliar with it. However, have you looked at the
globus-gatekeeper logs? I think it is located at $VDT_LOCATION/globus/var/logs. This
might give you some hints. Let me know if you see anything, I will hunt down an OSG
SGE admin if you are still seeing issues.

Thanks,
Rob</pre></div><div class='update_description'><i onclick="document.location='4867#1207853104'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-04-10T18:45:04+00:00">Apr 10, 2008 06:45 PM UTC</time> by <b>echism</b><a class="anchor" name="1207853104">&nbsp;</a></div><pre>Description of Problem&#58;
A user reports that a significant number of jobs submitted to our SGE job manager
through our gatekeeper receive a fatal gridmanager error. No record of the submission
is logged anywhere on our gatekeeper, nor do these jobs ever show up in the SGE queue
as either pending or running. The occurrences seem to be more frequent when the SGE
queue is full, but some submitted jobs are held in the queue and run later on. The
user reports the following error&#58;

018 (2713.000.000) 03/30 02&#58;13&#58;21 Globus job submission failed!
Reason&#58; 111 the job manager timed out while waiting for a commit
signal
...
012 (2713.000.000) 03/30 02&#58;13&#58;26 Job was held.
Unspecified gridmanager error
Code 0 Subcode 0

Because there is no logged info here pertaining to these events, I&#39;m at a loss as to
which software component is actually generating this error. I would greatly
appreciate any help in tracking this problem down.</pre></div><div class='update_description'><i onclick="document.location='4867#1207852268'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2008-04-10T18:31:08+00:00">Apr 10, 2008 06:31 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1207852268">&nbsp;</a></div><pre>Queues = Grid

Submitter Name&#58; Elisabeth Atems

Application System&#58; atems@....

Your Resource/Service Name&#58;
STAR-WSU

Your VO Support Center&#58;
STAR

Your Phone Number&#58;
313-577-1203

Description of Problem&#58;
A user reports that a significant number of jobs submitted to our SGE job manager through our gatekeeper receive a fatal gridmanager error. No record of the submission is logged anywhere on our gatekeeper, nor do these jobs ever show up in the SGE queue as either pending or running. The occurrences seem to be more frequent when the SGE queue is full, but some submitted jobs are held in the queue and run later on. The user reports the following error&#58;

018 (2713.000.000) 03/30 02&#58;13&#58;21 Globus job submission failed!
Reason&#58; 111 the job manager timed out while waiting for a commit
signal
...
012 (2713.000.000) 03/30 02&#58;13&#58;26 Job was held.
Unspecified gridmanager error
Code 0 Subcode 0

Because there is no logged info here pertaining to these events, I&#39;m at a loss as to which software component is actually generating this error. I would greatly appreciate any help in tracking this problem down.</pre></div><legend>Similar Recent Tickets <small>modified within the last 30 days</small></legend><div id="similar_tickets"><p class="muted">No similar tickets found.</p></div>
</div>
<script type="text/javascript">
function reset_anchor() {
    $("#updates .selected").removeClass("selected");
    var urls = document.location.toString().split('#'); 
    var anchor = urls[1];
    if(anchor) {
        $("a[name='"+anchor+"']").parents(".update_description").addClass("selected");
    }
}
function submitspam(ticket_id) {
    myret = confirm("Would you like to close this ticket as a security ticket, and submit the ticket content to akismet?");
    if(myret == true) {
        $.ajax("viewer/processspam?id="+ticket_id).done(function() {
            window.location.reload();
        });
    }
}

$(function() {
    reset_anchor();
    var ADDITIONAL_COOKIE_NAME = 'gocticket';
    var options = { path: '/', expires: 365};

    if(window.opener && window.opener.name == "gocticket_list") {
        v = $.cookie("closewindow");
        if(!v) {
            $("#closewindow").attr("checked", "checked"); //on by default
        } else {
            if(v == "checked") {
                $("#closewindow").attr("checked", "checked");
            }
        }
        $("#closewindow").click(function() {
            $.cookie("closewindow", $(this).attr('checked'), options);
        });
    } else {
        $("#closewindow_area").hide();
    }
    function updateTimeago() {
        $("time").timeago();
        setTimeout(updateTimeago, 30*1000);
    }
    updateTimeago();
    $(".description").focus(expand_description);
});
</script>
<hr/>
<footer>
<p>GOC Ticket Version 2.2 | <a href="https://ticket.opensciencegrid.org/goc/submit?app_issue_check=on&amp;app_issue_type=goc&amp;app_goc_url=https%3A%2F%2Fticket.opensciencegrid.org%3A443%2F4867">Report Bugs</a>
 | <a href="https://github.com/opensciencegrid/operations/blob/master/docs/privacy.md">Privacy Policy</a>
</p>

<p> <img align="top" src="images/tag_orange.png"/> Copyright 2018 The Trustees of Indiana University - Developed for Open Science Grid</p>
</footer>


</div><!--container-fluid-->
<script src="https://ticket1.grid.iu.edu:8443/socket.io/socket.io.js"></script>
<script>
var chat = io.connect('https://ticket1.grid.iu.edu:8443');
chat.on('connect', function() {
    chat.emit('authenticate', {nodekey:'', ticketid: 4867});
});
chat.on('peers', function(peers) {
    $("#peers").html("");
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('peer_disconnect', function(pid) {
    $("#peer_"+pid).hide("slow");
});
chat.on('peer_connected', function(peers) {
    //expect only 1 peer connecting, but..
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('submit', function() {
    if(confirm("This ticket was updated. Do you want to refresh?")) {
        history.go(0);
    }
});

function addPeer(pid, peer) {
    var ipinfo = "";
    if(peer.ip != undefined) {
        ipinfo = "<span class=\"ip\">"+peer.ip+"</span>";
    }
    if(chat.io.engine.id == pid) {
        //don't display myself
        return;
    }
    var html = "<li class=\"new\" id=\"peer_"+pid+"\" class=\"peer\">"+peer.name+ipinfo+"</li>";
    $("#peers").prepend(html);
    $("#peers .new").animate({bottom: 0}, 1000, function() {$(this).removeClass("new")});
}

$(function() {
    $("#ticket_form").submit(function() {
        chat.emit('submit');
        return true;
    });
});
</script>
<script>
//used by searchbox
function parseValue(value) {
    var obj = new Object();
    var tokens = value.split("\t");
    obj.str = tokens[0];
    obj.count = tokens[1];
    return obj;
}

$(function() {
    //bootstrap-2.0.4 stuff
    $(".alert-message").alert();
    $(".dropdown-toggle").dropdown();
    $("span[rel='tooltip']").tooltip();
    $("a[rel=popover]").popover();

    //activate menu that user is currently on
    $("#menu_navigator").addClass("active"); 
    $("#submenu_").addClass("active"); 

    //translate zend validation error message to bootstrap
    $(".errors").addClass("alert").addClass("alert-error");

    //enable autocomplete for search box
    $("#search").autocomplete({
        source: function( request, response ) {
            $.ajax({
                url: "search/autocomplete",
                dataType: "text",
                data: {
                    //featureClass: "P",
                    //style: "full",
                    //maxRows: 12,
                    //name_startsWith: request.term
                    q: request.term
                },
                success: function( data ) {
                    response( $.map( data.split("\n"), function( item ) {
                        if(item == "") return null;
                        return {
                            value: item
                        }
                    }));
                }
            });
        },
        select: function(event, ui) {
            document.location = "search?q="+ui.item.value;
        }
    });
    
});
</script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-69012-13");
pageTracker._trackPageview();
} catch(err) {}
</script>

</body>
