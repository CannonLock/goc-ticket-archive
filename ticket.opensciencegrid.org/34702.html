<!DOCTYPE html>
<html lang="en">
  <head>
  <base href="">
    <title>[34702] HoldReason = &#34;Error parsing classad or job not found&#34;</title>    <meta charset="utf-8" />
    <meta name="verify-v1" content="na5IcAJsZVOfEkboRxuIiZ1zpZgnZiWra+nKcS7nA/o=" />
    <meta name="google-site-verification" content="DLrk3ft4s8b-S2TloLCL2LD_t6wcTjgSluf5pmiu2kA" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <link href="https://ticket.opensciencegrid.org/rss" rel="alternate" type="application/rss+xml" title="GOC Ticket Update feed" />

    <style type="text/css">
      body {
        padding-top: 50px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
     #search {
            width: 300px;
     }

    </style>

<script src="https://code.jquery.com/jquery-3.0.0.js"></script>
<script src="https://code.jquery.com/jquery-migrate-3.0.1.js"></script>

   <link href="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet"/>
    <script src="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"/>
    <link href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.min.css" rel="stylesheet"/>
 <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>


    <link href="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/css/select2.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/js/select2.min.js"></script>

    <link href="css/ticket.css" rel="stylesheet" />
    <script src="lib/jquery.cookie.js"></script>

    <link href="images/tag_orange.png" rel="icon" type="image/png"/>
    <script src='https://www.google.com/recaptcha/api.js'></script>
  </head>

  <body>
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
            <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </a>

            <a class="brand" style="padding: 6px 0px 0px 6px;" href="http://opensciencegrid.org"><img src="images/osglogo.40x30.png"/></a>
            <ul class="nav">
                <li class="dropdown"><a href="https://ticket.opensciencegrid.org/#" class="dropdown-toggle" data-toggle="dropdown">GOC Ticket <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    <li><a href="https://my.opensciencegrid.org">MyOSG</a></li>
                    <li><a href="https://oim.opensciencegrid.org">OIM</a></li>
                    <li class="active"><a href="https://ticket.opensciencegrid.org/index">Ticket</a></li>
	<li class="divider"></li>
	<li><a href="http://repo.grid.iu.edu">Repo</a></li>
	<li class="divider"></li>
	<li><a href="http://blogs.grid.iu.edu">Blog</a></li>
                    <li><a href="http://display.grid.iu.edu">Display</a></li>
                    <li><a href="http://osggoc.blogspot.com/">News</a></li>
                    </ul>
                </li>
            </ul>
            <ul class="nav pull-right">
                <li><a href="https://ticket.opensciencegrid.org/sso/">Login</a></li>            </ul>

            <div class="nav-collapse">
                <ul class="nav">
			 <li id="menu_submit"><a href="https://ticket.opensciencegrid.org/submit">Submit</a></li><li id="menu_view" class="dropdown"><a href="https://ticket.opensciencegrid.org/\#" class="dropdown-toggle" data-toggle="dropdown">View <b class="caret"></b></a><ul class="dropdown-menu"><li id="submenu_listopen"><a href="https://ticket.opensciencegrid.org/list/open">Open Tickets</a></li><li id="submenu_listrecentclose"><a href="https://ticket.opensciencegrid.org/list/recentclose">Recently Closed Tickets</a></li><li class="divider"></li><li id="submenu_alltickets"><a href="https://ticket.opensciencegrid.org/search?q=&amp;sort=id">All Tickets</a></li></ul></li>                </ul>

                <form class="navbar-search pull-right" action="https://ticket.opensciencegrid.org/viewer">
                    <input id="search" type="text" name="id" class="search-query span2" placeholder="Search Ticket" value=""/>
                </form>
            </div>
        </div>
      </div>
    </div>

<script type='text/javascript' src='lib/jquery.timeago.js'></script>
<script type='text/javascript' src='lib/byte2size.js'></script>
<style>
#updates .toolbar {
position: relative;
margin-top: 0px;
top: -10px;
font-weight: normal;
}
#updates a.anchor {
position: relative;
top: -50px;
}
#updates .selected pre {
animation:selected 2s;
animation-iteration-count: 2;
animation-direction: alternate;
-webkit-animation:selected 2s; 
-webkit-animation-iteration-count: 2;
-webkit-animation-direction: alternate;
box-shadow: inset 1px 1px 20px #9ad;
border: 1px solid #9ab;
margin: 5px 0px;
padding-left: 10px;
}
@keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ab;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
@-webkit-keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ad;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
#updates pre {
background-color: inherit;
line-height: 15px;
padding: 5px;
}
#updates .header {
color: #999;
}
#updates .update_history pre {
background-color: #eee;
color: #666;
font-size: 85%;
}
#updates .clickable {
cursor: pointer;
}
#updates .clickable:hover {
color: #D98719;
}
#updates .meta_information pre {
background-color: #fed;
}
#similar_tickets {
max-height: 300px;
overflow-y: auto;
pointer-events: none;
padding: 5px;
background-color: #f4f4f4;
}
.btn-toolbar {
margin-bottom: 0;
height: 30px;
}
#peers {
position: fixed;
bottom: 0px;
right: 0px;
z-index: 100;
list-style: none;
padding: 5px 0px 0px 5px;
margin: 0px;
background-color: white;
box-shadow: 0px 0px 10px white;
}
#peers li {
background-color: #ccc;
color: #000;
display: inline-block;
padding: 5px 10px;
margin-right: 5px;
position: relative;
}
/*
#peers li:hover {
background-color: #999;
cursor: pointer;
}
*/
#peers span.ip {
padding-left: 5px;
color: #666;
}
#peers .new {
bottom: -30px;
}
/*
#peers .me {
background-color: red;
}
*/
</style>

<div class="container-fluid">
<ul id="peers"></ul>
<div class="alert alert-danger"><a class="close" href="https://ticket.opensciencegrid.org/#" data-dismiss="alert">&times;</a>By the end of May 2018, the ticketing system at https://ticket.opensciencegrid.org will be retired and support will be provided at https://support.opensciencegrid.org. Throughout this transition the support email (help@opensciencegrid.org) will be available as a point of contact.<br><br>                                                   
                                                                                                                                                                                   
Please see the service migration page for details: https://opensciencegrid.github.io/technology/policy/service-migrations-spring-2018/#ticket</div><div id="presence" class="pull-right"></div><div class="ticketgui"><script type="text/javascript" src="lib/checktab.js"></script>

<script>
var expanded = false;
function expand_description() {
    var desc = $(".description");
    if(!expanded) {
        expanded = true;
        //expand to minheight
        var min = 250;
        if(desc.height() < min) {
            desc.animate({height: min}, 200);
        }
    }
}

$(document).ready(function() {
    $("input[name='nad']").datepicker({
        dateFormat: 'yy-mm-dd'
    });
});

</script>



<style>
.form-horizontal .control-label {
padding-top: inherit;
font-size:90%;
color:#666;
}
label {
margin-bottom: 0px;
}
.controls {
line-height: 18px;
}
</style>
<form class="form-horizontal" action="https://ticket.opensciencegrid.org/viewer/updatebasic?id=34702" method="post">
<div class="page-header">
    <h3><span class="muted">34702</span> / HoldReason = &#34;Error parsing classad or job not found&#34;</h3>
</div>

<div class="row-fluid">
<div class="span5">
    <legend>Contact</legend>
    <div class="control-group">
        <label class="control-label">Full Name</label>
        <div class="controls">Evgeniy Kuznetsov</div>
    </div>
    <div class="control-group">
        <label class="control-label">Email</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">Phone</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">CC</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>

    <legend>Details</legend>
    <div class="control-group"><label class="control-label">Submitted Via</label><div class="controls">GOC Ticket/submit</div></div><div class="control-group"><label class="control-label">Submitter</label><div class="controls">Evgeniy Kuznetsov</div></div><div class="control-group"><label class="control-label">Ticket Links</label><div class="controls"></div></div>
    <div class="control-group">
        <label class="control-label">Ticket Type</label>
        <div class="controls">Problem/Request</div>
    </div>
    <div class="control-group">
        <label class="control-label">Priority</label>
        <div class="controls">Normal</div>
    </div>
    <div class="control-group">
        <label class="control-label">Status</label>
        <div class="controls">
Closed</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action</label>
        <div class="controls">ENG Action</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action Deadline</label>
        <div class="controls flag_red">2017-10-27</div>
    </div>

</div><!--span-->
<div class="span7">
    <legend>Assignees</legend>
    <div class="assignee" style="width: 60%">Software Support (Triage) <span class="muted"> / OSG Software Team</span></div><div class="assignee" style="width: 60%">Brian Lin <span class="muted"> / OSG Software Team</span></div><div class="assignee" style="width: 60%">Derek Weitzel <span class="muted"> / OSG Software Team</span></div>    <br>

    <legend>Assignees</legend>
    TODO
    <br>

    <style>
legend.noborder {
border-bottom: none;
}
</style>

<div id="attachment-list"/>
<script>
$(function () {
    var first = true;
    $.getJSON("attachment/list/34702", function (files) {
        //console.dir(files);
        var html = "<table class=\"table table-condensed\">";
        $(files).each(function() {
            if(first) {
                first = false;
                html += "<legend class=\"noborder\">Attachmenets</legend>";
            }
            html += "<tr class=\"attachment\">";
            html += "<td><img src="\&quot;&quot;+this.thumbnail_url+&quot;\&quot;/"></td>";
            html += "<td><a href="\&quot;&quot;+this.url+&quot;\&quot;" target=\"_blank\">"+this.name+"</a></td>";
            html += "<td>"+bytesToSize(this.size, 1)+"</td>";
            html += "</tr>";
        });
        html += "</table>";
        $("#attachment-list").html(html);
    });
});

function download(url) {
    window.open(url, "_blank");
}
</script>


</div><!--span-->
</div><!--row-fluid-->


</form>

</div>
<div id="updates" style="clear: both;">
    <legend>Past Updates
    <div class="btn-toolbar pull-right toolbar">
        <div class="btn-group">
                <a class="btn btn-small" href="https://ticket.opensciencegrid.org/34702?sort=up&amp;"><i class="icon-arrow-up"></i> Sort</a>

        
        <a class="btn btn-small" href="https://ticket.opensciencegrid.org/34702?expandall=true&amp;">Expand Descriptions</a>        <a class="btn btn-small" target="_blank" href="mailto:osg@tick.globalnoc.iu.edu?subject=Open%20Science%20Grid%3A%20HoldReason%20%3D%20%26%2334%3BError%20parsing%20classad%20or%20job%20not%20found%26%2334%3B%20ISSUE%3D34702%20PROJ%3D71"><i class="icon-envelope"></i> Update w/Email</a>
        </div>
    </div><!--btn-toolbar-->
    </legend>

    <div class='update_description'><i onclick="document.location='34702#1509027433'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-26T14:17:13+00:00">Oct 26, 2017 02:17 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1509027433">&nbsp;</a></div><pre>Evgeniy,

Great to hear! I&#39;m sorry that it took so long to get to the bottom of this issue. The fix I gave you will be included in the November release of the OSG stack. Please let us know if you have any more issues.

- Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1509020520'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-26T12:22:00+00:00">Oct 26, 2017 12:22 PM UTC</time><a class="anchor" name="1509020520">&nbsp;</a></div><pre>Brian,
I&#39;ve got correctly the file blah.py and it&#39;s DONE !

$ condor_ce_trace --debug osgce.jinr.ru
...
Job status&#58; Idle
Querying job status (157/600)
10/26/17 09&#58;04&#58;30 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 1682_7d39_3
Job status&#58; Idle
Querying job status (158/600)
10/26/17 09&#58;04&#58;32 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 1682_7d39_3
Job status&#58; Completed

***** Job output *****
MKLROOT=/opt/intel/composer_xe_2013.5.192/mkl
MANPATH=&#58;/opt/openmpi/1.6.5/man&#58;/opt/intel/composer_xe_2013.5.192/man/en_US&#58;/opt/glite/share/man
HOSTNAME=wn214.jinr.ru
PBS_VERSION=TORQUE-4.2.10
INTEL_LICENSE_FILE=/opt/intel/licenses
IPPROOT=/opt/intel/composer_xe_2013.5.192/ipp
VO_ALICE_SW_DIR=/cvmfs/alice.cern.ch
VO_STAR_DEFAULT_SE=lxse-dc01.jinr.ru
GRID_ENV_LOCATION=/usr/libexec
SHELL=/bin/bash
VO_FERMILAB_DEFAULT_SE=lxse-dc01.jinr.ru
...
so the job is finished.

bash-4.1$ /usr/libexec/blahp/pbs_status.py pbs/20171026/13059887
0[BatchJobId=&#34;13059887&#34;; JobStatus=1; ]

Also there are some test jobs from VO star that HTCondor-CE considers as completed.

This ticket can be closed.

<div id='show_1496599559' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1496599559'>THANK YOU very much !

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_1496599559').click(function() {
            $('#detail_1496599559').slideDown("normal");
            $('#show_1496599559').hide();
            $('#hide_1496599559').show();
        });
        $('#hide_1496599559').click(function() {
            $('#detail_1496599559').slideUp();
            $('#hide_1496599559').hide();
            $('#show_1496599559').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1508942150'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-25T14:35:50+00:00">Oct 25, 2017 02:35 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1508942150">&nbsp;</a></div><pre>Evgeniy,

Did you mean that you updated blah.py? It looks like you did and may have wget&#39;ed the link I provided, which contains html and may explain the error. Instead, you can grab the raw file here&#58; <a href='https&#58;//raw.githubusercontent.com/brianhlin/BLAH/fix_unicode_str_err/src/scripts/blah.py' target='_blank' rel='nofollow'>https&#58;//raw.githubusercontent.com/brianhlin/BLAH/fix_unicode_str_err/src/scripts/blah.py</a>

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1508941765'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-25T14:29:25+00:00">Oct 25, 2017 02:29 PM UTC</time><a class="anchor" name="1508941765">&nbsp;</a></div><pre>Brian,
I&#39;ve replaced the file /usr/libexec/blahp/pbs_status.py with the one you provided.
I chmoded it from 644 to 755 as my original file was.

Here is the output

bash-4.1$ /usr/libexec/blahp/pbs_status.py pbs/20171025/13045312
Traceback (most recent call last)&#58;
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 48, in &#60;module&#62;
import blah
File &#34;/usr/libexec/blahp/blah.py&#34;, line 7
&#60;!DOCTYPE html&#62;
^
SyntaxError&#58; invalid syntax

Also I&#39;ve attached my /etc/blah.config as blah_config.txt since blah.config format is not allowed.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1508857941'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-24T15:12:21+00:00">Oct 24, 2017 03:12 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1508857941">&nbsp;</a></div><pre>Evgeniy,

I think I found the bug. Please replace /usr/libexec/blah/blah.py with this file&#58; <a href='https&#58;//github.com/brianhlin/BLAH/blob/fix_unicode_str_err/src/scripts/blah.py' target='_blank' rel='nofollow'>https&#58;//github.com/brianhlin/BLAH/blob/fix_unicode_str_err/src/scripts/blah.py</a>

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1508854962'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-24T14:22:42+00:00">Oct 24, 2017 02:22 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1508854962">&nbsp;</a></div><pre>Evgeniy,

Perfect, that&#39;s exactly what we needed. Could you upload your blah.config? There seems to be something about that file that StringIO is choking on.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1508775232'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-23T16:13:52+00:00">Oct 23, 2017 04:13 PM UTC</time><a class="anchor" name="1508775232">&nbsp;</a></div><pre>Brian,

Can you look at this?

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020</pre></div><div class='update_description'><i onclick="document.location='34702#1508751871'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-23T09:44:31+00:00">Oct 23, 2017 09:44 AM UTC</time><a class="anchor" name="1508751871">&nbsp;</a></div><pre>Brian,
Oh, sorry, I missed touching pbs_status.debug file.
I fixed it and re-ran these tests.
This time the file /var/tmp/qstat_cache_star001/pbs_status.log has got new records, and it&#39;s size is doubled ! I&#39;ve attached the last pbs_status.log

...
08/28/17 16&#58;06&#58;49 51501 Starting qstat.
10/23/17 11&#58;58&#58;41 164098 Traceback (most recent call last)&#58;
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 564, in &#60;module&#62;
sys.exit(main())
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 531, in main
&#39;pbs_binpath&#39;&#58; &#39;/usr/bin&#39;})
File &#34;/usr/libexec/blahp/blah.py&#34;, line 15, in __init__
vfile = StringIO(u&#39;[%s]&#92;n%s&#39; % (self.header, config))
UnicodeDecodeError&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)

10/23/17 11&#58;59&#58;07 164142 Traceback (most recent call last)&#58;
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 564, in &#60;module&#62;
sys.exit(main())
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 531, in main
&#39;pbs_binpath&#39;&#58; &#39;/usr/bin&#39;})
File &#34;/usr/libexec/blahp/blah.py&#34;, line 15, in __init__
vfile = StringIO(u&#39;[%s]&#92;n%s&#39; % (self.header, config))
UnicodeDecodeError&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)

10/23/17 12&#58;00&#58;07 164223 Traceback (most recent call last)&#58;
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 564, in &#60;module&#62;
sys.exit(main())
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 531, in main
&#39;pbs_binpath&#39;&#58; &#39;/usr/bin&#39;})
File &#34;/usr/libexec/blahp/blah.py&#34;, line 15, in __init__
vfile = StringIO(u&#39;[%s]&#92;n%s&#39; % (self.header, config))
UnicodeDecodeError&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)

10/23/17 12&#58;01&#58;07 164305 Traceback (most recent call last)&#58;
<div id='show_2070060191' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_2070060191'>File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 564, in &#60;module&#62;
sys.exit(main())
File &#34;/usr/libexec/blahp/pbs_status.py&#34;, line 531, in main
&#39;pbs_binpath&#39;&#58; &#39;/usr/bin&#39;})
File &#34;/usr/libexec/blahp/blah.py&#34;, line 15, in __init__
vfile = StringIO(u&#39;[%s]&#92;n%s&#39; % (self.header, config))
UnicodeDecodeError&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)
...

And here is the output of pbs_status.py&#58;

bash-4.1$ /usr/libexec/blahp/pbs_status.py pbs/20171023/12988670
1ERROR&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)
bash-4.1$ qstat -f -1 12988670
Job Id&#58; 12988670.lxbsrv01.jinr.ru
Job_Name = bl_4df2bf861282
Job_Owner = star001@....
job_state = Q
queue = star
server = lxbsrv01.jinr.ru
Checkpoint = u
ctime = Mon Oct 23 11&#58;58&#58;04 2017
Error_Path = osgce.jinr.ru&#58;/dev/null
Hold_Types = n
Join_Path = n
Keep_Files = n
Mail_Points = n
mtime = Mon Oct 23 11&#58;58&#58;04 2017
Output_Path = osgce.jinr.ru&#58;/dev/null
Priority = 0
qtime = Mon Oct 23 11&#58;58&#58;04 2017
Rerunable = True
Resource_List.cput = 168&#58;00&#58;00
Resource_List.mem = 2000mb
Resource_List.ncpus = 1
Resource_List.nice = 10
Resource_List.nodect = 1
Resource_List.nodes = 1&#58;ppn=1
Resource_List.pmem = 2000mb
Resource_List.walltime = 172&#58;00&#58;00
Shell_Path_List = /bin/bash
Variable_List = PBS_O_QUEUE=star,PBS_O_HOME=/home/star001,PBS_O_PATH=/sbin&#58;/usr/sbin&#58;/bin&#58;/usr/bin,PBS_O_LANG=en_US,PBS_O_WORKDIR=/opt/exp_soft/star/spool/7427/0/cluster57427.proc0.subproc0,PBS_O_HOST=osgce.jinr.ru,PBS_O_SERVER=lxbsrv01
euser = star001
egroup = lstar
queue_type = E
etime = Mon Oct 23 11&#58;58&#58;04 2017
submit_args = /tmp/condor_g_scratch.0x7f5a0f8478e0.1775/bl_4df2bf861282
fault_tolerant = False
job_radix = 0
submit_host = osgce.jinr.ru

bash-4.1$ whoami
star001

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_2070060191').click(function() {
            $('#detail_2070060191').slideDown("normal");
            $('#show_2070060191').hide();
            $('#hide_2070060191').show();
        });
        $('#hide_2070060191').click(function() {
            $('#detail_2070060191').slideUp();
            $('#hide_2070060191').hide();
            $('#show_2070060191').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1508521053'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-20T17:37:33+00:00">Oct 20, 2017 05:37 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1508521053">&nbsp;</a></div><pre>Looking at a previous comment it looks like you&#39;re missing the /var/tmp/qstat_cache_star001/pbs_status.debug file. Could you touch it and re-run your tests?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1508491197'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-20T09:19:57+00:00">Oct 20, 2017 09:19 AM UTC</time><a class="anchor" name="1508491197">&nbsp;</a></div><pre>Brian,
Yes, I was ranning the pbs_status.py command as the root user.
This time I ran the pbs_status.py command as the star001 user.

osgce&#58;/home/star001 # su star001
bash-4.1$ /usr/libexec/blahp/pbs_status.py pbs/20171020/12904294
1ERROR&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)
bash-4.1$ whoami
star001

And the pbs_status.log didn&#39;t change, i.e. there is no new records.

osgce&#58;~ # tail -f /var/tmp/qstat_cache_star001/pbs_status.log
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;05&#58;49 51492 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;05&#58;49 51492 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 Checking cache for jobid 10026122
08/28/17 16&#58;06&#58;49 51501 Starting query to fill cache.
08/28/17 16&#58;06&#58;49 51501 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;06&#58;49 51501 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;06&#58;49 51501 Starting qstat.

And here is the qstat output

bash-4.1$ qstat -f -1 12904294
Job Id&#58; 12904294.lxbsrv01.jinr.ru
Job_Name = bl_b3a73c8b3d50
Job_Owner = star001@....
resources_used.cput = 00&#58;00&#58;00
resources_used.mem = 0kb
resources_used.vmem = 0kb
resources_used.walltime = 00&#58;00&#58;00
job_state = C
<div id='show_210003871' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_210003871'>queue = star
server = lxbsrv01.jinr.ru
Checkpoint = u
ctime = Fri Oct 20 12&#58;11&#58;37 2017
Error_Path = osgce.jinr.ru&#58;/dev/null
exec_host = wn203.jinr.ru/0
exec_port = 15003
Hold_Types = n
Join_Path = n
Keep_Files = n
Mail_Points = n
mtime = Fri Oct 20 12&#58;11&#58;58 2017
Output_Path = osgce.jinr.ru&#58;/dev/null
Priority = 0
qtime = Fri Oct 20 12&#58;11&#58;37 2017
Rerunable = True
Resource_List.cput = 168&#58;00&#58;00
Resource_List.mem = 2000mb
Resource_List.ncpus = 1
Resource_List.nice = 10
Resource_List.nodect = 1
Resource_List.nodes = 1&#58;ppn=1
Resource_List.pmem = 2000mb
Resource_List.walltime = 172&#58;00&#58;00
session_id = 19837
Shell_Path_List = /bin/bash
Variable_List = PBS_O_QUEUE=star,PBS_O_HOME=/home/star001,PBS_O_PATH=/sbin&#58;/usr/sbin&#58;/bin&#58;/usr/bin,PBS_O_LANG=en_US,PBS_O_WORKDIR=/opt/exp_soft/star/spool/7161/0/cluster57161.proc0.subproc0,PBS_O_HOST=osgce.jinr.ru,PBS_O_SERVER=lxbsrv01
euser = star001
egroup = lstar
queue_type = E
etime = Fri Oct 20 12&#58;11&#58;37 2017
exit_status = 0
submit_args = /tmp/condor_g_scratch.0x7f5a0f62fb30.1775/bl_b3a73c8b3d50
start_time = Fri Oct 20 12&#58;11&#58;57 2017
start_count = 1
fault_tolerant = False
comp_time = Fri Oct 20 12&#58;11&#58;58 2017
job_radix = 0
total_runtime = 1.135861
submit_host = osgce.jinr.ru

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_210003871').click(function() {
            $('#detail_210003871').slideDown("normal");
            $('#show_210003871').hide();
            $('#hide_210003871').show();
        });
        $('#hide_210003871').click(function() {
            $('#detail_210003871').slideUp();
            $('#hide_210003871').hide();
            $('#show_210003871').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1508428680'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-19T15:58:00+00:00">Oct 19, 2017 03:58 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1508428680">&nbsp;</a></div><pre>Evgeniy,

It looks like you ran the pbs_status.py command as the root user, could you instead run it as a user whose jobs run as star001 and then upload the pbs_status.log file?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1508424628'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-19T14:50:28+00:00">Oct 19, 2017 02:50 PM UTC</time><a class="anchor" name="1508424628">&nbsp;</a></div><pre>Brian,
I&#39;ve attached current osg-profile.txt.
Here is the output about python packages installed

osgce&#58;~ # yum list installed | grep python
audit-libs-python.x86_64            2.4.5-6.el6            @sl6x/6.8
boost-python.x86_64                 1.41.0-28.el6          @sl6x/6.8
condor-python.x86_64                8.4.12-2.1.osg33.el6   @osg
dbus-python.x86_64                  0.83.0-6.1.el6         @sl6x/6.8
gfal2-python.x86_64                 1.9.3-1.el6            @epel
libproxy-python.x86_64              0.3.0-10.el6           @sl6x/6.8
libselinux-python.x86_64            2.0.94-7.el6           @sl6x/6.8
libsemanage-python.x86_64           2.0.43-5.1.el6         @sl6x/6.8
libxml2-python.x86_64               2.7.6-21.el6_8.1       @Scientific Linux update/6.8
newt-python.x86_64                  0.52.11-4.el6          @sl6x/6.8
policycoreutils-python.x86_64       2.0.83-30.1.el6_8      @sl6x-security/6.8
python.x86_64                       2.6.6-66.el6_8         @sl6x-security/6.8
python-argparse.noarch              1.2.1-2.1.el6          @Scientific Linux/6.8
python-decorator.noarch             3.0.1-3.1.el6          @sl6x/6.8
python-iniparse.noarch              0.3.1-2.1.el6          @Scientific Linux/6.8
python-libs.x86_64                  2.6.6-66.el6_8         @sl6x-security/6.8
python-pycurl.x86_64                7.19.0-9.el6           @Scientific Linux/6.8
python-simplejson.x86_64            2.0.9-3.1.el6          @sl6x/6.8
python-slip.noarch                  0.2.20-1.el6_2         @sl6x/6.8
python-slip-dbus.noarch             0.2.20-1.el6_2         @sl6x/6.8
python-urlgrabber.noarch            3.9.1-11.el6           @Scientific Linux/6.8
rpm-python.x86_64                   4.8.0-55.el6           @Scientific Linux/6.8
setools-libs-python.x86_64          3.3.7-4.el6            @sl6x/6.8

I replaced the original pbs_status.py(19639 bytes) with your variant (19692 bytes), chmod-ed it permissions as of original file and repeated previous test, here is the output

osgce&#58;~ # /usr/libexec/blahp/pbs_status.py pbs/20171019/12888949
1ERROR&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)

osgce&#58;~ # qstat -f -1 12888949
<div id='show_475698255' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_475698255'>Job Id&#58; 12888949.lxbsrv01.jinr.ru
Job_Name = bl_461b8bb11924
Job_Owner = star001@....
resources_used.cput = 00&#58;00&#58;00
resources_used.mem = 0kb
resources_used.vmem = 0kb
resources_used.walltime = 00&#58;00&#58;00
job_state = C
queue = star
server = lxbsrv01.jinr.ru
Checkpoint = u
ctime = Thu Oct 19 17&#58;13&#58;32 2017
Error_Path = osgce.jinr.ru&#58;/dev/null
exec_host = wn221.jinr.ru/3
exec_port = 15003
Hold_Types = n
Join_Path = n
Keep_Files = n
Mail_Points = n
mtime = Thu Oct 19 17&#58;14&#58;08 2017
Output_Path = osgce.jinr.ru&#58;/dev/null
Priority = 0
qtime = Thu Oct 19 17&#58;13&#58;32 2017
Rerunable = True
Resource_List.cput = 168&#58;00&#58;00
Resource_List.mem = 2000mb
Resource_List.ncpus = 1
Resource_List.nice = 10
Resource_List.nodect = 1
Resource_List.nodes = 1&#58;ppn=1
Resource_List.pmem = 2000mb
Resource_List.walltime = 172&#58;00&#58;00
session_id = 26928
Shell_Path_List = /bin/bash
euser = star001
egroup = lstar
queue_type = E
etime = Thu Oct 19 17&#58;13&#58;32 2017
exit_status = 0
submit_args = /tmp/condor_g_scratch.0x7f5a0f403700.1775/bl_461b8bb11924
start_time = Thu Oct 19 17&#58;14&#58;06 2017
start_count = 1
fault_tolerant = False
comp_time = Thu Oct 19 17&#58;14&#58;08 2017
job_radix = 0
total_runtime = 2.428434
submit_host = osgce.jinr.ru

While the file /var/tmp/qstat_cache_star001/pbs_status.log is not changed, I&#39;ve attached it yet.
osgce&#58;/var/tmp # ls -l
total 8
drwxr-xr-x 2 root    root  4096 Oct 16 15&#58;27 qstat_cache_root
drwxr-xr-x 2 star001 lstar 4096 Aug 28 13&#58;04 qstat_cache_star001

osgce&#58;/var/tmp/qstat_cache_star001 # ls -l
total 8
-rw-r--r-- 1 star001 lstar 6300 Aug 28 16&#58;06 pbs_status.log

So the file pbs_status.log and uplevel directory both are available for writing for user star001.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_475698255').click(function() {
            $('#detail_475698255').slideDown("normal");
            $('#show_475698255').hide();
            $('#hide_475698255').show();
        });
        $('#hide_475698255').click(function() {
            $('#detail_475698255').slideUp();
            $('#hide_475698255').hide();
            $('#show_475698255').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1508352779'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-18T18:52:59+00:00">Oct 18, 2017 06:52 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1508352779">&nbsp;</a></div><pre>Evgeniy,

I&#39;ve attached a new pbs_status.py. Could you try your previous pbs_status.py test with this version of the script and then attach /var/tmp/qstat_cache_star001/pbs_status.log?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1508352003'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-18T18:40:03+00:00">Oct 18, 2017 06:40 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1508352003">&nbsp;</a></div><pre>Evgeniy,

Could you post an updated osg-profile.txt? That last error may indicate that your system is using python3.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1508158084'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-16T12:48:04+00:00">Oct 16, 2017 12:48 PM UTC</time><a class="anchor" name="1508158084">&nbsp;</a></div><pre>Additionally, there is NO /var/tmp/qstat_cache_star001/blahp_results_cache.

osgce&#58;~ # ls -l  /var/tmp/qstat_cache_star001
total 8
-rw-r--r-- 1 star001 lstar 6300 Aug 28 16&#58;06 pbs_status.log

There is only pbs_status.log.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1508157797'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-16T12:43:17+00:00">Oct 16, 2017 12:43 PM UTC</time><a class="anchor" name="1508157797">&nbsp;</a></div><pre>Brian,

The PBS job id is 12780747, gotten from this line of Gridmanager log&#58;
GridJobId = &#34;batch pbs osgce.jinr.ru_9619_osgce.jinr.ru#56962.0#1508156719 pbs/20171016/12780747&#34;
and the outputs from the commands &#58;

osgce&#58;~ # /usr/libexec/blahp/pbs_status.py pbs/20171016/12780747
1ERROR&#58; &#39;ascii&#39; codec can&#39;t decode byte 0xb0 in position 3470&#58; ordinal not in range(128)

osgce&#58;~ # qstat -f -1 12780747

Job Id&#58; 12780747.lxbsrv01.jinr.ru
Job_Name = bl_aae55c39edde
Job_Owner = star001@....
resources_used.cput = 00&#58;00&#58;00
resources_used.mem = 0kb
resources_used.vmem = 0kb
resources_used.walltime = 00&#58;00&#58;00
job_state = C
queue = star
server = lxbsrv01.jinr.ru
Checkpoint = u
ctime = Mon Oct 16 15&#58;25&#58;28 2017
Error_Path = osgce.jinr.ru&#58;/dev/null
exec_host = wn269.jinr.ru/2
exec_port = 15003
Hold_Types = n
Join_Path = n
Keep_Files = n
Mail_Points = n
mtime = Mon Oct 16 15&#58;26&#58;42 2017
Output_Path = osgce.jinr.ru&#58;/dev/null
Priority = 0
qtime = Mon Oct 16 15&#58;25&#58;28 2017
Rerunable = True
<div id='show_224733893' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_224733893'>Resource_List.cput = 168&#58;00&#58;00
Resource_List.mem = 2000mb
Resource_List.ncpus = 1
Resource_List.nice = 10
Resource_List.nodect = 1
Resource_List.nodes = 1&#58;ppn=1
Resource_List.pmem = 2000mb
Resource_List.walltime = 172&#58;00&#58;00
session_id = 2977
Shell_Path_List = /bin/bash
euser = star001
egroup = lstar
queue_type = E
etime = Mon Oct 16 15&#58;25&#58;28 2017
exit_status = 0
submit_args = /tmp/condor_g_scratch.0x7f5a0f48c910.1775/bl_aae55c39edde
start_time = Mon Oct 16 15&#58;26&#58;41 2017
start_count = 1
fault_tolerant = False
comp_time = Mon Oct 16 15&#58;26&#58;42 2017
job_radix = 0
total_runtime = 1.587556
submit_host = osgce.jinr.ru

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_224733893').click(function() {
            $('#detail_224733893').slideDown("normal");
            $('#show_224733893').hide();
            $('#hide_224733893').show();
        });
        $('#hide_224733893').click(function() {
            $('#detail_224733893').slideUp();
            $('#hide_224733893').hide();
            $('#show_224733893').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1507911179'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-13T16:12:59+00:00">Oct 13, 2017 04:12 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1507911179">&nbsp;</a></div><pre>Additionally, there should be a /var/tmp/qstat_cache_star001/blahp_results_cache. Does that have contents?</pre></div><div class='update_description'><i onclick="document.location='34702#1507911017'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-13T16:10:17+00:00">Oct 13, 2017 04:10 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1507911017">&nbsp;</a></div><pre>Evgeniy,

Thanks for the Gridmanager log, that&#39;s helpful. Could you run the condor_ce_trace command again and retrieve the job ID of the PBS job that gets submitted as a result? Once you have that job ID, please run the following and attach the output&#58;

$ /usr/libexec/blahp/pbs_status.py pbs/20171013/&#60;PBS JOB ID&#62;
$ qstat -f -1 &#60;PBS JOB ID&#62;

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1507894593'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-13T11:36:33+00:00">Oct 13, 2017 11:36 AM UTC</time><a class="anchor" name="1507894593">&nbsp;</a></div><pre>Brian,
- the holdreason is still the same &#58; &#39;Error parsing classad or job not found&#39;.
-  Yes, there are errors in the Gridmanager log corresponding to  user star001.
Here is some output form GridmanagerLog.star001 &#58;
...
10/12/17 14&#58;22&#58;40 [32074] Using job type INFNBatch for job 56795.0
10/12/17 14&#58;22&#58;40 [32074] (56795.0) SetJobLeaseTimers()
10/12/17 14&#58;22&#58;40 [32074] Found job 56795.0 --- inserting
10/12/17 14&#58;22&#58;40 [32074] Fetched 1 new job ads from schedd
10/12/17 14&#58;22&#58;40 [32074] querying for removed/held jobs
10/12/17 14&#58;22&#58;40 [32074] Using constraint ((Owner=?=&#34;star001&#34;&&JobUniverse==9)) && ((Managed =!= &#34;ScheddDone&#34;)) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= &#34;External&#34;))
10/12/17 14&#58;22&#58;40 [32074] Fetched 0 job ads from schedd
10/12/17 14&#58;22&#58;40 [32074] leaving doContactSchedd()
10/12/17 14&#58;22&#58;40 [32074] gahp server not up yet, delaying ping
10/12/17 14&#58;22&#58;40 [32074] *** UpdateLeases called
10/12/17 14&#58;22&#58;40 [32074]     Leases not supported, cancelling timer
10/12/17 14&#58;22&#58;40 [32074] BaseResource&#58;&#58;UpdateResource&#58;
NumJobs = 1
HashName = &#34;batch PBS&#34;
Machine = &#34;osgce.jinr.ru&#34;
SubmitsAllowed = 0
Name = &#34;batch &#34;
CondorPlatform = &#34;$CondorPlatform&#58; X86_64-CentOS_6.9 $&#34;
RunningJobs = 0
Owner = &#34;star001&#34;
...
Also please note the record in GridmanagerLog about job id in pbs, which stands for 12633549
GridJobId = &#34;batch pbs osgce.jinr.ru_9619_osgce.jinr.ru#56795.0#1507807356 pbs/20171012/12633549&#34;

- NO, there is no corresponding error message in /var/tmp/qstat_cache_star001/pbs_status.log. The last record in pbs_status.log is still dated by &#34;08/28/17 16&#58;06&#58;49&#34;

I&#39;ve attached 3 files&#58;
- &#39;ce_trace_output.txt&#39;   the output of &#34;condor_ce_trace --debug osgce.jinr.ru&#34;
- &#39;GridmanagerLog.star001_56795.0.txt&#39;  - GridmanagerLog.star001 according to job id 56795.0.
- &#39;JobRouterLog_56794.0.txt&#39;  - JobRouterLog for job id 56794.0, where this job id transforms to job id 56795.0 while having the follow message &#58;
<div id='show_656604420' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_656604420'>
&#34;10/12/17 14&#58;22&#58;36 JobRouter (src=56794.0,dest=56795.0,route=Local_PBS_star)&#58; submitted job has not yet appeared in job queue mirror or was removed (submitted 0 seconds ago)&#34;

And finally the output from batch server for job 12633549 identified in GridmanagerLog&#58;

lxbsrv01&#58;/var/lib/torque/server_logs # less 20171012 | grep 12633549
10/12/2017 14&#58;22&#58;46;0100;PBS_Server.4322;Job;12633549.lxbsrv01.jinr.ru;enqueuing into star, state 1 hop 1
10/12/2017 14&#58;22&#58;46;0008;PBS_Server.4322;Job;req_commit;job_id&#58; 12633549.lxbsrv01.jinr.ru
10/12/2017 14&#58;22&#58;55;0008;PBS_Server.939;Job;12633549.lxbsrv01.jinr.ru;Job Run at request of root@....
10/12/2017 14&#58;22&#58;55;0008;PBS_Server.939;Job;12633549.lxbsrv01.jinr.ru;Not sending email&#58; job requested no e-mail
10/12/2017 14&#58;22&#58;56;0010;PBS_Server.31830;Job;12633549.lxbsrv01.jinr.ru;Exit_status=0 resources_used.cput=00&#58;00&#58;00 resources_used.mem=0kb resources_used.vmem=0kb resources_used.walltime=00&#58;00&#58;00
10/12/2017 14&#58;22&#58;56;0008;PBS_Server.31830;Job;12633549.lxbsrv01.jinr.ru;Not sending email&#58; job requested no e-mail
10/12/2017 14&#58;22&#58;56;0008;PBS_Server.931;Job;12633549.lxbsrv01.jinr.ru;on_job_exit valid pjob&#58; 12633549.lxbsrv01.jinr.ru (substate=50)
10/12/2017 14&#58;26&#58;03;0100;PBS_Server.31802;Job;12633549.lxbsrv01.jinr.ru;dequeuing from star, state COMPLETE

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_656604420').click(function() {
            $('#detail_656604420').slideDown("normal");
            $('#show_656604420').hide();
            $('#hide_656604420').show();
        });
        $('#hide_656604420').click(function() {
            $('#detail_656604420').slideUp();
            $('#hide_656604420').hide();
            $('#show_656604420').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1507739165'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-11T16:26:05+00:00">Oct 11, 2017 04:26 PM UTC</time> by <b>echism</b><a class="anchor" name="1507739165">&nbsp;</a></div><pre>Evgeniy,

Did you see that last bit?

Thanks</pre></div><div class='update_description'><i onclick="document.location='34702#1507563852'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-09T15:44:12+00:00">Oct 9, 2017 03:44 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1507563852">&nbsp;</a></div><pre>What&#39;s the hold reason? Are there corresponding errors in the Gridmanager log corresponding to your user? Is there a corresponding error message in /var/tmp/qstat_cache_star001/pbs_status.log?</pre></div><div class='update_description'><i onclick="document.location='34702#1507138350'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-04T17:32:30+00:00">Oct 4, 2017 05:32 PM UTC</time> by <b>echism</b><a class="anchor" name="1507138350">&nbsp;</a></div><pre>Hey Brian,

What now?

Thanks</pre></div><div class='update_description'><i onclick="document.location='34702#1506959752'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-02T15:55:52+00:00">Oct 2, 2017 03:55 PM UTC</time><a class="anchor" name="1506959752">&nbsp;</a></div><pre>Brian,

I&#39;ve been wait until the condor_ce_trace finished.
So the Job status&#58; Idle was until 327 job status query and become Held status at 328 query up to 600 query.

Final  output of condor_ce_trace --debug osgce.jinr.ru &#58;
...
Querying job status (598/600)
10/02/17 17&#58;49&#58;34 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Held
Querying job status (599/600)
10/02/17 17&#58;49&#58;35 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Held
Querying job status (600/600)
10/02/17 17&#58;49&#58;36 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Held

********************************************************************************
2017-10-02 17&#58;49&#58;37 Remote job, 56754.0, was held
********************************************************************************

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1506959129'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-10-02T15:45:29+00:00">Oct 2, 2017 03:45 PM UTC</time> by <b>echism</b><a class="anchor" name="1506959129">&nbsp;</a></div><pre>Evgeniy,

Did you see that last request?

Thanks</pre></div><div class='update_description'><i onclick="document.location='34702#1506614233'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-28T15:57:13+00:00">Sep 28, 2017 03:57 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1506614233">&nbsp;</a></div><pre>Evgeniy,

Please let the condor_ce_trace command run until completion. We need the
job to be picked up by the job router daemon, then the gridmanager
daemon before we can determine if your jobs are getting held for the
same reason.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1506613358'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-28T15:42:38+00:00">Sep 28, 2017 03:42 PM UTC</time><a class="anchor" name="1506613358">&nbsp;</a></div><pre>Brian,
The output of pbs_status.log didn&#39;t change, there is now new records since August 28th.
I cleaned  /usr/libexec/blahp/ of *.pyc *.pyo files.
Here is the output of the commands mentioned &#58;

osgce&#58;/usr/libexec/blahp # condor_ce_config_val -verbose BATCH_GAHP
BATCH_GAHP = /usr/bin/blahpd
# at&#58; /etc/condor-ce/condor_config, line 63
# raw&#58; BATCH_GAHP = $(BIN)/blahpd

osgce&#58;/usr/libexec/blahp # rpm -qf $(condor_ce_config_val BATCH_GAHP)
blahp-1.18.33.bosco-1.osg33.el6.x86_64

And finally
bash-4.1$ condor_ce_trace --debug osgce.jinr.ru
09/28/17 18&#58;31&#58;07 Result of reading /etc/issue&#58;  Scientific Linux release 6.9 (Carbon)

09/28/17 18&#58;31&#58;07 Using processor count&#58; 4 processors, 4 CPUs, 0 HTs
09/28/17 18&#58;31&#58;07 Enumerating interfaces&#58; lo 127.0.0.1 up
09/28/17 18&#58;31&#58;07 Enumerating interfaces&#58; eth0 159.93.226.250 up
09/28/17 18&#58;31&#58;07 Initializing Directory&#58; curr_dir = /usr/share/condor-ce/config.d
09/28/17 18&#58;31&#58;07 Initializing Directory&#58; curr_dir = /etc/condor-ce/config.d
Testing HTCondor-CE collector connectivity.
***** condor_ping output *****
Remote Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Local  Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Session ID&#58;                  osgce&#58;55048&#58;1506612667&#58;149
Instruction&#58;                 READ
Command&#58;                     60020
Encryption&#58;                  none
Integrity&#58;                   none
Authentication&#58;              none
Remote Mapping&#58;              unauthenticated@unmapped
Authorized&#58;                  TRUE

<div id='show_675454178' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_675454178'>********************
- Successful ping of collector on &#60;159.93.226.250&#58;9619&#62;.

09/28/17 18&#58;31&#58;07 Will use TCP to update collector osgce.jinr.ru &#60;159.93.226.250&#58;9619&#62;
09/28/17 18&#58;31&#58;07 Trying to query collector &#60;159.93.226.250&#58;9619&#62;
09/28/17 18&#58;31&#58;07 IPVERIFY&#58; checking osgce.jinr.ru against 159.93.226.250
09/28/17 18&#58;31&#58;07 IPVERIFY&#58; matched 159.93.226.250 to 159.93.226.250
09/28/17 18&#58;31&#58;07 IPVERIFY&#58; ip found is 1
Testing HTCondor-CE schedd connectivity.
***** condor_ping output *****
Remote Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Local  Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Session ID&#58;                  osgce&#58;55056&#58;1506612667&#58;985
Instruction&#58;                 WRITE
Command&#58;                     60021
Encryption&#58;                  none
Integrity&#58;                   MD5
Authenticated using&#58;         FS
All authentication methods&#58;  FS,GSI
Remote Mapping&#58;              star001@....
Authorized&#58;                  TRUE

********************
- Successful ping of schedd on &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;.

Job ad, pre-submit&#58;
[
Out = &#34;/home/star001/.stdout_129816_biFLa3&#34;;
x509UserProxyFQAN = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
Log = &#34;/home/star001/.log_129816_pUqH07&#34;;
x509UserProxyExpiration = 1506632808;
x509userproxysubject = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
Args = &#34;&#34;;
Cmd = &#34;/bin/env&#34;;
Err = &#34;/home/star001/.stderr_129816_C9aqBQ&#34;;
LeaveJobInQueue = ( StageOutFinish &#62; 0 ) isnt true;
x509userproxy = &#34;/tmp/x509up_u18361&#34;
]
Submitting job to schedd &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;
09/28/17 18&#58;31&#58;07 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
09/28/17 18&#58;31&#58;07 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
- Successful submission; cluster ID 56740
Resulting job ad&#58;
[
BufferSize = 524288;
NiceUser = false;
CoreSize = -1;
CumulativeSlotTime = 0;
OnExitHold = false;
RequestCpus = 1;
Err = &#34;_condor_stderr&#34;;
BufferBlockSize = 32768;
x509userproxy = &#34;/tmp/x509up_u18361&#34;;
TransferOutputRemaps = &#34;_condor_stdout=/home/star001/.stdout_129816_biFLa3;_condor_stderr=/home/star001/.stderr_129816_C9aqBQ&#34;;
ImageSize = 100;
WantCheckpoint = false;
CommittedTime = 0;
TargetType = &#34;Machine&#34;;
WhenToTransferOutput = &#34;ON_EXIT&#34;;
Cmd = &#34;/bin/env&#34;;
JobUniverse = 5;
ExitBySignal = false;
HoldReasonCode = 16;
Iwd = &#34;/home/star001&#34;;
NumRestarts = 0;
CommittedSuspensionTime = 0;
Owner = undefined;
NumSystemHolds = 0;
CumulativeSuspensionTime = 0;
RequestDisk = DiskUsage;
Requirements = true && TARGET.OPSYS == &#34;LINUX&#34; && TARGET.ARCH == &#34;X86_64&#34; && TARGET.HasFileTransfer && TARGET.Disk &#62;= RequestDisk && TARGET.Memory &#62;= RequestMemory;
MinHosts = 1;
JobNotification = 0;
NumCkpts = 0;
LastSuspensionTime = 0;
NumJobStarts = 0;
WantRemoteSyscalls = false;
JobPrio = 0;
RootDir = &#34;/&#34;;
CurrentHosts = 0;
x509UserProxyExpiration = 1506632808;
StreamOut = false;
WantRemoteIO = true;
OnExitRemove = true;
DiskUsage = 1;
In = &#34;/dev/null&#34;;
PeriodicRemove = false;
RemoteUserCpu = 0.0;
LocalUserCpu = 0.0;
RemoteSysCpu = 0.0;
LocalSysCpu = 0.0;
ClusterId = 56740;
Log = &#34;/home/star001/.log_129816_pUqH07&#34;;
CompletionDate = 0;
RemoteWallClockTime = 0.0;
x509UserProxyFQAN = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
LeaveJobInQueue = JobStatus == 4 && ( CompletionDate is UNDDEFINED || CompletionDate == 0 || ( ( time() - CompletionDate ) &#60; 864000 ) );
CondorVersion = &#34;$CondorVersion&#58; 8.4.12 Aug 07 2017 $&#34;;
MyType = &#34;Job&#34;;
StreamErr = false;
HoldReason = &#34;Spooling input data files&#34;;
PeriodicHold = false;
ProcId = 0;
Out = &#34;_condor_stdout&#34;;
JobStatus = 5;
PeriodicRelease = false;
RequestMemory = ifthenelse(MemoryUsage isnt undefined,MemoryUsage,( ImageSize + 1023 ) / 1024);
Args = &#34;&#34;;
MaxHosts = 1;
TotalSuspensions = 0;
CommittedSlotTime = 0;
x509userproxysubject = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
CondorPlatform = &#34;$CondorPlatform&#58; X86_64-CentOS_6.9 $&#34;;
ShouldTransferFiles = &#34;YES&#34;;
ExitStatus = 0;
QDate = 1506612667;
EnteredCurrentStatus = 1506612667
]

Spooling cluster 56740 files to schedd &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;
09/28/17 18&#58;31&#58;07 SharedPortClient&#58; sent connection request to &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
09/28/17 18&#58;31&#58;07 entering FileTransfer&#58;&#58;SimpleInit
09/28/17 18&#58;31&#58;07 Input files&#58;
09/28/17 18&#58;31&#58;08 FILETRANSFER&#58; protocol &#34;http&#34; handled by &#34;/usr/libexec/condor/curl_plugin&#34;
09/28/17 18&#58;31&#58;08 FILETRANSFER&#58; protocol &#34;ftp&#34; handled by &#34;/usr/libexec/condor/curl_plugin&#34;
09/28/17 18&#58;31&#58;08 FILETRANSFER&#58; protocol &#34;file&#34; handled by &#34;/usr/libexec/condor/curl_plugin&#34;
09/28/17 18&#58;31&#58;08 FILETRANSFER&#58; protocol &#34;data&#34; handled by &#34;/usr/libexec/condor/data_plugin&#34;
09/28/17 18&#58;31&#58;08 entering FileTransfer&#58;&#58;UploadFiles (final_transfer=0)
09/28/17 18&#58;31&#58;08 entering FileTransfer&#58;&#58;Upload
09/28/17 18&#58;31&#58;08 entering FileTransfer&#58;&#58;DoUpload
09/28/17 18&#58;31&#58;08 DoUpload&#58; sending file /tmp/x509up_u18361
09/28/17 18&#58;31&#58;08 FILETRANSFER&#58; outgoing file_command is 4 for /tmp/x509up_u18361
09/28/17 18&#58;31&#58;08 Received GoAhead from peer to send /tmp/x509up_u18361 and all further files.
09/28/17 18&#58;31&#58;08 Sending GoAhead for 159.93.226.250 to receive /tmp/x509up_u18361 and all further files.
09/28/17 18&#58;31&#58;08 DoUpload&#58; put_x509_delegation() returned 0
09/28/17 18&#58;31&#58;08 DoUpload&#58; sending file /bin/env
09/28/17 18&#58;31&#58;08 FILETRANSFER&#58; outgoing file_command is 1 for /bin/env
09/28/17 18&#58;31&#58;08 ReliSock&#58;&#58;put_file_with_permissions()&#58; going to send permissions 100755
09/28/17 18&#58;31&#58;08 put_file&#58; going to send from filename /bin/env
09/28/17 18&#58;31&#58;08 put_file&#58; Found file size 23832
09/28/17 18&#58;31&#58;08 put_file&#58; sending 23832 bytes
09/28/17 18&#58;31&#58;08 ReliSock&#58; put_file&#58; sent 23832 bytes
09/28/17 18&#58;31&#58;08 DoUpload&#58; exiting at 3380
- Successful spooling
Querying job status (1/600)
09/28/17 18&#58;31&#58;08 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Held
Querying job status (2/600)
09/28/17 18&#58;31&#58;10 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (3/600)
09/28/17 18&#58;31&#58;11 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (4/600)
09/28/17 18&#58;31&#58;12 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (5/600)
09/28/17 18&#58;31&#58;13 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (6/600)
09/28/17 18&#58;31&#58;14 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (7/600)
09/28/17 18&#58;31&#58;15 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (8/600)
09/28/17 18&#58;31&#58;16 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
^CTraceback (most recent call last)&#58;
File &#34;/usr/bin/condor_ce_trace&#34;, line 350, in &#60;module&#62;
main()
File &#34;/usr/bin/condor_ce_trace&#34;, line 343, in main
check_job_submit(job_info)
File &#34;/usr/bin/condor_ce_trace&#34;, line 287, in check_job_submit
time.sleep(1)
KeyboardInterrupt
bash-4.1$

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_675454178').click(function() {
            $('#detail_675454178').slideDown("normal");
            $('#show_675454178').hide();
            $('#hide_675454178').show();
        });
        $('#hide_675454178').click(function() {
            $('#detail_675454178').slideUp();
            $('#hide_675454178').hide();
            $('#show_675454178').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1506606026'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-28T13:40:26+00:00">Sep 28, 2017 01:40 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1506606026">&nbsp;</a></div><pre>Evgeniy,

Does the job getting held correspond to more of the same &#34;1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;&#34; messages in your pbs_status.log? Could you run the following commands?

# cd /usr/libexec/blahp/
# rm *.pyc *.pyo

I&#39;d also like to see the output from&#58;

$ condor_ce_config_val -verbose BATCH_GAHP
$ rpm -qf $(condor_ce_config_val BATCH_GAHP)

After that, could you run the same condor_ce_trace command from the same location?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1506596878'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-28T11:07:58+00:00">Sep 28, 2017 11:07 AM UTC</time><a class="anchor" name="1506596878">&nbsp;</a></div><pre>Brian,

Our CE uses sharing spool directory, not NFS server.
The CE spool directory is on the path &#34;/opt/exp_soft/star/spool&#34;

osgce&#58;/etc/condor-ce/config.d # grep -ri &#34;spool&#34;
99-local.conf&#58;SPOOL=/opt/exp_soft/star/spool

The shared spool directory is readable and writeable by the condor user as it mentioned in manual.

osgce&#58;/ # ls -l /opt/exp_soft/star/ | grep spool
drwxr-xr-x 6 condor condor 4096 Sep 28 13&#58;23 spool

I run the command from /home/star001 as user star001
Here is the permissions for this directory
osgce&#58;~ # ls -l /home | grep star001
drwxr-xr-x 4 star001 lstar   4096 Sep 28 12&#58;39 star001

So the user star001 can write and read from this directory

And the output of condor_ce_trace

bash-4.1$ condor_ce_trace --debug osgce.jinr.ru

09/28/17 12&#58;38&#58;00 Result of reading /etc/issue&#58;  Scientific Linux release 6.9 (Carbon)

09/28/17 12&#58;38&#58;00 Using processor count&#58; 4 processors, 4 CPUs, 0 HTs
09/28/17 12&#58;38&#58;00 Enumerating interfaces&#58; lo 127.0.0.1 up
09/28/17 12&#58;38&#58;00 Enumerating interfaces&#58; eth0 159.93.226.250 up
09/28/17 12&#58;38&#58;00 Initializing Directory&#58; curr_dir = /usr/share/condor-ce/config.d
09/28/17 12&#58;38&#58;00 Initializing Directory&#58; curr_dir = /etc/condor-ce/config.d
Testing HTCondor-CE collector connectivity.
***** condor_ping output *****
Remote Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Local  Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
<div id='show_1912009635' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1912009635'>Session ID&#58;                  osgce&#58;55048&#58;1506591480&#58;144
Instruction&#58;                 READ
Command&#58;                     60020
Encryption&#58;                  none
Integrity&#58;                   none
Authentication&#58;              none
Remote Mapping&#58;              unauthenticated@unmapped
Authorized&#58;                  TRUE

********************
- Successful ping of collector on &#60;159.93.226.250&#58;9619&#62;.

09/28/17 12&#58;38&#58;00 Will use TCP to update collector osgce.jinr.ru &#60;159.93.226.250&#58;9619&#62;
09/28/17 12&#58;38&#58;00 Trying to query collector &#60;159.93.226.250&#58;9619&#62;
09/28/17 12&#58;38&#58;00 IPVERIFY&#58; checking osgce.jinr.ru against 159.93.226.250
09/28/17 12&#58;38&#58;00 IPVERIFY&#58; matched 159.93.226.250 to 159.93.226.250
09/28/17 12&#58;38&#58;00 IPVERIFY&#58; ip found is 1
Testing HTCondor-CE schedd connectivity.
***** condor_ping output *****
Remote Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Local  Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Session ID&#58;                  osgce&#58;55056&#58;1506591481&#58;832
Instruction&#58;                 WRITE
Command&#58;                     60021
Encryption&#58;                  none
Integrity&#58;                   MD5
Authenticated using&#58;         FS
All authentication methods&#58;  FS,GSI
Remote Mapping&#58;              star001@....
Authorized&#58;                  TRUE

********************
- Successful ping of schedd on &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;.

Job ad, pre-submit&#58;
[
Out = &#34;/home/star001/.stdout_127216_S81XqH&#34;;
x509UserProxyFQAN = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
Log = &#34;/home/star001/.log_127216_QhUCl0&#34;;
x509UserProxyExpiration = 1506632808;
x509userproxysubject = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
Args = &#34;&#34;;
Cmd = &#34;/bin/env&#34;;
Err = &#34;/home/star001/.stderr_127216_49zupr&#34;;
LeaveJobInQueue = ( StageOutFinish &#62; 0 ) isnt true;
x509userproxy = &#34;/tmp/x509up_u18361&#34;
]
Submitting job to schedd &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;
09/28/17 12&#58;38&#58;01 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
09/28/17 12&#58;38&#58;01 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
- Successful submission; cluster ID 56738
Resulting job ad&#58;
[
BufferSize = 524288;
NiceUser = false;
CoreSize = -1;
CumulativeSlotTime = 0;
OnExitHold = false;
RequestCpus = 1;
Err = &#34;_condor_stderr&#34;;
BufferBlockSize = 32768;
x509userproxy = &#34;/tmp/x509up_u18361&#34;;
TransferOutputRemaps = &#34;_condor_stdout=/home/star001/.stdout_127216_S81XqH;_condor_stderr=/home/star001/.stderr_12721$
ImageSize = 100;
WantCheckpoint = false;
CommittedTime = 0;
TargetType = &#34;Machine&#34;;
WhenToTransferOutput = &#34;ON_EXIT&#34;;
Cmd = &#34;/bin/env&#34;;
JobUniverse = 5;
ExitBySignal = false;
HoldReasonCode = 16;
Iwd = &#34;/home/star001&#34;;
NumRestarts = 0;
CommittedSuspensionTime = 0;
Owner = undefined;
NumSystemHolds = 0;
CumulativeSuspensionTime = 0;
RequestDisk = DiskUsage;
Requirements = true && TARGET.OPSYS == &#34;LINUX&#34; && TARGET.ARCH == &#34;X86_64&#34; && TARGET.HasFileTransfer && TARGET.Disk &#62;=$
MinHosts = 1;
JobNotification = 0;
NumCkpts = 0;
LastSuspensionTime = 0;
NumJobStarts = 0;
WantRemoteSyscalls = false;
JobPrio = 0;
RootDir = &#34;/&#34;;
CurrentHosts = 0;
x509UserProxyExpiration = 1506632808;
StreamOut = false;
WantRemoteIO = true;
OnExitRemove = true;
DiskUsage = 1;
In = &#34;/dev/null&#34;;
PeriodicRemove = false;
RemoteUserCpu = 0.0;
LocalUserCpu = 0.0;
RemoteSysCpu = 0.0;
LocalSysCpu = 0.0;
ClusterId = 56738;
Log = &#34;/home/star001/.log_127216_QhUCl0&#34;;
CompletionDate = 0;
RemoteWallClockTime = 0.0;
x509UserProxyFQAN = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
LeaveJobInQueue = JobStatus == 4 && ( CompletionDate is UNDDEFINED || CompletionDate == 0 || ( ( time() - CompletionD$
CondorVersion = &#34;$CondorVersion&#58; 8.4.12 Aug 07 2017 $&#34;;
MyType = &#34;Job&#34;;
StreamErr = false;
HoldReason = &#34;Spooling input data files&#34;;
PeriodicHold = false;
ProcId = 0;
Out = &#34;_condor_stdout&#34;;
JobStatus = 5;
PeriodicRelease = false;
RequestMemory = ifthenelse(MemoryUsage isnt undefined,MemoryUsage,( ImageSize + 1023 ) / 1024);
Args = &#34;&#34;;
MaxHosts = 1;
TotalSuspensions = 0;
CommittedSlotTime = 0;
x509userproxysubject = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1871758380&#34;;
CondorPlatform = &#34;$CondorPlatform&#58; X86_64-CentOS_6.9 $&#34;;
ShouldTransferFiles = &#34;YES&#34;;
ExitStatus = 0;
QDate = 1506591481;
EnteredCurrentStatus = 1506591481
]
Spooling cluster 56738 files to schedd &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;
09/28/17 12&#58;38&#58;01 SharedPortClient&#58; sent connection request to &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
09/28/17 12&#58;38&#58;01 entering FileTransfer&#58;&#58;SimpleInit
09/28/17 12&#58;38&#58;01 Input files&#58;
09/28/17 12&#58;38&#58;01 FILETRANSFER&#58; protocol &#34;http&#34; handled by &#34;/usr/libexec/condor/curl_plugin&#34;
09/28/17 12&#58;38&#58;01 FILETRANSFER&#58; protocol &#34;ftp&#34; handled by &#34;/usr/libexec/condor/curl_plugin&#34;
09/28/17 12&#58;38&#58;01 FILETRANSFER&#58; protocol &#34;file&#34; handled by &#34;/usr/libexec/condor/curl_plugin&#34;
09/28/17 12&#58;38&#58;01 FILETRANSFER&#58; protocol &#34;data&#34; handled by &#34;/usr/libexec/condor/data_plugin&#34;
09/28/17 12&#58;38&#58;01 entering FileTransfer&#58;&#58;UploadFiles (final_transfer=0)
09/28/17 12&#58;38&#58;01 entering FileTransfer&#58;&#58;Upload
09/28/17 12&#58;38&#58;01 entering FileTransfer&#58;&#58;DoUpload
09/28/17 12&#58;38&#58;01 DoUpload&#58; sending file /tmp/x509up_u18361
09/28/17 12&#58;38&#58;01 FILETRANSFER&#58; outgoing file_command is 4 for /tmp/x509up_u18361
09/28/17 12&#58;38&#58;01 Received GoAhead from peer to send /tmp/x509up_u18361 and all further files.
09/28/17 12&#58;38&#58;01 Sending GoAhead for 159.93.226.250 to receive /tmp/x509up_u18361 and all further files.
09/28/17 12&#58;38&#58;01 DoUpload&#58; put_x509_delegation() returned 0
09/28/17 12&#58;38&#58;01 DoUpload&#58; sending file /bin/env
09/28/17 12&#58;38&#58;01 FILETRANSFER&#58; outgoing file_command is 1 for /bin/env
09/28/17 12&#58;38&#58;01 ReliSock&#58;&#58;put_file_with_permissions()&#58; going to send permissions 100755
09/28/17 12&#58;38&#58;01 put_file&#58; going to send from filename /bin/env
09/28/17 12&#58;38&#58;01 put_file&#58; Found file size 23832
09/28/17 12&#58;38&#58;01 put_file&#58; sending 23832 bytes
09/28/17 12&#58;38&#58;01 ReliSock&#58; put_file&#58; sent 23832 bytes
09/28/17 12&#58;38&#58;01 DoUpload&#58; exiting at 3380
- Successful spooling
Querying job status (1/600)
09/28/17 12&#58;38&#58;02 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Held
Querying job status (2/600)
09/28/17 12&#58;38&#58;04 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (3/600)
09/28/17 12&#58;38&#58;05 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
Querying job status (4/600)
09/28/17 12&#58;38&#58;06 SharedPortClient&#58; sent connection request to schedd at &#60;159.93.226.250&#58;9619&#62; for shared port id 55018_8c81_3
Job status&#58; Idle
....

and so on

Then I checked the job 56738 status, it is also has holdreason=&#34;Error parsing classad or job not found&#34;
So simple test did not passed.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_1912009635').click(function() {
            $('#detail_1912009635').slideDown("normal");
            $('#show_1912009635').hide();
            $('#hide_1912009635').show();
        });
        $('#hide_1912009635').click(function() {
            $('#detail_1912009635').slideUp();
            $('#hide_1912009635').hide();
            $('#show_1912009635').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1506443052'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-26T16:24:12+00:00">Sep 26, 2017 04:24 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1506443052">&nbsp;</a></div><pre>Evgeniy,

Could you run the command from a folder where you have write access?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1506416701'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-26T09:05:01+00:00">Sep 26, 2017 09:05 AM UTC</time><a class="anchor" name="1506416701">&nbsp;</a></div><pre>Brian, Elizabeth,

I missed the &#34;--debug&#34; option in previous message. Here is the correspondent output &#58;

osgce&#58;~ # condor_ce_trace --debug osgce.jinr.ru
09/26/17 11&#58;58&#58;26 Result of reading /etc/issue&#58;  Scientific Linux release 6.9 (Carbon)

09/26/17 11&#58;58&#58;26 Using processor count&#58; 4 processors, 4 CPUs, 0 HTs
09/26/17 11&#58;58&#58;26 Enumerating interfaces&#58; lo 127.0.0.1 up
09/26/17 11&#58;58&#58;26 Enumerating interfaces&#58; eth0 159.93.226.250 up
09/26/17 11&#58;58&#58;26 Initializing Directory&#58; curr_dir = /usr/share/condor-ce/config.d
09/26/17 11&#58;58&#58;26 Initializing Directory&#58; curr_dir = /etc/condor-ce/config.d
Testing HTCondor-CE collector connectivity.
***** condor_ping output *****
Remote Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Local  Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Session ID&#58;                  osgce&#58;55048&#58;1506416306&#58;36
Instruction&#58;                 READ
Command&#58;                     60020
Encryption&#58;                  none
Integrity&#58;                   none
Authentication&#58;              none
Remote Mapping&#58;              unauthenticated@unmapped
Authorized&#58;                  TRUE

********************
- Successful ping of collector on &#60;159.93.226.250&#58;9619&#62;.

09/26/17 11&#58;58&#58;26 Will use TCP to update collector osgce.jinr.ru &#60;159.93.226.250&#58;9619&#62;
09/26/17 11&#58;58&#58;26 Trying to query collector &#60;159.93.226.250&#58;9619&#62;
09/26/17 11&#58;58&#58;26 IPVERIFY&#58; checking osgce.jinr.ru against 159.93.226.250
09/26/17 11&#58;58&#58;26 IPVERIFY&#58; matched 159.93.226.250 to 159.93.226.250
09/26/17 11&#58;58&#58;26 IPVERIFY&#58; ip found is 1
Testing HTCondor-CE schedd connectivity.
***** condor_ping output *****
<div id='show_1932179135' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1932179135'>Remote Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Local  Version&#58;              $CondorVersion&#58; 8.4.12 Aug 07 2017 $
Session ID&#58;                  osgce&#58;55056&#58;1506416306&#58;49
Instruction&#58;                 WRITE
Command&#58;                     60021
Encryption&#58;                  none
Integrity&#58;                   MD5
Authenticated using&#58;         FS
All authentication methods&#58;  FS,GSI
Remote Mapping&#58;              condor@....
Authorized&#58;                  TRUE

********************
- Successful ping of schedd on &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;.

********************************************************************************
Uncaught exception, please send the following error to goc@....
with a description of the issue&#58;
Traceback (most recent call last)&#58;
File &#34;/usr/bin/condor_ce_trace&#34;, line 350, in &#60;module&#62;
main()
File &#34;/usr/bin/condor_ce_trace&#34;, line 342, in main
generate_run_script(job_info)
File &#34;/usr/bin/condor_ce_trace&#34;, line 175, in generate_run_script
job_info.update(ce.generate_job_files())
File &#34;/usr/lib/python2.6/site-packages/htcondorce/tools.py&#34;, line 45, in
generate_job_files
fd, job_info[filetype + &#39;_file&#39;] = tempfile.mkstemp(dir=&#34;.&#34;,
prefix=&#34;.%s_%d_&#34; % (filetype, pid))
File &#34;/usr/lib64/python2.6/tempfile.py&#34;, line 293, in mkstemp
return _mkstemp_inner(dir, prefix, suffix, flags)
File &#34;/usr/lib64/python2.6/tempfile.py&#34;, line 228, in _mkstemp_inner
fd = _os.open(file, flags, 0600)
OSError&#58; [Errno 13] Permission denied&#58; &#39;./.stdout_108702_OMSF89&#39;

********************************************************************************

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_1932179135').click(function() {
            $('#detail_1932179135').slideDown("normal");
            $('#show_1932179135').hide();
            $('#hide_1932179135').show();
        });
        $('#hide_1932179135').click(function() {
            $('#detail_1932179135').slideUp();
            $('#hide_1932179135').hide();
            $('#show_1932179135').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1506416238'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-26T08:57:18+00:00">Sep 26, 2017 08:57 AM UTC</time><a class="anchor" name="1506416238">&nbsp;</a></div><pre>Brian, Elizabet,

that is my output of condor_ce_trace  &#58;
osgce&#58;~ # condor_ce_trace osgce.jinr.ru
Testing HTCondor-CE collector connectivity.
- Successful ping of collector on &#60;159.93.226.250&#58;9619&#62;.

Testing HTCondor-CE schedd connectivity.
- Successful ping of schedd on &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;.

Submitting job to schedd &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;
- Successful submission; cluster ID 56731
Resulting job ad&#58;
[
BufferSize = 524288;
NiceUser = false;
CoreSize = -1;
CumulativeSlotTime = 0;
OnExitHold = false;
RequestCpus = 1;
Err = &#34;_condor_stderr&#34;;
BufferBlockSize = 32768;
x509userproxy = &#34;/tmp/x509up_u0&#34;;
TransferOutputRemaps = &#34;_condor_stdout=/root/.stdout_108631_yqtm_8;_condor_stderr=/root/.stderr_108631_UJ4YIx&#34;;
ImageSize = 100;
WantCheckpoint = false;
CommittedTime = 0;
TargetType = &#34;Machine&#34;;
WhenToTransferOutput = &#34;ON_EXIT&#34;;
Cmd = &#34;/bin/env&#34;;
JobUniverse = 5;
ExitBySignal = false;
HoldReasonCode = 16;
Iwd = &#34;/root&#34;;
NumRestarts = 0;
<div id='show_2000487570' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_2000487570'>CommittedSuspensionTime = 0;
Owner = undefined;
NumSystemHolds = 0;
CumulativeSuspensionTime = 0;
RequestDisk = DiskUsage;
Requirements = true && TARGET.OPSYS == &#34;LINUX&#34; && TARGET.ARCH == &#34;X86_64&#34; && TARGET.HasFileTransfer && TARGET.Disk &#62;= RequestDisk && TARGET.Memory &#62;= RequestMemory;
MinHosts = 1;
JobNotification = 0;
NumCkpts = 0;
LastSuspensionTime = 0;
NumJobStarts = 0;
WantRemoteSyscalls = false;
JobPrio = 0;
RootDir = &#34;/&#34;;
CurrentHosts = 0;
x509UserProxyExpiration = 1506458305;
StreamOut = false;
WantRemoteIO = true;
OnExitRemove = true;
DiskUsage = 1;
In = &#34;/dev/null&#34;;
PeriodicRemove = false;
RemoteUserCpu = 0.0;
LocalUserCpu = 0.0;
RemoteSysCpu = 0.0;
LocalSysCpu = 0.0;
ClusterId = 56731;
Log = &#34;/root/.log_108631_fmlPoS&#34;;
CompletionDate = 0;
RemoteWallClockTime = 0.0;
x509UserProxyFQAN = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1848901731&#34;;
LeaveJobInQueue = JobStatus == 4 && ( CompletionDate is UNDDEFINED || CompletionDate == 0 || ( ( time() - CompletionDate ) &#60; 864000 ) );
CondorVersion = &#34;$CondorVersion&#58; 8.4.12 Aug 07 2017 $&#34;;
MyType = &#34;Job&#34;;
StreamErr = false;
HoldReason = &#34;Spooling input data files&#34;;
PeriodicHold = false;
ProcId = 0;
Out = &#34;_condor_stdout&#34;;
JobStatus = 5;
PeriodicRelease = false;
RequestMemory = ifthenelse(MemoryUsage isnt undefined,MemoryUsage,( ImageSize + 1023 ) / 1024);
Args = &#34;&#34;;
MaxHosts = 1;
TotalSuspensions = 0;
CommittedSlotTime = 0;
x509userproxysubject = &#34;/C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov/CN=1848901731&#34;;
CondorPlatform = &#34;$CondorPlatform&#58; X86_64-CentOS_6.9 $&#34;;
ShouldTransferFiles = &#34;YES&#34;;
ExitStatus = 0;
QDate = 1506415730;
EnteredCurrentStatus = 1506415730
]
Spooling cluster 56731 files to schedd &#60;159.93.226.250&#58;9619?addrs=159.93.226.250-9619&noUDP&sock=55018_8c81_3&#62;
********************************************************************************
Unable to write stackfile due to the following error&#58;
Traceback (most recent call last)&#58;
File &#34;/usr/bin/condor_ce_trace&#34;, line 361, in &#60;module&#62;
FD, STACK_FILE = tempfile.mkstemp(dir=&#34;.&#34;, prefix=&#34;.stack_%d_&#34; % PID)
File &#34;/usr/lib64/python2.6/tempfile.py&#34;, line 293, in mkstemp
return _mkstemp_inner(dir, prefix, suffix, flags)
File &#34;/usr/lib64/python2.6/tempfile.py&#34;, line 228, in _mkstemp_inner
fd = _os.open(file, flags, 0600)
OSError&#58; [Errno 13] Permission denied&#58; &#39;./.stack_108631_ax8vO_&#39;

********************************************************************************
So there are wrong permissions somewhere for writing stackfile. Which permissions are intended, or what folder should be checked for permissions ?
Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_2000487570').click(function() {
            $('#detail_2000487570').slideDown("normal");
            $('#show_2000487570').hide();
            $('#hide_2000487570').show();
        });
        $('#hide_2000487570').click(function() {
            $('#detail_2000487570').slideUp();
            $('#hide_2000487570').hide();
            $('#show_2000487570').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1506110627'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-22T20:03:47+00:00">Sep 22, 2017 08:03 PM UTC</time> by <b>echism</b><a class="anchor" name="1506110627">&nbsp;</a></div><pre>Evgeniy,

Did you have any luck?

Thanks</pre></div><div class='update_description'><i onclick="document.location='34702#1505918352'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-20T14:39:12+00:00">Sep 20, 2017 02:39 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1505918352">&nbsp;</a></div><pre>Also, updating to the latest blahp available in OSG&#39;s repos
(1.18.33.bosco-1) should fix that specific problem in the logs.</pre></div><div class='update_description'><i onclick="document.location='34702#1505918193'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-20T14:36:33+00:00">Sep 20, 2017 02:36 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1505918193">&nbsp;</a></div><pre>Evgeniy,

If you generate a proxy, can you try submitting a job with condor_ce_trace (<a href='https&#58;//twiki.opensciencegrid.org/bin/view/Documentation/Release3/TroubleshootingHTCondorCE#condor_ce_trace' target='_blank' rel='nofollow'>https&#58;//twiki.opensciencegrid.org/bin/view/Documentation/Release3/TroubleshootingHTCondorCE#condor_ce_trace</a>) and seeing how that fares?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1505896655'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-20T08:37:35+00:00">Sep 20, 2017 08:37 AM UTC</time><a class="anchor" name="1505896655">&nbsp;</a></div><pre>Hi, all
About blahp &#58;

osgce&#58;~ # rpm -q blahp
blahp-1.18.33.bosco-1.osg33.el6.x86_64

The output from pbs_status.log have not changed since August 28 2017. There is no new records after 08/28/17.
osgce&#58;~ # tail -f /var/tmp/qstat_cache_star001/pbs_status.log
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;05&#58;49 51492 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;05&#58;49 51492 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 Checking cache for jobid 10026122
08/28/17 16&#58;06&#58;49 51501 Starting query to fill cache.
08/28/17 16&#58;06&#58;49 51501 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;06&#58;49 51501 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;06&#58;49 51501 Starting qstat.

^C
osgce&#58;~ #
I restarted condor-ce a few times with no result. I&#39;ve still get holdreason = &#34;Error parsing classad or job not found&#34;.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1505742408'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-18T13:46:48+00:00">Sep 18, 2017 01:46 PM UTC</time> by <b>echism</b><a class="anchor" name="1505742408">&nbsp;</a></div><pre>Evgeniy,

Do you have any updates on this?

Thank you</pre></div><div class='update_description'><i onclick="document.location='34702#1505157272'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-11T19:14:32+00:00">Sep 11, 2017 07:14 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1505157272">&nbsp;</a></div><pre>Additionally, you can try installing blahp-1.18.33 out of osg-testing (yum install blahp --enablerepo=osg-testing), which should handle when pbs_pro is not specified.</pre></div><div class='update_description'><i onclick="document.location='34702#1505147216'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-11T16:26:56+00:00">Sep 11, 2017 04:26 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1505147216">&nbsp;</a></div><pre>Hum... from that output, it looks like the blahp hasn&#39;t been updated.  Just to confirm, can you run&#58;

$ rpm -q blahp

Then, you may need to restart the condor-ce.  The pbs_status.log is the best indicator to tell of the problem has been fixed.</pre></div><div class='update_description'><i onclick="document.location='34702#1505144983'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-11T15:49:43+00:00">Sep 11, 2017 03:49 PM UTC</time><a class="anchor" name="1505144983">&nbsp;</a></div><pre>No, the output is empty.

osgce&#58;~ # date
Mon Sep 11 18&#58;48&#58;13 MSK 2017
osgce&#58;~ # tail -f /var/tmp/qstat_cache_star001/pbs_status.log
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;05&#58;49 51492 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;05&#58;49 51492 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 Checking cache for jobid 10026122
08/28/17 16&#58;06&#58;49 51501 Starting query to fill cache.
08/28/17 16&#58;06&#58;49 51501 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;06&#58;49 51501 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;06&#58;49 51501 Starting qstat.

^C
osgce&#58;~ # date
Mon Sep 11 18&#58;48&#58;23 MSK 2017
osgce&#58;~ #

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1504797101'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-07T15:11:41+00:00">Sep 7, 2017 03:11 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1504797101">&nbsp;</a></div><pre>Are you still getting the same messages in pbs_status.log ?</pre></div><div class='update_description'><i onclick="document.location='34702#1504786184'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-07T12:09:44+00:00">Sep 7, 2017 12:09 PM UTC</time><a class="anchor" name="1504786184">&nbsp;</a></div><pre>Hi Derek,
this update didn&#39;t help. There is still holdreason = &#34;Error parsing classad or job not found&#34;

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1504637213'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-05T18:46:53+00:00">Sep 5, 2017 06:46 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1504637213">&nbsp;</a></div><pre>Hi Evgeniy,

Your configuration looks good, especially blah.config_02.txt.  It has a strange unicode character on the line above the pbs_pro=no option, but it should be good.

If you would like, we have fixed this bug in the new release of Blahp.  The new release is available if you install with yum enabling the osg-testing repo&#58;

$ yum update --enablerepo=osg-testing blahp

After updating, be sure to release all of your held jobs.</pre></div><div class='update_description'><i onclick="document.location='34702#1504510084'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-09-04T07:28:04+00:00">Sep 4, 2017 07:28 AM UTC</time><a class="anchor" name="1504510084">&nbsp;</a></div><pre>Brian, the new attached file named blah.config_02.txt

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1504082476'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-30T08:41:16+00:00">Aug 30, 2017 08:41 AM UTC</time><a class="anchor" name="1504082476">&nbsp;</a></div><pre>Done. The setting is in &#34;PBS common variables&#34;  section of  /etc/blah.config

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1504016804'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-29T14:26:44+00:00">Aug 29, 2017 02:26 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1504016804">&nbsp;</a></div><pre>That&#39;s odd, I just verified that I saw the same errors as you without &#34;pbs_pro=no&#34; in my configuration and that it went away after setting it. Could you attach /etc/blah.config again?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1503992530'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-29T07:42:10+00:00">Aug 29, 2017 07:42 AM UTC</time><a class="anchor" name="1503992530">&nbsp;</a></div><pre>OK. I removed those jobs. The new jobs are with the same error.

osgce&#58;/etc # condor_ce_q

-- Schedd&#58; osgce.jinr.ru &#58; &#60;159.93.226.250&#58;35614&#62;
ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD
56462.0   star001         8/28 18&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56463.0   star001         8/28 18&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
...
56483.0   star001         8/29 04&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56484.0   star001         8/29 05&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56485.0   star001         8/29 05&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56486.0   star001         8/29 06&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56487.0   star001         8/29 06&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56488.0   star001         8/29 07&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56489.0   star001         8/29 07&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56490.0   star001         8/29 08&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56491.0   star001         8/29 08&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56492.0   star001         8/29 09&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56493.0   star001         8/29 09&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56494.0   star001         8/29 10&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh
56495.0   star001         8/29 10&#58;01   0+00&#58;00&#58;00 H  0   0.0  test.csh

34 jobs; 0 completed, 0 removed, 0 idle, 0 running, 34 held, 0 suspended
osgce&#58;/etc # condor_ce_q -l 56490.0 -attr holdreason
holdreason = &#34;Error parsing classad or job not found&#34;

GridmanagerLog.* for job id 56493.0&#58;
...
08/29/17 09&#58;01&#58;48 [60164] Updating classad values for 56493.0&#58;
08/29/17 09&#58;01&#58;48 [60164]    DelegatedProxyExpiration = 1504850482
08/29/17 09&#58;01&#58;48 [60164]    GridJobId = &#34;batch pbs osgce.jinr.ru_9619_osgce.jinr.ru#56493.0#1503986490 pbs/20170829/10142654&#34;
08/29/17 09&#58;01&#58;48 [60164] leaving doContactSchedd()
08/29/17 09&#58;01&#58;48 [60164] (56493.0) doEvaluateState called&#58; gmState GM_SUBMIT_SAVE, remoteState 0
08/29/17 09&#58;01&#58;48 [60164] (56493.0) gm state change&#58; GM_SUBMIT_SAVE -&#62; GM_SUBMITTED
<div id='show_1624119275' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1624119275'>08/29/17 09&#58;02&#58;36 [60164] Received CHECK_LEASES signal
08/29/17 09&#58;02&#58;36 [60164] in doContactSchedd()
08/29/17 09&#58;02&#58;36 [60164] querying for renewed leases
08/29/17 09&#58;02&#58;36 [60164] querying for removed/held jobs
08/29/17 09&#58;02&#58;36 [60164] Using constraint ((Owner=?=&#34;star001&#34;&&JobUniverse==9)) && ((Managed =!= &#34;ScheddDone&#34;)) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= &#34;External&#34;))
08/29/17 09&#58;02&#58;36 [60164] Fetched 0 job ads from schedd
08/29/17 09&#58;02&#58;36 [60164] leaving doContactSchedd()
08/29/17 09&#58;02&#58;38 [60164] GAHP[60166] &#60;- &#39;RESULTS&#39;
08/29/17 09&#58;02&#58;38 [60164] GAHP[60166] -&#62; &#39;S&#39; &#39;0&#39;
08/29/17 09&#58;02&#58;40 [60164] Evaluating staleness of remote job statuses.
08/29/17 09&#58;02&#58;48 [60164] (56493.0) doEvaluateState called&#58; gmState GM_SUBMITTED, remoteState 0
08/29/17 09&#58;02&#58;48 [60164] (56493.0) gm state change&#58; GM_SUBMITTED -&#62; GM_POLL_ACTIVE
08/29/17 09&#58;02&#58;48 [60164] GAHP[60166] &#60;- &#39;BLAH_JOB_STATUS 3 pbs/20170829/10142654&#39;
08/29/17 09&#58;02&#58;48 [60164] GAHP[60166] -&#62; &#39;S&#39;
08/29/17 09&#58;02&#58;48 [60164] GAHP[60166] &#60;- &#39;RESULTS&#39;
08/29/17 09&#58;02&#58;48 [60164] GAHP[60166] -&#62; &#39;R&#39;
08/29/17 09&#58;02&#58;48 [60164] GAHP[60166] -&#62; &#39;S&#39; &#39;1&#39;
08/29/17 09&#58;02&#58;48 [60164] GAHP[60166] -&#62; &#39;3&#39; &#39;1&#39; &#39;Error parsing classad or job not found&#39; &#39;0&#39; &#39;N/A&#39;
08/29/17 09&#58;02&#58;48 [60164] (56493.0) doEvaluateState called&#58; gmState GM_POLL_ACTIVE, remoteState 0
08/29/17 09&#58;02&#58;48 [60164] (56493.0) gm state change&#58; GM_POLL_ACTIVE -&#62; GM_SUBMITTED
...
++++++++++++++
pbs server log for corresponded job 10142654&#58;

lxbsrv01&#58;/var/lib/torque/server_logs # less 20170829 | grep 10142654
08/29/2017 09&#58;01&#58;44;0100;PBS_Server.25182;Job;10142654.lxbsrv01.jinr.ru;enqueuing into star, state 1 hop 1
08/29/2017 09&#58;01&#58;44;0008;PBS_Server.25182;Job;req_commit;job_id&#58; 10142654.lxbsrv01.jinr.ru
08/29/2017 09&#58;02&#58;49;0008;PBS_Server.15113;Job;10142654.lxbsrv01.jinr.ru;Job Run at request of root@....
08/29/2017 09&#58;02&#58;50;0008;PBS_Server.15113;Job;10142654.lxbsrv01.jinr.ru;Not sending email&#58; job requested no e-mail
08/29/2017 09&#58;07&#58;51;0010;PBS_Server.15108;Job;10142654.lxbsrv01.jinr.ru;Exit_status=0 resources_used.cput=00&#58;00&#58;00 resources_used.mem=5468kb resources_used.vmem=33528kb resources_used.walltime=00&#58;05&#58;01
08/29/2017 09&#58;07&#58;51;0008;PBS_Server.15108;Job;10142654.lxbsrv01.jinr.ru;Not sending email&#58; job requested no e-mail
08/29/2017 09&#58;07&#58;51;0008;PBS_Server.25212;Job;10142654.lxbsrv01.jinr.ru;on_job_exit valid pjob&#58; 10142654.lxbsrv01.jinr.ru (substate=50)
08/29/2017 09&#58;10&#58;59;0100;PBS_Server.25189;Job;10142654.lxbsrv01.jinr.ru;dequeuing from star, state COMPLETE

And pbs_status.log on CE doesn&#39;t have any new records since yesterday.

osgce&#58;/var/tmp/qstat_cache_star001 # tail -f pbs_status.log
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;05&#58;49 51492 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;05&#58;49 51492 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 Checking cache for jobid 10026122
08/28/17 16&#58;06&#58;49 51501 Starting query to fill cache.
08/28/17 16&#58;06&#58;49 51501 Starting qstat.
08/28/17 16&#58;06&#58;49 51501 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;06&#58;49 51501 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;06&#58;49 51501 Starting qstat.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_1624119275').click(function() {
            $('#detail_1624119275').slideDown("normal");
            $('#show_1624119275').hide();
            $('#hide_1624119275').show();
        });
        $('#hide_1624119275').click(function() {
            $('#detail_1624119275').slideUp();
            $('#hide_1624119275').hide();
            $('#show_1624119275').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1503930194'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-28T14:23:14+00:00">Aug 28, 2017 02:23 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1503930194">&nbsp;</a></div><pre>Evgeniy,

I don&#39;t believe releasing jobs will cut it. Go ahead and remove them and
watch out for the behavior of new jobs.

- Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1503929896'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-28T14:18:16+00:00">Aug 28, 2017 02:18 PM UTC</time><a class="anchor" name="1503929896">&nbsp;</a></div><pre>Done.
I restarted CE and &#39;condor_ce_released -all&#39; jobs.
But the last 2 jobs are still with &#34;Error parsing classad or job not found&#34;, while other previous jobs with holdreason = &#34;Failed to get expiration time of proxy&#58; unable to read proxy file&#34;
Also there is no new records in  output of /var/tmp/qstat_cache_star001/pbs_status.log

Eveniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1503928031'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-28T13:47:11+00:00">Aug 28, 2017 01:47 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1503928031">&nbsp;</a></div><pre>Evgeniy,

This should be a simple fix, set &#34;pbs_pro=no&#34; in /etc/blah.config. Let
me know if that works.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1503927594'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-28T13:39:54+00:00">Aug 28, 2017 01:39 PM UTC</time><a class="anchor" name="1503927594">&nbsp;</a></div><pre>Answer for the request &#58;
&#34;Are you still getting new jobs? Do you have /var/tmp/qstat_cache_* directories? Could you &#96;touch /var/tmp/qstat_cache_*/pbs_status.debug&#96;? That should result in pbs_status.log files that should give us more info.&#34;
Yes, I asked our VO users to send just test jobs for tuning HTCondor-ce.
I have /var/tmp/qstat_cache_star001/ directory.
I touched pbs_status.debug, so

# ls -l /var/tmp/qstat_cache_star001/
total 532
-rw------- 1 star001 lstar 536724 Aug  9 12&#58;33 blahp_results_cache
-rw------- 1 star001 lstar      0 Aug  9 12&#58;33 cluster_type
-rw-r--r-- 1 star001 lstar      0 Aug 28 13&#58;04 pbs_status.debug
-rw-r--r-- 1 star001 lstar   1050 Aug 28 13&#58;06 pbs_status.log

Here is some ouput from pbs_status.log &#58;

08/28/17 16&#58;02&#58;48 51473 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;02&#58;48 51473 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;02&#58;48 51473 Starting qstat.
08/28/17 16&#58;03&#58;48 51478 Checking cache for jobid 10026122
08/28/17 16&#58;03&#58;48 51478 Starting query to fill cache.
08/28/17 16&#58;03&#58;48 51478 Starting qstat.
08/28/17 16&#58;03&#58;48 51478 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;03&#58;48 51478 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;03&#58;48 51478 Starting qstat.
08/28/17 16&#58;04&#58;49 51487 Checking cache for jobid 10026122
08/28/17 16&#58;04&#58;49 51487 Starting query to fill cache.
08/28/17 16&#58;04&#58;49 51487 Starting qstat.
08/28/17 16&#58;04&#58;49 51487 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
08/28/17 16&#58;04&#58;49 51487 Jobid 10026122 not in cache; querying PBS
08/28/17 16&#58;04&#58;49 51487 Starting qstat.
08/28/17 16&#58;05&#58;49 51492 Checking cache for jobid 10026122
08/28/17 16&#58;05&#58;49 51492 Starting query to fill cache.
08/28/17 16&#58;05&#58;49 51492 Starting qstat.
08/28/17 16&#58;05&#58;49 51492 1ERROR&#58; Internal exception, No option &#39;pbs_pro&#39; in section&#58; &#39;blahp&#39;
...
<div id='show_788616360' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_788616360'>
Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_788616360').click(function() {
            $('#detail_788616360').slideDown("normal");
            $('#show_788616360').hide();
            $('#hide_788616360').show();
        });
        $('#hide_788616360').click(function() {
            $('#detail_788616360').slideUp();
            $('#hide_788616360').hide();
            $('#show_788616360').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1503675013'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-25T15:30:13+00:00">Aug 25, 2017 03:30 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1503675013">&nbsp;</a></div><pre>Hrm, could you just attach the GridmanagerLog* files from that directory
then?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1503674548'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-25T15:22:28+00:00">Aug 25, 2017 03:22 PM UTC</time><a class="anchor" name="1503674548">&nbsp;</a></div><pre>Brian,
My log level was always maximum from the begining  -  D_FULLDEBUG

osgce&#58;/etc/condor-ce/config.d # less 99-local.conf | grep DEBUG
MASTER_DEBUG = D_FULLDEBUG
SCHEDD_DEBUG = D_FULLDEBUG
JOB_ROUTER_DEBUG = D_FULLDEBUG
GRIDMANAGER_DEBUG = D_FULLDEBUG
ALL_DEBUG=D_FULLDEBUG

So the contents of &#39;/var/log/condor-ce&#39; covers about 7.6Mbytes in *.tat.gz archive and I can not attach such a file  - &#34;File is too large&#34;, how can I pass over this ?

I&#39;ve attached osg-system-profiler output.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1503672709'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-25T14:51:49+00:00">Aug 25, 2017 02:51 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1503672709">&nbsp;</a></div><pre>Evgeniy,

Could you increase the debug level of your CE (<a href='https&#58;//twiki.opensciencegrid.org/bin/view/Documentation/Release3/TroubleshootingHTCondorCE#HTCondor_CE_Troubleshooting_Item' target='_blank' rel='nofollow'>https&#58;//twiki.opensciencegrid.org/bin/view/Documentation/Release3/TroubleshootingHTCondorCE#HTCondor_CE_Troubleshooting_Item</a>), restart the CE, then attach the contents of /var/log/condor-ce? Also, could you provide the output of osg-system-profiler?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1503571237'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-24T10:40:37+00:00">Aug 24, 2017 10:40 AM UTC</time><a class="anchor" name="1503571237">&nbsp;</a></div><pre>Brian,
Yes, I have pbs backend on CE host.
osgce&#58;/etc # yum list installed | grep pbs
gratia-probe-pbs-lsf.x86_64          1.18.1-1.osg33.el6   @osg
htcondor-ce-pbs.noarch               2.2.2-1.osg33.el6    @osg
osg-base-ce-pbs.x86_64               3.3-13.osg33.el6     @osg
osg-ce-pbs.x86_64                    3.3-13.osg33.el6     @osg
osg-configure-pbs.noarch             1.9.1-1.osg33.el6    @osg
osg-htcondor-ce-pbs.x86_64           3.3-13.osg33.el6     @osg

The pbs binaries are in common path&#58;

osgce&#58;/usr # ls -l /usr/bin | grep &#39;qstat&#92;|qsub&#39;
-rwxr-xr-x   1 root root      41411 Aug  6 02&#58;00 condor_qsub
-rwxr-xr-x   1 root root      58650 Dec 16  2016 qstat
-rwxr-xr-x   1 root root      75549 Dec 16  2016 qsub

I forget to use osg-configure before opening this ticket, sorry for that.
First the output was as follows&#58;

osgce&#58;/etc/blahp # osg-configure -c

WARNING  No allowed_vos specified for section &#39;Subcluster&#39;.
WARNING  In OSG 3.4, you will be required to specify either a list of VOs, or a &#39;*&#39; to use an
autodetected VO list based on the user accounts available on your CE.
WARNING  No allowed_vos specified for section &#39;Subcluster&#39;.
WARNING  In OSG 3.4, you will be required to specify either a list of VOs, or a &#39;*&#39; to use an
autodetected VO list based on the user accounts available on your CE.
CRLs exist, skipping fetch-crl invocation
WARNING  Backing up /etc/lcmaps.db to /etc/lcmaps.db.pre-configure
/usr/sbin/osg-configure completed

So I&#39;ve set the setting &#39;allowed_vos = star&#39;, and after that &#58;

osgce&#58;/etc/condor-ce/config.d # osg-configure -c
<div id='show_1818174595' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1818174595'>CRLs exist, skipping fetch-crl invocation
/usr/sbin/osg-configure completed
osgce&#58;/etc/condor-ce/config.d # osg-configure -v
Configuration verified successfully

So it&#39;s OK with configuration.
Then I&#39;ve restarted the htcondor-ce, but all the jobs are still got to hold status with the same error &#39;Error parsing classad or job not found&#39;.
And pbs is still consider these jobs like completed.
So adding the setting &#39;allowed_vos = star&#39; not helped.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_1818174595').click(function() {
            $('#detail_1818174595').slideDown("normal");
            $('#show_1818174595').hide();
            $('#hide_1818174595').show();
        });
        $('#hide_1818174595').click(function() {
            $('#detail_1818174595').slideUp();
            $('#hide_1818174595').hide();
            $('#show_1818174595').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1503504010'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-23T16:00:10+00:00">Aug 23, 2017 04:00 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1503504010">&nbsp;</a></div><pre>Evgeniy,

A few questions&#58;

- Do you have a PBS backend? If so, where are your PBS binaries (e.g. qstat, qsub)? If it&#39;s not &#34;/usr/bin&#34;, you should set the pbs_binpath in /etc/blah.config to the correct location.
- Are you using osg-configure to manage the configuration of your CE?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='34702#1503474428'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-23T07:47:08+00:00">Aug 23, 2017 07:47 AM UTC</time><a class="anchor" name="1503474428">&nbsp;</a></div><pre>blahp version  - blahp.x86_64                         1.18.32.bosco-1.osg33.el6

blah.config renamed to blah.config.txt otherwise it can not be attached.
Also attached /etc/blahp/pbs_local_submit_attributes.sh, just in case.

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov</pre></div><div class='update_description'><i onclick="document.location='34702#1503421355'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-22T17:02:35+00:00">Aug 22, 2017 05:02 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1503421355">&nbsp;</a></div><pre>What version of the blahp do you have installed? Can you attach /etc/blah.config?</pre></div><div class='update_description'><i onclick="document.location='34702#1503070103'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-18T15:28:23+00:00">Aug 18, 2017 03:28 PM UTC</time> by <b>Matyas Selmeci</b><a class="anchor" name="1503070103">&nbsp;</a></div><pre>Brian, can you take a look when you get back?

Thanks,
-Mat</pre></div><div class='update_description'><i onclick="document.location='34702#1502965720'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-17T10:28:40+00:00">Aug 17, 2017 10:28 AM UTC</time><a class="anchor" name="1502965720">&nbsp;</a></div><pre>Today installed packages are as follows&#58;

# yum list installed | grep condor
condor.x86_64                        8.4.12-2.1.osg33.el6 @osg
condor-classads.x86_64               8.4.12-2.1.osg33.el6 @osg
condor-procd.x86_64                  8.4.12-2.1.osg33.el6 @osg
condor-python.x86_64                 8.4.12-2.1.osg33.el6 @osg
htcondor-ce.noarch                   2.2.2-1.osg33.el6    @osg
htcondor-ce-client.noarch            2.2.2-1.osg33.el6    @osg
htcondor-ce-pbs.noarch               2.2.2-1.osg33.el6    @osg
osg-htcondor-ce.x86_64               3.3-13.osg33.el6     @osg
osg-htcondor-ce-pbs.x86_64           3.3-13.osg33.el6     @osg

On 13&#39;th July 2017 was previous update and htcondor-ce updated packages were as follows&#58;

htcondor-ce.noarch 0&#58;2.2.1-1.osg33.el6
htcondor-ce-client.noarch 0&#58;2.2.1-1.osg33.el6
htcondor-ce-pbs.noarch 0&#58;2.2.1-1.osg33.el6

After this update HTCondor-CE was working correctly.

I&#39;ve found my records on 28 April 2017 &#58;

#yum list installed | grep condor
condor.x86_64                        8.4.11-1.1.osg33.el6 @osg/6.8
condor-classads.x86_64               8.4.11-1.1.osg33.el6 @osg/6.8
condor-procd.x86_64                  8.4.11-1.1.osg33.el6 @osg/6.8
condor-python.x86_64                 8.4.11-1.1.osg33.el6 @osg/6.8
htcondor-ce.noarch                   2.1.5-1.osg33.el6    @osg/6.8
htcondor-ce-client.noarch            2.1.5-1.osg33.el6    @osg/6.8
htcondor-ce-pbs.noarch               2.1.5-1.osg33.el6    @osg/6.8
osg-htcondor-ce.x86_64               3.3-12.osg33.el6     @osg/6.8
osg-htcondor-ce-pbs.x86_64           3.3-12.osg33.el6     @osg/6.8

So I guess the last working HTCondor-CE version  was as follows&#58;
<div id='show_1488315131' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1488315131'>osg-htcondor-ce-pbs.x86_64           3.3-12.osg33.el6     @osg/6.8
condor.x86_64                        8.4.11-1.1.osg33.el6 @osg/6.8
htcondor-ce-pbs.noarch 2.2.1-1.osg33.el6

I can not say more exactly because all logs are overwriten. I&#39;ll tune later the HTCondor-CE to keep more old logs.

Evgeniy

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov
</div><script type='text/javascript'>
        $('#show_1488315131').click(function() {
            $('#detail_1488315131').slideDown("normal");
            $('#show_1488315131').hide();
            $('#hide_1488315131').show();
        });
        $('#hide_1488315131').click(function() {
            $('#detail_1488315131').slideUp();
            $('#hide_1488315131').hide();
            $('#show_1488315131').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34702#1502916416'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-16T20:46:56+00:00">Aug 16, 2017 08:46 PM UTC</time> by <b>echism</b><a class="anchor" name="1502916416">&nbsp;</a></div><pre>Which was the last working HTCondor-CE version for you?

Thanks</pre></div><div class='update_description'><i onclick="document.location='34702#1502869324'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-08-16T07:42:04+00:00">Aug 16, 2017 07:42 AM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1502869324">&nbsp;</a></div><pre>Hi, all.
Our team has been using HTCondor-CE as CE and PBS as batch system.
After recent update of OSG packages on CE all the jobs are going instantly to hold status, while pbs consider theese jobs are all been completed.

That&#39;s how it looks like on CE
# condor_ce_q -l 55813.0 -attr HoldReason
HoldReason = &#34;Error parsing classad or job not found&#34;

Herein after are some outputs form CE Gridmanager log for job number 55813.0

# less GridmanagerLog.star001.20170815T150000
/55813.0

08/15/17 20&#58;01&#58;43 [2950329] Using job type INFNBatch for job 55813.0
08/15/17 20&#58;01&#58;43 [2950329] (55813.0) SetJobLeaseTimers()
08/15/17 20&#58;01&#58;44 [2950329] Found job 55813.0 --- inserting
08/15/17 20&#58;01&#58;44 [2950329] Fetched 1 new job ads from schedd

...

08/15/17 20&#58;01&#58;54 [2950329] Updating classad values for 55813.0&#58;
08/15/17 20&#58;01&#58;54 [2950329]    DelegatedProxyExpiration = 1503680486
08/15/17 20&#58;01&#58;54 [2950329]    GridJobId = &#34;batch pbs osgce.jinr.ru_9619_osgce.jinr.ru#55813.0#1502816498 pbs/20170815/9151649&#34;
08/15/17 20&#58;01&#58;54 [2950329] leaving doContactSchedd()
08/15/17 20&#58;01&#58;54 [2950329] (55813.0) doEvaluateState called&#58; gmState GM_SUBMIT_SAVE, remoteState 0
08/15/17 20&#58;01&#58;54 [2950329] (55813.0) gm state change&#58; GM_SUBMIT_SAVE -&#62; GM_SUBMITTED
08/15/17 20&#58;02&#58;40 [2950329] Received CHECK_LEASES signal
08/15/17 20&#58;02&#58;40 [2950329] in doContactSchedd()
08/15/17 20&#58;02&#58;40 [2950329] querying for renewed leases
08/15/17 20&#58;02&#58;40 [2950329] querying for removed/held jobs
08/15/17 20&#58;02&#58;40 [2950329] Using constraint ((Owner=?=&#34;star001&#34;&&JobUniverse==9)) && ((Managed =!= &#34;ScheddDone&#34;)) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= &#34;External&#34;))
08/15/17 20&#58;02&#58;40 [2950329] Fetched 0 job ads from schedd
08/15/17 20&#58;02&#58;40 [2950329] leaving doContactSchedd()
08/15/17 20&#58;02&#58;44 [2950329] GAHP[2950331] &#60;- &#39;RESULTS&#39;
08/15/17 20&#58;02&#58;44 [2950329] GAHP[2950331] -&#62; &#39;S&#39; &#39;0&#39;
<div id='show_756553704' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_756553704'>08/15/17 20&#58;02&#58;45 [2950329] Evaluating staleness of remote job statuses.
08/15/17 20&#58;02&#58;54 [2950329] (55813.0) doEvaluateState called&#58; gmState GM_SUBMITTED, remoteState 0
08/15/17 20&#58;02&#58;54 [2950329] (55813.0) gm state change&#58; GM_SUBMITTED -&#62; GM_POLL_ACTIVE
08/15/17 20&#58;02&#58;54 [2950329] GAHP[2950331] &#60;- &#39;BLAH_JOB_STATUS 3 pbs/20170815/9151649&#39;
08/15/17 20&#58;02&#58;54 [2950329] GAHP[2950331] -&#62; &#39;S&#39;
08/15/17 20&#58;02&#58;54 [2950329] GAHP[2950331] &#60;- &#39;RESULTS&#39;
08/15/17 20&#58;02&#58;54 [2950329] GAHP[2950331] -&#62; &#39;R&#39;
08/15/17 20&#58;02&#58;54 [2950329] GAHP[2950331] -&#62; &#39;S&#39; &#39;1&#39;
08/15/17 20&#58;02&#58;54 [2950329] GAHP[2950331] -&#62; &#39;3&#39; &#39;1&#39; &#39;Error parsing classad or job not found&#39; &#39;0&#39; &#39;N/A&#39;
08/15/17 20&#58;02&#58;54 [2950329] (55813.0) doEvaluateState called&#58; gmState GM_POLL_ACTIVE, remoteState 0
08/15/17 20&#58;02&#58;54 [2950329] (55813.0) gm state change&#58; GM_POLL_ACTIVE -&#62; GM_SUBMITTED
08/15/17 20&#58;03&#58;40 [2950329] Received CHECK_LEASES signal
08/15/17 20&#58;03&#58;40 [2950329] in doContactSchedd()

I see the job 55813.0 has been submitted to batch and have got the number 9151649, and then have got &#39;Error parsing classad or job not found&#39;

Go to batch server.
Herein after are some outputs form pbs log for job 9151649.

# less 20170815 | grep 9151649
08/15/2017 20&#58;01&#58;53;0100;PBS_Server.12645;Job;9151649.lxbsrv01.jinr.ru;enqueuing into star, state 1 hop 1
08/15/2017 20&#58;01&#58;53;0008;PBS_Server.12645;Job;req_commit;job_id&#58; 9151649.lxbsrv01.jinr.ru
08/15/2017 20&#58;02&#58;14;0008;PBS_Server.12685;Job;9151649.lxbsrv01.jinr.ru;Job Run at request of root@....
08/15/2017 20&#58;02&#58;14;0008;PBS_Server.12685;Job;9151649.lxbsrv01.jinr.ru;Not sending email&#58; job requested no e-mail
08/15/2017 20&#58;07&#58;18;0010;PBS_Server.16379;Job;9151649.lxbsrv01.jinr.ru;Exit_status=0 resources_used.cput=00&#58;00&#58;00 resources_used.mem=3424kb resources_used.vmem=33528kb resources_used.walltime=00&#58;05&#58;03
08/15/2017 20&#58;07&#58;18;0008;PBS_Server.16379;Job;9151649.lxbsrv01.jinr.ru;Not sending email&#58; job requested no e-mail
08/15/2017 20&#58;07&#58;18;0008;PBS_Server.12664;Job;9151649.lxbsrv01.jinr.ru;on_job_exit valid pjob&#58; 9151649.lxbsrv01.jinr.ru (substate=50)
08/15/2017 20&#58;10&#58;27;0100;PBS_Server.12632;Job;9151649.lxbsrv01.jinr.ru;dequeuing from star, state COMPLETE

So the batch consider the job 9151649 as COMPLETE and everything looks OK for batch, but not for CE. CE keeps the corresponding job 55813.0 in hold status.

How to make CE to process the jobs correctly ?

Evgeniy.

by /C=RU/O=RDIG/OU=users/OU=jinr.ru/CN=Evgeniy Kuznetsov

</div><script type='text/javascript'>
        $('#show_756553704').click(function() {
            $('#detail_756553704').slideDown("normal");
            $('#show_756553704').hide();
            $('#hide_756553704').show();
        });
        $('#hide_756553704').click(function() {
            $('#detail_756553704').slideUp();
            $('#hide_756553704').hide();
            $('#show_756553704').show();
        });
        </script></pre></div><legend>Similar Recent Tickets <small>modified within the last 30 days</small></legend><div id="similar_tickets"><p class="muted">No similar tickets found.</p></div>
</div>
<script type="text/javascript">
function reset_anchor() {
    $("#updates .selected").removeClass("selected");
    var urls = document.location.toString().split('#'); 
    var anchor = urls[1];
    if(anchor) {
        $("a[name='"+anchor+"']").parents(".update_description").addClass("selected");
    }
}
function submitspam(ticket_id) {
    myret = confirm("Would you like to close this ticket as a security ticket, and submit the ticket content to akismet?");
    if(myret == true) {
        $.ajax("viewer/processspam?id="+ticket_id).done(function() {
            window.location.reload();
        });
    }
}

$(function() {
    reset_anchor();
    var ADDITIONAL_COOKIE_NAME = 'gocticket';
    var options = { path: '/', expires: 365};

    if(window.opener && window.opener.name == "gocticket_list") {
        v = $.cookie("closewindow");
        if(!v) {
            $("#closewindow").attr("checked", "checked"); //on by default
        } else {
            if(v == "checked") {
                $("#closewindow").attr("checked", "checked");
            }
        }
        $("#closewindow").click(function() {
            $.cookie("closewindow", $(this).attr('checked'), options);
        });
    } else {
        $("#closewindow_area").hide();
    }
    function updateTimeago() {
        $("time").timeago();
        setTimeout(updateTimeago, 30*1000);
    }
    updateTimeago();
    $(".description").focus(expand_description);
});
</script>
<hr/>
<footer>
<p>GOC Ticket Version 2.2 | <a href="https://ticket.opensciencegrid.org/goc/submit?app_issue_check=on&amp;app_issue_type=goc&amp;app_goc_url=https%3A%2F%2Fticket.opensciencegrid.org%3A443%2F34702">Report Bugs</a>
 | <a href="https://github.com/opensciencegrid/operations/blob/master/docs/privacy.md">Privacy Policy</a>
</p>

<p> <img align="top" src="images/tag_orange.png"/> Copyright 2018 The Trustees of Indiana University - Developed for Open Science Grid</p>
</footer>


</div><!--container-fluid-->
<script src="https://ticket1.grid.iu.edu:8443/socket.io/socket.io.js"></script>
<script>
var chat = io.connect('https://ticket1.grid.iu.edu:8443');
chat.on('connect', function() {
    chat.emit('authenticate', {nodekey:'', ticketid: 34702});
});
chat.on('peers', function(peers) {
    $("#peers").html("");
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('peer_disconnect', function(pid) {
    $("#peer_"+pid).hide("slow");
});
chat.on('peer_connected', function(peers) {
    //expect only 1 peer connecting, but..
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('submit', function() {
    if(confirm("This ticket was updated. Do you want to refresh?")) {
        history.go(0);
    }
});

function addPeer(pid, peer) {
    var ipinfo = "";
    if(peer.ip != undefined) {
        ipinfo = "<span class=\"ip\">"+peer.ip+"</span>";
    }
    if(chat.io.engine.id == pid) {
        //don't display myself
        return;
    }
    var html = "<li class=\"new\" id=\"peer_"+pid+"\" class=\"peer\">"+peer.name+ipinfo+"</li>";
    $("#peers").prepend(html);
    $("#peers .new").animate({bottom: 0}, 1000, function() {$(this).removeClass("new")});
}

$(function() {
    $("#ticket_form").submit(function() {
        chat.emit('submit');
        return true;
    });
});
</script>
<script>
//used by searchbox
function parseValue(value) {
    var obj = new Object();
    var tokens = value.split("\t");
    obj.str = tokens[0];
    obj.count = tokens[1];
    return obj;
}

$(function() {
    //bootstrap-2.0.4 stuff
    $(".alert-message").alert();
    $(".dropdown-toggle").dropdown();
    $("span[rel='tooltip']").tooltip();
    $("a[rel=popover]").popover();

    //activate menu that user is currently on
    $("#menu_navigator").addClass("active"); 
    $("#submenu_").addClass("active"); 

    //translate zend validation error message to bootstrap
    $(".errors").addClass("alert").addClass("alert-error");

    //enable autocomplete for search box
    $("#search").autocomplete({
        source: function( request, response ) {
            $.ajax({
                url: "search/autocomplete",
                dataType: "text",
                data: {
                    //featureClass: "P",
                    //style: "full",
                    //maxRows: 12,
                    //name_startsWith: request.term
                    q: request.term
                },
                success: function( data ) {
                    response( $.map( data.split("\n"), function( item ) {
                        if(item == "") return null;
                        return {
                            value: item
                        }
                    }));
                }
            });
        },
        select: function(event, ui) {
            document.location = "search?q="+ui.item.value;
        }
    });
    
});
</script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-69012-13");
pageTracker._trackPageview();
} catch(err) {}
</script>

</body>
