<!DOCTYPE html>
<html lang="en">
  <head>
  <base href="">
    <title>[6567] TTU overwhelmed with jobs</title>    <meta charset="utf-8" />
    <meta name="verify-v1" content="na5IcAJsZVOfEkboRxuIiZ1zpZgnZiWra+nKcS7nA/o=" />
    <meta name="google-site-verification" content="DLrk3ft4s8b-S2TloLCL2LD_t6wcTjgSluf5pmiu2kA" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <link href="https://ticket.opensciencegrid.org/rss" rel="alternate" type="application/rss+xml" title="GOC Ticket Update feed" />

    <style type="text/css">
      body {
        padding-top: 50px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
     #search {
            width: 300px;
     }

    </style>

<script src="https://code.jquery.com/jquery-3.0.0.js"></script>
<script src="https://code.jquery.com/jquery-migrate-3.0.1.js"></script>

   <link href="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet"/>
    <script src="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"/>
    <link href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.min.css" rel="stylesheet"/>
 <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>


    <link href="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/css/select2.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/js/select2.min.js"></script>

    <link href="css/ticket.css" rel="stylesheet" />
    <script src="lib/jquery.cookie.js"></script>

    <link href="images/tag_orange.png" rel="icon" type="image/png"/>
    <script src='https://www.google.com/recaptcha/api.js'></script>
  </head>

  <body>
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
            <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </a>

            <a class="brand" style="padding: 6px 0px 0px 6px;" href="http://opensciencegrid.org"><img src="images/osglogo.40x30.png"/></a>
            <ul class="nav">
                <li class="dropdown"><a href="https://ticket.opensciencegrid.org/#" class="dropdown-toggle" data-toggle="dropdown">GOC Ticket <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    <li><a href="https://my.opensciencegrid.org">MyOSG</a></li>
                    <li><a href="https://oim.opensciencegrid.org">OIM</a></li>
                    <li class="active"><a href="https://ticket.opensciencegrid.org/index">Ticket</a></li>
	<li class="divider"></li>
	<li><a href="http://repo.grid.iu.edu">Repo</a></li>
	<li class="divider"></li>
	<li><a href="http://blogs.grid.iu.edu">Blog</a></li>
                    <li><a href="http://display.grid.iu.edu">Display</a></li>
                    <li><a href="http://osggoc.blogspot.com/">News</a></li>
                    </ul>
                </li>
            </ul>
            <ul class="nav pull-right">
                <li><a href="https://ticket.opensciencegrid.org/sso/">Login</a></li>            </ul>

            <div class="nav-collapse">
                <ul class="nav">
			 <li id="menu_submit"><a href="https://ticket.opensciencegrid.org/submit">Submit</a></li><li id="menu_view" class="dropdown"><a href="https://ticket.opensciencegrid.org/\#" class="dropdown-toggle" data-toggle="dropdown">View <b class="caret"></b></a><ul class="dropdown-menu"><li id="submenu_listopen"><a href="https://ticket.opensciencegrid.org/list/open">Open Tickets</a></li><li id="submenu_listrecentclose"><a href="https://ticket.opensciencegrid.org/list/recentclose">Recently Closed Tickets</a></li><li class="divider"></li><li id="submenu_alltickets"><a href="https://ticket.opensciencegrid.org/search?q=&amp;sort=id">All Tickets</a></li></ul></li>                </ul>

                <form class="navbar-search pull-right" action="https://ticket.opensciencegrid.org/viewer">
                    <input id="search" type="text" name="id" class="search-query span2" placeholder="Search Ticket" value=""/>
                </form>
            </div>
        </div>
      </div>
    </div>

<script type='text/javascript' src='lib/jquery.timeago.js'></script>
<script type='text/javascript' src='lib/byte2size.js'></script>
<style>
#updates .toolbar {
position: relative;
margin-top: 0px;
top: -10px;
font-weight: normal;
}
#updates a.anchor {
position: relative;
top: -50px;
}
#updates .selected pre {
animation:selected 2s;
animation-iteration-count: 2;
animation-direction: alternate;
-webkit-animation:selected 2s; 
-webkit-animation-iteration-count: 2;
-webkit-animation-direction: alternate;
box-shadow: inset 1px 1px 20px #9ad;
border: 1px solid #9ab;
margin: 5px 0px;
padding-left: 10px;
}
@keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ab;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
@-webkit-keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ad;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
#updates pre {
background-color: inherit;
line-height: 15px;
padding: 5px;
}
#updates .header {
color: #999;
}
#updates .update_history pre {
background-color: #eee;
color: #666;
font-size: 85%;
}
#updates .clickable {
cursor: pointer;
}
#updates .clickable:hover {
color: #D98719;
}
#updates .meta_information pre {
background-color: #fed;
}
#similar_tickets {
max-height: 300px;
overflow-y: auto;
pointer-events: none;
padding: 5px;
background-color: #f4f4f4;
}
.btn-toolbar {
margin-bottom: 0;
height: 30px;
}
#peers {
position: fixed;
bottom: 0px;
right: 0px;
z-index: 100;
list-style: none;
padding: 5px 0px 0px 5px;
margin: 0px;
background-color: white;
box-shadow: 0px 0px 10px white;
}
#peers li {
background-color: #ccc;
color: #000;
display: inline-block;
padding: 5px 10px;
margin-right: 5px;
position: relative;
}
/*
#peers li:hover {
background-color: #999;
cursor: pointer;
}
*/
#peers span.ip {
padding-left: 5px;
color: #666;
}
#peers .new {
bottom: -30px;
}
/*
#peers .me {
background-color: red;
}
*/
</style>

<div class="container-fluid">
<ul id="peers"></ul>
<div class="alert alert-danger"><a class="close" href="https://ticket.opensciencegrid.org/#" data-dismiss="alert">&times;</a>By the end of May 2018, the ticketing system at https://ticket.opensciencegrid.org will be retired and support will be provided at https://support.opensciencegrid.org. Throughout this transition the support email (help@opensciencegrid.org) will be available as a point of contact.<br><br>                                                   
                                                                                                                                                                                   
Please see the service migration page for details: https://opensciencegrid.github.io/technology/policy/service-migrations-spring-2018/#ticket</div><div id="presence" class="pull-right"></div><div class="ticketgui"><script type="text/javascript" src="lib/checktab.js"></script>

<script>
var expanded = false;
function expand_description() {
    var desc = $(".description");
    if(!expanded) {
        expanded = true;
        //expand to minheight
        var min = 250;
        if(desc.height() < min) {
            desc.animate({height: min}, 200);
        }
    }
}

$(document).ready(function() {
    $("input[name='nad']").datepicker({
        dateFormat: 'yy-mm-dd'
    });
});

</script>



<style>
.form-horizontal .control-label {
padding-top: inherit;
font-size:90%;
color:#666;
}
label {
margin-bottom: 0px;
}
.controls {
line-height: 18px;
}
</style>
<form class="form-horizontal" action="https://ticket.opensciencegrid.org/viewer/updatebasic?id=6567" method="post">
<div class="page-header">
    <h3><span class="muted">6567</span> / TTU overwhelmed with jobs</h3>
</div>

<div class="row-fluid">
<div class="span5">
    <legend>Contact</legend>
    <div class="control-group">
        <label class="control-label">Full Name</label>
        <div class="controls">Alan Sill</div>
    </div>
    <div class="control-group">
        <label class="control-label">Email</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">Phone</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">CC</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>

    <legend>Details</legend>
    
    <div class="control-group">
        <label class="control-label">Ticket Type</label>
        <div class="controls">Problem/Request</div>
    </div>
    <div class="control-group">
        <label class="control-label">Priority</label>
        <div class="controls">Normal</div>
    </div>
    <div class="control-group">
        <label class="control-label">Status</label>
        <div class="controls">
Closed</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action</label>
        <div class="controls">review</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action Deadline</label>
        <div class="controls flag_red">2009-04-08</div>
    </div>

</div><!--span-->
<div class="span7">
    <legend>Assignees</legend>
    <div class="assignee" style="width: 60%">Kyle Gross <span class="muted"> / OSG GOC Support Team</span></div><div class="assignee" style="width: 60%">Thomas Lee <span class="muted"> / OSG Operations Infrastructure</span></div>    <br>

    <legend>Assignees</legend>
    TODO
    <br>

    <style>
legend.noborder {
border-bottom: none;
}
</style>

<div id="attachment-list"/>
<script>
$(function () {
    var first = true;
    $.getJSON("https://ticket.opensciencegrid.org/attachment/list?id=6567", function (files) {
        //console.dir(files);
        var html = "<table class=\"table table-condensed\">";
        $(files).each(function() {
            if(first) {
                first = false;
                html += "<legend class=\"noborder\">Attachmenets</legend>";
            }
            html += "<tr class=\"attachment\">";
            html += "<td><img src="https://ticket.opensciencegrid.org/\&quot;&quot;+this.thumbnail_url+&quot;\&quot;/"></td>";
            html += "<td><a href="https://ticket.opensciencegrid.org/\&quot;&quot;+this.url+&quot;\&quot;" target=\"_blank\">"+this.name+"</a></td>";
            html += "<td>"+bytesToSize(this.size, 1)+"</td>";
            html += "</tr>";
        });
        html += "</table>";
        $("#attachment-list").html(html);
    });
});

function download(url) {
    window.open(url, "_blank");
}
</script>


</div><!--span-->
</div><!--row-fluid-->


</form>

</div>
<div id="updates" style="clear: both;">
    <legend>Past Updates
    <div class="btn-toolbar pull-right toolbar">
        <div class="btn-group">
                <a class="btn btn-small" href="https://ticket.opensciencegrid.org/6567?sort=up&amp;"><i class="icon-arrow-up"></i> Sort</a>

        
        <a class="btn btn-small" href="https://ticket.opensciencegrid.org/6567?expandall=true&amp;">Expand Descriptions</a>        <a class="btn btn-small" target="_blank" href="mailto:osg@tick.globalnoc.iu.edu?subject=Open%20Science%20Grid%3A%20TTU%20overwhelmed%20with%20jobs%20ISSUE%3D6567%20PROJ%3D71"><i class="icon-envelope"></i> Update w/Email</a>
        </div>
    </div><!--btn-toolbar-->
    </legend>

    <div class='update_description'><i onclick="document.location='6567#1239301663'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-04-09T18:27:43+00:00">Apr 9, 2009 06:27 PM UTC</time> by <b>Kyle Gross</b><a class="anchor" name="1239301663">&nbsp;</a></div><pre>Resolving for lack of response.</pre></div><div class='update_description'><i onclick="document.location='6567#1238592571'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-04-01T13:29:31+00:00">Apr 1, 2009 01:29 PM UTC</time> by <b>Kyle Gross</b><a class="anchor" name="1238592571">&nbsp;</a></div><pre>Pinging again&#58;</pre></div><div class='update_description'><i onclick="document.location='6567#1238164329'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-27T14:32:09+00:00">Mar 27, 2009 02:32 PM UTC</time> by <b>Kyle Gross</b><a class="anchor" name="1238164329">&nbsp;</a></div><pre>Alan,

Are there three tickets would you like us to make for this?

Please submit problems, requests, and questions at&#58;
<a href='http&#58;//oim.grid.iu.edu/gocticket' target='_blank' rel='nofollow'>http&#58;//oim.grid.iu.edu/gocticket</a>

Thank You,
OSG Grid Operations Center
goc@...., 317-278-9699
Visit the OSG Support Page&#58;
<a href='http&#58;//www.opensciencegrid.org/ops' target='_blank' rel='nofollow'>http&#58;//www.opensciencegrid.org/ops</a>
RSS&#58; <a href='http&#58;//www.grid.iu.edu/news' target='_blank' rel='nofollow'>http&#58;//www.grid.iu.edu/news</a></pre></div><div class='update_description'><i onclick="document.location='6567#1237848843'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-23T22:54:03+00:00">Mar 23, 2009 10:54 PM UTC</time> by <b>Christopher Pipes</b><a class="anchor" name="1237848843">&nbsp;</a></div><pre>Hi,

This ticket has apparently nothing to do with GridScan, at least here at TTU, but has
uncovered many issues.

It seems that practically no one in OSG has the regular habit of cleaning out their
old gram log files and scratch directories.  We, as a collaboratory effort, should
publish a good script for doing this and get VO sign-off on it so that we can put it
into force.  Temporarily Ai have asked Steve Timm if he could share his.

Better procedures for temporarily blocking individual users would also be helpful.
IN this case, it took a while, but I believe our problems mostly boil down to a glow
user, and Frank&#39;s jobs were caught in the diagnostic flux while we tried to figure it
out.

This does not reduce my concernts about the behavior of the job sthat were running
under Frank&#39;s proxy!  These have an odd characteristic of apparently trying to submit
themselves to any queue available on teh cluster, and not to the queues advertised
through BDII, so I would like to understand that also.

In any case it looks like we have at least 3 different tickets here.

Thanks,
Alan</pre></div><div class='update_description'><i onclick="document.location='6567#1237836400'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-23T19:26:40+00:00">Mar 23, 2009 07:26 PM UTC</time> by <b>echism</b><a class="anchor" name="1237836400">&nbsp;</a></div><pre>Alan&#58;

Gridscan runs once every four hours, so the Gridscan metrics look about right from where I&#39;m sitting. Our jobs
may look pretty large in number, but they are all going to be insignificant jobs like echo $SOME_LOCATION. I
understand that the small jobs can contribute to a larger problem when you&#39;re just completely overwhelmed,
but other than that scenario, are you comfortable with what we&#39;re doing?

Iwona&#58;

I am glad that it was just a false alarm and thank you for the update. I will go ahead and drop you from this
ticket and keep looking into Alan&#39;s issue.

Thank you all,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='6567#1237390211'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-18T15:30:11+00:00">Mar 18, 2009 03:30 PM UTC</time> by <b>VDT</b><a class="anchor" name="1237390211">&nbsp;</a></div><pre>On Mar 17, 2009, at 10&#58;30 PM, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; Frank&#39;s jobs seem to set or be launched from a glide-in and seem to</font>
<font color='#7F7E6F'>&#62; loop over all of our queues, ignoring the BDII info or our advertised</font>
<font color='#7F7E6F'>&#62; OSG queue set.  This causes each of them to consume time on our head</font>
<font color='#7F7E6F'>&#62; node, and also eventually seem to run on jobmanager-fork?  It&#39;s hard</font>
<font color='#7F7E6F'>&#62; to tell from the gram job logs.  Also, he needs to clear out log files</font>
<font color='#7F7E6F'>&#62; in his home directory, as these are piling up.</font>

I think the GOC (and/or you) needs to contact Frank about this.

<font color='#7F7E6F'>&#62; Also, interestingly, the log file rotation seems not to be working,</font>
<font color='#7F7E6F'>&#62; even though it is enabled in vdt-control.   How did that stop</font>
<font color='#7F7E6F'>&#62; working?  It doesn&#39;t actually seem to have rotated this log file since</font>
<font color='#7F7E6F'>&#62; Oct. 2008!</font>

I have recently become aware of a bug in our logrotate setup, and I
wasn&#39;t aware it was a problem for anyone yet. If the VDT updates the
logrotate package, we lose the configuration. This is bad. &#58;( We&#39;ll
fix it.

-alain</pre></div><div class='update_description'><i onclick="document.location='6567#1237347011'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-18T03:30:11+00:00">Mar 18, 2009 03:30 AM UTC</time> by <b>TIGRE</b><a class="anchor" name="1237347011">&nbsp;</a></div><pre>I checked the very large (2 GB!) globus-gatekeeper.log that was in
place when the problem occurred today, and found that over time, there
were more samgrid jobs than ones from Frank, but today there was a
surge, as shown below.

We did have some unexpected downtime over the weekend due to chilled
water work, but I thought things were stable after that was over.

Frank&#39;s jobs seem to set or be launched from a glide-in and seem to
loop over all of our queues, ignoring the BDII info or our advertised
OSG queue set.  This causes each of them to consume time on our head
node, and also eventually seem to run on jobmanager-fork?  It&#39;s hard
to tell from the gram job logs.  Also, he needs to clear out log files
in his home directory, as these are piling up.

Also, interestingly, the log file rotation seems not to be working,
even though it is enabled in vdt-control.   How did that stop
working?  It doesn&#39;t actually seem to have rotated this log file since
Oct. 2008!

None of this rules out Gridscan, but it does not seem to be the
dominant job.  Has Iwona seen anythhing, and/or should we split these
tickets?

Top 10 on TTU-Antaeus since Oct 7, 2008&#58;

459415 /DC=org/DC=doegrids/OU=People/CN=Joel M. Snow 647405
231750 /DC=org/DC=doegrids/OU=People/CN=Frank Wuerthwein 699373
74207 /DC=org/DC=doegrids/OU=People/CN=Mark Silberstein 410390
39862 /DC=org/DC=doegrids/OU=Services/CN=rsv/antaeus.hpcc.ttu.edu
11747 /DC=gov/DC=fnal/O=Fermilab/OU=Robots/CN=fermigrid0.fnal.gov/
CN=cron/CN=Keith Chadwick/CN=UID&#58;chadwick
10178 /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/
CN=Andrea Sciaba
9590 /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=asciaba/CN=430796/
<div id='show_1520837489' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1520837489'>CN=Andrea Sciaba
8512 /C=IT/O=INFN/OU=Personal Certificate/L=Roma 1/CN=Alessandro
Di Girolamo
7351 /DC=org/DC=doegrids/OU=People/CN=Feng Zhao 232454
6311 /DC=org/DC=doegrids/OU=People/CN=Rob Quick (GridScan) 445196

Top 10 in March&#58;

[root@antaeus save_2009-03-17]# grep &#34;2009-03-&#34; globus-gatekeeper.log
| grep PERMIT | cut -f 2 -d &#92;&#34; | sort | uniq -c | sort -rn
239422 /DC=org/DC=doegrids/OU=People/CN=Joel M. Snow 647405
231750 /DC=org/DC=doegrids/OU=People/CN=Frank Wuerthwein 699373
5922 /DC=org/DC=doegrids/OU=People/CN=Mark Silberstein 410390
5621 /DC=org/DC=doegrids/OU=Services/CN=rsv/antaeus.hpcc.ttu.edu
4245 /DC=org/DC=doegrids/OU=People/CN=Feng Zhao 232454
2449 /DC=org/DC=doegrids/OU=People/CN=Andrew J. Schultz 19084
1832 /CN=nanoHUB Service01/OU=Purdue TeraGrid/O=Purdue University/
ST=Indiana/C=US
1197 /DC=gov/DC=fnal/O=Fermilab/OU=Robots/CN=fermigrid0.fnal.gov/
CN=cron/CN=Keith Chadwick/CN=UID&#58;chadwick
1154 /C=IT/O=INFN/OU=Personal Certificate/L=Roma 1/CN=Alessandro
Di Girolamo
874 /DC=org/DC=doegrids/OU=People/CN=Rob Quick (GridScan) 445196

Top 10 today&#58;

[root@antaeus save_2009-03-17]# grep &#34;2009-03-17&#34; globus-
gatekeeper.log | grep PERMIT | cut -f 2 -d &#92;&#34; | sort | uniq -c | sort -
rn
22169 /DC=org/DC=doegrids/OU=People/CN=Frank Wuerthwein 699373
7774 /DC=org/DC=doegrids/OU=People/CN=Joel M. Snow 647405
1165 /DC=org/DC=doegrids/OU=People/CN=Mark Silberstein 410390
205 /DC=org/DC=doegrids/OU=People/CN=Feng Zhao 232454
188 /DC=org/DC=doegrids/OU=Services/CN=rsv/antaeus.hpcc.ttu.edu
187 /CN=nanoHUB Service01/OU=Purdue TeraGrid/O=Purdue University/
ST=Indiana/C=US
104 /DC=org/DC=doegrids/OU=People/CN=Andrew J. Schultz 19084
75 /DC=org/DC=doegrids/OU=People/CN=zhi sun 343390
42 /DC=org/DC=doegrids/OU=People/CN=Mats Rynge 722233
35 /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=asciaba/CN=430796/
CN=Andrea Sciaba
</div><script type='text/javascript'>
        $('#show_1520837489').click(function() {
            $('#detail_1520837489').slideDown("normal");
            $('#show_1520837489').hide();
            $('#hide_1520837489').show();
        });
        $('#hide_1520837489').click(function() {
            $('#detail_1520837489').slideUp();
            $('#hide_1520837489').hide();
            $('#show_1520837489').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='6567#1237343771'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-18T02:36:11+00:00">Mar 18, 2009 02:36 AM UTC</time> by <b>Alain Roy</b><a class="anchor" name="1237343771">&nbsp;</a></div><pre>On Mar 17, 2009, at 6&#58;51 PM, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; root@antaeus grid]# grep PERMIT globus/var/globus-gatekeeper.log | cut</font>
<font color='#7F7E6F'>&#62; -f 2 -d &#92;&#34; | sort | uniq -c | sort -rn</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;     452 /DC=org/DC=doegrids/OU=People/CN=Frank Wuerthwein 699373</font>
<font color='#7F7E6F'>&#62;      79 /DC=org/DC=doegrids/OU=People/CN=Joel M. Snow 647405</font>
<font color='#7F7E6F'>&#62;      62 /DC=org/DC=doegrids/OU=People/CN=Mark Silberstein 410390</font>
<font color='#7F7E6F'>&#62;      58 /DC=org/DC=doegrids/OU=People/CN=Andrew J. Schultz 19084</font>
<font color='#7F7E6F'>&#62;      10 /CN=nanoHUB Service01/OU=Purdue TeraGrid/O=Purdue University/</font>
<font color='#7F7E6F'>&#62; ST=Indiana/C=US</font>
<font color='#7F7E6F'>&#62;       7 /DC=org/DC=doegrids/OU=Services/CN=osgmm-geant4/</font>
<font color='#7F7E6F'>&#62; gratia01.fnal.gov</font>
<font color='#7F7E6F'>&#62;       7 /DC=org/DC=doegrids/OU=People/CN=Feng Zhao 232454</font>
<font color='#7F7E6F'>&#62;       3 /DC=org/DC=doegrids/OU=Services/CN=rsv/antaeus.hpcc.ttu.edu</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; So I got 452 jobs from Frank in a few minutes after turning things</font>
<font color='#7F7E6F'>&#62; on.  As antaeus has only 192 cpus total, and about half that many</font>
<font color='#7F7E6F'>&#62; batch slots for general grid jobs, this many jobs in that short of a</font>
<font color='#7F7E6F'>&#62; time may be a problem.</font>

Were they fork jobs or batch jobs? It&#39;s not clear from the data above.
I would expect Frank to use Condor-G with the grid monitor, so I
wouldn&#39;t expect a particularly high load from them. Frank&#39;s a nice
guy--we should just talk to him about his jobs.

<font color='#7F7E6F'>&#62; I may have to learn how to configure managed-fork, or otherwise limit</font>
<font color='#7F7E6F'>&#62; OSG job submissions to a reasonable value to avoid overloads.  Is</font>
<font color='#7F7E6F'>&#62; there something we can configure here, or does Frank have some advice</font>
<font color='#7F7E6F'>&#62; about his jobs?</font>

We should talk to Frank about limiting his jobs, as a first step. It&#39;s
not obvious to me that managed fork will help here, unless they are
fork jobs. In the VDT, we have a way of limiting the number of job
managers started, but it&#39;s a bit experimental, so I don&#39;t know if I
should recommend it or not.
<div id='show_559690866' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_559690866'>
-alain
</div><script type='text/javascript'>
        $('#show_559690866').click(function() {
            $('#detail_559690866').slideDown("normal");
            $('#show_559690866').hide();
            $('#hide_559690866').show();
        });
        $('#hide_559690866').click(function() {
            $('#detail_559690866').slideUp();
            $('#hide_559690866').hide();
            $('#show_559690866').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='6567#1237333872'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T23:51:12+00:00">Mar 17, 2009 11:51 PM UTC</time> by <b>TIGRE</b><a class="anchor" name="1237333872">&nbsp;</a></div><pre>My issue on antaeus may be different.  Looking at our log file from a
short  few-minute period in which I turned the VDT services back on, I
see the following.  (I had cleared the globus-gatekeeper.log file
before this, just to be sure.)

root@antaeus grid]# grep PERMIT globus/var/globus-gatekeeper.log | cut
-f 2 -d &#92;&#34; | sort | uniq -c | sort -rn

452 /DC=org/DC=doegrids/OU=People/CN=Frank Wuerthwein 699373
79 /DC=org/DC=doegrids/OU=People/CN=Joel M. Snow 647405
62 /DC=org/DC=doegrids/OU=People/CN=Mark Silberstein 410390
58 /DC=org/DC=doegrids/OU=People/CN=Andrew J. Schultz 19084
10 /CN=nanoHUB Service01/OU=Purdue TeraGrid/O=Purdue University/
ST=Indiana/C=US
7 /DC=org/DC=doegrids/OU=Services/CN=osgmm-geant4/
gratia01.fnal.gov
7 /DC=org/DC=doegrids/OU=People/CN=Feng Zhao 232454
3 /DC=org/DC=doegrids/OU=Services/CN=rsv/antaeus.hpcc.ttu.edu

So I got 452 jobs from Frank in a few minutes after turning things
on.  As antaeus has only 192 cpus total, and about half that many
batch slots for general grid jobs, this many jobs in that short of a
time may be a problem.

I may have to learn how to configure managed-fork, or otherwise limit
OSG job submissions to a reasonable value to avoid overloads.  Is
there something we can configure here, or does Frank have some advice
about his jobs?

I have VDT services off at the moment, and am allowing current jobs to
finish.

Thanks,
Alan

<div id='show_391972631' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_391972631'>On Mar 17, 2009, at 6&#58;36 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_391972631').click(function() {
            $('#detail_391972631').slideDown("normal");
            $('#show_391972631').hide();
            $('#hide_391972631').show();
        });
        $('#hide_391972631').click(function() {
            $('#detail_391972631').slideUp();
            $('#hide_391972631').hide();
            $('#show_391972631').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='6567#1237332971'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T23:36:11+00:00">Mar 17, 2009 11:36 PM UTC</time> by <b>sakrejda@....</b><a class="anchor" name="1237332971">&nbsp;</a></div><pre>Hi,

I just looked at  the accounting log. It&#39;s a pretty regular event,
happened again at 13&#58;17PDT and did not cause any
problems.

I saw in the morning&#58;
[root@pdsfgrid4 root]# lastcomm|grep quick|grep &#34;Tue Mar 17 07&#34;|wc -l
3224

and again early afternoon&#58;
[root@pdsfgrid4 root]# lastcomm|grep quick|grep &#34;Tue Mar 17 13&#34;|wc -l
3224
[root@pdsfgrid4 root]#

Here is a short example of those entries&#58;
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
sed                     rquick   ??         0.00 secs Tue Mar 17 07&#58;18
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
sed                     rquick   ??         0.01 secs Tue Mar 17 07&#58;18
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
sed                     rquick   ??         0.00 secs Tue Mar 17 07&#58;18
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
globus-job-mana    F    rquick   ??         0.00 secs Tue Mar 17 07&#58;18
sed                     rquick   ??         0.00 secs Tue Mar 17 07&#58;18

There are only 18 connections each time and somehow they blossom into
all this activity in the unix accounting files.
So the collapse  of my system at 7&#58;17am must have been a coincidence.

I apologize for this false alarm and withdraw my case,

<div id='show_111646522' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_111646522'>Iwona

On 3/17/09 1&#58;39 PM, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_111646522').click(function() {
            $('#detail_111646522').slideDown("normal");
            $('#show_111646522').hide();
            $('#hide_111646522').show();
        });
        $('#hide_111646522').click(function() {
            $('#detail_111646522').slideUp();
            $('#hide_111646522').hide();
            $('#show_111646522').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='6567#1237332431'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T23:27:11+00:00">Mar 17, 2009 11:27 PM UTC</time> by <b>TIGRE</b><a class="anchor" name="1237332431">&nbsp;</a></div><pre>I kept VDT services off for a while to investigate, but can&#39;t
determine the reason for teh overload yet.

I have kept a copy of the (very large) log files from today in the area

/usr/local/grid/globus/var/save_2009-03-17/

on antaeus, cleared the working ones and restarted with vdt-control --
on.  I am not really happy not knowing what went wrong or how to keep
it from happening again, though.

I have two requests&#58;

1) Can recommended settings for managed-fork be posted, rather than
the present general instructions?   Or if I missed the current
recommendations, would you be kind enough to point them out to us,
please?

2) If you would like to look through our log files, they are in the
area mentioned above.  These can either be pulled over by a grid job,
or you can log on via gsissh and take a look at them directly
yourself.  To log on, set up any VDT client (e.g., the OSG client),
get a proxy and log on using

gsissh antaeus.hpcc.ttu.edu -p 49922

Whichever method you use, here is what you should find.  These files
are really much too large to send, so looking at them or fetching them
directly yourself would be better&#58;

[root@antaeus grid]# du -h /usr/local/grid/globus/var/save_2009-03-17/*
123M	/usr/local/grid/globus/var/save_2009-03-17/accounting.log
4.0K	/usr/local/grid/globus/var/save_2009-03-17/container.log
676K	/usr/local/grid/globus/var/save_2009-03-17/container-real.log
108K	/usr/local/grid/globus/var/save_2009-03-17/globus-condor.log
<div id='show_1190678526' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1190678526'>504K	/usr/local/grid/globus/var/save_2009-03-17/globus-fork.log
1.9G	/usr/local/grid/globus/var/save_2009-03-17/globus-gatekeeper.log
8.6M	/usr/local/grid/globus/var/save_2009-03-17/gridftp.log
</div><script type='text/javascript'>
        $('#show_1190678526').click(function() {
            $('#detail_1190678526').slideDown("normal");
            $('#show_1190678526').hide();
            $('#hide_1190678526').show();
        });
        $('#hide_1190678526').click(function() {
            $('#detail_1190678526').slideUp();
            $('#hide_1190678526').hide();
            $('#show_1190678526').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='6567#1237322351'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T20:39:11+00:00">Mar 17, 2009 08:39 PM UTC</time> by <b>Craig Prescott</b><a class="anchor" name="1237322351">&nbsp;</a></div><pre>Hi Elizabeth;

No, I can&#39;t think of anything in site-verify.pl which would cause this.

When gridscan runs, is it possible that there are outstanding
site-verify runs that are leftover from prior runs?  Seems like a couple
thousand job managers is hard to get to that way, though.

Cheers,
Craig

Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='6567#1237321409'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T20:23:29+00:00">Mar 17, 2009 08:23 PM UTC</time> by <b>echism</b><a class="anchor" name="1237321409">&nbsp;</a></div><pre>Alan,

I understand. Antaeus should be scanned again in about an hour. If you don&#39;t see any trouble from our scan cert
&#34;/DC=org/DC=doegrids/OU=People/CN=Rob Quick (GridScan) 445196&#34; then let me know and I will move this into
another ticket.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='6567#1237320193'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T20:03:13+00:00">Mar 17, 2009 08:03 PM UTC</time> by <b>TIGRE</b><a class="anchor" name="1237320193">&nbsp;</a></div><pre>On Mar 17, 2009, at 2&#58;27 PM, Open Science Grid FootPrints wrote&#58;

<font color='#7F7E6F'>&#62; Please provide us with anything/everything relevant in your logs as</font>
<font color='#7F7E6F'>&#62; it relates to the gridscan problem.</font>

Not sure if it is completely a grid-scan problem -- in fact, all I saw
in terms of globus-gob-maanger at the time were below, which appear to
be samrid and nysgrid jobs primarily.

I can say that repeated (brutal, but effective) &#34;killall globus-job-
manager&#34; commands brought the load back to normal.

I have left VDT services off for now and may have to reboot to recover
all local functionality.  At present, load is fine, and only a few
samgrid jobs remain.

Before the above steps, here were the jobs running with &#34;grid&#34; in them

[root@antaeus ~]# ps -efwww | grep -i grid
samgrid    615     1  0 12&#58;47 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   1612     1  0 12&#58;33 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   1912     1  0 12&#58;47 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   1996     1  0 12&#58;50 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid   2067 31741  0 14&#58;16 ?        00&#58;00&#58;00 [globus-job-mana]
&#60;defunct&#62;
samgrid   2196     1  0 12&#58;36 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
<div id='show_518420534' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_518420534'>-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid   2901     1  0 14&#58;11 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   3261  4550  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollqrLkoM -c poll
samgrid   3277     1  0 14&#58;13 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid   3708     1  0 12&#58;32 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   4070     1  0 12&#58;46 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   4550     1  0 12&#58;46 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   4693     1  0 12&#58;36 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid   5957     1  0 14&#58;13 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid   6507     1  0 14&#58;09 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   6942     1  0 12&#58;36 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   7029     1  0 14&#58;13 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid   7080     1  0 14&#58;13 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   7455 27497  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollpj7FfS -c poll
samgrid   7646     1  0 14&#58;06 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   7791  3261  2 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollqrLkoM -c poll
nysgrid   8237     1  0 13&#58;38 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid   8433     1  0 14&#58;10 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid   8889  6942  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollx8bRA6 -c poll
samgrid   9652  7791  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 181882 2&#62;/dev/null
samgrid   9720  7455  2 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollpj7FfS -c poll
samgrid  10329  8889  3 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollx8bRA6 -c poll
samgrid  10638  9720  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186195 2&#62;/dev/null
nysgrid  11064 19560  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_poll1jV0vP -c poll
nysgrid  11065  3708  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollWHcQV8 -c poll
nysgrid  11066  8237  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollqwibTu -c poll
samgrid  11248     1  0 12&#58;46 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  11569 10329  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 178360 2&#62;/dev/null
samgrid  12289     1  0 12&#58;34 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  12359     1  0 13&#58;38 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  12959     1  0 14&#58;13 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  13101 11065  4 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollWHcQV8 -c poll
nysgrid  13165 11064  5 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_poll1jV0vP -c poll
nysgrid  13231 11066  5 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollqwibTu -c poll
samgrid  13487 19486  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollyfLSpp -c poll
samgrid  13622  4070  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollwGfAwk -c poll
samgrid  13634 12289  0 14&#58;17 ?        00&#58;00&#58;00 /bin/sh /usr/local/
OSG_1_0_0/globus/libexec/globus-job-manager-script.pl -m lsf -f /tmp/
gram_pollIlJybo -c poll
samgrid  14131     1  0 12&#58;46 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  14313 13165  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186275 2&#62;/dev/null
samgrid  14517     1  0 12&#58;46 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  14749  9652  0 14&#58;17 ?        00&#58;00&#58;00 /share/nfsapps/lsf/7.0/
linux2.6-glibc2.3-x86_64/bin/bjobs 181882
nysgrid  14792 13101  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186271 2&#62;/dev/null
nysgrid  14892     1  0 14&#58;14 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  14924 13231  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186274 2&#62;/dev/null
samgrid  15304 13622 10 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollwGfAwk -c poll
samgrid  15348 10638  0 14&#58;17 ?        00&#58;00&#58;00 /share/nfsapps/lsf/7.0/
linux2.6-glibc2.3-x86_64/bin/bjobs 186195
samgrid  15858 11569  0 14&#58;17 ?        00&#58;00&#58;00 /share/nfsapps/lsf/7.0/
linux2.6-glibc2.3-x86_64/bin/bjobs 178360
samgrid  15875 13487  7 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollyfLSpp -c poll
samgrid  15988 13634  0 14&#58;17 ?        00&#58;00&#58;00 /usr/bin/perl /usr/
local/OSG_1_0_0/globus/libexec/globus-job-manager-script-real.pl -m
lsf -f /tmp/gram_pollIlJybo -c poll
root     16002 19885  0 14&#58;17 pts/1    00&#58;00&#58;00 grep -i grid
nysgrid  16097 14313  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186275 2&#62;/dev/null
nysgrid  16123 16097  0 14&#58;17 ?        00&#58;00&#58;00 [sh] &#60;defunct&#62;
nysgrid  16150 14792  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186271 2&#62;/dev/null
nysgrid  16165 16097  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186275 2&#62;/dev/null
nysgrid  16168 14924  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186274 2&#62;/dev/null
nysgrid  16173 16150  0 14&#58;17 ?        00&#58;00&#58;00 [sh] &#60;defunct&#62;
nysgrid  16200 16150  0 14&#58;17 ?        00&#58;00&#58;00 sh -c . /share/nfsapps/
lsf/conf/profile.lsf && /share/nfsapps/lsf/7.0/linux2.6-glibc2.3-
x86_64/bin/bjobs 186271 2&#62;/dev/null
samgrid  17237     1  0 13&#58;00 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  19486     1  0 12&#58;35 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  19560     1  0 13&#58;53 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  24382     1  0 13&#58;51 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  25253     1  0 14&#58;12 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  25355  8433  0 14&#58;12 ?        00&#58;00&#58;00 [globus-job-mana]
&#60;defunct&#62;
samgrid  27279     1  0 12&#58;45 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  27497     1  0 14&#58;12 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  28237  6507  0 14&#58;12 ?        00&#58;00&#58;00 [globus-job-mana]
&#60;defunct&#62;
samgrid  29460     1  0 12&#58;35 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  29950     1  0 14&#58;11 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  31174     1  0 12&#58;34 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  31667     1  0 12&#58;46 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  31741     1  0 14&#58;14 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
samgrid  32458     1  0 12&#58;38 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
nysgrid  32504     1  0 14&#58;11 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type
managedfork -rdn jobmanager-managedfork -machine-type unknown -publish-
jobs

Unfortunately, I foolishly did not grep for &#34;osg&#34; and so only got a
small fraction, probably, of the globus-job-manager jobs running.

After the fact, there were the following processes running that showed
up on a grep for &#34;osg&#34; (which flagged ones based on our installation
directory, which happens to be named /usr/local/OSG...)

[root@antaeus ~]# ps -efwww | grep -i osg
root      2164     1  0 13&#58;00 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2166     1  0 13&#58;00 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2167     1  0 13&#58;00 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2168     1  0 13&#58;00 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2169     1  0 13&#58;00 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2523     1  0 13&#58;50 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2630     1  0 13&#58;44 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2703     1  0 11&#58;47 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      2704     1  0 11&#58;47 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      3018     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      3153     1  0 12&#58;50 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      3154     1  0 12&#58;50 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      4061     1  0 13&#58;16 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      4576     1  0 13&#58;44 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      5196     1  0 11&#58;46 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      5308     1  0 12&#58;44 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      6732     1  0 12&#58;50 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      6930     1  0 12&#58;35 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      7354     1  0 12&#58;44 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      7417     1  0 13&#58;11 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      7523     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      7612     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      8089     1  0 12&#58;44 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      8437     1  0 12&#58;01 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      8702     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      8703     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      8704     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      8814     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      8966     1  0 12&#58;17 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      9191     1  0 12&#58;33 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      9442     1  0 12&#58;44 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      9673     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      9674     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root      9719     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     10131     1  0 13&#58;57 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     10184     1  0 12&#58;14 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     10377     1  0 12&#58;56 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     10478     1  0 13&#58;11 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     10801     1  0 13&#58;15 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     10813     1  0 12&#58;33 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     10971     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     11193     1  0 12&#58;14 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     11677     1  0 13&#58;50 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     12305     1  0 11&#58;51 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     12458     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     12904     1  0 14&#58;18 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     12987     1  0 13&#58;57 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13086     1  0 12&#58;20 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13137     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13138     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13158     1  0 11&#58;49 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13379     1  0 11&#58;54 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13381     1  0 14&#58;18 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13619     1  0 11&#58;49 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     13985     1  0 12&#58;05 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     14221     1  0 14&#58;09 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     14281     1  0 13&#58;09 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     14463     1  0 14&#58;09 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     15288     1  0 13&#58;10 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     15438     1  0 11&#58;46 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     15653     1  0 13&#58;57 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     15780     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     15781     1  0 12&#58;07 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     15803     1  0 11&#58;58 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     16597     1  0 13&#58;00 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     16696     1  0 13&#58;23 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     16845     1  0 12&#58;15 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     17109     1  0 13&#58;58 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     17276     1  0 11&#58;58 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     17371     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     17567     1  0 13&#58;58 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     17922     1  0 13&#58;13 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18090     1  0 13&#58;58 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18091     1  0 12&#58;18 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18407     1  0 14&#58;18 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18447     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18471     1  0 12&#58;15 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18649     1  0 14&#58;18 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18764     1  0 12&#58;57 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18770     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     18888     1  0 13&#58;50 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     19476     1  0 13&#58;58 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     19526     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     20664     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     20786     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     20834     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     20839     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     20843     1  0 13&#58;23 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     20846     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     20902     1  0 12&#58;10 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     21273     1  0 12&#58;34 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     21467     1  0 13&#58;15 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     21850     1  0 13&#58;15 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     22542     1  0 11&#58;54 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     22953     1  0 12&#58;10 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     22965     1  0 12&#58;34 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     23037 21361  0 14&#58;51 pts/2    00&#58;00&#58;00 grep -i osg
root     23397     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     23398     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     23400     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     23521     1  0 Mar16 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     24446     1  0 14&#58;01 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     24459     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     24460     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     24461     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     24596     1  0 13&#58;59 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     24635     1  0 12&#58;27 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     24668     1  0 13&#58;10 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
samgrid  25018     1  0 14&#58;19 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
root     25266     1  0 13&#58;59 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     25267     1  0 13&#58;59 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     25471     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     25529     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     25899     1  0 11&#58;49 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
samgrid  26038     1  0 14&#58;19 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
root     26132     1  0 13&#58;23 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     26152     1  0 12&#58;18 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     26386     1  0 12&#58;58 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     26396     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     26397     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     26398     1  0 11&#58;39 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
samgrid  26423     1  0 14&#58;19 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
root     26434     1  0 14&#58;09 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     27086     1  0 12&#58;28 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
samgrid  27112     1  0 14&#58;19 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
root     27294     1  0 12&#58;14 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
samgrid  27656     1  0 14&#58;19 ?        00&#58;00&#58;00 globus-job-manager -
conf /usr/local/OSG_1_0_0/globus/etc/globus-job-manager.conf -type lsf
-rdn jobmanager-lsf -machine-type unknown -publish-jobs
root     28171     1  0 12&#58;00 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     28275     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     28276     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     28277     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     28863     1  0 11&#58;47 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     29885     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     29886     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     30761     1  0 13&#58;37 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     30909     1  0 11&#58;40 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     31553     1  0 11&#58;47 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     31831     1  0 11&#58;47 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     31918     1  0 12&#58;56 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     32020     1  0 13&#58;37 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     32195     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     32196     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf
root     32197     1  0 11&#58;24 ?        00&#58;00&#58;00 /usr/local/OSG_1_0_0/
globus/sbin/globus-gatekeeper -conf /usr/local/OSG_1_0_0/globus/etc/
globus-gatekeeper.conf

I also have a *huge* globus-gatekeeper.log&#58;

-rw-r--r--  1 root   root  2016502699 Mar 17 14&#58;20 globus-gatekeeper.log

Looking at the tail of that shows a bunch of jobs submitted by

PID&#58; 27627 -- PRIMA INFO ts=2009-03-17T14&#58;19&#58;35-06&#58;00
event=org.osg.prima.authz.end status=0 decision=PERMIT DN=&#34;/DC=org/
DC=doegrids/OU=People/CN=Frank Wuerthwein 699373&#34; FQAN=&#34;/cms/Role=NULL/
Capability=NULL&#34; FQAN_Issuer=&#34;/DC=ch/DC=cern/OU=computers/
CN=voms.cern.ch&#34; Service_URL=&#34;<a href='https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSAuthorizationServicePort' target='_blank' rel='nofollow'>https&#58;//gums.hpcc.ttu.edu&#58;8443/gums/services/GUMSAuthorizationServicePort</a>
&#34; local_user=uscmspool0429
</div><script type='text/javascript'>
        $('#show_518420534').click(function() {
            $('#detail_518420534').slideDown("normal");
            $('#show_518420534').hide();
            $('#hide_518420534').show();
        });
        $('#hide_518420534').click(function() {
            $('#detail_518420534').slideUp();
            $('#hide_518420534').hide();
            $('#show_518420534').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='6567#1237319888'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T19:58:08+00:00">Mar 17, 2009 07:58 PM UTC</time> by <b>echism</b><a class="anchor" name="1237319888">&nbsp;</a></div><pre>Craig and Alain,

We are getting strange reports of excessive resource usage by our gridscan system. From a developer
standpoint can you think of any reason why the site_verify.pl script might start in excess of 2000 job managers
with a resource? Is there an obvious way to stop that from happening either from here or on the CE side of
things? Any assistance is greatly appreciated.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='6567#1237318026'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T19:27:06+00:00">Mar 17, 2009 07:27 PM UTC</time> by <b>echism</b><a class="anchor" name="1237318026">&nbsp;</a></div><pre>Alan,

Please provide us with anything/everything relevant in your logs as it relates to the gridscan problem.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='6567#1237317031'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T19:10:31+00:00">Mar 17, 2009 07:10 PM UTC</time> by <b>echism</b><a class="anchor" name="1237317031">&nbsp;</a></div><pre>Iwona,

Please provide us with anything/everything relevant in the logs.

Thank you,
Elizabeth</pre></div><div class='update_description'><i onclick="document.location='6567#1237315871'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T18:51:11+00:00">Mar 17, 2009 06:51 PM UTC</time> by <b>Iwona Sakrejda</b><a class="anchor" name="1237315871">&nbsp;</a></div><pre>Actually it was pdsf, I have not heard from other sys admins so
must have been not a problem there (but also they have more memory).

Iwona

On 3/17/09 11&#58;42 AM, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='6567#1237315347'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T18:42:27+00:00">Mar 17, 2009 06:42 PM UTC</time> by <b>Christopher Pipes</b><a class="anchor" name="1237315347">&nbsp;</a></div><pre>Iwona, was it Davinci, Jacquard or Franklin that was hit?

Thank you,

Christopher</pre></div><div class='update_description'><i onclick="document.location='6567#1237314791'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2009-03-17T18:33:11+00:00">Mar 17, 2009 06:33 PM UTC</time> by <b>Christopher Pipes</b><a class="anchor" name="1237314791">&nbsp;</a></div><pre>Hi,
This morning at 7&#58;17 PDT user /DC=org/DC=doegrids/OU=People/CN=Rob Quick (GridScan)
started ~2k of job managers and then my gatekeeper ran out of memory? Was it some
monitoring going bonkers? Is it normal (not the collaps, it might have been a
coincidence).
I know I should run a manged fork. I will, next install. I am just curious what
happened...

Iwona</pre></div><div class='update_description'><i onclick="document.location='6567#0'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="1970-01-01T00:00:00+00:00">Jan 1, 1970 12:00 AM UTC</time> by <b>Kyle Gross&#58;</b><a class="anchor" name="0">&nbsp;</a></div><pre>Alan,

Are there three tickets would you like us to make for this?

Please submit problems, requests, and questions at&#58;
<a href='http&#58;//oim.grid.iu.edu/gocticket' target='_blank' rel='nofollow'>http&#58;//oim.grid.iu.edu/gocticket</a>

Thank You,
OSG Grid Operations Center
goc@...., 317-278-9699
Visit the OSG Support Page&#58;
<a href='http&#58;//www.opensciencegrid.org/ops' target='_blank' rel='nofollow'>http&#58;//www.opensciencegrid.org/ops</a>
RSS&#58; <a href='http&#58;//www.grid.iu.edu/news' target='_blank' rel='nofollow'>http&#58;//www.grid.iu.edu/news</a></pre></div><legend>Similar Recent Tickets <small>modified within the last 30 days</small></legend><div id="similar_tickets"><p class="muted">No similar tickets found.</p></div>
</div>
<script type="text/javascript">
function reset_anchor() {
    $("#updates .selected").removeClass("selected");
    var urls = document.location.toString().split('#'); 
    var anchor = urls[1];
    if(anchor) {
        $("a[name='"+anchor+"']").parents(".update_description").addClass("selected");
    }
}
function submitspam(ticket_id) {
    myret = confirm("Would you like to close this ticket as a security ticket, and submit the ticket content to akismet?");
    if(myret == true) {
        $.ajax("viewer/processspam?id="+ticket_id).done(function() {
            window.location.reload();
        });
    }
}

$(function() {
    reset_anchor();
    var ADDITIONAL_COOKIE_NAME = 'gocticket';
    var options = { path: '/', expires: 365};

    if(window.opener && window.opener.name == "gocticket_list") {
        v = $.cookie("closewindow");
        if(!v) {
            $("#closewindow").attr("checked", "checked"); //on by default
        } else {
            if(v == "checked") {
                $("#closewindow").attr("checked", "checked");
            }
        }
        $("#closewindow").click(function() {
            $.cookie("closewindow", $(this).attr('checked'), options);
        });
    } else {
        $("#closewindow_area").hide();
    }
    function updateTimeago() {
        $("time").timeago();
        setTimeout(updateTimeago, 30*1000);
    }
    updateTimeago();
    $(".description").focus(expand_description);
});
</script>
<hr/>
<footer>
<p>GOC Ticket Version 2.2 | <a href="https://ticket.opensciencegrid.org/goc/submit?app_issue_check=on&amp;app_issue_type=goc&amp;app_goc_url=https%3A%2F%2Fticket.opensciencegrid.org%3A443%2F6567">Report Bugs</a>
 | <a href="https://github.com/opensciencegrid/operations/blob/master/docs/privacy.md">Privacy Policy</a>
</p>

<p> <img align="top" src="images/tag_orange.png"/> Copyright 2018 The Trustees of Indiana University - Developed for Open Science Grid</p>
</footer>


</div><!--container-fluid-->
<script src="https://ticket1.grid.iu.edu:8443/socket.io/socket.io.js"></script>
<script>
var chat = io.connect('https://ticket1.grid.iu.edu:8443');
chat.on('connect', function() {
    chat.emit('authenticate', {nodekey:'', ticketid: 6567});
});
chat.on('peers', function(peers) {
    $("#peers").html("");
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('peer_disconnect', function(pid) {
    $("#peer_"+pid).hide("slow");
});
chat.on('peer_connected', function(peers) {
    //expect only 1 peer connecting, but..
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('submit', function() {
    if(confirm("This ticket was updated. Do you want to refresh?")) {
        history.go(0);
    }
});

function addPeer(pid, peer) {
    var ipinfo = "";
    if(peer.ip != undefined) {
        ipinfo = "<span class=\"ip\">"+peer.ip+"</span>";
    }
    if(chat.io.engine.id == pid) {
        //don't display myself
        return;
    }
    var html = "<li class=\"new\" id=\"peer_"+pid+"\" class=\"peer\">"+peer.name+ipinfo+"</li>";
    $("#peers").prepend(html);
    $("#peers .new").animate({bottom: 0}, 1000, function() {$(this).removeClass("new")});
}

$(function() {
    $("#ticket_form").submit(function() {
        chat.emit('submit');
        return true;
    });
});
</script>
<script>
//used by searchbox
function parseValue(value) {
    var obj = new Object();
    var tokens = value.split("\t");
    obj.str = tokens[0];
    obj.count = tokens[1];
    return obj;
}

$(function() {
    //bootstrap-2.0.4 stuff
    $(".alert-message").alert();
    $(".dropdown-toggle").dropdown();
    $("span[rel='tooltip']").tooltip();
    $("a[rel=popover]").popover();

    //activate menu that user is currently on
    $("#menu_navigator").addClass("active"); 
    $("#submenu_").addClass("active"); 

    //translate zend validation error message to bootstrap
    $(".errors").addClass("alert").addClass("alert-error");

    //enable autocomplete for search box
    $("#search").autocomplete({
        source: function( request, response ) {
            $.ajax({
                url: "search/autocomplete",
                dataType: "text",
                data: {
                    //featureClass: "P",
                    //style: "full",
                    //maxRows: 12,
                    //name_startsWith: request.term
                    q: request.term
                },
                success: function( data ) {
                    response( $.map( data.split("\n"), function( item ) {
                        if(item == "") return null;
                        return {
                            value: item
                        }
                    }));
                }
            });
        },
        select: function(event, ui) {
            document.location = "search?q="+ui.item.value;
        }
    });
    
});
</script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-69012-13");
pageTracker._trackPageview();
} catch(err) {}
</script>

</body>
