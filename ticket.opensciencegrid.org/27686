<!DOCTYPE html>
<html lang="en">
  <head>
  <base href="">
    <title>[27686] Missing output files after job completion</title>    <meta charset="utf-8" />
    <meta name="verify-v1" content="na5IcAJsZVOfEkboRxuIiZ1zpZgnZiWra+nKcS7nA/o=" />
    <meta name="google-site-verification" content="DLrk3ft4s8b-S2TloLCL2LD_t6wcTjgSluf5pmiu2kA" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <link href="https://ticket.opensciencegrid.org/rss" rel="alternate" type="application/rss+xml" title="GOC Ticket Update feed" />

    <style type="text/css">
      body {
        padding-top: 50px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
     #search {
            width: 300px;
     }

    </style>

<script src="https://code.jquery.com/jquery-3.0.0.js"></script>
<script src="https://code.jquery.com/jquery-migrate-3.0.1.js"></script>

   <link href="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet"/>
    <script src="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"/>
    <link href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.min.css" rel="stylesheet"/>
 <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>


    <link href="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/css/select2.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/js/select2.min.js"></script>

    <link href="css/ticket.css" rel="stylesheet" />
    <script src="lib/jquery.cookie.js"></script>

    <link href="images/tag_orange.png" rel="icon" type="image/png"/>
    <script src='https://www.google.com/recaptcha/api.js'></script>
  </head>

  <body>
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
            <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </a>

            <a class="brand" style="padding: 6px 0px 0px 6px;" href="http://opensciencegrid.org"><img src="images/osglogo.40x30.png"/></a>
            <ul class="nav">
                <li class="dropdown"><a href="https://ticket.opensciencegrid.org/#" class="dropdown-toggle" data-toggle="dropdown">GOC Ticket <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    <li><a href="https://my.opensciencegrid.org">MyOSG</a></li>
                    <li><a href="https://oim.opensciencegrid.org">OIM</a></li>
                    <li class="active"><a href="https://ticket.opensciencegrid.org/index">Ticket</a></li>
	<li class="divider"></li>
	<li><a href="http://repo.grid.iu.edu">Repo</a></li>
	<li class="divider"></li>
	<li><a href="http://blogs.grid.iu.edu">Blog</a></li>
                    <li><a href="http://display.grid.iu.edu">Display</a></li>
                    <li><a href="http://osggoc.blogspot.com/">News</a></li>
                    </ul>
                </li>
            </ul>
            <ul class="nav pull-right">
                <li><a href="https://ticket.opensciencegrid.org/sso/">Login</a></li>            </ul>

            <div class="nav-collapse">
                <ul class="nav">
			 <li id="menu_submit"><a href="https://ticket.opensciencegrid.org/submit">Submit</a></li><li id="menu_view" class="dropdown"><a href="https://ticket.opensciencegrid.org/\#" class="dropdown-toggle" data-toggle="dropdown">View <b class="caret"></b></a><ul class="dropdown-menu"><li id="submenu_listopen"><a href="https://ticket.opensciencegrid.org/list/open">Open Tickets</a></li><li id="submenu_listrecentclose"><a href="https://ticket.opensciencegrid.org/list/recentclose">Recently Closed Tickets</a></li><li class="divider"></li><li id="submenu_alltickets"><a href="https://ticket.opensciencegrid.org/search?q=&amp;sort=id">All Tickets</a></li></ul></li>                </ul>

                <form class="navbar-search pull-right" action="https://ticket.opensciencegrid.org/viewer">
                    <input id="search" type="text" name="id" class="search-query span2" placeholder="Search Ticket" value=""/>
                </form>
            </div>
        </div>
      </div>
    </div>

<script type='text/javascript' src='lib/jquery.timeago.js'></script>
<script type='text/javascript' src='lib/byte2size.js'></script>
<style>
#updates .toolbar {
position: relative;
margin-top: 0px;
top: -10px;
font-weight: normal;
}
#updates a.anchor {
position: relative;
top: -50px;
}
#updates .selected pre {
animation:selected 2s;
animation-iteration-count: 2;
animation-direction: alternate;
-webkit-animation:selected 2s; 
-webkit-animation-iteration-count: 2;
-webkit-animation-direction: alternate;
box-shadow: inset 1px 1px 20px #9ad;
border: 1px solid #9ab;
margin: 5px 0px;
padding-left: 10px;
}
@keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ab;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
@-webkit-keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ad;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
#updates pre {
background-color: inherit;
line-height: 15px;
padding: 5px;
}
#updates .header {
color: #999;
}
#updates .update_history pre {
background-color: #eee;
color: #666;
font-size: 85%;
}
#updates .clickable {
cursor: pointer;
}
#updates .clickable:hover {
color: #D98719;
}
#updates .meta_information pre {
background-color: #fed;
}
#similar_tickets {
max-height: 300px;
overflow-y: auto;
pointer-events: none;
padding: 5px;
background-color: #f4f4f4;
}
.btn-toolbar {
margin-bottom: 0;
height: 30px;
}
#peers {
position: fixed;
bottom: 0px;
right: 0px;
z-index: 100;
list-style: none;
padding: 5px 0px 0px 5px;
margin: 0px;
background-color: white;
box-shadow: 0px 0px 10px white;
}
#peers li {
background-color: #ccc;
color: #000;
display: inline-block;
padding: 5px 10px;
margin-right: 5px;
position: relative;
}
/*
#peers li:hover {
background-color: #999;
cursor: pointer;
}
*/
#peers span.ip {
padding-left: 5px;
color: #666;
}
#peers .new {
bottom: -30px;
}
/*
#peers .me {
background-color: red;
}
*/
</style>

<div class="container-fluid">
<ul id="peers"></ul>
<div class="alert alert-danger"><a class="close" href="https://ticket.opensciencegrid.org/#" data-dismiss="alert">&times;</a>By the end of May 2018, the ticketing system at https://ticket.opensciencegrid.org will be retired and support will be provided at https://support.opensciencegrid.org. Throughout this transition the support email (help@opensciencegrid.org) will be available as a point of contact.<br><br>                                                   
                                                                                                                                                                                   
Please see the service migration page for details: https://opensciencegrid.github.io/technology/policy/service-migrations-spring-2018/#ticket</div><div id="presence" class="pull-right"></div><div class="ticketgui"><script type="text/javascript" src="lib/checktab.js"></script>

<script>
var expanded = false;
function expand_description() {
    var desc = $(".description");
    if(!expanded) {
        expanded = true;
        //expand to minheight
        var min = 250;
        if(desc.height() < min) {
            desc.animate({height: min}, 200);
        }
    }
}

$(document).ready(function() {
    $("input[name='nad']").datepicker({
        dateFormat: 'yy-mm-dd'
    });
});

</script>



<style>
.form-horizontal .control-label {
padding-top: inherit;
font-size:90%;
color:#666;
}
label {
margin-bottom: 0px;
}
.controls {
line-height: 18px;
}
</style>
<form class="form-horizontal" action="https://ticket.opensciencegrid.org/viewer/updatebasic?id=27686" method="post">
<div class="page-header">
    <h3><span class="muted">27686</span> / Missing output files after job completion</h3>
</div>

<div class="row-fluid">
<div class="span5">
    <legend>Contact</legend>
    <div class="control-group">
        <label class="control-label">Full Name</label>
        <div class="controls">Aaron Moate</div>
    </div>
    <div class="control-group">
        <label class="control-label">Email</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">Phone</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">CC</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>

    <legend>Details</legend>
    <div class="control-group"><label class="control-label">Resource Name</label><div class="controls">Clemson-Palmetto</div></div><div class="control-group"><label class="control-label">Associated VO</label><div class="controls">OSG</div></div><div class="control-group"><label class="control-label">Submitted Via</label><div class="controls">GOC Ticket/submit</div></div><div class="control-group"><label class="control-label">Submitter</label><div class="controls">Aaron Moate</div></div><div class="control-group"><label class="control-label">Ticket Links</label><div class="controls"></div></div>
    <div class="control-group">
        <label class="control-label">Ticket Type</label>
        <div class="controls">Problem/Request</div>
    </div>
    <div class="control-group">
        <label class="control-label">Priority</label>
        <div class="controls">Normal</div>
    </div>
    <div class="control-group">
        <label class="control-label">Status</label>
        <div class="controls">
Closed</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action</label>
        <div class="controls">Looks solved check back if everything is fine</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action Deadline</label>
        <div class="controls flag_red">2016-03-31</div>
    </div>

</div><!--span-->
<div class="span7">
    <legend>Assignees</legend>
    <div class="assignee" style="width: 60%">Software Support (Triage) <span class="muted"> / OSG Software Team</span></div><div class="assignee" style="width: 60%">Brian Lin <span class="muted"> / OSG Software Team</span></div>    <br>

    <legend>Assignees</legend>
    TODO
    <br>

    <style>
legend.noborder {
border-bottom: none;
}
</style>

<div id="attachment-list"/>
<script>
$(function () {
    var first = true;
    $.getJSON("attachment/list/27686", function (files) {
        //console.dir(files);
        var html = "<table class=\"table table-condensed\">";
        $(files).each(function() {
            if(first) {
                first = false;
                html += "<legend class=\"noborder\">Attachmenets</legend>";
            }
            html += "<tr class=\"attachment\">";
            html += "<td><img src="https://ticket.opensciencegrid.org/\&quot;&quot;+this.thumbnail_url+&quot;\&quot;/"></td>";
            html += "<td><a href="https://ticket.opensciencegrid.org/\&quot;&quot;+this.url+&quot;\&quot;" target=\"_blank\">"+this.name+"</a></td>";
            html += "<td>"+bytesToSize(this.size, 1)+"</td>";
            html += "</tr>";
        });
        html += "</table>";
        $("#attachment-list").html(html);
    });
});

function download(url) {
    window.open(url, "_blank");
}
</script>


</div><!--span-->
</div><!--row-fluid-->


</form>

</div>
<div id="updates" style="clear: both;">
    <legend>Past Updates
    <div class="btn-toolbar pull-right toolbar">
        <div class="btn-group">
                <a class="btn btn-small" href="https://ticket.opensciencegrid.org/27686?sort=up&amp;"><i class="icon-arrow-up"></i> Sort</a>

        
        <a class="btn btn-small" href="https://ticket.opensciencegrid.org/27686?expandall=true&amp;">Expand Descriptions</a>        <a class="btn btn-small" target="_blank" href="mailto:osg@tick.globalnoc.iu.edu?subject=Open%20Science%20Grid%3A%20Missing%20output%20files%20after%20job%20completion%20ISSUE%3D27686%20PROJ%3D71"><i class="icon-envelope"></i> Update w/Email</a>
        </div>
    </div><!--btn-toolbar-->
    </legend>

    <div class='update_description'><i onclick="document.location='27686#1459360693'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-30T17:58:13+00:00">Mar 30, 2016 05:58 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1459360693">&nbsp;</a></div><pre>It&#39;s been a week, without any complaints, so I&#39;m going to close this ticket. Thanks for the patience, everyone.</pre></div><div class='update_description'><i onclick="document.location='27686#1459360688'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-30T17:58:08+00:00">Mar 30, 2016 05:58 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1459360688">&nbsp;</a></div><pre>It&#39;s been a week, without any complaints, so I&#39;m going to close this ticket. Thanks for the patience, everyone.</pre></div><div class='update_description'><i onclick="document.location='27686#1458746045'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-23T15:14:05+00:00">Mar 23, 2016 03:14 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1458746045">&nbsp;</a></div><pre>Thanks Moate! We can re-enable GLOW jobs being sent to Clemson and keep an eye on them. If we don&#39;t hear any complaints in a week, I think we can finally close this ticket.</pre></div><div class='update_description'><i onclick="document.location='27686#1458745571'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-23T15:06:11+00:00">Mar 23, 2016 03:06 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1458745571">&nbsp;</a></div><pre>I submitted a round of test jobs that write to stdout, sleep for
an hour, then write to standard out.  They all behaved correctly.

Cheers!
-Moate

On Tue, Mar 22, 2016 at 06&#58;47&#58;00PM +0000, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62;    [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1458672456'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-22T18:47:36+00:00">Mar 22, 2016 06:47 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1458672456">&nbsp;</a></div><pre>At this point, we&#39;d like Moate to submit his GLOW test jobs to see if they no longer fail. If they&#39;re ok, I think this ticket can be closed.

There are also some other internal issues at Clemson with memory leaks and jobs exceeding their walltime, that don&#39;t really appear to be affecting us for now but we may have to keep an eye on the pilots. Additionaly, Nova pilots seem to be killed due to memory but that requires more investigation and should probably end up in a separate ticket.</pre></div><div class='update_description'><i onclick="document.location='27686#1458667810'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-22T17:30:10+00:00">Mar 22, 2016 05:30 PM UTC</time><a class="anchor" name="1458667810">&nbsp;</a></div><pre>Hi Brian,

Where do we take it from here? You want me to submit some jobs to Clemson again? of certain lenght?

Edgar
OSG Software Support

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020</pre></div><div class='update_description'><i onclick="document.location='27686#1458322379'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-18T17:32:59+00:00">Mar 18, 2016 05:32 PM UTC</time> by <b>echism</b><a class="anchor" name="1458322379">&nbsp;</a></div><pre>Is there anything new on this?

Thank you</pre></div><div class='update_description'><i onclick="document.location='27686#1457996230'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-14T22:57:10+00:00">Mar 14, 2016 10:57 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1457996230">&nbsp;</a></div><pre>Go for it.

-Moate

On Fri, Mar 11, 2016 at 10&#58;14&#58;00PM +0000, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62;    [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1457963472'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-14T13:51:12+00:00">Mar 14, 2016 01:51 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1457963472">&nbsp;</a></div><pre>Done.

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1457960952'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-14T13:09:12+00:00">Mar 14, 2016 01:09 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1457960952">&nbsp;</a></div><pre>Xizhou,

That&#39;s something you&#39;ll need to do on your end. Not hearing any
complaints from anyone at GLOW, I think you can go ahead with it using
this command&#58;

condor_ce_rm -const &#39;jobstatus == 4 && qdate &#60; 1451628000&#39; glow

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1457737390'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-11T23:03:10+00:00">Mar 11, 2016 11:03 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1457737390">&nbsp;</a></div><pre>Hi Brian,

Do we need to take actions or you can remove them on your side?

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1457734464'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-11T22:14:24+00:00">Mar 11, 2016 10:14 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1457734464">&nbsp;</a></div><pre>Xizhou/Randy,

You guys have a lot of &#39;completed&#39; jobs that are sitting in your queue because of some failures to retrieve the output and we should probably clean them up. It looks like they&#39;re all from GLOW pilots and I vote that we just remove all of the &#39;completed&#39; but &#39;stuck&#39; jobs from 2015. Anyone from UW/GLOW disagree?

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1457552947'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-09T19:49:07+00:00">Mar 9, 2016 07:49 PM UTC</time><a class="anchor" name="1457552947">&nbsp;</a></div><pre>Xizhou, Brian,

I&#39;ve lifted the glidein limits on both the GOC and SDSC factories. We&#39;ll check back on glidein performance in a few days.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1457551930'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-09T19:32:10+00:00">Mar 9, 2016 07:32 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1457551930">&nbsp;</a></div><pre>Yes, please.

Sent from my iPhone

On Mar 9, 2016, at 1&#58;34 PM, Open Science Grid FootPrints &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;

[Duplicate message snipped]</pre></div><div class='update_description'><i onclick="document.location='27686#1457548413'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-09T18:33:33+00:00">Mar 9, 2016 06:33 PM UTC</time><a class="anchor" name="1457548413">&nbsp;</a></div><pre>Xizhou,

May we lift the per frontend glidein limits at the site and ramp up the number glideins again?

Marty Kandes
UCSD Glidein Factory

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1457548325'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-09T18:32:05+00:00">Mar 9, 2016 06:32 PM UTC</time><a class="anchor" name="1457548325">&nbsp;</a></div><pre>Brian,

I may have spoken too soon in my email yesterday. It looks like the most recent glidein logs look quite good, i.e., normal.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1457385866'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-07T21:24:26+00:00">Mar 7, 2016 09:24 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1457385866">&nbsp;</a></div><pre>Xizhou and I worked on this today and it looks like we&#39;ve resolved the memory request and job submission bugs (<a href='https&#58;//github.com/osg-bosco/BLAH/pull/27' target='_blank' rel='nofollow'>https&#58;//github.com/osg-bosco/BLAH/pull/27</a>). Pilots since the change have been running for 30 min+ so far. Could we remove the held pilots and slowly ramp up the number of pilots at Clemson again?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1457023094'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-03T16:38:14+00:00">Mar 3, 2016 04:38 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1457023094">&nbsp;</a></div><pre>I would be available on both Monday and Wednesday.

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1457020806'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-03T16:00:06+00:00">Mar 3, 2016 04:00 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1457020806">&nbsp;</a></div><pre>Xizhou,

Do you have any time over the next week to troubleshoot why we&#39;re having issues submitting to your local batch system?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456946401'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-02T19:20:01+00:00">Mar 2, 2016 07:20 PM UTC</time><a class="anchor" name="1456946401">&nbsp;</a></div><pre>Brian,

Great news. Nice work troubleshooting this for us. I&#39;ve put in place limits on glidein submission to Clemson. You should begin to see glidein numbers decrease as the currently running ones die off naturally. If you don&#39;t see this happen over the next day or so, please let me know.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1456938131'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-02T17:02:11+00:00">Mar 2, 2016 05:02 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456938131">&nbsp;</a></div><pre>Thanks Marty, jobs looks much better now. Would it be possible to spin
down on the number of pilots that we&#39;re sending to Clemson (both for the
OSG and the FIFE VO&#39;s)? We need to start troubleshooting why the &#39;new&#39;
blahp is broken for Clemson and I don&#39;t want to mess with OSG throughput.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456879384'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-02T00:43:04+00:00">Mar 2, 2016 12:43 AM UTC</time><a class="anchor" name="1456879384">&nbsp;</a></div><pre>Brian, Xizhou,

I&#39;ve cleared out the held glideins on both the GOC and SDSC factories. We should now be guaranteed the new glideins will be submitted to Clemson using the fixed PBS submission attributes. Fingers crossed. I&#39;ll check back with you all tomorrow.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1456874231'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-01T23:17:11+00:00">Mar 1, 2016 11:17 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1456874231">&nbsp;</a></div><pre>Thanks. Brian.

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1456873150'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-01T22:59:10+00:00">Mar 1, 2016 10:59 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456873150">&nbsp;</a></div><pre>You shouldn&#39;t need to do anything, newly submited jobs will pick up the
new scripts.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456872971'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-01T22:56:11+00:00">Mar 1, 2016 10:56 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1456872971">&nbsp;</a></div><pre>Hi Brian,

I have just replaced the two scripts on the node osg-ce.clemson.edu. Do I need to run other command to get the change effective?

In PBS, you can request the memory as follows.
#PBS -l select=1&#58;ncpus=1&#58;mem=2gb
#PBS -l walltime=1&#58;00&#58;00

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1456869246'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-03-01T21:54:06+00:00">Mar 1, 2016 09:54 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456869246">&nbsp;</a></div><pre>Xizhou,

This certainly made things worse, all of the pilots are being held now with &#34;Attempts to submit failed&#58;&#34;. This is likely because the changes in our JIRA ticket (<a href='https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-1958' target='_blank' rel='nofollow'>https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-1958</a>) are incomplete. Xizhou, if you grab the pbs_submit.sh and pbs_status.sh files from that ticket and replace your current versions at /usr/libexec/blahp/, that should get pilots running again. Can you tell us what the proper way to request memory is for your PBS cluster? We&#39;re currently trying with this&#58;

#PBS -l select=1&#58;mem=&#60;request memory&#62;mb

Is this incorrect?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456785130'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-29T22:32:10+00:00">Feb 29, 2016 10:32 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1456785130">&nbsp;</a></div><pre>Thank you, Brian.

I just did a yum update.

Before the update, the verification results are&#58;
[root@osg-ce ~]# rpm -q --verify blahp
S.5....T.  c /etc/blah.config
S.5....T.  c /etc/blahp/pbs_local_submit_attributes.sh
S.5....T.    /usr/libexec/blahp/pbs_status.py
S.5....T.    /usr/libexec/blahp/pbs_submit.sh

After the update, the results become&#58;
[root@osg-ce ~]# rpm -q --verify blahp
S.5....T.  c /etc/blah.config
S.5....T.  c /etc/blahp/pbs_local_submit_attributes.sh

[root@osg-ce ~]# rpm -qa | grep blah
blahp-1.18.16.bosco-1.osg32.el6.x86_64

Hope this will resolve this problem.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1456780570'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-29T21:16:10+00:00">Feb 29, 2016 09:16 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456780570">&nbsp;</a></div><pre>Xizhou,

I&#39;ve done some poking around on your CE and noticed that you&#39;re running an old (and modified) version of the blahp. Can you verify the package with &#96;rpm -q --verify blahp&#96;? I believe your /usr/libexec/blahp/pbs_submit.sh file is preventing any memory requests from making it to PBS (if you check your accounting logs, you&#39;ll see that almost every job has requested the default mem of 1GB). You need a newer version of the blahp and should accordingly update all your OSG packages (&#96;yum update&#96;). If you do that, I think that may actually solve many of our problems.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456514683'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-26T19:24:43+00:00">Feb 26, 2016 07:24 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456514683">&nbsp;</a></div><pre>Marty,

In the meantime, could you answer my other questions?

1) Here are some job attrs of a job that was terminated by Clemson&#39;s PBS. Do you have any logs on your end corresponding to this job? I would like to know if there are errors in the StarterLog like the ones we&#39;ve been seeing (missing PID&#39;s). If the logs are gone, I can get you a more recent job.

GlideinClient = &#34;fifebatchgpvmhead1_OSG_gWMSFrontend.OSG_nova&#34;
GlideinCredentialIdentifier = &#34;714925&#34;
GlideinEntryName = &#34;OSG_US_Clemson-Palmetto_condce&#34;
GlideinFactory = &#34;SDSC&#34;
GlideinFrontendName = &#34;Fermilab-fifebatch&#58;frontend&#34;
GlideinLogNr = &#34;20160224&#34;
GlideinName = &#34;gfactory_instance&#34;
GlideinSecurityClass = &#34;frontend&#34;
GlideinSlotsLayout = &#34;fixed&#34;
GlideinWebBase = &#34;<a href='http&#58;//gfactory-1.t2.ucsd.edu/factory/stage&#34' target='_blank' rel='nofollow'>http&#58;//gfactory-1.t2.ucsd.edu/factory/stage&#34</a>;;
GlideinWorkDir = &#34;OSG&#34;

2) More than half the pilots are completing successfully. I&#39;ve looked at the last 10 that completed successfully and there were a few that ran for only a few minutes but many of them ran for a few hours&#58;

resources_used.walltime = 26&#58;23&#58;15
resources_used.walltime = 14&#58;56&#58;55
resources_used.walltime = 14&#58;30&#58;30
resources_used.walltime = 00&#58;00&#58;02
resources_used.walltime = 00&#58;06&#58;30
resources_used.walltime = 26&#58;24&#58;06
resources_used.walltime = 26&#58;24&#58;37
resources_used.walltime = 26&#58;25&#58;12
resources_used.walltime = 26&#58;24&#58;55
resources_used.walltime = 00&#58;23&#58;18

The graphs that you provided suggest that the vast majority of pilots are not matching to user jobs. How are glideins even running that long if they aren&#39;t matching to any jobs? I could likely get you the corresponding CE job ID&#39;s, which I imagine we would be able to trace back to their logs on the factory. What&#39;s going on here?

All,

<div id='show_1671037501' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1671037501'>If we can&#39;t get anywhere with this before the AHM, we should consider a live troubleshooting session to get to the bottom of this.

Thanks,
Brian
</div><script type='text/javascript'>
        $('#show_1671037501').click(function() {
            $('#detail_1671037501').slideDown("normal");
            $('#show_1671037501').hide();
            $('#hide_1671037501').show();
        });
        $('#hide_1671037501').click(function() {
            $('#detail_1671037501').slideUp();
            $('#hide_1671037501').hide();
            $('#show_1671037501').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1456432037'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-25T20:27:17+00:00">Feb 25, 2016 08:27 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456432037">&nbsp;</a></div><pre>Jeff,

Could you let us know if you you guys can change the startd config of the glideins?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456343570'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-24T19:52:50+00:00">Feb 24, 2016 07:52 PM UTC</time><a class="anchor" name="1456343570">&nbsp;</a></div><pre>Brian,

I believe the submit attributes in the factory config are used to explicitly communicate to the CE (and the batch) the requirements for the glidein. e.g., we usually only set +maxMemory and +maxWallTime. I don&#39;t think they can actually set the local configuration of condor that lands on the glidein. And I&#39;m not sure how we would do that exactly. Jeff is working Factory Ops this afternoon. I&#39;ll have him reply to the ticket and have him offer his glideinWMS expertise.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1456342513'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-24T19:35:13+00:00">Feb 24, 2016 07:35 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456342513">&nbsp;</a></div><pre>Marty,

Does that submit the job itself with a MachineMaxVacateTime attribute?
If so, I think that&#39;s incorrect. What we need is the actual condor
config of the pilots to be changed so &#39;MachineMaxVacateTime = 30&#39; (or less).

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456342110'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-24T19:28:30+00:00">Feb 24, 2016 07:28 PM UTC</time><a class="anchor" name="1456342110">&nbsp;</a></div><pre>Brian,

I&#39;ve added

&#60;submit_attr name=&#34;+MachineMaxVacateTime&#34; value=&#34;20&#34;/&#62;

to the GOC factory config. Not sure if this is the correct formatting though.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1456339812'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-24T18:50:12+00:00">Feb 24, 2016 06:50 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1456339812">&nbsp;</a></div><pre>Hi Brian, thank you for sharing the information.

I also noticed that many of the jobs consumed very short CPU time but the reported wall time values seem much large. Is it possible that PBS report one value for the payload and the other for pilot?

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1456337872'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-24T18:17:52+00:00">Feb 24, 2016 06:17 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456337872">&nbsp;</a></div><pre>Xizhou,

I looked at all the PBS records on your CE over the past month and most of them seem to have completed from PBS&#39; perspective (only ~100 were killed) with the following exit status breakdown&#58;

5
34624 0
932 1
1 139
1872 271

I&#39;m not sure the glideins return an exit status of 271 and I found this&#58; <a href='http&#58;//www.clusterresources.com/pipermail/torqueusers/2007-May/005604.html.' target='_blank' rel='nofollow'>http&#58;//www.clusterresources.com/pipermail/torqueusers/2007-May/005604.html.</a> Indicating that the jobs were SIGTERM&#39;ed by torque for resource overconsumption. For example, job 2505888.pbs02 looks like it was using 1048576kb, which would be over the limit of 1gb.

Factory folks (anybody that knows the ins and outs of glideins, please chime in)&#58;

* Do the 1872 jobs that were SIGTERM&#39;ed correspond to the errors where pilots can&#39;t find PID&#39;s? I was able to trace job 2505888.pbs02 to the initial glidein job on the CE and it has the following attributes, can we crosscheck the glidein logs for errors?

GlideinClient = &#34;fifebatchgpvmhead1_OSG_gWMSFrontend.OSG_nova&#34;
GlideinCredentialIdentifier = &#34;714925&#34;
GlideinEntryName = &#34;OSG_US_Clemson-Palmetto_condce&#34;
GlideinFactory = &#34;SDSC&#34;
GlideinFrontendName = &#34;Fermilab-fifebatch&#58;frontend&#34;
GlideinLogNr = &#34;20160224&#34;
GlideinName = &#34;gfactory_instance&#34;
GlideinSecurityClass = &#34;frontend&#34;
GlideinSlotsLayout = &#34;fixed&#34;
GlideinWebBase = &#34;<a href='http&#58;//gfactory-1.t2.ucsd.edu/factory/stage&#34' target='_blank' rel='nofollow'>http&#58;//gfactory-1.t2.ucsd.edu/factory/stage&#34</a>;
GlideinWorkDir = &#34;OSG&#34;

* If the missing PID errors correspond to jobs being SIGTERM&#39;ed (which has been our theory for quite some time now...), WHY are the pilots handling this so poorly? Clemson only gives us 30s after their warning shot SIGTERM before outright killing the job and removing the sandbox. Has the MachineMaxVacateTime been set to &#60; 30s on Clemson&#39;s pilots?
* 34624 pilots ran to successful completion. Why are none of them matching? Factory ops should be getting logs back for these successful jobs. I&#39;ll poke around some more to see if jobs are running for any reasonable amount of time.

Let&#39;s try to resolve this before the All Hands meeting...

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456330090'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-24T16:08:10+00:00">Feb 24, 2016 04:08 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1456330090">&nbsp;</a></div><pre>I went through various type of logs, I found that the incident that some reimaged nodes didn’t start autofs server. I don’t know if this is one reason for the missing output files after the job completion. I will work with our team to get the autofs issue fixed and keep monitoring the job failures.

Thanks,

Xizhou

[root@pbs02 ~]# grep fermilab /var/spool/PBS/server_priv/accounting/20160220 | grep Exit_status=0 | wc
2244   69564 1626161

[root@pbs02 ~]# grep fermilab /var/spool/PBS/server_priv/accounting/20160220 | grep Exit_status=1 | wc
27     837   19535

[root@pbs02 ~]# grep fermilab /var/spool/PBS/server_priv/accounting/20160224 | grep Exit_status=0 | wc
375   11625  271762

[root@pbs02 ~]# grep fermilab /var/spool/PBS/server_priv/accounting/20160224 | grep Exit_status=1 | wc
5     155    3619

[root@pbs02 ~]# qstat -xf 2501996
Job Id&#58; 2501996.pbs02
Job_Name = bl_417f107a5f3d
Job_Owner = fermilab@....
resources_used.cpupercent = 27
resources_used.cput = 00&#58;00&#58;03
resources_used.mem = 6232kb
resources_used.ncpus = 1
resources_used.vmem = 336144kb
resources_used.walltime = 00&#58;40&#58;44
job_state = F
queue = osg_e
server = pbs02
Checkpoint = u
ctime = Tue Feb 23 20&#58;43&#58;36 2016
Error_Path = osg-ce.clemson.edu&#58;/dev/null
exec_host = node0060/6
<div id='show_1160683295' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1160683295'>exec_vnode = (node0060&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0)
group_list = osg
Hold_Types = n
Join_Path = n
Keep_Files = n
Mail_Points = n
Mail_Users = fermilab@....
mtime = Wed Feb 24 10&#58;43&#58;31 2016
Output_Path = osg-ce.clemson.edu&#58;/dev/null
Priority = 0
qtime = Wed Feb 24 10&#58;00&#58;06 2016
Rerunable = True
Resource_List.mem = 1gb
Resource_List.ncpus = 1
Resource_List.ngpus = 0
Resource_List.nodect = 1
Resource_List.nphis = 0
Resource_List.place = free&#58;shared
Resource_List.qcat = osg_qcat
Resource_List.select = 1
Resource_List.walltime = 23&#58;30&#58;00
schedselect = 1&#58;mem=1gb&#58;qcat=osg_qcat&#58;ncpus=1&#58;ngpus=0&#58;nphis=0
stime = Wed Feb 24 10&#58;02&#58;47 2016
session_id = 15065
Shell_Path_List = /bin/bash
jobdir = /home/fermilab
substate = 93
Variable_List = PBS_O_HOME=/home/fermilab,
PBS_O_WORKDIR=/usr/local/osg/8034/0/cluster2208034.proc0.subproc0,
PBS_O_LANG=en_US.UTF-8,PBS_O_PATH=/sbin&#58;/usr/sbin&#58;/bin&#58;/usr/bin,
PBS_O_SYSTEM=Linux,PBS_O_QUEUE=osg,
PBS_O_HOST=osg001.palmetto.clemson.edu
euser = fermilab
egroup = osg
hashname = 2501996.pbs02
queue_rank = 1462822
queue_type = E
comment = Job run at Wed Feb 24 at 10&#58;01 on (node0060&#58;mem=1048576kb&#58;ncpus=1
&#58;ngpus=0&#58;nphis=0) and failed
etime = Wed Feb 24 10&#58;00&#58;06 2016
run_count = 1
Stageout_status = 1
Exit_status = 1
Submit_arguments = /tmp/condor_g_scratch.0x7f25187f04d0.7550/bl_417f107a5f3
d
history_timestamp = 1456328611
project = _pbs_project_default
run_version = 1

[root@osg-ce cluster2208034.proc0.subproc0]# cat _condor_stderr
------- Initial environment ---------------
MANPATH=&#58;
HOSTNAME=node0060
OSG_GLEXEC_LOCATION=None
SHELL=/bin/bash
HISTSIZE=1000
TMPDIR=/local_scratch/pbs.2501996.pbs02
PBS_JOBNAME=bl_417f107a5f3d
GLOBUS_LOCATION=/usr
LIBRARY_PATH=/opt/orangefs/lib
PBS_ENVIRONMENT=PBS_BATCH
QTDIR=/usr/lib64/qt-3.3
QTINC=/usr/lib64/qt-3.3/include
OSG_WN_TMP=/local_scratch
PBS_O_WORKDIR=/usr/local/osg/8034/0/cluster2208034.proc0.subproc0
NCPUS=1
OSG_SQUID_LOCATION=10.125.40.22&#58;3128
PBS_TASKNUM=1
USER=fermilab
OSG_SITE_WRITE=/common/osg/data
LD_LIBRARY_PATH=/opt/orangefs/lib&#58;/opt/mx/lib
PBS_O_HOME=/home/fermilab
PBS_MOMPORT=15003
CONDORCE_COLLECTOR_HOST=osg-ce.clemson.edu&#58;9619
OSG_SITE_READ=/common/osg/data
PBS_O_QUEUE=osg
MAIL=/var/spool/mail/fermilab
PATH=/usr/lib64/qt-3.3/bin&#58;/opt/gold/bin&#58;/bin&#58;/usr/bin&#58;/usr/bin/X11&#58;/usr/local/sbin&#58;/usr/sbin&#58;/sbin&#58;/opt/ibutils/bin&#58;/opt/mx/bin&#58;/opt/pbs/default/bin&#58;/opt/pbs/default/sbin&#58;/opt/dell/srvadmin/bin&#58;/home/fermilab/bin
PBS_O_LANG=en_US.UTF-8
PBS_JOBCOOKIE=0000000002AE6BAA0000000027B755FA
C_INCLUDE_PATH=/opt/orangefs/include
OSG_DATA=/common/osg/data
PWD=/home/fermilab/home_bl_osg-ce.clemson.edu_9619_osg-ce.clemson.edu#2208040.0#1456278190
OSG_APP=/common/osg/app
LANG=C
PBS_NODENUM=0
PBS_TZID=America/New_York
MODULEPATH=/software/modulefiles&#58;/usr/share/Modules/modulefiles&#58;/etc/modulefiles
LOADEDMODULES=
PBS_JOBDIR=/home/fermilab
TZ=America/New_York
OSG_GRID=/software/osg-wn-client
PBS_JOBID=2501996.pbs02
HISTCONTROL=ignoredups
ENVIRONMENT=BATCH
OSG_HOSTNAME=osg-ce.clemson.edu
SHLVL=3
HOME=/home/fermilab/home_bl_osg-ce.clemson.edu_9619_osg-ce.clemson.edu#2208040.0#1456278190
OSG_STORAGE_ELEMENT=False
X509_USER_PROXY=/home/fermilab/home_bl_osg-ce.clemson.edu_9619_osg-ce.clemson.edu#2208040.0#1456278190/bl_417f107a5f3d.proxy
PBS_O_HOST=osg001.palmetto.clemson.edu
OSG_DEFAULT_SE=None
LOGNAME=fermilab
QTLIB=/usr/lib64/qt-3.3/lib
CVS_RSH=ssh
PBS_QUEUE=osg_e
OSG_SITE_NAME=osg-ce
MODULESHOME=/usr/share/Modules
LESSOPEN=||/usr/bin/lesspipe.sh %s
OMP_NUM_THREADS=1
PBS_O_SYSTEM=Linux
G_BROKEN_FILENAMES=1
PBS_NODEFILE=/var/spool/PBS/aux/2501996.pbs02
PBS_O_PATH=/sbin&#58;/usr/sbin&#58;/bin&#58;/usr/bin
BASH_FUNC_module()=() {  eval &#96;/usr/bin/modulecmd bash $*&#96;
}
_=/bin/env
------- =================== ---------------
Setting X509_USER_PROXY  to canonical path /home/fermilab/home_bl_osg-ce.clemson.edu_9619_osg-ce.clemson.edu#2208040.0#1456278190/bl_417f107a5f3d.proxy
signature.g2nn5F.sha1&#58; OK
signature.g2nn5F.sha1&#58; OK
signature.g1dgyv.sha1&#58; OK
signature.ebieLp.sha1&#58; OK
description.g2nn5F.cfg&#58; OK
Signature OK for main&#58;description.g2nn5F.cfg.
description.g2nn5F.cfg&#58; OK
Signature OK for entry&#58;description.g2nn5F.cfg.
description.g1dgyv.cfg&#58; OK
Signature OK for client&#58;description.g1dgyv.cfg.
description.ebieLp.cfg&#58; OK
Signature OK for client_group&#58;description.ebieLp.cfg.
file_list.g2nn5F.lst&#58; OK
Signature OK for main&#58;file_list.g2nn5F.lst.
error_gen.g2nn5F.sh&#58; OK
Signature OK for main&#58;error_gen.g2nn5F.sh.
error_augment.g2nn5F.sh&#58; OK
Signature OK for main&#58;error_augment.g2nn5F.sh.
setup_script.g2nn5F.sh&#58; OK
Signature OK for main&#58;setup_script.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/main/setup_script.sh (0) ===
constants.g25lGH.cfg&#58; OK
Signature OK for main&#58;constants.g25lGH.cfg.
condor_vars.fadiVb.lst&#58; OK
Signature OK for main&#58;condor_vars.fadiVb.lst.
untar.g25lGH.cfg&#58; OK
Signature OK for main&#58;untar.g25lGH.cfg.
grid-mapfile.f2amKi&#58; OK
Signature OK for main&#58;grid-mapfile.f2amKi.
parse_starterlog.g2nn5F.awk&#58; OK
Signature OK for main&#58;parse_starterlog.g2nn5F.awk.
advertise_failure.g2nn5F.helper&#58; OK
Signature OK for main&#58;advertise_failure.g2nn5F.helper.
condor_config.g2nn5F&#58; OK
Signature OK for main&#58;condor_config.g2nn5F.
condor_config.multi_schedd.g2nn5F.include&#58; OK
Signature OK for main&#58;condor_config.multi_schedd.g2nn5F.include.
condor_config.dedicated_starter.g2nn5F.include&#58; OK
Signature OK for main&#58;condor_config.dedicated_starter.g2nn5F.include.
condor_config.check.g2nn5F.include&#58; OK
Signature OK for main&#58;condor_config.check.g2nn5F.include.
condor_config.monitor.g2nn5F.include&#58; OK
Signature OK for main&#58;condor_config.monitor.g2nn5F.include.
cat_consts.g2nn5F.sh&#58; OK
Signature OK for main&#58;cat_consts.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/main/cat_consts.sh (0) ===
condor_platform_select.g2nn5F.sh&#58; OK
Signature OK for main&#58;condor_platform_select.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/main/condor_platform_select.sh (0) ===
condor_bin_8.g2nn5F.tgz&#58; OK
Signature OK for main&#58;condor_bin_8.g2nn5F.tgz.
collector_setup.g2nn5F.sh&#58; OK
Signature OK for main&#58;collector_setup.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/main/collector_setup.sh (0) ===
create_temp_mapfile.g2nn5F.sh&#58; OK
Signature OK for main&#58;create_temp_mapfile.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/main/create_temp_mapfile.sh (0) ===
setup_x509.g2nn5F.sh&#58; OK
Signature OK for main&#58;setup_x509.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/main/setup_x509.sh (0) ===
condor_startup.g2nn5F.sh&#58; OK
Signature OK for main&#58;condor_startup.g2nn5F.sh.
Skipping last script condor_startup.sh
sethome.g2nn5F.source&#58; OK
Signature OK for main&#58;sethome.g2nn5F.source.
preentry_file_list.g1dgyv.lst&#58; OK
Signature OK for client&#58;preentry_file_list.g1dgyv.lst.
constants.g1dgyv.cfg&#58; OK
Signature OK for client&#58;constants.g1dgyv.cfg.
condor_vars.g1dgyv.lst&#58; OK
Signature OK for client&#58;condor_vars.g1dgyv.lst.
untar.e6udMS.cfg&#58; OK
Signature OK for client&#58;untar.e6udMS.cfg.
No signature for /local_scratch/glide_luDCt9/client/nodes.blacklist.
cat_consts.e6udMS.sh&#58; OK
Signature OK for client&#58;cat_consts.e6udMS.sh.
=== validation OK in /local_scratch/glide_luDCt9/client/cat_consts.sh (0) ===
check_blacklist.e6udMS.sh&#58; OK
Signature OK for client&#58;check_blacklist.e6udMS.sh.
=== validation OK in /local_scratch/glide_luDCt9/client/check_blacklist.sh (0) ===
grid-mapfile.f5lefm&#58; OK
Signature OK for client&#58;grid-mapfile.f5lefm.
preentry_file_list.ebieLp.lst&#58; OK
Signature OK for client_group&#58;preentry_file_list.ebieLp.lst.
constants.ebieLp.cfg&#58; OK
Signature OK for client_group&#58;constants.ebieLp.cfg.
condor_vars.e6udMS.lst&#58; OK
Signature OK for client_group&#58;condor_vars.e6udMS.lst.
untar.e6udMS.cfg&#58; OK
Signature OK for client_group&#58;untar.e6udMS.cfg.
No signature for /local_scratch/glide_luDCt9/client_group_OSG_mu2e/nodes.blacklist.
cat_consts.e6udMS.sh&#58; OK
Signature OK for client_group&#58;cat_consts.e6udMS.sh.
=== validation OK in /local_scratch/glide_luDCt9/client_group_OSG_mu2e/cat_consts.sh (0) ===
check_blacklist.e6udMS.sh&#58; OK
Signature OK for client_group&#58;check_blacklist.e6udMS.sh.
=== validation OK in /local_scratch/glide_luDCt9/client_group_OSG_mu2e/check_blacklist.sh (0) ===
aftergroup_preentry_file_list.e6udMS.lst&#58; OK
Signature OK for client&#58;aftergroup_preentry_file_list.e6udMS.lst.
file_list.g2nn5F.lst&#58; OK
Signature OK for entry&#58;file_list.g2nn5F.lst.
constants.g2a18K.cfg&#58; OK
Signature OK for entry&#58;constants.g2a18K.cfg.
condor_vars.g1liGX.lst&#58; OK
Signature OK for entry&#58;condor_vars.g1liGX.lst.
untar.f7vggL.cfg&#58; OK
Signature OK for entry&#58;untar.f7vggL.cfg.
No signature for /local_scratch/glide_luDCt9/entry_OSG_US_Clemson-Palmetto_condce/nodes.blacklist.
cat_consts.g2nn5F.sh&#58; OK
Signature OK for entry&#58;cat_consts.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/entry_OSG_US_Clemson-Palmetto_condce/cat_consts.sh (0) ===
check_blacklist.g2nn5F.sh&#58; OK
Signature OK for entry&#58;check_blacklist.g2nn5F.sh.
=== validation OK in /local_scratch/glide_luDCt9/entry_OSG_US_Clemson-Palmetto_condce/check_blacklist.sh (0) ===
file_list.f8ramT.lst&#58; OK
Signature OK for client&#58;file_list.f8ramT.lst.
advertise_os.e6udMS.sh&#58; OK
Signature OK for client&#58;advertise_os.e6udMS.sh.
=== validation OK in /local_scratch/glide_luDCt9/client/advertise_os.sh (0) ===
IFFE_script.f8ramT.sh&#58; OK
Signature OK for client&#58;IFFE_script.f8ramT.sh.
Wed Feb 24 10&#58;02&#58;56 EST 2016 ERROR&#58; Did not find CERN file /cvmfs/grid.cern.ch/util/cvmfs-uptodate
=== Validation error in /local_scratch/glide_luDCt9/client/IFFE_script.sh ===
Wed Feb 24 10&#58;23&#58;30 EST 2016 Error running &#39;/local_scratch/glide_luDCt9/client/IFFE_script.sh&#39;
Did not find CERN file /cvmfs/grid.cern.ch/util/cvmfs-uptodate
Wed Feb 24 10&#58;23&#58;31 EST 2016 Sleeping 255
Wed Feb 24 10&#58;27&#58;46 EST 2016 Sleeping 317
Wed Feb 24 10&#58;33&#58;03 EST 2016 Sleeping 312
Wed Feb 24 10&#58;38&#58;15 EST 2016 Sleeping 315

=== Encoded XML description of glidein activity ===
begin-base64 644 -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====
=== End encoded XML description of glidein activity ===
[root@osg-ce cluster2208034.proc0.subproc0]#

02/20/2016 23&#58;59&#58;40;E;2446157.pbs02;user=fermilab group=osg project=_pbs_project_default jobname=bl_7eabf17768fd queue=osg_e ctime=1455967577 qtime=1456029497 etime=1456029497 start=1456029498 exec_host=node0242/6 exec_vnode=(node0242&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0 Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=24&#58;00&#58;00 session=19395 end=1456030780 Exit_status=0 resources_used.cpupercent=21 resources_used.cput=00&#58;00&#58;27 resources_used.mem=27160kb resources_used.ncpus=1 resources_used.vmem=562336kb resources_used.walltime=00&#58;21&#58;23 run_count=1

/6 exec_vnode=(node0155&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0 Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=23&#58;30&#58;00 session=11498 end=1456030758 Exit_status=1 resources_used.cpupercent=27 resources_used.cput=00&#58;00&#58;03 resources_used.mem=6232kb resources_used.ncpus=1 resources_used.vmem=336144kb resources_used.walltime=00&#58;40&#58;43 run_count=1

[root@pbs02 ~]# tracejob -n 6 2450077

Job&#58; 2450077.pbs02

02/20/2016 22&#58;27&#58;34  S    Job Queued at request of fermilab@...., owner = fermilab@...., job name = bl_0ce5afadda02, queue = osg
02/20/2016 22&#58;27&#58;34  A    queue=osg
02/20/2016 23&#58;18&#58;34  A    queue=osg_e
02/20/2016 23&#58;18&#58;35  L    Considering job to run
02/20/2016 23&#58;18&#58;35  S    Job Run at request of Scheduler@.... on exec_vnode (node0155&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0)
02/20/2016 23&#58;18&#58;35  S    Job Modified at request of Scheduler@....
02/20/2016 23&#58;18&#58;35  L    Job run
02/20/2016 23&#58;18&#58;35  A    user=fermilab group=osg project=_pbs_project_default jobname=bl_0ce5afadda02 queue=osg_e ctime=1456025254 qtime=1456028314 etime=1456028314 start=1456028315 exec_host=node0155/6
exec_vnode=(node0155&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0
Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=23&#58;30&#58;00 resource_assigned.mem=1048576kb resource_assigned.ncpus=1
resource_assigned.ngpus=0 resource_assigned.nphis=0
02/20/2016 23&#58;59&#58;18  S    Obit received momhop&#58;1 serverhop&#58;1 state&#58;4 substate&#58;42
02/20/2016 23&#58;59&#58;18  S    Exit_status=1 resources_used.cpupercent=27 resources_used.cput=00&#58;00&#58;03 resources_used.mem=6232kb resources_used.ncpus=1 resources_used.vmem=336144kb resources_used.walltime=00&#58;40&#58;43
02/20/2016 23&#58;59&#58;18  A    user=fermilab group=osg project=_pbs_project_default jobname=bl_0ce5afadda02 queue=osg_e ctime=1456025254 qtime=1456028314 etime=1456028314 start=1456028315 exec_host=node0155/6
exec_vnode=(node0155&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0
Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=23&#58;30&#58;00 session=11498 end=1456030758 Exit_status=1
resources_used.cpupercent=27 resources_used.cput=00&#58;00&#58;03 resources_used.mem=6232kb resources_used.ncpus=1 resources_used.vmem=336144kb resources_used.walltime=00&#58;40&#58;43 run_count=1

[root@node0155 mom_logs]# grep 2450077 20160220
02/20/2016 23&#58;18&#58;35;0080;pbs_mom;Job;2450077.pbs02;running prologue
02/20/2016 23&#58;18&#58;36;0008;pbs_mom;Job;2450077.pbs02;Started, pid = 11498
02/20/2016 23&#58;59&#58;17;0080;pbs_mom;Job;2450077.pbs02;task 00000001 terminated
02/20/2016 23&#58;59&#58;17;0008;pbs_mom;Job;2450077.pbs02;Terminated
02/20/2016 23&#58;59&#58;17;0100;pbs_mom;Job;2450077.pbs02;task 00000001 cput= 0&#58;00&#58;03
02/20/2016 23&#58;59&#58;17;0008;pbs_mom;Job;2450077.pbs02;kill_job
02/20/2016 23&#58;59&#58;17;0100;pbs_mom;Job;2450077.pbs02;node0155 cput= 0&#58;00&#58;03 mem=6232kb
02/20/2016 23&#58;59&#58;17;0080;pbs_mom;Job;2450077.pbs02;running epilogue
02/20/2016 23&#58;59&#58;18;0008;pbs_mom;Job;2450077.pbs02;no active tasks
02/20/2016 23&#58;59&#58;18;0100;pbs_mom;Job;2450077.pbs02;Obit sent
02/20/2016 23&#58;59&#58;18;0080;pbs_mom;Job;2450077.pbs02;copy file request received
02/20/2016 23&#58;59&#58;18;0100;pbs_mom;Job;2450077.pbs02;staged 2 items out over 0&#58;00&#58;00
02/20/2016 23&#58;59&#58;18;0008;pbs_mom;Job;2450077.pbs02;no active tasks
02/20/2016 23&#58;59&#58;18;0080;pbs_mom;Job;2450077.pbs02;delete job request received
02/20/2016 23&#58;59&#58;18;0008;pbs_mom;Job;2450077.pbs02;kill_job

[root@pbs02 ~]# grep fermilab /var/spool/PBS/server_priv/accounting/20160224 | grep Exit_status=1
02/24/2016 00&#58;51&#58;06;E;2502265.pbs02;user=fermilab group=osg project=_pbs_project_default jobname=bl_7a8293dd5826 queue=osg_e ctime=1456283410 qtime=1456290610 etime=1456290610 start=1456290617 exec_host=node0155/4 exec_vnode=(node0155&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0 Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=24&#58;00&#58;00 session=11996 end=1456293066 Exit_status=1 resources_used.cpupercent=19 resources_used.cput=00&#58;00&#58;03 resources_used.mem=6240kb resources_used.ncpus=1 resources_used.vmem=336144kb resources_used.walltime=00&#58;40&#58;49 run_count=1
02/24/2016 01&#58;40&#58;55;E;2503236.pbs02;user=fermilab group=osg project=_pbs_project_default jobname=bl_bc6c433b64a5 queue=osg_e ctime=1456292612 qtime=1456293602 etime=1456293602 start=1456293612 exec_host=node0172/6 exec_vnode=(node0172&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0 Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=23&#58;30&#58;00 session=21319 end=1456296055 Exit_status=1 resources_used.cpupercent=30 resources_used.cput=00&#58;00&#58;05 resources_used.mem=6236kb resources_used.ncpus=1 resources_used.vmem=336144kb resources_used.walltime=00&#58;40&#58;43 run_count=1
02/24/2016 05&#58;04&#58;14;E;2503315.pbs02;user=fermilab group=osg project=_pbs_project_default jobname=bl_3282fdc9a4b0 queue=osg_e ctime=1456296823 qtime=1456303214 etime=1456303214 start=1456305811 exec_host=node0060/7 exec_vnode=(node0060&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0 Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=23&#58;30&#58;00 session=27553 end=1456308254 Exit_status=1 resources_used.cpupercent=23 resources_used.cput=00&#58;00&#58;03 resources_used.mem=6236kb resources_used.ncpus=1 resources_used.vmem=336144kb resources_used.walltime=00&#58;40&#58;45 run_count=1
02/24/2016 07&#58;29&#58;56;E;2502223.pbs02;user=fermilab group=osg project=_pbs_project_default jobname=bl_ff7ccfbad697 queue=osg_e ctime=1456282539 qtime=1456314549 etime=1456314549 start=1456314553 exec_host=node0172/1 exec_vnode=(node0172&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0 Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=23&#58;30&#58;00 session=7487 end=1456316996 Exit_status=1 resources_used.cpupercent=28 resources_used.cput=00&#58;00&#58;04 resources_used.mem=6228kb resources_used.ncpus=1 resources_used.vmem=336144kb resources_used.walltime=00&#58;40&#58;43 run_count=1
02/24/2016 08&#58;48&#58;38;E;2505117.pbs02;user=fermilab group=osg project=_pbs_project_default jobname=bl_2527be2f8144 queue=osg_e ctime=1456314892 qtime=1456319092 etime=1456319092 start=1456319271 exec_host=node0060/3 exec_vnode=(node0060&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0) Resource_List.mem=1gb Resource_List.ncpus=1 Resource_List.ngpus=0 Resource_List.nodect=1 Resource_List.nphis=0 Resource_List.place=free&#58;shared Resource_List.qcat=osg_qcat Resource_List.select=1 Resource_List.walltime=24&#58;00&#58;00 session=19365 end=1456321718 Exit_status=1 resources_used.cpupercent=19 resources_used.cput=00&#58;00&#58;03 resources_used.mem=6236kb resources_used.ncpus=1 resources_used.vmem=363424kb resources_used.walltime=00&#58;40&#58;47 run_count=1

Xizhou
</div><script type='text/javascript'>
        $('#show_1160683295').click(function() {
            $('#detail_1160683295').slideDown("normal");
            $('#show_1160683295').hide();
            $('#hide_1160683295').show();
        });
        $('#hide_1160683295').click(function() {
            $('#detail_1160683295').slideUp();
            $('#hide_1160683295').hide();
            $('#show_1160683295').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1456257010'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-23T19:50:10+00:00">Feb 23, 2016 07:50 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456257010">&nbsp;</a></div><pre>Ah yes, they&#39;re on an old version of HTCondor-CE so they don&#39;t show up
in condor_ce_info_status. Not sure they would have time to help as it
is. Xizhou, could you start posting some PBS logs? Maybe we can help.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456256812'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-23T19:46:52+00:00">Feb 23, 2016 07:46 PM UTC</time> by <b>echism</b><a class="anchor" name="1456256812">&nbsp;</a></div><pre>What about Vanderbilt?</pre></div><div class='update_description'><i onclick="document.location='27686#1456256474'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-23T19:41:14+00:00">Feb 23, 2016 07:41 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456256474">&nbsp;</a></div><pre>Unfortunately, I don&#39;t really know of any sites running like yours. With
PBS Pro and the &#39;rogue process killer&#39;, you guys are very unique. I
could find some contacts at other torque sites, though.

Nebraska, PBS, GT, Purdue, Hyak are the HTCondor-CE PBS sites I see in
condor_ce_info_status.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1456256250'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-23T19:37:30+00:00">Feb 23, 2016 07:37 PM UTC</time><a class="anchor" name="1456256250">&nbsp;</a></div><pre>Brian,

Do you know who else runs HTCondor-CE with PBS? Maybe Florida, if I remember correctly?

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1456255750'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-23T19:29:10+00:00">Feb 23, 2016 07:29 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1456255750">&nbsp;</a></div><pre>I will check the PBS logs to find the possible reasons that the jobs were being killed. Meanwhile, is there another site which has a similar setting as Clemson that we may consult with?

Thanks,

Xizhou

<font color='#7F7E6F'>&#62; On Feb 23, 2016, at 2&#58;13 PM, Open Science Grid FootPrints &#60;osg@....&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1456254780'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-23T19:13:00+00:00">Feb 23, 2016 07:13 PM UTC</time><a class="anchor" name="1456254780">&nbsp;</a></div><pre>Hi Brian, Xizhou, et al.,

There is no change in OSG glidein performance. Occasionally, a few glideins appear to run normally, accepting user jobs. But most of the time we still see them being killed off by PBS [1].

I also suspect the site metric report for the Fermilab glideins is bogus. We see Fermilab glideins mostly sitting idle, never running a user job. It looks like there may be a matchmaking problem with the Fermilab frontend [2] [3]. Maybe the Fermilab folks can comment on this matching problem? If they actually had any jobs running on the glideins for a significant amount of time, their glideins may also still be getting killed off by PBS too. But since they only idle 20 min and kill themselves, we&#39;re probably beating PBS to the punch. At least, that is my guess as this seems to be what is happening for the GLOW glideins.

Marty Kandes
UCSD Glidein Factory Operations

[1]

Received kill signal... shutting down child processes

[2]

<a href='http&#58;//glidein.grid.iu.edu/factory/monitor/factoryStatus.html?entry=OSG_US_Clemson-Palmetto_condce&frontend=Fermilab-fifebatch_frontend&infoGroup=running&elements=StatusRunning' target='_blank' rel='nofollow'>http&#58;//glidein.grid.iu.edu/factory/monitor/factoryStatus.html?entry=OSG_US_Clemson-Palmetto_condce&frontend=Fermilab-fifebatch_frontend&infoGroup=running&elements=StatusRunning</a>,ClientGlideRunning,ClientGlideIdle,&rra=2&window_min=0&window_max=0&timezone=-8

[3]

<a href='http&#58;//gfactory-1.t2.ucsd.edu/factory/monitor/factoryStatus.html?entry=OSG_US_Clemson-Palmetto_condce&frontend=Fermilab-fifebatch_frontend&infoGroup=running&elements=StatusRunning' target='_blank' rel='nofollow'>http&#58;//gfactory-1.t2.ucsd.edu/factory/monitor/factoryStatus.html?entry=OSG_US_Clemson-Palmetto_condce&frontend=Fermilab-fifebatch_frontend&infoGroup=running&elements=StatusRunning</a>,ClientGlideRunning,ClientGlideIdle,&rra=2&window_min=0&window_max=0&timezone=-8

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1456170429'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-22T19:47:09+00:00">Feb 22, 2016 07:47 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1456170429">&nbsp;</a></div><pre>Hi Brian,

I think we may close this ticket at this point. If we continue seeing similar problems, we can reopen the ticket.

This morning, I received the OSG site metric report for Clemson-Palmetto site during last week period. It looks like most jobs completed except a large failures from the VO  OSG as shown in the table pasted below. The small percent job failures from other VO’s may be related to the preemption policy that has been enforced on the Palmetto cluster. I will keep watching the report and if we continuously see a high OSG job failure rate, we will work with you and the OSG support team to get the issue resolved.

Summary of active reporting names for VO Fermilab ()&#58;
_________________________________________________________________________________________________________________________
|    VO    | Field(s) of Science | Reporting Name |    Job   | # Sites | # Jobs |   Wall   | Delta | Delta | Delta Wall |
|          |                     |                | Success? |         |        | Time (h) | Sites | Jobs  |  Time (h)  |
|__________|_____________________|________________|__________|_________|________|__________|_______|_______|____________|
| ATLAS    |                     |    atlas       |    Yes   |    1    |      4 |      0.4 |   0   |  -122 |     -12.4  |
| ATLAS    |                     |    atlas       |    No    |    1    |      5 |      0.5 |   0   |    +5 |      +0.5  |
| Fermilab |                     |    fermilab    |    Yes   |    1    |  11310 |   4379.7 |   0   | +6200 |   -1214.2  |
| Fermilab |                     |    fermilab    |    No    |    1    |    351 |    153.3 |   0   |  -720 |    -382.0  |
| Fermilab |                     |    minos       |    Yes   |    1    |      1 |      0.4 |  +1   |    +1 |      +0.4  |
| Fermilab |                     |    mu2e        |    Yes   |    1    |   3084 |   1190.4 |   0   | +1006 |    +348.4  |
| Fermilab |                     |    mu2e        |    No    |    1    |    155 |     47.5 |   0   |  -385 |    -262.3  |
| Fermilab |                     |    nova        |    Yes   |    1    |    398 |    154.9 |   0   |  -748 |   -2768.1  |
| Fermilab |                     |    nova        |    No    |    1    |      5 |      3.4 |   0   |   -43 |     -26.8  |
| GLOW     |                     |    glow        |    Yes   |    1    |    737 |   1391.6 |   0   |  -740 |  -11622.4  |
| GLOW     |                     |    glow        |    No    |    1    |    218 |    100.6 |   0   |  +113 |     +59.6  |
| OSG      |                     |    osg         |    Yes   |    1    |   5922 | 109871.3 |   0   | -2110 |  -23870.7  |
| OSG      |                     |    osg         |    No    |    1    |   9283 |  46896.4 |   0   | +4240 |  +11248.7

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1456154658'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-22T15:24:18+00:00">Feb 22, 2016 03:24 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1456154658">&nbsp;</a></div><pre>Marty/Factory Ops,

Any word on this?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1455565579'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-15T19:46:19+00:00">Feb 15, 2016 07:46 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1455565579">&nbsp;</a></div><pre>Marty/Factory ops,

Are you still seeing pilots being removed after Xizhou cleaned up some of his WN&#39;s?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1455223866'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-11T20:51:06+00:00">Feb 11, 2016 08:51 PM UTC</time> by <b>echism</b><a class="anchor" name="1455223866">&nbsp;</a></div><pre>Hi all,

What direction do you want to take from here?

Thank you</pre></div><div class='update_description'><i onclick="document.location='27686#1455114551'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-10T14:29:11+00:00">Feb 10, 2016 02:29 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1455114551">&nbsp;</a></div><pre>Hi Brian,

We found that the system logs generated by a system library have filled up some compute node. This could explain the glidein errors that you just found. The developers are aware of this problem. Hope they will have a fix soon.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1455060191'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-09T23:23:11+00:00">Feb 9, 2016 11:23 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1455060191">&nbsp;</a></div><pre>1. Greg, could you take a look at the stacktrace for us?
2. At a quick glance, this looks like the same error as before&#58; PBS
kills the glidein, the glidein passes along the SIGTERM, HTCondor tries
to clean up but it&#39;s had the rug pulled out from under it.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1455058639'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-09T22:57:19+00:00">Feb 9, 2016 10:57 PM UTC</time><a class="anchor" name="1455058639">&nbsp;</a></div><pre>Brian, Xizhou,

Overall, things look much better. However, we&#39;re still observing some oddities with OSG gldeins. First, we still see the occasional segfault [1] upon startup. I&#39;m not sure why this would be occurring if we&#39;re within the PBS limits. Second, even when glideins run for some time and appear to be doing useful work, many exit in an unusual way [2]. Not sure if this is affecting user jobs, but seems to occur on a large percentage of glideins.

Marty Kandes
UCSD Glidein Factory Operations

[1]

Unsetting X509_USER_PROXY
Received kill signal... shutting down child processes
Stack dump for process 20356 at timestamp 1455002195 (24 frames)
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(dprintf_dump_stack+0x12d)[0x2b685002daed]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(+0x176432)[0x2b6850020432]
/lib64/libpthread.so.0[0x37b200f710]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN16ProcFamilyClient13signal_familyEi21proc_family_command_tRb+0x33)[0x2b6850165bf3]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN15ProcFamilyProxy11kill_familyEi+0x3f)[0x2b685001092f]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN10DaemonCore11Kill_FamilyEi+0x16)[0x2b685013b8d6]
/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master(_ZN6daemon10KillFamilyEv+0x1f)[0x40da0f]
/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master(_ZN6daemon8HardKillEv+0x53)[0x40da83]
/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master(_ZN7Daemons18HardKillAllDaemonsEv+0x59)[0x40f209]
/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master(DoCleanup+0x46)[0x40ac56]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_EXCEPT_+0x11e)[0x2b685006cdbe]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN15ProcFamilyProxy24recover_from_procd_errorEv+0x1bb)[0x2b685001073b]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN15ProcFamilyProxy11kill_familyEi+0x30)[0x2b6850010920]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN10DaemonCore11Kill_FamilyEi+0x16)[0x2b685013b8d6]
/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master(_ZN6daemon10KillFamilyEv+0x1f)[0x40da0f]
/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master(_ZN6daemon6ExitedEi+0x149)[0x40e059]
/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master(_ZN7Daemons9AllReaperEii+0x109)[0x40fb69]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN10DaemonCore10CallReaperEiPKcii+0x195)[0x2b685013f555]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN10DaemonCore17HandleProcessExitEii+0x2e9)[0x2b6850147679]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN10DaemonCore24HandleDC_SERVICEWAITPIDSEi+0x7e)[0x2b685014787e]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_ZN10DaemonCore6DriverEv+0x823)[0x2b685014bae3]
/local_scratch/glide_IC6Zuc/main/condor/lib/libcondor_utils_8_4_4.so(_Z7dc_mainiPPc+0x1799)[0x2b685012cbd9]
/lib64/libc.so.6(__libc_start_main+0xfd)[0x37b141ed5d]
<div id='show_197783789' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_197783789'>/local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master[0x4090e9]
/local_scratch/glide_IC6Zuc/main/condor_startup.sh&#58; line 13&#58; 20356 Segmentation fault      $CONDOR_DIR/sbin/condor_master -f -pidfile $PWD/condor_master2.pid
/local_scratch/glide_IC6Zuc/main/condor_startup.sh&#58; line 15&#58; /local_scratch/glide_IC6Zuc/main/condor/sbin/condor_master&#58; No such file or directory
ls&#58; cannot access log&#58; No such file or directory
/local_scratch/glide_IC6Zuc/main/condor_startup.sh&#58; line 1002&#58; /local_scratch/glide_IC6Zuc/main/error_gen.sh&#58; No such file or directory
rm&#58; cannot remove &#96;/local_scratch/glide_IC6Zuc/execute/dir_20381&#39;&#58; Directory not empty

[2]

02/09/16 08&#58;57&#58;44 (pid&#58;8400) Buf&#58;&#58;write()&#58; condor_write() failed
02/09/16 08&#58;57&#58;44 (pid&#58;8400) Failed to send non-blocking update to &#60;130.127.160.109&#58;9619&#62;.
02/09/16 08&#58;59&#58;42 (pid&#58;8400) Updated job ClassAd&#58;
02/09/16 09&#58;00&#58;17 (pid&#58;8400) Got SIGTERM. Performing graceful shutdown.
02/09/16 09&#58;00&#58;17 (pid&#58;8400) shutdown graceful
02/09/16 09&#58;00&#58;17 (pid&#58;8400) Cron&#58; Killing all jobs
02/09/16 09&#58;00&#58;17 (pid&#58;8400) Cron&#58; Killing all jobs
02/09/16 09&#58;00&#58;17 (pid&#58;8400) Killing job mips
02/09/16 09&#58;00&#58;17 (pid&#58;8400) Killing job kflops
02/09/16 09&#58;00&#58;17 (pid&#58;8400) Changing activity&#58; Busy -&#62; Retiring
02/09/16 09&#58;00&#58;19 (pid&#58;8400) error writing to named pipe&#58; watchdog pipe has closed
02/09/16 09&#58;00&#58;19 (pid&#58;8400) LocalClient&#58; error sending message to server
02/09/16 09&#58;00&#58;19 (pid&#58;8400) ProcFamilyClient&#58; failed to start connection with ProcD
02/09/16 09&#58;00&#58;19 (pid&#58;8400) get_usage&#58; ProcD communication error
02/09/16 09&#58;00&#58;19 (pid&#58;8400) waiting a second to allow the ProcD to be restarted
02/09/16 09&#58;00&#58;20 (pid&#58;8400) Result of &#34;get_usage&#34; operation from ProcD&#58; ERROR&#58; No family with the given PID is registered
02/09/16 09&#58;00&#58;20 (pid&#58;8400) ERROR &#34;Starter&#58;&#58;percentCpuUsage()&#58; Fatal error getting process info for the starter and decendents&#34; at line 1273 in file /slots/01/dir_4917/userdir/src/condor_startd.V6/Starter.cpp
02/09/16 09&#58;00&#58;20 (pid&#58;8400) Changing state and activity&#58; Claimed/Retiring -&#62; Preempting/Killing
02/09/16 09&#58;00&#58;20 (pid&#58;8400) startd exiting because of fatal exception.

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049
</div><script type='text/javascript'>
        $('#show_197783789').click(function() {
            $('#detail_197783789').slideDown("normal");
            $('#show_197783789').hide();
            $('#hide_197783789').show();
        });
        $('#hide_197783789').click(function() {
            $('#detail_197783789').slideUp();
            $('#hide_197783789').hide();
            $('#show_197783789').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1454949017'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-08T16:30:17+00:00">Feb 8, 2016 04:30 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1454949017">&nbsp;</a></div><pre>Marty,

How do the glideins look now?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1454948233'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-08T16:17:13+00:00">Feb 8, 2016 04:17 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1454948233">&nbsp;</a></div><pre>Hi Brian.

We have cleaned up the local scratch last week. Everything is looking fine now.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1454341827'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-02-01T15:50:27+00:00">Feb 1, 2016 03:50 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1454341827">&nbsp;</a></div><pre>Xizhou,

How&#39;s the cleanup of your local scratch going?

Cheers,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1454019005'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T22:10:05+00:00">Jan 28, 2016 10:10 PM UTC</time> by <b>Marco Mambelli</b><a class="anchor" name="1454019005">&nbsp;</a></div><pre>Hi all,
policing of local disks cannot be handled by the glidein that runs as the same user running the misbehaving job.
It can be handled by the local job manager (PBS) that can create a temporary directory deleted after the job&#39;s completion&#58;
<a href='https&#58;//twiki.grid.iu.edu/bin/view/Documentation/Release3/PbsBatchSystemHints#Create_a_Dynamic_OSG_WN_TMP_Dire' target='_blank' rel='nofollow'>https&#58;//twiki.grid.iu.edu/bin/view/Documentation/Release3/PbsBatchSystemHints#Create_a_Dynamic_OSG_WN_TMP_Dire</a>

The bad job will still crash the glidein and maybe all the jobs currently running on the node but will be cleaned up after completion.
Not respecting the disk limits is against OSG AUP. The user should be called to fix the script and depending on the level of havoc caused (one job, many jobs, affecting one, few or many worker nodes, ...) the site can protect itself by suspending temporarily the VO of the jobs filling the nodes.
Marco</pre></div><div class='update_description'><i onclick="document.location='27686#1454012898'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T20:28:18+00:00">Jan 28, 2016 08:28 PM UTC</time><a class="anchor" name="1454012898">&nbsp;</a></div><pre>Brian, Xizhou,

I&#39;m not able to find any sign of the original problem with PBS killing of the glideins. It looks like using the explicit submit attributes did the trick.

I can confirm we&#39;ve seen recent glideins failing node validation due to the full scratch space issue. So it sounds like perhaps the same job just kept trying to run at Clemson, filling up the local scratch on each node it tried to run on, killing glidein, preventing clean up, then moving on to the next node to repeat? Unfortunately, I&#39;m not sure how we prevent user jobs from filling up the available scratch.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1454004734'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T18:12:14+00:00">Jan 28, 2016 06:12 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1454004734">&nbsp;</a></div><pre>Thanks, Brian.

However, this particular issue we saw today is likely because a single OSG job creates a large output file.

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1454004370'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T18:06:10+00:00">Jan 28, 2016 06:06 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1454004370">&nbsp;</a></div><pre>Xizhou,

Your OSG_WN_TMP dir filling up is likely due to the issues we&#39;ve been
discussing in this thread. OSG_WN_TMP (/local_scratch in your case) is
for user jobs as a scratch dir. Most of the time, user jobs are cleaned
up by HTCondor and the pilots but since the pilot jobs have been killed,
they&#39;re not getting a chance to clean up.

Feel free to remove files from that dir that were created by OSG jobs
that are older than a few days. In the future, you should think about
automating this process in case there are problems with the glideins
again. I created a ticket to improve our documentation regarding this&#58;

<a href='https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-2183?filter=12359' target='_blank' rel='nofollow'>https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-2183?filter=12359</a>

Sorry for the trouble,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1453999482'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T16:44:42+00:00">Jan 28, 2016 04:44 PM UTC</time> by <b>echism</b><a class="anchor" name="1453999482">&nbsp;</a></div><pre>Reopening this as requested</pre></div><div class='update_description'><i onclick="document.location='27686#1453997531'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T16:12:11+00:00">Jan 28, 2016 04:12 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1453997531">&nbsp;</a></div><pre>This morning, we saw a different OSG job problem that is unrelated to this issue in this thread.

In this morning, one of our HPC admins found that some osg jobs were filling up the /local_scratch on 263 computer nodes. I pasted some text from the email. As our site don’t direct communicate with OSG users, please advise what is the typical route to resolve this type of issues.

Thanks,

Xizhou

--------------Text from our admin’s email-----------------------

<font color='#7F7E6F'>&#62;[root@node1062 dir_3093]# df -k /local_scratch/</font>

<font color='#7F7E6F'>&#62;Filesystem     1K-blocks      Used Available Use% Mounted on</font>

<font color='#7F7E6F'>&#62;/dev/sda3      115515868 109415140    226144 100% /local_scratch</font>

<font color='#7F7E6F'>&#62;</font>

<font color='#7F7E6F'>&#62;[root@node1062 dir_3093]# du -sh *</font>

<font color='#7F7E6F'>&#62;4.0K     1.json</font>

<font color='#7F7E6F'>&#62;4.0K     condor_exec.exe</font>

<font color='#7F7E6F'>&#62;0           _condor_stderr</font>

<font color='#7F7E6F'>&#62;0           _condor_stdout</font>

<font color='#7F7E6F'>&#62;220K    jq</font>

<font color='#7F7E6F'>&#62;101G   out0.txt</font>

<font color='#7F7E6F'>&#62;4.0K     sim0.json</font>

<font color='#7F7E6F'>&#62;4.0K     sim.json</font>
<div id='show_1503540879' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1503540879'>
<font color='#7F7E6F'>&#62;12M    spt_z2</font>

<font color='#7F7E6F'>&#62;</font>

<font color='#7F7E6F'>&#62;[root@node1062 dir_3093]# tail -50 out0.txt worm count 0</font>

<font color='#7F7E6F'>&#62;</font>

<font color='#7F7E6F'>&#62;0%   10   20   30   40   50   60   70   80   90   100%</font>

<font color='#7F7E6F'>&#62;|----|----|----|----|----|----|----|----|----|----|</font>

<font color='#7F7E6F'>&#62;worm count 0</font>
</div><script type='text/javascript'>
        $('#show_1503540879').click(function() {
            $('#detail_1503540879').slideDown("normal");
            $('#show_1503540879').hide();
            $('#hide_1503540879').show();
        });
        $('#hide_1503540879').click(function() {
            $('#detail_1503540879').slideUp();
            $('#hide_1503540879').hide();
            $('#show_1503540879').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1453997116'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T16:05:16+00:00">Jan 28, 2016 04:05 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1453997116">&nbsp;</a></div><pre>Jeff/Marty,

Are you guys still seeing pilots randomly removed? If you are then this ticket needs to be re-opened.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1453996777'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T15:59:37+00:00">Jan 28, 2016 03:59 PM UTC</time> by <b>echism</b><a class="anchor" name="1453996777">&nbsp;</a></div><pre>Closing as requested!

Thank you</pre></div><div class='update_description'><i onclick="document.location='27686#1453996272'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T15:51:12+00:00">Jan 28, 2016 03:51 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1453996272">&nbsp;</a></div><pre>Yes, please.

For a record, the cause for this issue mainly came from some mismatches between OSG jobs and PBS scheduler at the Clemson site. Our PBS scheduler requires all jobs specify both memory and wall time requirements explicitly. Previously the OSG jobs didn’t include those requirements. Once the resources used by some OSG jobs exceed the default limits, such jobs will be terminated by the scheduler.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1453996249'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T15:50:49+00:00">Jan 28, 2016 03:50 PM UTC</time><a class="anchor" name="1453996249">&nbsp;</a></div><pre>Hi,

Yes, I *think* we are OK (at least we are from the Fermilab VO point of view) but others should comment.

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Kenneth Herner 1385</pre></div><div class='update_description'><i onclick="document.location='27686#1453995172'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-28T15:32:52+00:00">Jan 28, 2016 03:32 PM UTC</time> by <b>echism</b><a class="anchor" name="1453995172">&nbsp;</a></div><pre>Should this be closed?

Thank you</pre></div><div class='update_description'><i onclick="document.location='27686#1453405217'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-21T19:40:17+00:00">Jan 21, 2016 07:40 PM UTC</time><a class="anchor" name="1453405217">&nbsp;</a></div><pre>Xizhou et al.,

We&#39;ve gone ahead and explicitly set the memory-related attributes in the factory config to request +maxMemory = 2048 (in MBs) upon glidein submission at Clemson, while setting
GLIDEIN_MaxMemMBs = 2048 to advertise back to the VO frontends this is the max allowed per user job, which had previously been set at the factory default of 2500 MBs. If the glideins were originally defaulting to requesting 1 GB upon submission to PBS, but the VO frontends were sending jobs with larger memory requirements, then this could explain where we were running into the problems.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1453404564'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-21T19:29:24+00:00">Jan 21, 2016 07:29 PM UTC</time><a class="anchor" name="1453404564">&nbsp;</a></div><pre>Marco,

As requested, I&#39;ve attached a set of sample *.out and *.err from OSG, Fermilab and Glow glideins.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1453392493'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-21T16:08:13+00:00">Jan 21, 2016 04:08 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1453392493">&nbsp;</a></div><pre>I talked with Corey who manages the PBS scheduler on our site. Here are what I have found&#58;

(1)  The PBS scheduler will kill the jobs if they used more memory than what they request in the same way as the scheduler deals with the wall time. The default allocated memory for the OSG job is 1GB.

(2)  The PBS mom log files on the compute node will show the reason that a job ended.

(3)  The PBS server logs on the PBS server node will provide information whether the job is preempted.

I will try to write a script to analyze those log files and share the results with you in the next a few days.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1453391164'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-21T15:46:04+00:00">Jan 21, 2016 03:46 PM UTC</time> by <b>Marco Mambelli</b><a class="anchor" name="1453391164">&nbsp;</a></div><pre>Hi Jeff, Xizhou,
with the current information it is difficult to pinpoint the problem. It could be the site (some failure or busy and preempting), it could be the glidein failing somewhere or the job (failing or using too many resources).

Jeff,
is it still the case that the early ending (fermilab and glow) jobs do not return stdout and stderr from the glideins?
If not, If you have some stdout/err files I&#39;d like to have a look now that other problems have been fixed.

Xizhou,
from the PBS logs is it there any way to understand why/how those early terminating jobs ended?
They exited with an error code, on their own, they were preempted, they were kicked out because they used too many resources

Thank you, Marco</pre></div><div class='update_description'><i onclick="document.location='27686#1453337411'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-21T00:50:11+00:00">Jan 21, 2016 12:50 AM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1453337411">&nbsp;</a></div><pre>Hi Jeff,

I hadn’t seen cases in Clemson where jobs are killed because of using too much memory. I will check our admins to get more details on this matter.

The default chunk_memory is set to 1GB and the max memory is set to 5TB for osg jobs. If you think memory could be a reason, you may try to request more memory for the jobs.

There are two other possibilities for OSG jobs to be killed.

First, the cluster uses a CONDO model, which implies that jobs with lower priority might be preempted by owner jobs.

Second, some issues on the file systems may also cause job failures.

Xizhou

<font color='#7F7E6F'>&#62; On Jan 20, 2016, at 7&#58;17 PM, Open Science Grid FootPrints &#60;osg@....&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1453335419'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-21T00:16:59+00:00">Jan 21, 2016 12:16 AM UTC</time><a class="anchor" name="1453335419">&nbsp;</a></div><pre>Hi Xizhou,

GLIDEIN_Max_Walltime is a setting that gets into the pilot, you won&#39;t see it on the CE config side.  It&#39;s correctly configured, I always make it 30 minutes shorter than the submit file walltime (the 86400 you see) to give time to cleanup.

I&#39;m starting to suspect something else is at fault here.  The pilots I&#39;m looking through that fail with the segfault happen at any time, I see some happen on the order of minutes, others hours, all happen below 24h since I made the walltime fixes.

Just a wild guess, do you kill jobs that use too much memory?  What is the allowed amount of memory per slot at Clemson?

by /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=jdost/CN=732648/CN=Jeffrey Michael Dost</pre></div><div class='update_description'><i onclick="document.location='27686#1453319771'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-20T19:56:11+00:00">Jan 20, 2016 07:56 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1453319771">&nbsp;</a></div><pre>a.    The osg jobs are able to run long hours like the snapshot below. The glow and fermilab jobs exits soon after they are launched. They are running much less than the maxWallTime. I looked at the PBS job submit file and now confirm that the walltime in that file is set to  86400 seconds or 24 hours.

2033716.pbs02     bl_87cf1c192fa6  osg               17&#58;26&#58;19 F osg_e
2033717.pbs02     bl_52097c9af02f  osg               03&#58;39&#58;32 F osg_e
2033718.pbs02     bl_4e4b6991b0ae  osg               03&#58;31&#58;06 F osg_e
2033719.pbs02     bl_b85e7ed3fe64  osg               03&#58;23&#58;38 F osg_e
2033720.pbs02     bl_54a2bc5c30b4  fermilab          00&#58;00&#58;24 F osg_e
2033721.pbs02     bl_4f41978a2db9  glow              00&#58;01&#58;35 F osg_e
2033722.pbs02     bl_8da6ca87bbac  glow              00&#58;01&#58;18 F osg_e

b.    I didn’t see the variable GLIDEIN_Max_Walltime either from “condor_ce_config_val –dump” or in the glidein_startup.sh file. How can I find this variable?

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1453317462'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-20T19:17:42+00:00">Jan 20, 2016 07:17 PM UTC</time> by <b>Marco Mambelli</b><a class="anchor" name="1453317462">&nbsp;</a></div><pre>+maxWallTime &#62; GLIDEIN_Max_Walltime and glideins are still being killed by PBS instead of shutting down.
Can you check why? Checking after how long the glideins (PBS jobs) were killed.
a. is the job still running for less than maxWallTime?
somehow the walltime request is not propagating or PBS is not honoring it. This should be investigated to see where the chain is broken.
Maybe there is preemption that is ignoring the walltime requests
b. is the glidein running longer than GLIDEIN_Max_Walltime?
There may be a bug in the code or there may be long condor/glidein response times. Try to reduce GLIDEIN_Max_Walltime of about 10min and see if that fixes it.

The correct solution should be 0 so please help us understand why is not working.
The other 2 options are pure condor options (no special glidein code/option)&#58;
<a href='http&#58;//research.cs.wisc.edu/htcondor/manual/v8.4/12_Appendix_A.html#101501' target='_blank' rel='nofollow'>http&#58;//research.cs.wisc.edu/htcondor/manual/v8.4/12_Appendix_A.html#101501</a>

JobMaxVacateTime (#2) should be set in the submit host connected to the frontend or the users submitting those jobs can do it.
Nothing that the factory or the site can affect.
MachineMaxVacateTime (#1) is a submit attribute you can set on the factory side (<a href='http&#58;//glideinwms.fnal.gov/doc.prd/factory/configuration.html#attrs' target='_blank' rel='nofollow'>http&#58;//glideinwms.fnal.gov/doc.prd/factory/configuration.html#attrs</a>). There is no special description about this attribute in the glidinwms documentation because you could put there anything that you want to end in the condor configuration and this is not something that is used normally.</pre></div><div class='update_description'><i onclick="document.location='27686#1453315833'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-20T18:50:33+00:00">Jan 20, 2016 06:50 PM UTC</time><a class="anchor" name="1453315833">&nbsp;</a></div><pre>Xizhou,

From Marco&#39;s options, it sounds like JobMaxVacateTime (Option #2) may be configurable on your end, while MachineMaxVacateTime (Option #1) may be configurable from the factory as a submit attribute we pass to you, but I&#39;m entirely and could not find documentation on it. Let&#39;s wait and see what Marco has to see, he knows the software internals.

Marty Kandes
UCSD Glidein Factory Operations

P.S. +maxWallTime &#62; GLIDEIN_Max_Walltime (Option #0) is already in place, but we&#39;re still observing glideins being killed off without shutting down properly.

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1453315272'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-20T18:41:12+00:00">Jan 20, 2016 06:41 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1453315272">&nbsp;</a></div><pre>Hi Marty,

No. I didn’t. I am on Clemson’s HPC team and have no knowledge on user’s jobs submit files. Should the JobMaxVacateTime variable be set at the factory configuration or at the OSG_CE configuration at the Clemson site? If it needs to be set on our site, please let me know.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1453314636'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-20T18:30:36+00:00">Jan 20, 2016 06:30 PM UTC</time><a class="anchor" name="1453314636">&nbsp;</a></div><pre>Marco,

Is MachineMaxVacateTime a submit attribute we can set on the factory side?

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1453313382'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-20T18:09:42+00:00">Jan 20, 2016 06:09 PM UTC</time><a class="anchor" name="1453313382">&nbsp;</a></div><pre>Xizhou,

Did you attempt to add JobMaxVacateTime in the jobs submit files?

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1452909190'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-16T01:53:10+00:00">Jan 16, 2016 01:53 AM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1452909190">&nbsp;</a></div><pre>Hi Marco, Jeff, and Brian,

Thank all of you for solving the mystery.

Cheers!

Xizhou

<font color='#7F7E6F'>&#62; On Jan 15, 2016, at 7&#58;42 PM, Open Science Grid FootPrints &#60;osg@....&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1452904920'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-16T00:42:00+00:00">Jan 16, 2016 12:42 AM UTC</time> by <b>Marco Mambelli</b><a class="anchor" name="1452904920">&nbsp;</a></div><pre>Hi Xizhou,
Jeff should have been taking care of the max wall time that was not set (see his message below).
About the shoutdown/eviction&#58; GLIDEIN_Graceful_Shutdown is only used when the glidein decides it is time to close and gives a warning to the job. You can set it to a long value so that the glidein will initiate the shutdown of the job well in advance of being terminated by PBS and the jobs will have plenty of time to terminate. This is the normal behavior of the glideins when GLIDEIN_MAX_WALLTIME is &#60;= of the wall time in PBS. It should close shop before being kicked out.

In the case of PBS initiated shutdown (e.g. wall time was not requested and the default was shorter than GLIDEIN_MAX_WALLTIME) it receives the sigterm and propagates it to condor. You have to change the condor settings, you can do that in the startd configuration setting MachineMaxVacateTime (1) or in your user jobs via JobMaxVacateTime (2). The smaller of the 2 will be used by condor.

So your options are&#58;
0. avoid PBS initiated shutdowns by requesting a wall time (+maxWallTime) bigger than what you set for GLIDEIN_MAX_WALLTIME (both of these are factory configurations)
1. add MachineMaxVacateTime in the attributes of that entry (factory configuration). Condor default is 300, you want 30 or less.
2. add JobMaxVacateTime in the jobs submit files (from GLOW?). Also here you want 30 or less.

Cheers,
Marco</pre></div><div class='update_description'><i onclick="document.location='27686#1452884678'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-15T19:04:38+00:00">Jan 15, 2016 07:04 PM UTC</time><a class="anchor" name="1452884678">&nbsp;</a></div><pre>Hi guys,

I haven&#39;t been following this in too much detail, but we should have been smarter about configuring clemson in the factory all along to avoid getting killed.

If I understand from discussion, we need to explicitly set max walltime, but I don&#39;t see it in the factory entry. Normally for condor ce entries we add the following condor attribute&#58;
&#60;submit_attr name=&#34;+maxWallTime&#34; value=&#34;1440&#34;/&#62;

But factory ops failed to do this, so I wouldn&#39;t be surprised PBS is trying to kill them in 30 min.  I will add this shortly and hopefully it will solve all our problems.

Sorry, Factory ops should have been better about catching this sooner.

Jeff Dost
OSG Glidein Factory Operations

by /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=jdost/CN=732648/CN=Jeffrey Michael Dost</pre></div><div class='update_description'><i onclick="document.location='27686#1452883286'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-15T18:41:26+00:00">Jan 15, 2016 06:41 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1452883286">&nbsp;</a></div><pre>Xizhou,

After a quick look at the pilot startup script, it looks like it does intercept SIGTERM and tries to pass it off to the forked processes (Condor daemons). When the Condor daemons receive SIGTERMs, they try to give jobs time to complete before exiting and it looks like the default grace period that pilots expect is 120s. We should change that to  accommodate your site&#39;s 30s grace period.

Parag/Marco&#58; I see GLIDEIN_Graceful_Shutdown in the glidein source. Is that the correct value to set to change the graceful shutdown timeout for glideins?

Factory ops&#58; If that is the proper value, can we change Clemson&#39;s value to 30s? Maybe even less?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1452877152'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-15T16:59:12+00:00">Jan 15, 2016 04:59 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1452877152">&nbsp;</a></div><pre>Hi Brian,

I agree with your analysis. The problem arises when PBS only terminates the pilot job but not the payload. I don’t know how the pilot job creates the payload. If they are in the same session, killing the pilot job should terminate the payload. I haven’t noticed similar problems for our local user jobs on the Palmetto cluster. However, in the past I did see some orphan MPI processes residing on some compute nodes for long time after the leading process was normally terminated. This could have the same cause as the one we are dealing with. Some jobs like MPI jobs do fork processes. The MPI launcher such as mpiexec normally handles the cleanup when the job was killed.

For the walltime limit issue, I forget to mention that we currently set the default walltime for osg jobs to 24 hours. That might be the reason why your job working fine. If you can check the PBS job script corresponding to your job, that would verify if the “#PBS -l walltime hh&#58;mm&#58;ss” is inserted or not.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1452875715'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-15T16:35:15+00:00">Jan 15, 2016 04:35 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1452875715">&nbsp;</a></div><pre>Xizhou,

Technically it&#39;s not that the payload job needs to be killed, it&#39;s that
the pilot job is not being killed completely&#58; the initial pilot script
is killed but that pilot script forks other processes that don&#39;t receive
SIGTERMs from PBS. Putting SIGTERM handling logic into the pilot could
be something we add for robustness but this seems like it&#39;s a real
problem that could affect Clemson users&#39; jobs. Has this not been a
problem until now? Are your users not forking processes?

I just submitted a job that sleeps for an hour and requests a walltime
limit of 1 minute. The job completed 10 minutes after my submission so I
think the walltime requests are working fine, it&#39;s just that the
incoming pilot jobs you&#39;re seeing don&#39;t request a maxwalltime.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1452869412'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-15T14:50:12+00:00">Jan 15, 2016 02:50 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1452869412">&nbsp;</a></div><pre>Hi Brian,

Thank you for the testing and analysis. Below is the information I collected and hope it would be useful for fixing the problem.

The problem is not related to the rogue pid killer. The rogue pid killer periodically checks user processes which are not associated with any user having a running PBS job on the compute node. If such a process is found, the rogue pid killer will terminate it.
For the issue we are looking, the pilot job and the payload process are associated with the “osg” user which has an active PBS job on the compute node, before the requested walltime expires, the rogue pid killer will do nothing with both the pilot job and the payload.

PBS enforces the walltime policy by checking the walltime used by a job, if the value is beyond the assigned walltime, PBS will send a SIGTERM signal to the process it started and give the process 30 seconds buffer to exit smoothly. If the process doesn’t terminate by itself, PBS will kill the process right away.

PBS can use a hook to handle the termination. However, because PBS doesn’t know the mapping between the pilot job and the payload and it is possible that there are other OSG jobs running on the same node, the PBS hook solution would not work for this case.

One candidate solution might be for the pilot job handle the SIGTERM signal and smoothly stop the payload after receiving the signal.

For the time limit, I looked the PBS job scripts generated two days ago, there is no “#PBS -l walltime=hh&#58;mm&#58;ss”  line there.

Hope the above information will be helpful.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1452727235'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-13T23:20:35+00:00">Jan 13, 2016 11:20 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1452727235">&nbsp;</a></div><pre>Xizhou/Randy,

After getting around a few bugs in my tests, it looks like Greg&#39;s theory is right that jobs are not &#39;cleaning up&#39; after themselves properly. For example, I submitted a job that runs a script that starts a client program in a new session. The job completed and returned it&#39;s output with the initial PID 10027, PPID 10022, and PGRP 9985. However, the client still seems to be running and contacting my test server every ten minutes&#58;

2016-01-13 16&#58;24&#58;24,970 - Connection address&#58; (&#39;130.127.255.222&#39;, 34401)
2016-01-13 16&#58;24&#58;24,971 - PID&#58; 10029, PPID&#58; 10027, PGRP&#58; 10029
2016-01-13 16&#58;34&#58;25,069 - PID&#58; 10029, PPID&#58; 10027, PGRP&#58; 10029
2016-01-13 16&#58;44&#58;25,169 - PID&#58; 10029, PPID&#58; 10027, PGRP&#58; 10029
2016-01-13 16&#58;54&#58;25,269 - PID&#58; 10029, PPID&#58; 10027, PGRP&#58; 10029

This explains the issue with the missing output&#58; The pilot job lands on a PBS compute node, forks the payload into a new session, and when the job is removed due to walltime restrictions, it removes files required by the payload. The still-running payload then throws errors because its files are no longer there. How exactly is the walltime being enforced? How does this rogue pid killer work? We need the details to get to the bottom of this.

As for the walltime not being passed on, are you sure? I just submitted my test job again with the one minute wall time and at least on the HTCondor side, it looks right to me. How are you testing this exactly?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1452109631'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-06T19:47:11+00:00">Jan 6, 2016 07:47 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1452109631">&nbsp;</a></div><pre>Hi Brian,

I looked one of the script generated today by pbs_submit.sh. There is no #PBS directive in the script specifying the walltime requirement. As a result, the job will use the default walltime for osg-ce jobs. This explains why your earlier job requesting 1 minute was not terminated before completion.

Below is the script I looked at.

[root@osg-ce tmp]# cat bl_2016-01-06.sub
#!/bin/bash
# PBS job wrapper generated by pbs_submit.sh
# on Wed Jan  6 14&#58;33&#58;01 EST 2016
#
# stgcmd = no
# proxy_string = /usr/local/osg/8872/0/cluster1758872.proc0.subproc0/credential_chtc2.IceCube_727485.lmt
# proxy_local_file = /usr/local/osg/8872/0/cluster1758872.proc0.subproc0/credential_chtc2.IceCube_727485.lmt
#
# PBS directives&#58;
#PBS -S /bin/bash
#PBS -o /dev/null
#PBS -e /dev/null
#PBS -q osg
#PBS -l select=1
#PBS -m n

If you can provide me an updated version of the pbs_submit.sh script, I can deploy it to our site for further testing.

Thanks,

Xizhou
From&#58; Open Science Grid FootPrints [mailto&#58;osg@....]
Sent&#58; Wednesday, January 06, 2016 2&#58;14 PM
To&#58; Xizhou Feng
Subject&#58; Open Science Grid&#58; Missing output files after job completion ISSUE=27686 PROJ=71

[Duplicate message snipped]</pre></div><div class='update_description'><i onclick="document.location='27686#1452108011'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-06T19:20:11+00:00">Jan 6, 2016 07:20 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1452108011">&nbsp;</a></div><pre>That last command can be simplified as follows&#58;

&#96;condor_ce_history -const &#39;routedfromjobid isnt null && cmd == &#34;sleep&#34;&#39; osg&#96;

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1452107649'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-06T19:14:09+00:00">Jan 6, 2016 07:14 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1452107649">&nbsp;</a></div><pre>Xizhou,

If you run &#96;condor_ce_history -const &#39;routedfromjobid isnt null && cmd
== &#34;sleep&#34;&#39; osg | grep sleep&#96; on the CE, this will give you the list of
CE job IDs that then submitted corresponding jobs into PBS.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1452106753'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-06T18:59:13+00:00">Jan 6, 2016 06:59 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1452106753">&nbsp;</a></div><pre>Hi Brian,

I am not sure about the exact granularity. It should be in the order of seconds unless the server is very busy.

I just did a test with the following PBS script.
#!/bin/bash

#PBS -N test
#PBS -l select=1&#58;ncpus=8
#PBS -l walltime=00&#58;01&#58;00
#PBS -j oe

module load gcc/4.8.1 mpich/3.1.4

export PBS_O_WORKDIR=/scratch1/$USER/$PBS_JOBID
mkdir -p $PBS_O_WORKDIR
cd $PBS_O_WORKDIR

date
mpirun -n 8 /home/xizhouf/testing/mpich/hello &#62; output
sleep 2m
date

The job was terminated by PBS after 67 seconds as shown below.
[xizhouf@user001 mpich]$ tail -n 2 test.o1872109
Wed Jan  6 13&#58;36&#58;34 EST 2016
=&#62;&#62; PBS&#58; job killed&#58; walltime 67 exceeded limit 60

For the OSG jobs, we previously found that the corresponding PBS script didn’t have the walltime specification. I wonder if the script which converts Codor job to PBS job needs to be upgraded. We upgrade our PBS scheduler to version 13 last October which may cause some incompatibility.

If you can provide me your condor job id on the osg-ce node, I may be able to check the exact PBS job script mapping to that job.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1452104162'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-06T18:16:02+00:00">Jan 6, 2016 06:16 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1452104162">&nbsp;</a></div><pre>Xizhou/Randy,

I submitted a job requesting a MaxWallTime of 1 minute that ran a 2 minute sleep and it looks like my job was allowed to run to completion, which was unexpected. Is there a minimum wall time that jobs are allowed to run for? Or perhaps PBS&#39;s wall time check is less granular than a minute?

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1451924690'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2016-01-04T16:24:50+00:00">Jan 4, 2016 04:24 PM UTC</time> by <b>echism</b><a class="anchor" name="1451924690">&nbsp;</a></div><pre>Hello,

Could everyone revisit this ticket, please?

Thank you</pre></div><div class='update_description'><i onclick="document.location='27686#1450382739'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-17T20:05:39+00:00">Dec 17, 2015 08:05 PM UTC</time> by <b>Greg Thain</b><a class="anchor" name="1450382739">&nbsp;</a></div><pre>On 12/17/2015 01&#58;45 PM, Open Science Grid FootPrints wrote&#58;

It seems like the root problem here is that PBS isn&#39;t doing a good job
of preempting the glidein.  From these logs, it looks like PBS is
removing all the files in the sandbox before sending a TERM signal to
the process it started.  Because of this, the glided-in condor has the
rug pulled out from under it before it has the opportunity to gracefully
shut down, and gets confused. Glideins are expected to be preempted from
time to time, and live in that environment with no manual intervention
to user jobs required, but the preemption mechanism needs to cleanly
preempt what it started.

Is there a way to get a log of PBS is doing on the worker node -- when
it sends the TERM signal, and when it removes the sandbox?

<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; *Notification of Ticket Change*</font>
<font color='#7F7E6F'>&#62;</font></pre></div><div class='update_description'><i onclick="document.location='27686#1450381533'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-17T19:45:33+00:00">Dec 17, 2015 07:45 PM UTC</time><a class="anchor" name="1450381533">&nbsp;</a></div><pre>Xizhou,

Unfortunately, the glideins and/or user jobs are still being killed off by the original segfault issue [1]. It sounds like from the discussion that the HTCondor-CE + PBS preemption / eviction may be the source of the problem. I wouldn&#39;t be surprised by that. Preemption at sites always seems to eventually cause trouble at some level. It&#39;s usually just not as hard a break as seen here. Usually we just see glideins get stuck in a long-term held state that needs to be manually cleaned up at some point.

Is there anyway you could setup a special queue for us with a dedicated node (i.e., with preemption / eviction off) to test this hypothesis? I&#39;m not sure if that would be useful. It sounds like you need preemption at Clemson for the condo model y&#39;all have setup, and HTCondor-CE + PBS may not quite like each other enough yet to make this work.

Maybe someone else has a better idea of how to proceed here?

Marty Kandes
UCSD Glidein Factory Operations

[1]

Received kill signal... shutting down child processes

=== End encoded XML description of glidein activity ===
Stack dump for process 8650 at timestamp 1450378002 (24 frames)
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(dprintf_dump_stack+0x12d)[0x2ae079eb993d]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(+0x199542)[0x2ae079ec4542]
/lib64/libpthread.so.0[0x375720f710]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN16ProcFamilyClient13signal_familyEi21proc_family_command_tRb+0x33)[0x2ae079fbaaf3]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN15ProcFamilyProxy11kill_familyEi+0x3f)[0x2ae079ea65bf]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN10DaemonCore11Kill_FamilyEi+0x16)[0x2ae079f80f76]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master(_ZN6daemon10KillFamilyEv+0x1c)[0x40c6fc]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master(_ZN6daemon8HardKillEv+0x51)[0x40c761]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master(_ZN7Daemons18HardKillAllDaemonsEv+0x59)[0x40ddd9]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master(DoCleanup+0x44)[0x409c44]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_EXCEPT_+0x12d)[0x2ae079e9815d]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN15ProcFamilyProxy24recover_from_procd_errorEv+0x1bb)[0x2ae079ea63cb]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN15ProcFamilyProxy11kill_familyEi+0x30)[0x2ae079ea65b0]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN10DaemonCore11Kill_FamilyEi+0x16)[0x2ae079f80f76]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master(_ZN6daemon10KillFamilyEv+0x1c)[0x40c6fc]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master(_ZN6daemon6ExitedEi+0x137)[0x40ccf7]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master(_ZN7Daemons9AllReaperEii+0x109)[0x40e709]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN10DaemonCore10CallReaperEiPKcii+0x195)[0x2ae079f848e5]
<div id='show_177109596' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_177109596'>/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN10DaemonCore17HandleProcessExitEii+0x2e9)[0x2ae079f8bbc9]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN10DaemonCore24HandleDC_SERVICEWAITPIDSEi+0x7e)[0x2ae079f8bdce]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_ZN10DaemonCore6DriverEv+0x82d)[0x2ae079f8fc3d]
/local_scratch/glide_lzu2Mp/main/condor/lib/libcondor_utils_8_2_8.so(_Z7dc_mainiPPc+0x1799)[0x2ae079fafaa9]
/lib64/libc.so.6(__libc_start_main+0xfd)[0x375661ed5d]
/local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master[0x408399]
/local_scratch/glide_lzu2Mp/main/condor_startup.sh&#58; line 13&#58;  8650 Segmentation fault      $CONDOR_DIR/sbin/condor_master -f -pidfile $PWD/condor_master2.pid
/local_scratch/glide_lzu2Mp/main/condor_startup.sh&#58; line 15&#58; /local_scratch/glide_lzu2Mp/main/condor/sbin/condor_master&#58; No such file or directory
ls&#58; cannot access log&#58; No such file or directory
/local_scratch/glide_lzu2Mp/main/condor_startup.sh&#58; line 1013&#58; /local_scratch/glide_lzu2Mp/main/error_gen.sh&#58; No such file or directory

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049
</div><script type='text/javascript'>
        $('#show_177109596').click(function() {
            $('#detail_177109596').slideDown("normal");
            $('#show_177109596').hide();
            $('#hide_177109596').show();
        });
        $('#hide_177109596').click(function() {
            $('#detail_177109596').slideUp();
            $('#hide_177109596').hide();
            $('#show_177109596').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1450375445'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-17T18:04:05+00:00">Dec 17, 2015 06:04 PM UTC</time><a class="anchor" name="1450375445">&nbsp;</a></div><pre>Xizhou,

I&#39;ve submitted around of test jobs. I&#39;ll also have a look around at the latest glidein logs from the production factories and get back to you.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1450368780'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-17T16:13:00+00:00">Dec 17, 2015 04:13 PM UTC</time><a class="anchor" name="1450368780">&nbsp;</a></div><pre>Hi Marty,

When you have chance, please try some new jobs.

We looked at the previous issue that work nodes can not access the osg-ce node. Randy found that Red Hat/CentOS/SL not allowing asymmetrically routed packets by default may cause the problem. The problem has been fixed. Maybe this will fix some of the OSG job issues.

Thanks,

Xizhou

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Xizhou Feng 3402</pre></div><div class='update_description'><i onclick="document.location='27686#1450368778'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-17T16:12:58+00:00">Dec 17, 2015 04:12 PM UTC</time><a class="anchor" name="1450368778">&nbsp;</a></div><pre>Hi Marty,

When you have chance, please try some new jobs.

We looked at the previous issue that work nodes can not access the osg-ce node. Randy found that Red Hat/CentOS/SL not allowing asymmetrically routed packets by default may cause the problem. The problem has been fixed. Maybe this will fix some of the OSG job issues.

Thanks,

Xizhou

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Xizhou Feng 3402</pre></div><div class='update_description'><i onclick="document.location='27686#1450368769'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-17T16:12:49+00:00">Dec 17, 2015 04:12 PM UTC</time><a class="anchor" name="1450368769">&nbsp;</a></div><pre>Hi Marty,

When you have chance, please try some new jobs.

We looked at the previous issue that work nodes can not access the osg-ce node. Randy found that Red Hat/CentOS/SL not allowing asymmetrically routed packets by default may cause the problem. The problem has been fixed. Maybe this will fix some of the OSG job issues.

Thanks,

Xizhou

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Xizhou Feng 3402</pre></div><div class='update_description'><i onclick="document.location='27686#1450364228'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-17T14:57:08+00:00">Dec 17, 2015 02:57 PM UTC</time> by <b>echism</b><a class="anchor" name="1450364228">&nbsp;</a></div><pre>Xizhou will be out from tomorrow until after the holidays.</pre></div><div class='update_description'><i onclick="document.location='27686#1450292206'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-16T18:56:46+00:00">Dec 16, 2015 06:56 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1450292206">&nbsp;</a></div><pre>I don’t know how CONDOR_CE handles the evicted PBS jobs submitted by CONDOR_CE. From my understand, PBS doesn’t directly interact with the glidein jobs. The CONDOR_CE installed on OSG-CE node will translate the glidein job descriptions to PBS job scripts and submit them to the PBS queue. Therefore, the PBS scheduler might not know the existence of condor system and shadow processes.

Beside the 30 minutes default time limits, another possible cause is PBS evictions due to job priority. Clemson adopts a condo model for the cluster which gives owner of the work nodes higher priority than standard users including OSG. Also, we allocate resources by processor count instead of node count, which will leads to several jobs running on the same node. As a result, there are chances that OSG jobs might be evicted by owner jobs.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1450289893'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-16T18:18:13+00:00">Dec 16, 2015 06:18 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1450289893">&nbsp;</a></div><pre>We are running Scientific Linux 6.6 (Carbon), the kernel version is 2.6.32-504.16.2.el6.x86_64.

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1450289869'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-16T18:17:49+00:00">Dec 16, 2015 06:17 PM UTC</time><a class="anchor" name="1450289869">&nbsp;</a></div><pre>In the meantime I can try setting the wall time to see if that improves things at all (though it won&#39;t completely solve the issue as already mentioned.) Exactly which variable do you need us to set?

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Kenneth Herner 1385</pre></div><div class='update_description'><i onclick="document.location='27686#1450289442'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-16T18:10:42+00:00">Dec 16, 2015 06:10 PM UTC</time><a class="anchor" name="1450289442">&nbsp;</a></div><pre>Xizhou,

What OS are you running are you running at Clemson?

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1450276695'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-16T14:38:15+00:00">Dec 16, 2015 02:38 PM UTC</time> by <b>Greg Thain</b><a class="anchor" name="1450276695">&nbsp;</a></div><pre>On 12/15/2015 06&#58;05 PM, Open Science Grid FootPrints wrote&#58;

THIS IS NOT THE REAL PROBLEM!

Yes, it does sound like the glideins are landing in the 30 minute queue
when they should be landing in a longer queue, and that can be fixed.
BUT THAT WON&#34;T FIX THE UNDERLYING PROBLEM, JUST MAKE IT MORE RARE!

HOWEVER!  When PBS decides to evict the glidein, at whatever time it
does, if PBS does the job correctly, it needs to first kill all of the
processes involved in the glidein -- the condor_master, the startd, the
starter, all of the processes involved in the job itself.  On the submit
side, the payload job will notice that the execute side has gone away,
transition back to the &#34;I&#34;dle state, and run somewhere else.

Remember, even if the glidein lands in the 24 hour queue, one glidein
can run many payload jobs in sequence, if the jobs are less than 24
hours, so it is likely that some job will be running when the glidein is
evicted, and this needs to be handled without any user intervention.

<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; *Notification of Ticket Change*</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; <a href='27686' target='_blank' rel='nofollow'>https&#58;//ticket.opensciencegrid.org/27686</a></font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; *Description&#58;*</font>
<font color='#7F7E6F'>&#62; Can you try setting the wall time requirement in the job script? If it</font>
<font color='#7F7E6F'>&#62; succeeds, then we may have an answer.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Xizhou</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; On Dec 15, 2015, at 7&#58;01 PM, Open Science Grid FootPrints</font>
<font color='#7F7E6F'>&#62; &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<div id='show_667854979' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_667854979'><font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; xizhouf@.... xizhouf@....</font>
<font color='#7F7E6F'>&#62;</font>
</div><script type='text/javascript'>
        $('#show_667854979').click(function() {
            $('#detail_667854979').slideDown("normal");
            $('#show_667854979').hide();
            $('#hide_667854979').show();
        });
        $('#hide_667854979').click(function() {
            $('#detail_667854979').slideUp();
            $('#hide_667854979').hide();
            $('#show_667854979').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1450224314'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-16T00:05:14+00:00">Dec 16, 2015 12:05 AM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1450224314">&nbsp;</a></div><pre>Can you try setting the wall time requirement in the job script? If it succeeds, then we may have an answer.

Xizhou

On Dec 15, 2015, at 7&#58;01 PM, Open Science Grid FootPrints &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;

[Duplicate message snipped]</pre></div><div class='update_description'><i onclick="document.location='27686#1450223973'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-15T23:59:33+00:00">Dec 15, 2015 11:59 PM UTC</time><a class="anchor" name="1450223973">&nbsp;</a></div><pre>Hi,

I wrote this in another ticket (<a href='27650' target='_blank' rel='nofollow'>https&#58;//ticket.opensciencegrid.org/27650</a>) but thought it might be useful here too&#58;

Hi everyone,

Here is an additional data point. I sent a couple dozen test jobs today, and those that were designed to only run a few minutes completed without incident, including copying some output files back to Fermilab. I then sent another set of jobs which were identical but contained a 40-minute sleep command in the middle. I haven&#39;t gotten any of those to complete successfully yet (they are still in the run state after a few hours even though they only do about one minute&#39;s worth of work other than the sleep statement.)  Also one of them sent me back the following error (received at 16&#58;36 CST)&#58;

Error from glidein_6279_251586783@node0666&#58; STARTER at 10.125.3.155 failed to send file(s) to &#60;131.225.67.139&#58;9615&#62;&#58; error reading from /local_scratch/glide_jnMyVn/execute/dir_11422/_condor_stderr&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;60315&#62;

So indeed it does look like something strange happens at some time between a few and 40 minutes. Perhaps it is the PBS scheduler terminating (or trying to but not quite succeeding?) jobs after the 30 minutes.

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Kenneth Herner 1385</pre></div><div class='update_description'><i onclick="document.location='27686#1449777937'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-10T20:05:37+00:00">Dec 10, 2015 08:05 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1449777937">&nbsp;</a></div><pre>Additionally, a quick test job shows that I was able to grab to grab a
file from repo.grid.iu.edu from one of your execute nodes, so outbound
traffic seems ok&#58;

$ condor_ce_run -r osg-ce.clemson.edu&#58;9619 wget
<a href='http&#58;//repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm' target='_blank' rel='nofollow'>http&#58;//repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm</a>
--2015-12-10 14&#58;53&#58;44--
<a href='http&#58;//repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm' target='_blank' rel='nofollow'>http&#58;//repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm</a>
Resolving repo.grid.iu.edu... 129.79.53.71
Connecting to repo.grid.iu.edu|129.79.53.71|&#58;80... connected.
HTTP request sent, awaiting response... 200 OK
Length&#58; 10112 (9.9K) [application/x-rpm]
Saving to&#58; “osg-3.3-el7-release-latest.rpm”

0K .........                                             100% 381M=0s

2015-12-10 14&#58;53&#58;44 (381 MB/s) - “osg-3.3-el7-release-latest.rpm” saved
[10112/10112]

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1449776655'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-10T19:44:15+00:00">Dec 10, 2015 07:44 PM UTC</time> by <b>Greg Thain</b><a class="anchor" name="1449776655">&nbsp;</a></div><pre>On 12/10/2015 01&#58;26 PM, Open Science Grid FootPrints wrote&#58;

Note that the glidein payload from Wisconsin landed and started running
on this node.  Clearly it does have networking.

<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; *Notification of Ticket Change*</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; <a href='27686' target='_blank' rel='nofollow'>https&#58;//ticket.opensciencegrid.org/27686</a></font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; *Description&#58;*</font>
<font color='#7F7E6F'>&#62; This worker node on Palmetto doesn&#39;t have access to external network.</font>
<font color='#7F7E6F'>&#62; This is likely the cause of the problem.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Is there a reference network setup or requirement for the OSG</font>
<font color='#7F7E6F'>&#62; installation? I will check what we can do to meet the requirement.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Thanks,</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Xizhou</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; On Dec 10, 2015, at 12&#58;57 PM, Open Science Grid FootPrints</font>
<font color='#7F7E6F'>&#62; &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; xizhouf@.... xizhouf@....</font>
<font color='#7F7E6F'>&#62;</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449776135'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-10T19:35:35+00:00">Dec 10, 2015 07:35 PM UTC</time><a class="anchor" name="1449776135">&nbsp;</a></div><pre>Xizhou,

The recommendation is to allow all outbound traffic. If that&#39;s not possible, we&#39;ll have to figure out what systems and IP ranges should be whitelisted at your firewalls.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1449775610'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-10T19:26:50+00:00">Dec 10, 2015 07:26 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1449775610">&nbsp;</a></div><pre>This worker node on Palmetto doesn&#39;t have access to external network. This is likely the cause of the problem.

Is there a reference network setup or requirement for the OSG installation? I will check what we can do to meet the requirement.

Thanks,

Xizhou

On Dec 10, 2015, at 12&#58;57 PM, Open Science Grid FootPrints &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;

[Duplicate message snipped]</pre></div><div class='update_description'><i onclick="document.location='27686#1449770209'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-10T17:56:49+00:00">Dec 10, 2015 05:56 PM UTC</time><a class="anchor" name="1449770209">&nbsp;</a></div><pre>There appears to be no difference in my new batch of test jobs.  Several jobs go on hold with the same sort of error&#58;

50895043.0   moate          12/9  19&#58;34 Error from glidein_32465_58716238@....&#58; STARTER at 10.125.5.234 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_v3uPFB/execute/dir_2215/ZINC62967534_00_drd3-pde5a_scored.mol2&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;43332&#62;

Here&#39;s the job&#39;s log&#58;
000 (50895043.000.000) 12/09 10&#58;32&#58;29 Job submitted from host&#58; &#60;128.104.100.44&#58;9618?addrs=128.104.100.44-9618&noUDP&sock=1746300_7109_3&#62;
DAG Node&#58; drd3-0618.44
...
001 (50895043.000.000) 12/09 13&#58;59&#58;40 Job executing on host&#58; &#60;10.125.5.168&#58;55812?CCBID=128.104.100.30&#58;9656#93347&noUDP&#62;
...
006 (50895043.000.000) 12/09 13&#58;59&#58;49 Image size of job updated&#58; 138648
40  -  MemoryUsage of job (MB)
40476  -  ResidentSetSize of job (KB)
...
006 (50895043.000.000) 12/09 14&#58;04&#58;50 Image size of job updated&#58; 429148
278  -  MemoryUsage of job (MB)
283844  -  ResidentSetSize of job (KB)
...
006 (50895043.000.000) 12/09 14&#58;09&#58;50 Image size of job updated&#58; 493236
383  -  MemoryUsage of job (MB)
391560  -  ResidentSetSize of job (KB)
...
006 (50895043.000.000) 12/09 14&#58;14&#58;51 Image size of job updated&#58; 493500
386  -  MemoryUsage of job (MB)
394760  -  ResidentSetSize of job (KB)
...
007 (50895043.000.000) 12/09 14&#58;27&#58;07 Shadow exception!
Error from glidein_18172_593288759@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50895043.000.000) 12/09 15&#58;13&#58;50 Job executing on host&#58; &#60;10.125.7.131&#58;57543?CCBID=128.104.100.30&#58;9653#329381&noUDP&#62;
...
006 (50895043.000.000) 12/09 15&#58;28&#58;59 Image size of job updated&#58; 500000
392  -  MemoryUsage of job (MB)
<div id='show_295319248' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_295319248'>400924  -  ResidentSetSize of job (KB)
...
007 (50895043.000.000) 12/09 15&#58;42&#58;34 Shadow exception!
Error from glidein_10930_66923560@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50895043.000.000) 12/09 17&#58;38&#58;49 Job executing on host&#58; &#60;10.125.5.197&#58;56630?CCBID=128.104.100.30&#58;9628#314355&noUDP&#62;
...
007 (50895043.000.000) 12/09 18&#58;06&#58;59 Shadow exception!
Error from glidein_15547_67005932@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50895043.000.000) 12/09 19&#58;08&#58;14 Job executing on host&#58; &#60;10.125.5.234&#58;42129?CCBID=128.104.100.30&#58;9631#312458&noUDP&#62;
...
007 (50895043.000.000) 12/09 19&#58;34&#58;47 Shadow exception!
Error from glidein_32465_58716238@....&#58; STARTER at 10.125.5.234 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_v3uPFB/execute/dir_2215/ZINC62967534_00_drd3-pde5a_scored.mol2&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;43332&#62;
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
012 (50895043.000.000) 12/09 19&#58;34&#58;47 Job was held.
Error from glidein_32465_58716238@....&#58; STARTER at 10.125.5.234 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_v3uPFB/execute/dir_2215/ZINC62967534_00_drd3-pde5a_scored.mol2&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;43332&#62;
Code 13 Subcode 2
...

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Aaron Moate 894
</div><script type='text/javascript'>
        $('#show_295319248').click(function() {
            $('#detail_295319248').slideDown("normal");
            $('#show_295319248').hide();
            $('#hide_295319248').show();
        });
        $('#hide_295319248').click(function() {
            $('#detail_295319248').slideUp();
            $('#hide_295319248').hide();
            $('#show_295319248').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1449769662'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-10T17:47:42+00:00">Dec 10, 2015 05:47 PM UTC</time><a class="anchor" name="1449769662">&nbsp;</a></div><pre>Chiming in to note that Fermilab VO jobs (mu2e) have been seeing this shadow exception at Clemson also&#58;

001 (3827994.096.000) 12/09 23&#58;37&#58;55 Job executing on host&#58; &#60;10.125.5.145&#58;52461?CCBID=131.225.67.218&#58;9630%3faddrs%3d131.225.67.218-9630#1558720%20131.225.67.219&#58;9630%3faddrs%3d131.225.67.219-9630#1557620&
noUDP&#62;
...
028 (3827994.096.000) 12/09 23&#58;37&#58;55 Job ad information event triggered.
JOB_GLIDEIN_Name = &#34;gfactory_instance&#34;
JOB_Site = &#34;Clemson&#34;
Proc = 96
JOB_GLIDEIN_Entry_Name = &#34;OSG_US_Clemson-Palmetto_condce&#34;
EventTime = &#34;2015-12-09T23&#58;37&#58;55&#34;
TriggerEventTypeName = &#34;ULOG_EXECUTE&#34;
JOB_GLIDEIN_SiteWMS_Queue = &#34;osg_e&#34;
TriggerEventTypeNumber = 1
ExecuteHost = &#34;&#60;10.125.5.145&#58;52461?CCBID=131.225.67.218&#58;9630%3faddrs%3d131.225.67.218-9630#1558720%20131.225.67.219&#58;9630%3faddrs%3d131.225.67.219-9630#1557620&noUDP&#62;&#34;
JOB_GLIDEIN_Site = &#34;Clemson&#34;
JOB_GLIDEIN_SiteWMS_JobId = &#34;1120779.pbs02&#34;
MyType = &#34;ExecuteEvent&#34;
JOB_GLIDEIN_ProcId = &#34;0&#34;
JOB_GLIDEIN_Schedd = &#34;schedd_glideins4@....&#34;
JOB_GLIDEIN_ClusterId = &#34;2906420&#34;
Cluster = 3827994
JOB_GLIDEIN_Factory = &#34;OSGGOC&#34;
JOB_GLIDEIN_SiteWMS_Slot = &#34;Unknown&#34;
Subproc = 0
EventTypeNumber = 28
JOB_GLIDEIN_SiteWMS = &#34;PBS&#34;
...
007 (3827994.096.000) 12/10 00&#58;06&#58;49 Shadow exception!
Error from glidein_13205_967695876@node1153&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
34758  -  Run Bytes Received By Job

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Kevin Retzke 3130</pre></div><div class='update_description'><i onclick="document.location='27686#1449684876'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-09T18:14:36+00:00">Dec 9, 2015 06:14 PM UTC</time> by <b>Greg Thain</b><a class="anchor" name="1449684876">&nbsp;</a></div><pre>On 12/09/2015 10&#58;56 AM, Open Science Grid FootPrints wrote&#58;

How does the rogue pid killer know that any given process is not owned
by a running job?

<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62; *Notification of Ticket Change*</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; <a href='27686' target='_blank' rel='nofollow'>https&#58;//ticket.opensciencegrid.org/27686</a></font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; *Description&#58;*</font>
<font color='#7F7E6F'>&#62; Brian,</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; We have a rogue pid killer that runs and will periodically kill any</font>
<font color='#7F7E6F'>&#62; running processes that are not owned by a running job on the compute</font>
<font color='#7F7E6F'>&#62; nodes.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; -Randy</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; On Dec 9, 2015, at 11&#58;48 AM, Open Science Grid FootPrints</font>
<font color='#7F7E6F'>&#62; &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; wolf@.... wolf@....</font>
<font color='#7F7E6F'>&#62;</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449680738'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-09T17:05:38+00:00">Dec 9, 2015 05:05 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1449680738">&nbsp;</a></div><pre>How often does that run?

- Brian

On 12/09/2015 10&#58;56 AM, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449680207'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-09T16:56:47+00:00">Dec 9, 2015 04:56 PM UTC</time> by <b>Randy Martin</b><a class="anchor" name="1449680207">&nbsp;</a></div><pre>Brian,

We have a rogue pid killer that runs and will periodically kill any running processes that are not owned by a running job on the compute nodes.

-Randy

On Dec 9, 2015, at 11&#58;48 AM, Open Science Grid FootPrints &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;

[Duplicate message snipped]</pre></div><div class='update_description'><i onclick="document.location='27686#1449679291'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-09T16:41:31+00:00">Dec 9, 2015 04:41 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1449679291">&nbsp;</a></div><pre>After speaking with Greg about this, adjusting the wall time is only
putting a band-aid on the problem. The current working theory is that
when PBS kills the pilots, it&#39;s killing the pilot startup script and
cleaning up the spool directory but not killing the other processes that
the pilot starts (i.e. the HTCondor daemons). This means that we will
see the same breakage but only at the 24hr mark instead of the 30 min mark.

We can see if Moate&#39;s jobs complete but then I&#39;d like to remove the
config that you added to bring the walltime back down to 30 so I can
submit some test jobs to verify or disprove our theory.

- Brian

On 12/09/2015 10&#58;11 AM, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449678742'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-09T16:32:22+00:00">Dec 9, 2015 04:32 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1449678742">&nbsp;</a></div><pre>Thanks.  I&#39;ll resubmit my batch of jobs and let you all know
how it goes.

-Moate

On Wed, Dec 09, 2015 at 04&#58;11&#58;00PM +0000, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62;    [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449677490'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-09T16:11:30+00:00">Dec 9, 2015 04:11 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1449677490">&nbsp;</a></div><pre>Yes. All jobs without specifying the wall time requirement will be killed after 30 minutes.

The Palmetto cluster uses a single osg_e queue for all OSG VO jobs. The GLOW jobs will go to that queue.

I just changed the default wall time to 24 hours. Please try if this change can fix the problem.

[root@osg-ce ~]# condor_ce_reconfig
Sent &#34;Reconfig&#34; command to local master
[root@osg-ce ~]# cat /etc/condor-ce/config.d/99-local.conf
ALL_DEBUG = D_FULLDEBUG
SCHEDD.SEC_WRITE_AUTHENTICATION_METHODS = GSI,FS
#SPOOL=/common/osg/spool
SPOOL=/usr/local/osg
NETWORK_HOSTNAME=osg-ce.clemson.edu
LOWPORT=40000
HIGHPORT=45000
MERGE_JOB_ROUTER_DEFAULT_ADS=True
JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS_GENERATED) [default_maxWallTime = 1440;]
SCHEDD_INTERVAL=5
[root@osg-ce ~]#

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1449676475'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-09T15:54:35+00:00">Dec 9, 2015 03:54 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1449676475">&nbsp;</a></div><pre>Xizhou,

All jobs should be landing in the &#39;osg&#39; PBS queue. Do all of these jobs get killed after 30min?

If the max walltime is indeed the culprit, you can edit your CE config so that all jobs routed to PBS request a default max walltime. In /etc/condor-ce/config.d/99-local.conf, add the following to set a max walltime of 24 hours&#58;

MERGE_JOB_ROUTER_DEFAULT_ADS=True
JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS_GENERATED) [default_maxWallTime = 1440;]

If you want to set different walltimes for GLOW vs OSG jobs, we&#39;ll have to tinker more with your JOB_ROUTER_ENTRIES config.

- Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1449617891'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T23:38:11+00:00">Dec 8, 2015 11:38 PM UTC</time> by <b>Greg Thain</b><a class="anchor" name="1449617891">&nbsp;</a></div><pre>On 12/08/2015 04&#58;17 PM, Open Science Grid FootPrints wrote&#58;
When PBS evicts a job, especially a job composed of multiple processes,
how does it make sure that all processes in the job are killed?

<font color='#7F7E6F'>&#62; ------------------------------------------------------------------------</font>
<font color='#7F7E6F'>&#62;</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449616520'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T23:15:20+00:00">Dec 8, 2015 11:15 PM UTC</time><a class="anchor" name="1449616520">&nbsp;</a></div><pre>My guess is that this message is a little misleading&#58;

STARTER at 10.125.1.123 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_BYMzxg/execute/dir_32534/AuditLog.drd3-0618&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;16746&#62;

I don&#39;t think this is due to a network connectivity issue, although the &#34;failed to sendf file(s)&#34; message makes it sound like it is;  If the Starter was able to tell the Shadow &#34;error reading from /local_scratch/glide_BYMzxg/execute/dir_32534/AuditLog.drd3-0618&#34;, then I would guess that network communication is working.  I could be wrong; If outbound connections are being blocked and the file transfer is a separate connection, then this is still a possibility.

I&#39;m going to run some new tests that run just shy of 30 minutes and just longer than 30 minutes to gauge how they differ from our failed jobs.  Greg Thain thinks the PBS eviction mechanism may be the possible culprit.

It would be excellent if the configuration was changed to allow GLOW VO jobs to run for 24 hours.

Cheers!
-Aaron Moate

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Aaron Moate 894</pre></div><div class='update_description'><i onclick="document.location='27686#1449613032'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T22:17:12+00:00">Dec 8, 2015 10:17 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1449613032">&nbsp;</a></div><pre>I checked with our admins and learned that the default wall clock time for all jobs in the PBS queue is 30 minutes. If a job doesn’t specify the wall clock time, it will be terminated by PBS. The OSG jobs are allowed to run up to 24 hours. This explains why I haven’t seen glow jobs last longer than 30 minutes. I wonder if you can change the time requirements to a longer time.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1449612683'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T22:11:23+00:00">Dec 8, 2015 10:11 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1449612683">&nbsp;</a></div><pre>The worker nodes on the Palmetto cluster are configured to have no access to external network. I will ask Randy for his advice to see if we are able to work around this problem.

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1449611428'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T21:50:28+00:00">Dec 8, 2015 09:50 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1449611428">&nbsp;</a></div><pre>This information is very helpful. I will check what configuration might have caused the problem.

Thanks,

Xizhou</pre></div><div class='update_description'><i onclick="document.location='27686#1449611426'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T21:50:26+00:00">Dec 8, 2015 09:50 PM UTC</time><a class="anchor" name="1449611426">&nbsp;</a></div><pre>Hi ,

I am uploading the glidein logs that Marty asked me to.

Also Xizhou is this node&#58;

10.125.1.123

Behind a NAT or having outbound network problems.

From this&#58;

STARTER at 10.125.1.123 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_BYMzxg/execute/dir_32534/AuditLog.drd3-0618&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;16746&#62;

I can see that the glidein cannot contact the schedd.

Cheers

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020</pre></div><div class='update_description'><i onclick="document.location='27686#1449610917'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T21:41:57+00:00">Dec 8, 2015 09:41 PM UTC</time><a class="anchor" name="1449610917">&nbsp;</a></div><pre>Hi Xizhou,

OSG glideins are also failing at Clemson. I apologize. I&#39;m having trouble uploading the sample glidein logs I promised in my last post. I&#39;m waiting on someone else to try to upload them from his machine. This sample glidein log is an OSG glidein log.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1449609940'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T21:25:40+00:00">Dec 8, 2015 09:25 PM UTC</time><a class="anchor" name="1449609940">&nbsp;</a></div><pre>Can anyone verify if other OSG VO (e.g., osg) jobs are able to run on the Clemson Palmetto Cluster during these days? If only GLOW jobs fail, we can check if there are differences between glow and osg accounts.

I checked our system and there are 53 jobs glow jobs are running now. However, some job lasted 28 minutes and then failed as shown in the following job status report.

Also, if you have any suggestion that what Palmetto admins could do to help resolve this issue, please let us know.

Thanks,

Xizhou

[root@osg-ce cluster1345459.proc0.subproc0]# qstat -xf 1076105
Job Id&#58; 1076105.pbs02
Job_Name = bl_b950969564e7
Job_Owner = glow@....
resources_used.cpupercent = 99
resources_used.cput = 00&#58;28&#58;40
resources_used.mem = 431084kb
resources_used.ncpus = 1
resources_used.vmem = 1134932kb
resources_used.walltime = 00&#58;31&#58;58
job_state = F
queue = osg_e
server = pbs02
Checkpoint = u
ctime = Tue Dec  8 15&#58;12&#58;33 2015
Error_Path = osg-ce.clemson.edu&#58;/dev/null
exec_host = node0531/6
exec_vnode = (node0531&#58;mem=1048576kb&#58;ncpus=1&#58;ngpus=0&#58;nphis=0)
group_list = osg
Hold_Types = n
Join_Path = n
Keep_Files = n
Mail_Points = n
Mail_Users = glow@....
mtime = Tue Dec  8 15&#58;44&#58;34 2015
<div id='show_1455142703' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1455142703'>Output_Path = osg-ce.clemson.edu&#58;/dev/null
Priority = 0
qtime = Tue Dec  8 15&#58;12&#58;33 2015
Rerunable = True
Resource_List.mem = 1gb
Resource_List.ncpus = 1
Resource_List.ngpus = 0
Resource_List.nodect = 1
Resource_List.nphis = 0
Resource_List.place = free&#58;shared
Resource_List.qcat = osg_qcat
Resource_List.select = 1
Resource_List.walltime = 00&#58;30&#58;00
stime = Tue Dec  8 15&#58;12&#58;37 2015
session_id = 13616
Shell_Path_List = /bin/bash
jobdir = /home/glow
substate = 93
Variable_List = PBS_O_HOME=/home/glow,
PBS_O_WORKDIR=/usr/local/osg/5314/0/cluster1345314.proc0.subproc0,
PBS_O_LANG=en_US.UTF-8,PBS_O_PATH=/sbin&#58;/usr/sbin&#58;/bin&#58;/usr/bin,
PBS_O_SYSTEM=Linux,PBS_O_QUEUE=osg,
PBS_O_HOST=osg001.palmetto.clemson.edu
comment = Job run at Tue Dec 08 at 15&#58;12 on (node0531&#58;mem=1048576kb&#58;ncpus=1
&#58;ngpus=0&#58;nphis=0) and failed
etime = Tue Dec  8 15&#58;12&#58;33 2015
run_count = 1
Stageout_status = 1
Exit_status = 271
Submit_arguments = /tmp/condor_g_scratch.0x7f24e867a2b0.7550/bl_b950969564e
7
history_timestamp = 1449607474
project = _pbs_project_default

[root@osg-ce osg]# condor_ce_history 1345314
ID     OWNER          SUBMITTED   RUN_TIME     ST COMPLETED   CMD
1345314.0   glow           12/8  15&#58;11   0+00&#58;31&#58;26 C  12/8  15&#58;45 glidein_startup.sh -v std -name gfactory_instance -entry OSG_US_Clemson-Palmetto_condce -clientname chtc2.CHTCSubmit -schedd schedd_glideins4@g

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Xizhou Feng 3402
</div><script type='text/javascript'>
        $('#show_1455142703').click(function() {
            $('#detail_1455142703').slideDown("normal");
            $('#show_1455142703').hide();
            $('#hide_1455142703').show();
        });
        $('#hide_1455142703').click(function() {
            $('#detail_1455142703').slideUp();
            $('#hide_1455142703').hide();
            $('#show_1455142703').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1449604959'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T20:02:39+00:00">Dec 8, 2015 08:02 PM UTC</time><a class="anchor" name="1449604959">&nbsp;</a></div><pre>Hi all,

I apologize for the delay from us here. I have attached an example of the what the glidein logs we receive from Clemson look like. Please let us know if there is anything else we can do to help.

Marty Kandes
UCSD Glidein Factory Operations

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Marty Kandes 3049</pre></div><div class='update_description'><i onclick="document.location='27686#1449604216'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T19:50:16+00:00">Dec 8, 2015 07:50 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1449604216">&nbsp;</a></div><pre>Edgar,

Sorry for the fail in sentence completion.

I submitted a batch of 100 jobs.  Of them, 16 have at least
landed on a Clemson-Palmetto pilot and started (and likely been
kicked off due to the Procd error).  Of those 16, one has gone
on hold with the previous hold message.  84 of the jobs have not
yet started.

-Moate

On Tue, Dec 08, 2015 at 06&#58;40&#58;00PM +0000, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62;    [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449600028'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T18:40:28+00:00">Dec 8, 2015 06:40 PM UTC</time><a class="anchor" name="1449600028">&nbsp;</a></div><pre>Hi Moate,

If I understand correctly. One job had these problems. And the others were fine? Or all of them at Clemson had these issues.

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020</pre></div><div class='update_description'><i onclick="document.location='27686#1449597913'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-08T18:05:13+00:00">Dec 8, 2015 06:05 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1449597913">&nbsp;</a></div><pre>I&#39;ve got a batch of jobs running from submit-3.chtc.wisc.edu.  One of them went

50716242.0   moate          12/7  19&#58;23 Error from glidein_27353_318159621@....&#58; STARTER at 10.125.1.123 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_BYMzxg/execute/dir_32534/AuditLog.drd3-0618&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;16746&#62;

Here&#39;s the complete user log&#58;

000 (50716242.000.000) 12/07 14&#58;25&#58;44 Job submitted from host&#58; &#60;128.104.100.44&#58;9618?addrs=128.104.100.44-9618&noUDP&sock=1746300_7109_3&#62;
DAG Node&#58; drd3-0618.0
...
001 (50716242.000.000) 12/07 14&#58;33&#58;16 Job executing on host&#58; &#60;10.125.3.99&#58;48585?CCBID=128.104.100.30&#58;9627#107581&noUDP&#62;
...
006 (50716242.000.000) 12/07 14&#58;33&#58;25 Image size of job updated&#58; 129276
31  -  MemoryUsage of job (MB)
31020  -  ResidentSetSize of job (KB)
...
006 (50716242.000.000) 12/07 14&#58;38&#58;26 Image size of job updated&#58; 429148
51  -  MemoryUsage of job (MB)
51284  -  ResidentSetSize of job (KB)
...
006 (50716242.000.000) 12/07 14&#58;43&#58;26 Image size of job updated&#58; 493236
385  -  MemoryUsage of job (MB)
393384  -  ResidentSetSize of job (KB)
...
006 (50716242.000.000) 12/07 14&#58;53&#58;27 Image size of job updated&#58; 493500
389  -  MemoryUsage of job (MB)
397440  -  ResidentSetSize of job (KB)
...
007 (50716242.000.000) 12/07 14&#58;57&#58;36 Shadow exception!
Error from glidein_17913_86838414@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50716242.000.000) 12/07 15&#58;13&#58;50 Job executing on host&#58; &#60;10.125.5.115&#58;53118?CCBID=128.104.100.30&#58;9663#317228&noUDP&#62;
...
007 (50716242.000.000) 12/07 15&#58;43&#58;55 Shadow exception!
<div id='show_1576817306' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1576817306'>Error from glidein_27173_509636100@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50716242.000.000) 12/07 15&#58;51&#58;28 Job executing on host&#58; &#60;10.125.4.85&#58;50596?CCBID=128.104.100.30&#58;9624#314113&noUDP&#62;
...
007 (50716242.000.000) 12/07 16&#58;21&#58;56 Shadow exception!
Error from glidein_31705_753770440@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50716242.000.000) 12/07 16&#58;33&#58;06 Job executing on host&#58; &#60;10.125.7.121&#58;57008?CCBID=128.104.100.30&#58;9676#315082&noUDP&#62;
...
007 (50716242.000.000) 12/07 17&#58;01&#58;30 Shadow exception!
Error from glidein_31835_191581998@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50716242.000.000) 12/07 17&#58;27&#58;24 Job executing on host&#58; &#60;10.125.3.141&#58;59403?CCBID=128.104.100.30&#58;9629#314753&noUDP&#62;
...
007 (50716242.000.000) 12/07 17&#58;57&#58;20 Shadow exception!
Error from glidein_6918_390791240@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50716242.000.000) 12/07 18&#58;12&#58;57 Job executing on host&#58; &#60;10.125.4.117&#58;54849?CCBID=128.104.100.30&#58;9632#314623&noUDP&#62;
...
007 (50716242.000.000) 12/07 18&#58;42&#58;36 Shadow exception!
Error from glidein_6421_139592644@....&#58; unable to restart the ProcD after several tries
0  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
001 (50716242.000.000) 12/07 18&#58;54&#58;05 Job executing on host&#58; &#60;10.125.1.123&#58;36414?CCBID=128.104.100.30&#58;9623#317194&noUDP&#62;
...
007 (50716242.000.000) 12/07 19&#58;23&#58;12 Shadow exception!
Error from glidein_27353_318159621@....&#58; STARTER at 10.125.1.123 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_BYMzxg/execute/dir_32534/AuditLog.drd3-0618&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;16746&#62;
2825386  -  Run Bytes Sent By Job
118443336  -  Run Bytes Received By Job
...
012 (50716242.000.000) 12/07 19&#58;23&#58;12 Job was held.
Error from glidein_27353_318159621@....&#58; STARTER at 10.125.1.123 failed to send file(s) to &#60;128.104.100.44&#58;9618&#62;&#58; error reading from /local_scratch/glide_BYMzxg/execute/dir_32534/AuditLog.drd3-0618&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;16746&#62;
Code 13 Subcode 2

-Moate
</div><script type='text/javascript'>
        $('#show_1576817306').click(function() {
            $('#detail_1576817306').slideDown("normal");
            $('#show_1576817306').hide();
            $('#hide_1576817306').show();
        });
        $('#hide_1576817306').click(function() {
            $('#detail_1576817306').slideUp();
            $('#hide_1576817306').hide();
            $('#show_1576817306').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='27686#1449513853'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-07T18:44:13+00:00">Dec 7, 2015 06:44 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1449513853">&nbsp;</a></div><pre>Will do.  I&#39;m trying to run a larger batch of jobs.  I&#39;ll let everyone
know when they&#39;ve run.

-Moate

On Mon, Dec 07, 2015 at 04&#58;05&#58;00PM +0000, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62;    [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449504309'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-07T16:05:09+00:00">Dec 7, 2015 04:05 PM UTC</time><a class="anchor" name="1449504309">&nbsp;</a></div><pre>Hi Moate,

Let us know how the jobs ran, or if they did?

Edgar
OSG Software Support

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020</pre></div><div class='update_description'><i onclick="document.location='27686#1449271931'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T23:32:11+00:00">Dec 4, 2015 11:32 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1449271931">&nbsp;</a></div><pre>Sure.  I&#39;ll run some more test jobs on Monday, and I&#39;ll try
to get you what useful information I can.

-Moate

On Fri, Dec 04, 2015 at 11&#58;26&#58;00PM +0000, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62;    [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449271575'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T23:26:15+00:00">Dec 4, 2015 11:26 PM UTC</time> by <b>xizhouf@....</b><a class="anchor" name="1449271575">&nbsp;</a></div><pre>Could you send me one of the log files for me to see what might be the cause? The previous job logs are for November 13 when the cluster had storage issues.

Also, we switched the work node temp to /local_scatch yesterday. We want to check if the change is effective.

We have also updated the PBS scheduler on the cluster and added some new nodes in November. We want to make sure the errors are not related to these two changes.

Thanks,

Xizhou

On Dec 4, 2015, at 5&#58;51 PM, Open Science Grid FootPrints &#60;osg@....&#60;mailto&#58;osg@....&#62;&#62; wrote&#58;

[Duplicate message snipped]</pre></div><div class='update_description'><i onclick="document.location='27686#1449269414'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T22:50:14+00:00">Dec 4, 2015 10:50 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1449269414">&nbsp;</a></div><pre>Xizhou,

The date of our last failure was today, 12/4, 3&#58;55 AM CST;
If it doesn&#39;t make a difference, I&#39;ll unblock Clemson-Palmetto
on Monday and run some test jobs.

Cheers,
Aaron Moate</pre></div><div class='update_description'><i onclick="document.location='27686#1449259879'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T20:11:19+00:00">Dec 4, 2015 08:11 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1449259879">&nbsp;</a></div><pre>Xizhou,

If you&#39;re talking about moving your worker_node_temp dir from /tmp, Aaron has already submitted a set of test jobs since then. If you&#39;re talking about an unrelated-storage issue you guys have had today, we can run some more tests.

Thanks,
Brian</pre></div><div class='update_description'><i onclick="document.location='27686#1449259300'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T20:01:40+00:00">Dec 4, 2015 08:01 PM UTC</time><a class="anchor" name="1449259300">&nbsp;</a></div><pre>Hi Moate,

Can you remove the blocks to verify if the GLOW jobs can run on our site? We previously had some storage-related system issues but they should be fixed now.

Thanks,

Xizhou

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Xizhou Feng 3402</pre></div><div class='update_description'><i onclick="document.location='27686#1449256461'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T19:14:21+00:00">Dec 4, 2015 07:14 PM UTC</time> by <b>wiscmoate@....</b><a class="anchor" name="1449256461">&nbsp;</a></div><pre>I neglected to state that I&#39;ve configured our GWMS Frontend to
prohibit jobs from going to Clemson-Palmetto temporarily.  I&#39;ll
remove the block if it&#39;s necessary/helpful for troubleshooting.

-Moate

On Fri, Dec 04, 2015 at 06&#58;48&#58;00PM +0000, Open Science Grid FootPrints wrote&#58;
<font color='#7F7E6F'>&#62;    [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='27686#1449254906'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T18:48:26+00:00">Dec 4, 2015 06:48 PM UTC</time> by <b>Brian Lin</b><a class="anchor" name="1449254906">&nbsp;</a></div><pre>Updated CC&#39;s.

Factory folks&#58; Could you give us any logs that you&#39;re getting back from Clemson for GLOW to see if the errors are different than what we&#39;ve previously seen?</pre></div><div class='update_description'><i onclick="document.location='27686#1449254197'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2015-12-04T18:36:37+00:00">Dec 4, 2015 06:36 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1449254197">&nbsp;</a></div><pre>Greetings,

This issue has already been looked at a bit through back-channels;   We at CHTC/GLOW decided it would probably be better to open a formal ticket for continuity (this is not a slight towards the wonderful admins at Clemson-Palmetto,  they&#39;ve been very responsive and helpful).  The issue *could* be on our end, since I&#39;m not aware of any other OSG members having it.

We have GLOW jobs landing at Clemson-Palmetto and going on hold with these messages (first noticed a few weeks back)&#58;
5406865.0 jlange3 11/5 12&#58;04 Error from glidein_24793_194250044@....&#58; STARTER at 10.125.3.46 failed to send file(s) to &#60;128.104.101.92&#58;9618&#62;&#58; error reading from /tmp/glide_mCgAWd/execute/dir_29958/input_files2.tgz&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;46698&#62;
5408800.0 zflynn 10/15 16&#58;50 Error from glidein_32210_357415812@....&#58; STARTER at 10.125.1.122 failed to send file(s) to &#60;128.104.101.92&#58;9618&#62;&#58; error reading from /tmp/glide_Y96hZ9/execute/dir_5042/ChtcWrapper179_72.out&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;53284&#62;
6924118.0 vetsigian 11/8 16&#58;15 Error from glidein_24325_580346787@....&#58; STARTER at 10.125.4.18 failed to send file(s) to &#60;128.104.101.92&#58;9618&#62;&#58; error reading from /tmp/glide_Fij0xG/execute/dir_2340/result.mat&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;58572&#62;

I ran some test jobs against Clemson-Palmetto and got the same error.  In the user logs were a bunch of errors regarding the ProcD being unable to restart (not sure if these errors are actually relevant)&#58;

000 (50054419.000.000) 11/13 09&#58;33&#58;11 Job submitted from host&#58; &#60;128.104.100.44&#58;9618?addrs=128.104.100.44-9618&noUDP&sock=3134_8e4e_3&#62;
DAG Node&#58; drd3-0618
...
001 (50054419.000.000) 11/13 09&#58;49&#58;09 Job executing on host&#58; &#60;10.125.3.41&#58;53382?CCBID=128.104.100.30&#58;9668#291328&noUDP&#62;
...
006 (50054419.000.000) 11/13 09&#58;49&#58;17 Image size of job updated&#58; 134688
36 - MemoryUsage of job (MB)
36412 - ResidentSetSize of job (KB)
...
006 (50054419.000.000) 11/13 09&#58;54&#58;18 Image size of job updated&#58; 429148
51 - MemoryUsage of job (MB)
51284 - ResidentSetSize of job (KB)
...
006 (50054419.000.000) 11/13 09&#58;59&#58;18 Image size of job updated&#58; 493236
388 - MemoryUsage of job (MB)
396976 - ResidentSetSize of job (KB)
...
006 (50054419.000.000) 11/13 10&#58;09&#58;19 Image size of job updated&#58; 493500
392 - MemoryUsage of job (MB)
400900 - ResidentSetSize of job (KB)
...
007 (50054419.000.000) 11/13 10&#58;18&#58;22 Shadow exception!
Error from glidein_18016_325080620@....&#58; unable to restart the ProcD after several tries
0 - Run Bytes Sent By Job
<div id='show_41988450' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_41988450'>118443328 - Run Bytes Received By Job
...
001 (50054419.000.000) 11/13 11&#58;06&#58;32 Job executing on host&#58; &#60;10.125.3.15&#58;58414?CCBID=128.104.100.30&#58;9644#296875&noUDP&#62;
...
007 (50054419.000.000) 11/13 11&#58;33&#58;21 Shadow exception!
Error from glidein_2246_129498253@....&#58; unable to restart the ProcD after several tries
0 - Run Bytes Sent By Job
118443328 - Run Bytes Received By Job
...
001 (50054419.000.000) 11/13 17&#58;00&#58;00 Job executing on host&#58; &#60;10.125.2.185&#58;51589?CCBID=128.104.100.30&#58;9677#293059&noUDP&#62;
...
007 (50054419.000.000) 11/13 17&#58;30&#58;08 Shadow exception!
Error from glidein_13869_77526388@....&#58; unable to restart the ProcD after several tries
0 - Run Bytes Sent By Job
118443328 - Run Bytes Received By Job
...
001 (50054419.000.000) 11/14 20&#58;19&#58;02 Job executing on host&#58; &#60;10.125.3.69&#58;49065?CCBID=128.104.100.30&#58;9629#294698&noUDP&#62;
...
007 (50054419.000.000) 11/14 20&#58;49&#58;01 Shadow exception!
Error from glidein_6452_814641051@....&#58; unable to restart the ProcD after several tries
0 - Run Bytes Sent By Job
118443328 - Run Bytes Received By Job

It looks like the job runs fine for a while until it takes up a certain amount of memory, then bounces around from pilot to pilot while ProcD&#39;s fail.   I asked Brian Lin to help me out with retrieving HTCondor logs from the pilot.  He seng OSG Factory Support and email and retrieved some (I have the tarball available if anyone wants them)&#58;

<font color='#7F7E6F'>&#62; Hey Brian,</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; I was able to pull up some stdout and stderr logs for those glideins (which</font>
<font color='#7F7E6F'>&#62; are attached), but that&#39;s about it; I&#39;m not getting any condor logs for any</font>
<font color='#7F7E6F'>&#62; of them, so I can&#39;t provide you with startd or starter logs. However, I do</font>
<font color='#7F7E6F'>&#62; see some errors in what I did get returned that point to what&#39;s going on.</font>
<font color='#7F7E6F'>&#62; The glideins appear to make it through validation fine and condor is</font>
<font color='#7F7E6F'>&#62; started up, but at some point it is killed (or segfaults), quits, and spews</font>
<font color='#7F7E6F'>&#62; errors into different points in both stderr and stdout [1] [2] [3]. No</font>
<font color='#7F7E6F'>&#62; indication of what killed it, though.</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [1]</font>
<font color='#7F7E6F'>&#62; From stderr&#58;</font>
<font color='#7F7E6F'>&#62; Stack dump for process 26029 at timestamp 1447619565 (24 frames)</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(dprintf_dump_stack+0x12d)[0x2b9ea02a19ad]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(+0x1286f2)[0x2b9ea02086f2]</font>
<font color='#7F7E6F'>&#62; /lib64/libpthread.so.0[0x356780f710]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN16ProcFamilyClient11kill_familyEiRb+0x16)[0x2b9ea0380846]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN15ProcFamilyProxy11kill_familyEi+0x3f)[0x2b9ea026300f]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN10DaemonCore11Kill_FamilyEi+0x16)[0x2b9ea0356a86]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master(_ZN6daemon10KillFamilyEv+0x1c)[0x409ebc]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master(_ZN6daemon8HardKillEv+0x51)[0x409f21]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master(_ZN7Daemons18HardKillAllDaemonsEv+0x59)[0x40b5a9]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master(DoCleanup+0x44)[0x4104e4]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_EXCEPT_+0x12d)[0x2b9ea0253e0d]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN15ProcFamilyProxy24recover_from_procd_errorEv+0x1bb)[0x2b9ea0262e1b]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN15ProcFamilyProxy11kill_familyEi+0x30)[0x2b9ea0263000]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN10DaemonCore11Kill_FamilyEi+0x16)[0x2b9ea0356a86]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master(_ZN6daemon10KillFamilyEv+0x1c)[0x409ebc]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master(_ZN6daemon6ExitedEi+0x137)[0x40a4b7]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master(_ZN7Daemons9AllReaperEii+0x109)[0x40bed9]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN10DaemonCore10CallReaperEiPKcii+0x195)[0x2b9ea035a775]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN10DaemonCore17HandleProcessExitEii+0x2e9)[0x2b9ea0361d49]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN10DaemonCore24HandleDC_SERVICEWAITPIDSEi+0x7e)[0x2b9ea0361f4e]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_ZN10DaemonCore6DriverEv+0x823)[0x2b9ea0366073]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/lib/libcondor_utils_8_3_2.so(_Z7dc_mainiPPc+0x1799)[0x2b9ea034dbf9]</font>
<font color='#7F7E6F'>&#62; /lib64/libc.so.6(__libc_start_main+0xfd)[0x356701ed5d]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master[0x408649]</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor_startup.sh&#58; line 13&#58; 26029 Segmentation</font>
<font color='#7F7E6F'>&#62; fault      $CONDOR_DIR/sbin/condor_master -f -pidfile</font>
<font color='#7F7E6F'>&#62; $PWD/condor_master2.pid</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor_startup.sh&#58; line 15&#58;</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor/sbin/condor_master&#58; No such file or directory</font>
<font color='#7F7E6F'>&#62; ls&#58; cannot access log&#58; No such file or directory</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/condor_startup.sh&#58; line 1002&#58;</font>
<font color='#7F7E6F'>&#62; /tmp/glide_V7ugvC/main/error_gen.sh&#58; No such file or directory</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [2]</font>
<font color='#7F7E6F'>&#62; From stdout&#58;</font>
<font color='#7F7E6F'>&#62; === Condor starting Sun Nov 15 15&#58;01&#58;46 EST 2015 (1447617706) ===</font>
<font color='#7F7E6F'>&#62; === Condor started in background, now waiting on process 26029 ===</font>
<font color='#7F7E6F'>&#62; Condor startup received kill signal... shutting down condor processes</font>
<font color='#7F7E6F'>&#62; === Last script ended Sun Nov 15 15&#58;32&#58;36 EST 2015 (1447619556) with code 0</font>
<font color='#7F7E6F'>&#62; after 1851 ===</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [3]</font>
<font color='#7F7E6F'>&#62; From stdout&#58;</font>
<font color='#7F7E6F'>&#62; Condor startup received kill signal... shutting down condor processes</font>
<font color='#7F7E6F'>&#62; === Condor ended Sun Nov 15 15&#58;32&#58;48 EST 2015 (1447619568) after 1862 ===</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Total jobs/goodZ jobs/goodNZ jobs/badSignal jobs/badOther jobs below are</font>
<font color='#7F7E6F'>&#62; normalized to 1 Core</font>
<font color='#7F7E6F'>&#62; === Stats of main ===</font>
<font color='#7F7E6F'>&#62; === End Stats of main ===</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; Explicitly killed, exiting with return code 0 instead of 143</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; --</font>
<font color='#7F7E6F'>&#62; Brendan Dennis</font>
<font color='#7F7E6F'>&#62; UCSD Glidein Factory Operations</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; On Mon, Nov 16, 2015 at 12&#58;25 PM, Brian Lin &#60;blin@....&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; &#62; Hey,</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; &#62; Some users are having some issues here at UW where they&#39;re seeing their</font>
<font color='#7F7E6F'>&#62; &#62; jobs that land on Clemson pilots with the following hold message&#58;</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; &#62; 007 (50054419.000.000) 11/16 03&#58;29&#58;01 Shadow exception!</font>
<font color='#7F7E6F'>&#62; &#62; Error from glidein_12956_800180612@....&#58; unable</font>
<font color='#7F7E6F'>&#62; &#62; to restart the ProcD after several tries</font>
<font color='#7F7E6F'>&#62; &#62; 0 - Run Bytes Sent By Job</font>
<font color='#7F7E6F'>&#62; &#62; 118443328 - Run Bytes Received By Job</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; &#62; We&#39;d like to take a look at the ProcLog and StarterLogs to see if we can</font>
<font color='#7F7E6F'>&#62; &#62; gather why the ProcD is having issues. Here are some are glidein ID&#39;s</font>
<font color='#7F7E6F'>&#62; &#62; and hostnames to trim our search down&#58;</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; &#62; glidein_18016_325080620@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_2246_129498253@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_13869_77526388@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_6452_814641051@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_20869_480226428@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_15408_68645592@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_20591_565889760@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_16084_93291824@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_7137_315709848@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_31973_306537966@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_967_123990735@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_1977_30850502@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_12267_35188756@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_1236_404427987@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_9084_121845748@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_31841_72032296@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_7362_771347340@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_8670_974081424@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_6962_47300236@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_17886_62332920@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_32001_74922263@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_14553_424829778@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_31430_447823076@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_12956_800180612@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_1597_233541693@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_24099_415454517@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_720_279046670@....</font>
<font color='#7F7E6F'>&#62; &#62; glidein_22247_247910600@....</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; &#62; Thanks,</font>
<font color='#7F7E6F'>&#62; &#62; Brian</font>
<font color='#7F7E6F'>&#62; &#62; _______________________________________________</font>
<font color='#7F7E6F'>&#62; &#62; Osg-gfactory-support mailing list</font>
<font color='#7F7E6F'>&#62; &#62; Osg-gfactory-support@....</font>
<font color='#7F7E6F'>&#62; &#62; <a href='https&#58;//physics-mail.ucsd.edu/mailman/listinfo/osg-gfactory-support' target='_blank' rel='nofollow'>https&#58;//physics-mail.ucsd.edu/mailman/listinfo/osg-gfactory-support</a></font>
<font color='#7F7E6F'>&#62; &#62;</font>

I asked HTCondor expert Greg Thain for assistance&#58;

<font color='#7F7E6F'>&#62; On 11/17/2015 10&#58;12 AM, Aaron Moate wrote&#58;</font>
<font color='#7F7E6F'>&#62; &#62; Thanks Brian.  Gthain, here&#39;s what we have so far from Clemson.</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; &#62; -Moate</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; &#62;</font>
<font color='#7F7E6F'>&#62; Looks like condor is being unpacked and running under /tmp, and if /tmp</font>
<font color='#7F7E6F'>&#62; fills up, we&#39;re going to have problems.  Can we get clemson to not run</font>
<font color='#7F7E6F'>&#62; glideins under /tmp?</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; -greg</font>

Brian Lin talked to the admins at Clemson-Palmetto and asked them if they would move the working directory from /tmp to somewhere less volatile;  They did so and moved it to /local_scratch.

I ran some test jobs yesterday, and am still having the issue.  So are some of our users&#58;

7675071.0   crcox          12/4  03&#58;55 Error from glidein_10124_399004083@....&#58; STARTER at 10.125.5.122 failed to send file(s) to &#60;128.104.101.92&#58;9618&#62;&#58; error reading from /local_scratch/glide_Z7iELe/execute/dir_27653/ALLURLS&#58; (errno 2) No such file or directory; SHADOW failed to receive file(s) from &#60;130.127.255.222&#58;41100&#62;

I&#39;ve ran some diagnostic test jobs to try to gather data about why the output files aren&#39;t there after the job has completed, but no luck so far.

Cheers,
Aaron Moate

by /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Aaron Moate 894

</div><script type='text/javascript'>
        $('#show_41988450').click(function() {
            $('#detail_41988450').slideDown("normal");
            $('#show_41988450').hide();
            $('#hide_41988450').show();
        });
        $('#hide_41988450').click(function() {
            $('#detail_41988450').slideUp();
            $('#hide_41988450').hide();
            $('#show_41988450').show();
        });
        </script></pre></div><legend>Similar Recent Tickets <small>modified within the last 30 days</small></legend><div id="similar_tickets"><p class="muted">No similar tickets found.</p></div>
</div>
<script type="text/javascript">
function reset_anchor() {
    $("#updates .selected").removeClass("selected");
    var urls = document.location.toString().split('#'); 
    var anchor = urls[1];
    if(anchor) {
        $("a[name='"+anchor+"']").parents(".update_description").addClass("selected");
    }
}
function submitspam(ticket_id) {
    myret = confirm("Would you like to close this ticket as a security ticket, and submit the ticket content to akismet?");
    if(myret == true) {
        $.ajax("viewer/processspam?id="+ticket_id).done(function() {
            window.location.reload();
        });
    }
}

$(function() {
    reset_anchor();
    var ADDITIONAL_COOKIE_NAME = 'gocticket';
    var options = { path: '/', expires: 365};

    if(window.opener && window.opener.name == "gocticket_list") {
        v = $.cookie("closewindow");
        if(!v) {
            $("#closewindow").attr("checked", "checked"); //on by default
        } else {
            if(v == "checked") {
                $("#closewindow").attr("checked", "checked");
            }
        }
        $("#closewindow").click(function() {
            $.cookie("closewindow", $(this).attr('checked'), options);
        });
    } else {
        $("#closewindow_area").hide();
    }
    function updateTimeago() {
        $("time").timeago();
        setTimeout(updateTimeago, 30*1000);
    }
    updateTimeago();
    $(".description").focus(expand_description);
});
</script>
<hr/>
<footer>
<p>GOC Ticket Version 2.2 | <a href="https://ticket.opensciencegrid.org/goc/submit?app_issue_check=on&amp;app_issue_type=goc&amp;app_goc_url=https%3A%2F%2Fticket.opensciencegrid.org%3A443%2F27686">Report Bugs</a>
 | <a href="https://github.com/opensciencegrid/operations/blob/master/docs/privacy.md">Privacy Policy</a>
</p>

<p> <img align="top" src="images/tag_orange.png"/> Copyright 2018 The Trustees of Indiana University - Developed for Open Science Grid</p>
</footer>


</div><!--container-fluid-->
<script src="https://ticket1.grid.iu.edu:8443/socket.io/socket.io.js"></script>
<script>
var chat = io.connect('https://ticket1.grid.iu.edu:8443');
chat.on('connect', function() {
    chat.emit('authenticate', {nodekey:'', ticketid: 27686});
});
chat.on('peers', function(peers) {
    $("#peers").html("");
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('peer_disconnect', function(pid) {
    $("#peer_"+pid).hide("slow");
});
chat.on('peer_connected', function(peers) {
    //expect only 1 peer connecting, but..
    for(var pid in peers) {
        var peer = peers[pid];
        addPeer(pid, peer);
    }
});
chat.on('submit', function() {
    if(confirm("This ticket was updated. Do you want to refresh?")) {
        history.go(0);
    }
});

function addPeer(pid, peer) {
    var ipinfo = "";
    if(peer.ip != undefined) {
        ipinfo = "<span class=\"ip\">"+peer.ip+"</span>";
    }
    if(chat.io.engine.id == pid) {
        //don't display myself
        return;
    }
    var html = "<li class=\"new\" id=\"peer_"+pid+"\" class=\"peer\">"+peer.name+ipinfo+"</li>";
    $("#peers").prepend(html);
    $("#peers .new").animate({bottom: 0}, 1000, function() {$(this).removeClass("new")});
}

$(function() {
    $("#ticket_form").submit(function() {
        chat.emit('submit');
        return true;
    });
});
</script>
<script>
//used by searchbox
function parseValue(value) {
    var obj = new Object();
    var tokens = value.split("\t");
    obj.str = tokens[0];
    obj.count = tokens[1];
    return obj;
}

$(function() {
    //bootstrap-2.0.4 stuff
    $(".alert-message").alert();
    $(".dropdown-toggle").dropdown();
    $("span[rel='tooltip']").tooltip();
    $("a[rel=popover]").popover();

    //activate menu that user is currently on
    $("#menu_navigator").addClass("active"); 
    $("#submenu_").addClass("active"); 

    //translate zend validation error message to bootstrap
    $(".errors").addClass("alert").addClass("alert-error");

    //enable autocomplete for search box
    $("#search").autocomplete({
        source: function( request, response ) {
            $.ajax({
                url: "search/autocomplete",
                dataType: "text",
                data: {
                    //featureClass: "P",
                    //style: "full",
                    //maxRows: 12,
                    //name_startsWith: request.term
                    q: request.term
                },
                success: function( data ) {
                    response( $.map( data.split("\n"), function( item ) {
                        if(item == "") return null;
                        return {
                            value: item
                        }
                    }));
                }
            });
        },
        select: function(event, ui) {
            document.location = "search?q="+ui.item.value;
        }
    });
    
});
</script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-69012-13");
pageTracker._trackPageview();
} catch(err) {}
</script>

</body>
