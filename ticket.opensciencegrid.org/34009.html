<!DOCTYPE html>
<html lang="en">
  <head>
  <base href="">
    <title>[34009] osg-oasis-update not propagating updates to the cvmfs servers</title>    <meta charset="utf-8" />
    <meta name="verify-v1" content="na5IcAJsZVOfEkboRxuIiZ1zpZgnZiWra+nKcS7nA/o=" />
    <meta name="google-site-verification" content="DLrk3ft4s8b-S2TloLCL2LD_t6wcTjgSluf5pmiu2kA" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <link href="https://ticket.opensciencegrid.org/rss" rel="alternate" type="application/rss+xml" title="GOC Ticket Update feed" />

    <style type="text/css">
      body {
        padding-top: 50px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
     #search {
            width: 300px;
     }

    </style>

<script src="https://code.jquery.com/jquery-3.0.0.js"></script>
<script src="https://code.jquery.com/jquery-migrate-3.0.1.js"></script>

   <link href="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet"/>
    <script src="https://netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"/>
    <link href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.min.css" rel="stylesheet"/>
 <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>


    <link href="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/css/select2.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.0-rc.2/js/select2.min.js"></script>

    <link href="css/ticket.css" rel="stylesheet" />
    <script src="lib/jquery.cookie.js"></script>

    <link href="images/tag_orange.png" rel="icon" type="image/png"/>
  </head>

  <body>
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
            <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </a>

            <a class="brand" style="padding: 6px 0px 0px 6px;" href="http://opensciencegrid.org"><img src="images/osglogo.40x30.png"/></a>
            <ul class="nav">
                <li class="dropdown"><a href="https://ticket.opensciencegrid.org/#" class="dropdown-toggle" data-toggle="dropdown">GOC Ticket <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    <li><a href="https://my.opensciencegrid.org">MyOSG</a></li>
                    <li><a href="https://oim.opensciencegrid.org">OIM</a></li>
                    <li class="active"><a href="https://ticket.opensciencegrid.org/index">Ticket</a></li>
	<li class="divider"></li>
	<li><a href="http://repo.grid.iu.edu">Repo</a></li>
	<li class="divider"></li>
	<li><a href="http://blogs.grid.iu.edu">Blog</a></li>
                    <li><a href="http://display.grid.iu.edu">Display</a></li>
                    <li><a href="http://osggoc.blogspot.com/">News</a></li>
                    </ul>
                </li>
            </ul>
            <ul class="nav pull-right">
                <li><a href="https://ticket.opensciencegrid.org/sso/">Login</a></li>            </ul>

            <div class="nav-collapse">
                <ul class="nav">
			 <li id="menu_submit"><a href="https://ticket.opensciencegrid.org/submit">Submit</a></li><li id="menu_view" class="dropdown"><a href="https://ticket.opensciencegrid.org/\#" class="dropdown-toggle" data-toggle="dropdown">View <b class="caret"></b></a><ul class="dropdown-menu"><li id="submenu_listopen"><a href="https://ticket.opensciencegrid.org/list/open">Open Tickets</a></li><li id="submenu_listrecentclose"><a href="https://ticket.opensciencegrid.org/list/recentclose">Recently Closed Tickets</a></li><li class="divider"></li><li id="submenu_alltickets"><a href="https://ticket.opensciencegrid.org/search?q=&amp;sort=id">All Tickets</a></li></ul></li>                </ul>

                <form class="navbar-search pull-right" action="https://ticket.opensciencegrid.org/viewer">
                    <input id="search" type="text" name="id" class="search-query span2" placeholder="Search Ticket" value=""/>
                </form>
            </div>
        </div>
      </div>
    </div>

<script type='text/javascript' src='lib/jquery.timeago.js'></script>
<script type='text/javascript' src='lib/byte2size.js'></script>
<style>
#updates .toolbar {
position: relative;
margin-top: 0px;
top: -10px;
font-weight: normal;
}
#updates a.anchor {
position: relative;
top: -50px;
}
#updates .selected pre {
animation:selected 2s;
animation-iteration-count: 2;
animation-direction: alternate;
-webkit-animation:selected 2s; 
-webkit-animation-iteration-count: 2;
-webkit-animation-direction: alternate;
box-shadow: inset 1px 1px 20px #9ad;
border: 1px solid #9ab;
margin: 5px 0px;
padding-left: 10px;
}
@keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ab;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
@-webkit-keyframes selected {
    from  {
        box-shadow: inset 1px 1px 20px #9ad;
        border: 1px solid #9ad;
    }
    to {
        box-shadow: inset 1px 1px 20px #05c;
        border: 1px solid #05c;
    }
}
#updates pre {
background-color: inherit;
line-height: 15px;
padding: 5px;
}
#updates .header {
color: #999;
}
#updates .update_history pre {
background-color: #eee;
color: #666;
font-size: 85%;
}
#updates .clickable {
cursor: pointer;
}
#updates .clickable:hover {
color: #D98719;
}
#updates .meta_information pre {
background-color: #fed;
}
#similar_tickets {
max-height: 300px;
overflow-y: auto;
pointer-events: none;
padding: 5px;
background-color: #f4f4f4;
}
.btn-toolbar {
margin-bottom: 0;
height: 30px;
}
#peers {
position: fixed;
bottom: 0px;
right: 0px;
z-index: 100;
list-style: none;
padding: 5px 0px 0px 5px;
margin: 0px;
background-color: white;
box-shadow: 0px 0px 10px white;
}
#peers li {
background-color: #ccc;
color: #000;
display: inline-block;
padding: 5px 10px;
margin-right: 5px;
position: relative;
}
/*
#peers li:hover {
background-color: #999;
cursor: pointer;
}
*/
#peers span.ip {
padding-left: 5px;
color: #666;
}
#peers .new {
bottom: -30px;
}
/*
#peers .me {
background-color: red;
}
*/
</style>

<div class="container-fluid">
<ul id="peers"></ul>
<div class="alert alert-danger"><a class="close" href="https://ticket.opensciencegrid.org/#" data-dismiss="alert">&times;</a>By the end of May 2018, the ticketing system at https://ticket.opensciencegrid.org will be retired and support will be provided at https://support.opensciencegrid.org. Throughout this transition the support email (help@opensciencegrid.org) will be available as a point of contact.<br><br>                                                   
                                                                                                                                                                                   
Please see the service migration page for details: https://opensciencegrid.github.io/technology/policy/service-migrations-spring-2018/#ticket</div><div id="presence" class="pull-right"></div><div class="ticketgui"><script type="text/javascript" src="lib/checktab.js"></script>

<script>
var expanded = false;
function expand_description() {
    var desc = $(".description");
    if(!expanded) {
        expanded = true;
        //expand to minheight
        var min = 250;
        if(desc.height() < min) {
            desc.animate({height: min}, 200);
        }
    }
}

$(document).ready(function() {
    $("input[name='nad']").datepicker({
        dateFormat: 'yy-mm-dd'
    });
});

</script>



<style>
.form-horizontal .control-label {
padding-top: inherit;
font-size:90%;
color:#666;
}
label {
margin-bottom: 0px;
}
.controls {
line-height: 18px;
}
</style>
<form class="form-horizontal" action="https://ticket.opensciencegrid.org/viewer/updatebasic?id=34009" method="post">
<div class="page-header">
    <h3><span class="muted">34009</span> / osg-oasis-update not propagating updates to the cvmfs servers</h3>
</div>

<div class="row-fluid">
<div class="span5">
    <legend>Contact</legend>
    <div class="control-group">
        <label class="control-label">Full Name</label>
        <div class="controls">Richard T. Jones</div>
    </div>
    <div class="control-group">
        <label class="control-label">Email</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">Phone</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>
    <div class="control-group">
        <label class="control-label">CC</label>
        <div class="controls">
            <i class="icon-lock"></i>        </div>
    </div>

    <legend>Details</legend>
    <div class="control-group"><label class="control-label">Resource Name</label><div class="controls">Oasis</div></div><div class="control-group"><label class="control-label">Associated VO</label><div class="controls">Gluex</div></div><div class="control-group"><label class="control-label">Submitted Via</label><div class="controls">GOC Ticket/submit</div></div><div class="control-group"><label class="control-label">Submitter</label><div class="controls">Richard T. Jones</div></div><div class="control-group"><label class="control-label">Support Center</label><div class="controls">GLUEX</div></div><div class="control-group"><label class="control-label">Ticket Links</label><div class="controls"></div></div>
    <div class="control-group">
        <label class="control-label">Ticket Type</label>
        <div class="controls">Problem/Request</div>
    </div>
    <div class="control-group">
        <label class="control-label">Priority</label>
        <div class="controls">Normal</div>
    </div>
    <div class="control-group">
        <label class="control-label">Status</label>
        <div class="controls">
Closed</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action</label>
        <div class="controls">ENG Action</div>
    </div>
    <div class="control-group">
        <label class="control-label">Next Action Deadline</label>
        <div class="controls flag_red">2017-06-21</div>
    </div>

</div><!--span-->
<div class="span7">
    <legend>Assignees</legend>
    <div class="assignee" style="width: 60%">Scott Teige <span class="muted"> / OSG Operations Infrastructure</span></div><div class="assignee" style="width: 60%">GLUEX <span class="muted"> / OSG Support Centers</span></div><div class="assignee" style="width: 60%">OSG-GOC <span class="muted"> / OSG Support Centers</span></div><div class="assignee" style="width: 60%">Dave Dykstra <span class="muted"> / OSG Security Coordinators</span></div>    <br>

    <legend>Assignees</legend>
    TODO
    <br>

    <style>
legend.noborder {
border-bottom: none;
}
</style>

<div id="attachment-list"/>
<script>
$(function () {
    var first = true;
    $.getJSON("attachment/list/34009", function (files) {
        //console.dir(files);
        var html = "<table class=\"table table-condensed\">";
        $(files).each(function() {
            if(first) {
                first = false;
                html += "<legend class=\"noborder\">Attachmenets</legend>";
            }
            html += "<tr class=\"attachment\">";
            html += "<td><img src=\""+this.thumbnail_url+"\"/></td>";
            html += "<td><a href=\""+this.url+"\" target=\"_blank\">"+this.name+"</a></td>";
            html += "<td>"+bytesToSize(this.size, 1)+"</td>";
            html += "</tr>";
        });
        html += "</table>";
        $("#attachment-list").html(html);
    });
});

function download(url) {
    window.open(url, "_blank");
}
</script>


</div><!--span-->
</div><!--row-fluid-->


</form>

</div>
<div id="updates" style="clear: both;">
    <legend>Past Updates
    <div class="btn-toolbar pull-right toolbar">
    </div><!--btn-toolbar-->
    </legend>

    <div class='update_description'><i onclick="document.location='34009#1498475770'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-26T11:16:10+00:00">Jun 26, 2017 11:16 AM UTC</time> by <b>Scott Teige</b><a class="anchor" name="1498475770">&nbsp;</a></div><pre>closing per request</pre></div><div class='update_description'><i onclick="document.location='34009#1498249688'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-23T20:28:08+00:00">Jun 23, 2017 08:28 PM UTC</time><a class="anchor" name="1498249688">&nbsp;</a></div><pre>Ok, sounds good. We can close this ticket. Everything is working for me now, both centos6 and centos7 hosts giving the same behavior. The hang was caused by a bug on my end. Thanks for all of your help!
-Richard Jones

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498249582'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-23T20:26:22+00:00">Jun 23, 2017 08:26 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498249582">&nbsp;</a></div><pre>That&#39;s correct.  Scott updated the scratch filesystem to be a couple of hundred GB instead of 8GB.</pre></div><div class='update_description'><i onclick="document.location='34009#1498246333'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-23T19:32:13+00:00">Jun 23, 2017 07:32 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1498246333">&nbsp;</a></div><pre>Dave,

Yes, just one more question. I have been chunking up my updates to be less
than 5GB to avoid the errors I was seeing previously. Is this limitation no
longer in force?
-Richard

On Fri, Jun 23, 2017 at 3&#58;13 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1498245236'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-23T19:13:56+00:00">Jun 23, 2017 07:13 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498245236">&nbsp;</a></div><pre>gluex publications aren&#39;t failing anymore.  I also adjusted the .cvmfsdirtab so the catalogs are now in the correct place again and we no longer get the &#34;WARNING&#58; cannot apply pathspec /gluex/packages/*/*&#34; message.

Richard, does it look good to you, can we close the ticket?</pre></div><div class='update_description'><i onclick="document.location='34009#1498232989'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-23T15:49:49+00:00">Jun 23, 2017 03:49 PM UTC</time> by <b>Scott Teige</b><a class="anchor" name="1498232989">&nbsp;</a></div><pre>done.</pre></div><div class='update_description'><i onclick="document.location='34009#1498163240'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T20:27:20+00:00">Jun 22, 2017 08:27 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498163240">&nbsp;</a></div><pre>It&#39;s a sparse file, so to avoid causing it to grow I think rsync --sparse should work better than cp.</pre></div><div class='update_description'><i onclick="document.location='34009#1498163046'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T20:24:06+00:00">Jun 22, 2017 08:24 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498163046">&nbsp;</a></div><pre>Ok, sounds good.  If you want to save some time you could bring oasis down, copy oasis.12-hdb.qcow2 to oasis.13-hdb.qcow2, and bring it back up.</pre></div><div class='update_description'><i onclick="document.location='34009#1498160639'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T19:43:59+00:00">Jun 22, 2017 07:43 PM UTC</time> by <b>Scott Teige</b><a class="anchor" name="1498160639">&nbsp;</a></div><pre>OK, here is what I&#39;ll do&#58;
I&#39;ll ask Tom to stretch /usr/local to 200G. There will be an unscheduled reboot.
I&#39;ll modify the ansible yaml to use that value, this should make it persistent.

Tom is getting some heavy duty dental work done as I type and tomorrow is a Friday,
I&#39;ll let you know when this happens.</pre></div><div class='update_description'><i onclick="document.location='34009#1498160341'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T19:39:01+00:00">Jun 22, 2017 07:39 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498160341">&nbsp;</a></div><pre>Actually I think it was 225G, not 200G.  The current size on oasis-itb shows 207G raw, but the qcow2 file looks more like 225G.</pre></div><div class='update_description'><i onclick="document.location='34009#1498160170'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T19:36:10+00:00">Jun 22, 2017 07:36 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498160170">&nbsp;</a></div><pre>Yes, ansible/host_vars/oasis.grid.iu.edu.yaml has &#34;disk&#58; 8GB&#34;.  I checked the svn log and it looks like it never got updated.  I tried to check it in now and I got &#34;svn&#58; E220004&#58; Access denied
&#34; so maybe I never had permission to update it and that was the problem.

On the other hand, looking at the images on vm08 in /var/lib/libvirt/images, there was a larger /usr/local that was last used on May 9&#58;
# ls -ls oasis-itb.*-hdb.qcow2 oasis.*-hdb.qcow2
170416 -rw-r--r-- 1 root root   8591507456 Jul 18  2016 oasis-itb.18-hdb.qcow2
161156 -rw-r--r-- 1 root root   8001486848 Nov 30  2016 oasis-itb.19-hdb.qcow2
24128272 -rw-r--r-- 1 qemu qemu 225035091968 Jun 22 19&#58;10 oasis-itb.20-hdb.qcow2
3819928 -rw-r--r-- 1 root root   8591507456 Jan 24 14&#58;19 oasis.10-hdb.qcow2
161096 -rw-r--r-- 1 root root   8001486848 Feb 14 14&#58;09 oasis.11-hdb.qcow2
24655728 -rw-r--r-- 1 root root  25258950656 May  9 13&#58;02 oasis.12-hdb.qcow2
7764052 -rw-r--r-- 1 qemu qemu   8001486848 Jun 22 18&#58;24 oasis.13-hdb.qcow2

There were only minor updates that day that weren&#39;t supposed to affect this <a href='https&#58;//jira.opensciencegrid.org/browse/OO-185.' target='_blank' rel='nofollow'>https&#58;//jira.opensciencegrid.org/browse/OO-185.</a>

I am guessing that somehow when oasis was created on February 14 for <a href='https&#58;//jira.opensciencegrid.org/browse/OO-181' target='_blank' rel='nofollow'>https&#58;//jira.opensciencegrid.org/browse/OO-181</a> it then had the right size, but only the update of operating system to c7 got checked in to subversion, so when it was recreated May 9 it reverted.

Anyway I think the disk should be set to 200G and the VM recreated.  oasis-itb has been running that way stably since January or February, and I believe oasis was running that way February 14 through May 9.</pre></div><div class='update_description'><i onclick="document.location='34009#1498157806'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T18:56:46+00:00">Jun 22, 2017 06:56 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498157806">&nbsp;</a></div><pre>Ack, /usr/local is supposed to be 200G like on oasis-itb&#58;

/dev/vdb1       207G  3.8G  192G   2% /usr/local

I am checking the ansible configuration ...</pre></div><div class='update_description'><i onclick="document.location='34009#1498157493'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T18:51:33+00:00">Jun 22, 2017 06:51 PM UTC</time> by <b>Scott Teige</b><a class="anchor" name="1498157493">&nbsp;</a></div><pre>Filesystem                              Size  Used Avail Use% Mounted on
/dev/vdb1                               7.4G  2.1G  5.0G  30% /usr/local

Looks OK to me, how much do you need?</pre></div><div class='update_description'><i onclick="document.location='34009#1498157286'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T18:48:06+00:00">Jun 22, 2017 06:48 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498157286">&nbsp;</a></div><pre>Of all the files modified in the last 24 hours, the biggest directory is sim-recon/2.14.0 which is 12GB.  There should have been plenty of space for that in the 200GB filesystem, but I am guessing there&#39;s some junk left using up space.</pre></div><div class='update_description'><i onclick="document.location='34009#1498156875'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T18:41:15+00:00">Jun 22, 2017 06:41 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498156875">&nbsp;</a></div><pre>The first error was at 02&#58;38 UTC, but the publish succeeded even though not all the files were copied.

Thu Jun 22 02&#58;38&#58;01 UTC 2017 starting vo update for gluex
rsync&#58; writefd_unbuffered failed to write 4 bytes to socket [sender]&#58; Broken pip
e (32)
rsync&#58; write failed on &#34;/cvmfs/oasis.opensciencegrid.org/gluex/sim-recon/2.14.0/Linux_CentOS7-x86_64-gcc4.8.5/bin/hdevio_scan&#34;&#58; No space left on device (28)
rsync error&#58; error in file IO (code 11) at receiver.c(322) [receiver=3.0.9]
rsync&#58; connection unexpectedly closed (92472 bytes received so far) [sender]
rsync error&#58; error in rsync protocol data stream (code 12) at io.c(605) [sender=3.0.9]

That file was updated at 00&#58;38 UTC.

Then later when you tried again at 14&#58;35 UTC it tried to continue to update the rest of the unpublished files that were changed before, but that publish failed.  It may or may not fail again at the same point if you request a publish again while changing nothing, you could try it.  You could watch the log at /net/vm08/oasis-share/srv/oasis/log/oasis/updates.log if you want.</pre></div><div class='update_description'><i onclick="document.location='34009#1498150894'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T17:01:34+00:00">Jun 22, 2017 05:01 PM UTC</time><a class="anchor" name="1498150894">&nbsp;</a></div><pre>Maybe previous updates were failing, even though they seemed to complete?
-Richard

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498150831'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T17:00:31+00:00">Jun 22, 2017 05:00 PM UTC</time><a class="anchor" name="1498150831">&nbsp;</a></div><pre>The only thing I did was cd to /stage/oasis/gluex and edit two scripts in a text editor, then trigger the osg-oasis-update. How would any other files be changed, I wonder?
-Richard

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498150735'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T16:58:55+00:00">Jun 22, 2017 04:58 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498150735">&nbsp;</a></div><pre>The file it died on is 12MB.  It would include all files changed since the last successful update.</pre></div><div class='update_description'><i onclick="document.location='34009#1498149518'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T16:38:38+00:00">Jun 22, 2017 04:38 PM UTC</time><a class="anchor" name="1498149518">&nbsp;</a></div><pre>The two files were 3785 bytes and 3744 bytes.
-Richard

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498149426'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T16:37:06+00:00">Jun 22, 2017 04:37 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498149426">&nbsp;</a></div><pre>It&#39;s not actually the size difference that matters, it&#39;s the size of the new files, but in any case 2 executables should of course not be a problem.</pre></div><div class='update_description'><i onclick="document.location='34009#1498149210'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T16:33:30+00:00">Jun 22, 2017 04:33 PM UTC</time><a class="anchor" name="1498149210">&nbsp;</a></div><pre>You asked how much new space was asked for in this update. The size difference is less than 100 bytes.
-Richard

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498149145'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T16:32:25+00:00">Jun 22, 2017 04:32 PM UTC</time><a class="anchor" name="1498149145">&nbsp;</a></div><pre>In this update, I changed just two executables. I don&#39;t know how to tell the system that 99.8% of the files have not been touched.
-Richard

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498149063'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T16:31:03+00:00">Jun 22, 2017 04:31 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498149063">&nbsp;</a></div><pre>Indeed there is a problem&#58;

Thu Jun 22 14&#58;35&#58;01 UTC 2017 starting vo update for gluex
rsync&#58; writefd_unbuffered failed to write 4 bytes to socket [sender]&#58; Broken pipe (32)
rsync&#58; write failed on &#34;/cvmfs/oasis.opensciencegrid.org/gluex/sim-recon/2.14.0/src/.Linux_CentOS7-x86_64-gcc4.8.5/plugins/Analysis/p3pi_hists/DEventProcessor_p3pi_hists.os&#34;&#58; No space left on device (28)
rsync error&#58; error in file IO (code 11) at receiver.c(322) [receiver=3.0.9]
rsync&#58; connection unexpectedly closed (26048 bytes received so far) [sender]
rsync error&#58; error in rsync protocol data stream (code 12) at io.c(605) [sender=3.0.9]

Do you know how much new code you&#39;re trying to publish?  The scratch space on the oasis machine is in a 200G filesystem so that&#39;s the maximum that can be published at once.

I can&#39;t see the space used on the production oasis machine, but maybe there wasn&#39;t full cleanup from a previous faliure.  The oasis-itb machine does the same things as the oasis production machine and it didn&#39;t see a problem.

Scott, how much free space is there in /usr/local on oasis?  If it is nearly full, can you find out which subdirectory is using up most of the space?</pre></div><div class='update_description'><i onclick="document.location='34009#1498148052'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T16:14:12+00:00">Jun 22, 2017 04:14 PM UTC</time><a class="anchor" name="1498148052">&nbsp;</a></div><pre>ps. Can I ask that someone check the propagation of oasis updates to the oasis servers again? I have an update that I submitted a couple of hours ago that has still not shown up. My experience has been that when things are working it takes less than an hour to fully propagate back to my local caches.
-Richard J.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498128175'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-22T10:42:55+00:00">Jun 22, 2017 10:42 AM UTC</time><a class="anchor" name="1498128175">&nbsp;</a></div><pre>The issue with jobs hanging on Centos 6 nodes has been resolved. I discovered that there was a mutex that was not being properly initialized in the hd_root threads manager. The bug had been fixed in downstream versions, but had not been pushed upstream to the version I was running. The fact that it only bit us when running on osg worker nodes inside a container, and only on nodes running a 2.6 kernel misled me to suspect that the problem was somehow in the running environment. I think this issue is now resolved.
-Richard Jones

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1498046706'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-21T12:05:06+00:00">Jun 21, 2017 12:05 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1498046706">&nbsp;</a></div><pre>I think you&#39;re right, that&#39;s a &#34;not yet&#34;.  I don&#39;t think any of us had tried condor-ssh-to-job yet with singularity.</pre></div><div class='update_description'><i onclick="document.location='34009#1498041394'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-21T10:36:34+00:00">Jun 21, 2017 10:36 AM UTC</time><a class="anchor" name="1498041394">&nbsp;</a></div><pre>I have run into something that I don&#39;t know how to work around. I want to use condor-ssh-to-job to join a remote container where my job is running, so I can debug it while it is running, using gdb &#60;prog&#62; &#60;PID&#62;. However with PID isolation, I cannot see my running process from the bash shell started by my sshd daemon. I had to install my own build of sshd inside the container to make it work when run as a non-root user (standard sshd build for Centos fails unless run as root), and wrap it with a script to rewrite portions of the sshd_config that have the working directory rooted outside the container, but once that was done the condor-ssh-to-job command successfully connects with a shell running on the worker node inside a container with the same cwd as the job. Unfortunately it starts up with its own separate pid ns, so I cannot see the pid of the process I want to debug. Is there any way I can get it to start up in the same pid ns as the running job?

Looking through the mailing lists, it looks like this may be a feature of singularity v2.4, so I suspect the answer is going to be &#34;not yet&#34;.
-Richard

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497974555'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-20T16:02:35+00:00">Jun 20, 2017 04:02 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1497974555">&nbsp;</a></div><pre>Richard,

If the image works for you, then I wouldn&#39;t worry about these errors.  I&#39;ll ask around, but I don&#39;t think these errors important.

-Derek</pre></div><div class='update_description'><i onclick="document.location='34009#1497972373'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-20T15:26:13+00:00">Jun 20, 2017 03:26 PM UTC</time><a class="anchor" name="1497972373">&nbsp;</a></div><pre>Is there anything I can do to help resolve these errors? I don&#39;t think they are directly related to anything I do, because the files listed in this error report are just the results of a rpm install, and not touched directly by any commands I issue in the dockerfile.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497972191'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-20T15:23:11+00:00">Jun 20, 2017 03:23 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497972191">&nbsp;</a></div><pre>There were a number of errors during the publish of rjones30/gluex&#58;latest

Jun 20 09&#58;42&#58;03 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; usr/lib64/libthread_db.so&#58; Cannot unlink&#58; Permission denied
Jun 20 09&#58;42&#58;03 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; usr/lib64/librt.so&#58; Cannot unlink&#58; Permission denied
Jun 20 09&#58;42&#58;03 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; usr/lib64/libresolv.so&#58; Cannot unlink&#58; Permission denied
... [a bunch more similar deleted]...
Jun 20 09&#58;42&#58;03 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; Exiting with failure status due to previous errors
...
Jun 20 09&#58;42&#58;20 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; tmp&#58; Directory renamed before its status could be extracted
Jun 20 09&#58;42&#58;20 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; run&#58; Directory renamed before its status could be extracted
Jun 20 09&#58;42&#58;20 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; etc/pki/ca-trust/source/blacklist&#58; Directory renamed before its status could be extracted
Jun 20 09&#58;42&#58;20 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; etc/pki/ca-trust/source/anchors&#58; Directory renamed before its status could be extracted
Jun 20 09&#58;42&#58;20 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; tar&#58; Exiting with failure status due to previous errors

However they don&#39;t seem to be fatal because later it says that the repo is up to date&#58;
Jun 20 09&#58;52&#58;38 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; Docker image to publish&#58; rjones30/gluex&#58;latest
Jun 20 09&#58;52&#58;38 hcc-cvmfs-repo.unl.edu cvmfs-singularity-sync&#58; Image is already latest revision.</pre></div><div class='update_description'><i onclick="document.location='34009#1497971168'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-20T15:06:08+00:00">Jun 20, 2017 03:06 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497971168">&nbsp;</a></div><pre>The whole repository publishing system had a problem; the repository said &#34;Transport endpoint is not connected&#34; which usually indicates that the cvmfs process crashed for some reason.  I just did &#34;cvmfs_server transaction singularity.opensciencegrid.org&#34; and &#34;cvmfs_server abort singularity.opensciencegrid.org&#34; and that seems to have cleaned it up.  We&#39;ll see if the problem reappears.

Three other repositories on the same machine had the same problem&#58; uboone, mu2e, and ligo osgstorage.org.  I cleaned them up in the same way.</pre></div><div class='update_description'><i onclick="document.location='34009#1497970635'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-20T14:57:15+00:00">Jun 20, 2017 02:57 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1497970635">&nbsp;</a></div><pre>Hi Richard,

That was an issue on our end.  I have restarted the syncing.  It should be updated now, give or take 30 minutes for the updates to propagate to the caches.</pre></div><div class='update_description'><i onclick="document.location='34009#1497966966'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-20T13:56:06+00:00">Jun 20, 2017 01:56 PM UTC</time><a class="anchor" name="1497966966">&nbsp;</a></div><pre>Yes, I can do that. Right now I am stuck with my development because my singularity image on singularity.opensciencegrid.org has stopped updating. It would greatly help in my debugging if that were enabled again, so that changes I make to my base image show up on the cvmfs singularity area. What I intend to do is to set up a parallel image based on Centos 6 and see if that shows the same hanging behavior when running on top of a Centos 6 kernel as the Centos 7 image does. If I did something to screw up the singularity image refreshing scheme you created, I apologize. Let me know how to rectify it.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497477183'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-14T21:53:03+00:00">Jun 14, 2017 09:53 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497477183">&nbsp;</a></div><pre>Can you enable debugging messages in hddm to see if it is getting stuck there?

It&#39;s also odd that this can&#39;t be reproduced in a singularity container on a regular el6 machine, when it is so reproducible on the grid machines.

By the way, cvmfs is not intended for distributing source code that doesn&#39;t get read by jobs on worker nodes, so if you can do without distributing it in cvmfs, please do.  It tends to use up stratum 0 and stratum 1 resources and inflate catalog sizes, without getting used by things that make efficient use of cvmfs.</pre></div><div class='update_description'><i onclick="document.location='34009#1497474136'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-14T21:02:16+00:00">Jun 14, 2017 09:02 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1497474136">&nbsp;</a></div><pre>Dave,

Yes, it is correct that we have never seen this hang problem, before I ran
into it under singularity on these particular osg sites. Thanks for the
info about the Centos6 sites that do not reproduce the hang, very
interesting. One other bit of interesting bg info that may or may not be
relevant is that we have seen hd_root crash when running with the sqlite
file resident on a network lustre filesystem. We had to copy it over to a
local filesystem to make it work. In the case of lustre, the symptom was a
&#34;disk i/o error&#34; not a hang, so it is probably unrelated.

Your observation that the first thing that happens after the code gets
successfully past the sticking point is that it opens a dana_rest.hddm
output file. This is a completely different part of the code, unrelated to
the sqlite db input. The hddm library was written by me, so I know its
internals very well. It does multithreading using pthreads, using
pthread_mutex objects for interlocks. Could this code be dead-locking at
output, due to some race condition that I never imagined possible when I
wrote the code??? It sounds like this might be it, although why we have
never seen it outside containers in the millions of hours we have run on
the Jlab batch farm is a bit surprising.

The source code for hd_root is located in /cvmfs/
oasis.opensciencegrid.org/gluex/sim-recon/master/src/programs/Analysis/hd_root
with most of the meat of the code residing in library classes found under
/cvmfs/oasis.opensciencegrid.org/gluex/sim-recon/master/src/libraries.
There is a lot of code in there, but you can probably quickly find what you
want. Classes that make up the framework are under /cvmfs/
oasis.opensciencegrid.org/gluex/jana/0.7.5p2/src, and for the ccdb
part, /cvmfs/oasis.opensciencegrid.org/gluex/ccdb/1.06.02, it&#39;s all in
there if you have the time to poke around and find the relevant bits. The
source code for the hddm library is in /cvmfs/
oasis.opensciencegrid.org/gluex/sim-recon/master/src/.Linux_CentOS7-x86_64-gcc4.8.5/libraries/HDDM/
in source file hddm_r++.cpp (with header file hddm_r.hpp).

<div id='show_1697879266' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1697879266'>-Richard J.

On Wed, Jun 14, 2017 at 4&#58;09 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_1697879266').click(function() {
            $('#detail_1697879266').slideDown("normal");
            $('#show_1697879266').hide();
            $('#hide_1697879266').show();
        });
        $('#hide_1697879266').click(function() {
            $('#detail_1697879266').slideUp();
            $('#hide_1697879266').hide();
            $('#show_1697879266').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34009#1497473519'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-14T20:51:59+00:00">Jun 14, 2017 08:51 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497473519">&nbsp;</a></div><pre>Ah, the mwt2.org el6 systems are running a 3.18 kernel, 3.18.44-1.ultra.el6.x86_64.  So it is the kernel version that makes the difference.  The el7 systems (not all in unl.edu) run a Redhat 3.10 kernel which includes a number of backports from later kernel releases.</pre></div><div class='update_description'><i onclick="document.location='34009#1497470997'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-14T20:09:57+00:00">Jun 14, 2017 08:09 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497470997">&nbsp;</a></div><pre>Thanks, that helps.

I&#39;m consistently reproducing your results now, although I haven&#39;t made any progress getting to the root cause.

It&#39;s not an el6-only issue, since the jobs do not deadlock on mwt2.org el6 machines, in addition to the unl.edu el7 machines.

As you said, the code gets stuck after reading a bunch of data from the sqlite file in /tmp, doing fcntl(...F_SETLK...), and futex().

I haven&#39;t yet been able to determine any significant difference between the machines that get stuck and the ones that don&#39;t.

On machines that don&#39;t get stuck, after the corresponding read() and fcntl() call there are even more mprotect() calls followed by open(&#34;dana_rest.hddm&#34;...) instead of a call to futex().   Those that hang never get to open dana_rest.hddm.

Can you point me to the source code of hd_root?  I am assuming you&#39;ve never seen a hang problem except under singularity.</pre></div><div class='update_description'><i onclick="document.location='34009#1497400512'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-14T00:35:12+00:00">Jun 14, 2017 12:35 AM UTC</time> by <b>GLUEX</b><a class="anchor" name="1497400512">&nbsp;</a></div><pre>Dave,

It is too much to strace all of the processes in the chain. I suggest that
you not strace from osg-container.sh but go into mygridjob.py and find the
first occurrence of the command &#34;hd_root&#34; and place &#34;strace -f&#34; in front of
it. Even just stracing this one process produces a stderr log of 400MB for
jobs that run correctly, and multiplies the run time by some factor ~2. But
still by the time 10 minutes has elapsed you should know which jobs are
going to finish and which have stuck.

-Richard J.

On Tue, Jun 13, 2017 at 6&#58;31 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1497393085'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-13T22:31:25+00:00">Jun 13, 2017 10:31 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497393085">&nbsp;</a></div><pre>Richard,

I have been able to submit jobs to try this out today.  The first time I tried it, some jobs did run for a long time until I removed them.   The ones that succeeded were mostly el7, but there were two successful el6 jobs from mwt2.org.

Since then I have been trying it with strace inside osg-container.sh (the branch that does not call singularity), running the user code in the background but waiting for a fixed period of time before killing the background process.  I have had it up to 5 minutes and I don&#39;t yet see any significant difference between el6 & el7 machines. Some of them have many calls to nanosleep for 5 seconds, but about an equal number have no calls to nanosleep and both cases are mixed between el6 & el7.  I&#39;m trying it for 10 minutes now but will have to look at the results tomorrow because I am out of time for today.

Dave</pre></div><div class='update_description'><i onclick="document.location='34009#1497307690'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-12T22:48:10+00:00">Jun 12, 2017 10:48 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1497307690">&nbsp;</a></div><pre>Dave,

*&#62;&#62; Finally, I&#39;ve been going through the steps of getting an account on OSG
connect so I can submit the job myself just as you are. That process isn&#39;t
yet completed, so I&#39;ll try to continue with it tomorrow.*

Either that, or I could just add you to the Gluex vo. Let me know if that
would be better.

-Richard Jones

On Mon, Jun 12, 2017 at 6&#58;27 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1497306448'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-12T22:27:28+00:00">Jun 12, 2017 10:27 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497306448">&nbsp;</a></div><pre>Richard,

I did spend time on this today but haven&#39;t gotten very far yet.

First, I confirmed that it did not fail when running on my local host.

Next, I had a hypothesis that it might have been caused by singularity being run with /tmp mapped to a directory on an nfs filesystem.  Sqlite uses locking which has known issues on nfs filesystems.  However, I chatted with Mats about it, and it looks like the way OSG is using singularity it should be mapping the container&#39;s /tmp directory to /tmp/.singularity-session-XXXXX so that doesn&#39;t explain it.

Finally, I&#39;ve been going through the steps of getting an account on OSG connect so I can submit the job myself just as you are.  That process isn&#39;t yet completed, so I&#39;ll try to continue with it tomorrow.

Dave</pre></div><div class='update_description'><i onclick="document.location='34009#1497282356'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-12T15:45:56+00:00">Jun 12, 2017 03:45 PM UTC</time><a class="anchor" name="1497282356">&nbsp;</a></div><pre>Dave,

I attached it to the original email I sent to you, but it got stripped when the email got registered to the ticket. Here are (what I think are) complete instructions to reproduce the problem.

1) To run the job locally, do the following on a node with /cvmfs available
$ wget <a href='http&#58;//zeus.phys.uconn.edu/halld/gridwork/mygridjob.tar.gz' target='_blank' rel='nofollow'>http&#58;//zeus.phys.uconn.edu/halld/gridwork/mygridjob.tar.gz</a>
$ mkdir mygridjob_workdir
$ cd mygridjob_workdir
$ tar zxf ../mygridjob.tar.gz
$ bash osg-container.sh ./mygridjob.py doslice 0 0

2) To submit the job to osg, do the following
$ wget <a href='http&#58;//zeus.phys.uconn.edu/halld/gridwork/mygridjob.tar.gz' target='_blank' rel='nofollow'>http&#58;//zeus.phys.uconn.edu/halld/gridwork/mygridjob.tar.gz</a>
$ mkdir mygridjob_workdir
$ cd mygridjob_workdir
$ tar zxf ../mygridjob.tar.gz
$ ./mygridjob.py submit
$ condor_submit mygridjob.logs/mygridjob.sub0

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497280494'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-12T15:14:54+00:00">Jun 12, 2017 03:14 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497280494">&nbsp;</a></div><pre>Richard,

I&#39;m sorry, I don&#39;t see it.   Which file is it in?

Can you please give instructions for completely reproducing the problem?  At a minimum, where do I find mygridjob.py?

Dave</pre></div><div class='update_description'><i onclick="document.location='34009#1497136449'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T23:14:09+00:00">Jun 10, 2017 11:14 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1497136449">&nbsp;</a></div><pre>Dave,

Yes, I added that to my submit file after you suggested it. The modified
submit file is attached. That last clause in the Requirements was a vain
attempt I made a minute ago to force these jobs to land at UConn. It didn&#39;t
work, but I only added it a couple of minutes ago. The rest of the file,
without the &#34;uconn.edu&#34; clause is what I used for testing.

-Richard

On Sat, Jun 10, 2017 at 7&#58;06 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1497136017'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T23:06:57+00:00">Jun 10, 2017 11:06 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497136017">&nbsp;</a></div><pre>It seems like you may have found a new bug with singularity under el6.

Can you tell us how to reproduce it completely, that is how to run your application?

I don&#39;t see &#34;HAS_CVMFS_oasis_opensciencegrid_org == True&#34; in your job submission file, have you tried that?  The place it hangs doesn&#39;t appear to be related however.</pre></div><div class='update_description'><i onclick="document.location='34009#1497125076'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T20:04:36+00:00">Jun 10, 2017 08:04 PM UTC</time><a class="anchor" name="1497125076">&nbsp;</a></div><pre>I agree. Disabling autoload and starting the container myself does not fix the problem I am seeing. Once I hammered out the right set of options for starting my container, I got back to exactly the same situation as I had back when I was doing autoload. So I am back to doing all testing with autoload enabled. All results reported below are obtained when running with container /cvmfs/singularity.opensciencegrid.org/rjones30/gluex&#58;latest autoloaded on the client (see attachment .sub0)

Here is a summary of what I see.

1. My jobs ALWAYS seem to work correctly on osg Centos 7 nodes (identified inside the container with uname returning a 3.x kernel) -- see below for listing
2. My jobs ALWAYS seem to hang on osg Centos 6 nodes (identified inside the container with uname returning a 2.6 kernel) -- see below for listing
3. They either complete in 3-4 minutes (iff 3.x kernel) or hang forever (iff 2.6 kernel)
4. When they hang, they always hang in exactly the same place, in exactly the same way -- see below for description
5. The production code was tested on Centos 6 nodes, and I have never seen this problem before
6. Running the exact same scripts in the same container (on /cvmfs/singularity.opensciencegrid.org/rjones30/gluex&#58;latest) on my Centos 6 nodes at home does NOT hang
7. If I disable autoload and start the container myself on the osg workers using a wrapper bash script (osg-container.sh attached) I get the same result&#58; hangs on osg Centos 6 workers, runs to completion on osg Centos 7 hosts
(see attachments above for my wrapper script, that detects whether it start up inside my container, and if not, starts the container with what I think is a standard set of options.)

*** place in simulation program where the hang occurs (followed by strace)
In the reconstruction program, during initialization from the calibrations data base (local sqlite file copied to /tmp before using, copy completes without errors)
---------------------snipped from long section of read + lseek, accessing sqlite records in input file--------------------------------------
[pid   165] read(8, &#34;&#92;0&#92;1&#92;352&#92;26237343e-05|5.80536e-05|4.4360&#34;..., 1024) = 1024
[pid   165] lseek(8, 128631808, SEEK_SET) = 128631808
[pid   165] read(8, &#34;&#92;0&#92;1&#92;352&#92;263e-05|3.49894e-05|3.8943e-05|&#34;..., 1024) = 1024
[pid   165] lseek(8, 128632832, SEEK_SET) = 128632832
[pid   165] read(8, &#34;&#92;0&#92;0&#92;0&#92;000465e-05|2.41583e-05|4.90227e&#34;..., 1024) = 1024
[pid   165] lseek(8, 128611328, SEEK_SET) = 128611328
[pid   165] read(8, &#34;&#92;r&#92;0&#92;0&#92;0&#92;1&#92;0&#92;203&#92;0&#92;0&#92;203&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#34;..., 1024) = 1024
[pid   165] lseek(8, 128610304, SEEK_SET) = 128610304
[pid   165] read(8, &#34;&#92;r&#92;0&#92;0&#92;0&#92;2&#92;1&#39;&#92;0&#92;1&#39;&#92;3&#92;306&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#92;0&#34;..., 1024) = 1024
[pid   165] fcntl(8, F_SETLK, {type=F_UNLCK, whence=SEEK_SET, start=0, len=0}) = 0
[pid   165] mprotect(0x2b18c0fc1000, 4096, PROT_READ|PROT_WRITE) = 0
[pid   165] mprotect(0x2b18c0fc2000, 4096, PROT_READ|PROT_WRITE) = 0
[pid   165] mprotect(0x2b18c0fc3000, 4096, PROT_READ|PROT_WRITE) = 0
-----------------section cut out here, first argument of mprotect counts up by 0x1000--------------------
[pid   165] mprotect(0x2b18c1029000, 4096, PROT_READ|PROT_WRITE) = 0
[pid   165] mprotect(0x2b18c102a000, 4096, PROT_READ|PROT_WRITE) = 0
[pid   165] mprotect(0x2b18c102b000, 4096, PROT_READ|PROT_WRITE) = 0
[pid   165] futex(0x7fff52272bb8, FUTEX_WAIT_PRIVATE, 2, NULL &#60;unfinished ...&#62;                                  &#60;------------&#60;&#60;   I believe this line is where the deadlock occurs
[pid   155] &#60;... nanosleep resumed&#62; 0x7fff52272570) = 0
<div id='show_1451070046' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1451070046'>[pid   155] nanosleep({0, 500000000}, 0x7fff52272570) = 0
[pid   155] nanosleep({0, 500000000}, 0x7fff52272570) = 0
[pid   155] nanosleep({0, 500000000}, 0x7fff52272570) = 0
--------------this line repeats forever, waking up to print every 0.5 seconds-------------------

*** examples of osg workers where my jobs complete
1) Linux ligo-667629.0-red-c2313.unl.edu 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15&#58;04&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
2) Linux ligo-477905.0-red-c2203.unl.edu 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15&#58;04&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
3) Linux ligo-667629.0-red-c2313.unl.edu 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15&#58;04&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
4) Linux ligo-477905.0-red-c2203.unl.edu 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15&#58;04&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

*** examples of osg workers where my jobs hang forever
1) Linux lnxfarm267.colorado.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Tue Jan 10 11&#58;22&#58;50 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
2) Linux sdsc-43.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
3) Linux sdsc-42.t2.ucsd.edu 2.6.32-642.15.1.el6.x86_64 #1 SMP Fri Feb 24 14&#58;31&#58;22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
4) Linux cabinet-5-5-8.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
5) Linux sdsc-15.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
6) Linux node044.local 2.6.32-696.1.1.el6.x86_64 #1 SMP Tue Apr 11 17&#58;13&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
7) Linux node092.local 2.6.32-696.1.1.el6.x86_64 #1 SMP Tue Apr 11 17&#58;13&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
8) Linux node087.local 2.6.32-696.1.1.el6.x86_64 #1 SMP Tue Apr 11 17&#58;13&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
9) Linux g31n58.hep.wisc.edu 2.6.32-642.11.1.el6.x86_64 #1 SMP Tue Nov 15 14&#58;13&#58;21 CST 2016 x86_64 x86_64 x86_64 GNU/Linux
10) Linux cabinet-4-4-7.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
11) Linux cabinet-0-0-23.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
12) Linux cabinet-0-0-0.t2.ucsd.edu 2.6.32-642.15.1.el6.x86_64 #1 SMP Fri Feb 24 14&#58;31&#58;22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
13) Linux cabinet-4-4-25.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
14) Linux cabinet-0-0-32.t2.ucsd.edu 2.6.32-642.15.1.el6.x86_64 #1 SMP Fri Feb 24 14&#58;31&#58;22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
15) Linux cabinet-4-4-19.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
16) Linux lnxfarm287.colorado.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Tue Jan 10 11&#58;22&#58;50 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
17) Linux cabinet-5-5-10.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
18) Linux cabinet-0-0-36.t2.ucsd.edu 2.6.32-642.15.1.el6.x86_64 #1 SMP Fri Feb 24 14&#58;31&#58;22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
19) Linux lnxfarm243.colorado.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Tue Jan 10 11&#58;22&#58;50 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
20) Linux taub116 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
21) Linux sdsc-39.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
22) Linux node072.local 2.6.32-696.1.1.el6.x86_64 #1 SMP Tue Apr 11 17&#58;13&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
23) Linux umiss127.hep.olemiss.edu 2.6.32-696.3.1.el6.x86_64 #1 SMP Tue May 30 12&#58;42&#58;34 CDT 2017 x86_64 x86_64 x86_64 GNU/Linux
24) Linux cabinet-2-2-15.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
25) Linux cabinet-0-0-18.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
26) Linux cabinet-0-0-26.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
27) Linux cabinet-7-7-12.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
28) Linux lnxfarm211.colorado.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Tue Jan 10 11&#58;22&#58;50 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
29) Linux lnxfarm242.colorado.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Tue Jan 10 11&#58;22&#58;50 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
30) Linux sdsc-46.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
31) Linux cabinet-0-0-17.t2.ucsd.edu 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
32) Linux lnxfarm144.colorado.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Tue Jan 10 11&#58;22&#58;50 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
33) Linux lnxfarm243.colorado.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Tue Jan 10 11&#58;22&#58;50 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
34) Linux taub113 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
35) Linux taub118 2.6.32-642.6.2.el6.x86_64 #1 SMP Wed Oct 26 06&#58;52&#58;09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
36) Linux node027.local 2.6.32-696.1.1.el6.x86_64 #1 SMP Tue Apr 11 17&#58;13&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

*** for reference the same report for example Centos nodes at UConn where it does not hang (run locally, not through gwms)
1) Linux gluex.phys.uconn.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Wed Jan 11 20&#58;56&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
2) Linux gluey.phys.uconn.edu 2.6.32-642.13.1.el6.x86_64 #1 SMP Wed Jan 11 20&#58;56&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
3) Linux gluskap.phys.uconn.edu 2.6.32-696.1.1.el6.x86_64 #1 SMP Tue Apr 11 17&#58;13&#58;24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
4) Linux gandalf.phys.uconn.edu 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16&#58;42&#58;41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594
</div><script type='text/javascript'>
        $('#show_1451070046').click(function() {
            $('#detail_1451070046').slideDown("normal");
            $('#show_1451070046').hide();
            $('#hide_1451070046').show();
        });
        $('#hide_1451070046').click(function() {
            $('#detail_1451070046').slideUp();
            $('#hide_1451070046').hide();
            $('#show_1451070046').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34009#1497082497'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T08:14:57+00:00">Jun 10, 2017 08:14 AM UTC</time><a class="anchor" name="1497082497">&nbsp;</a></div><pre>Hi Richard,

My $0.02&#58; there&#39;s a significant number of workarounds and logic that we have deployed as a part of the auto-loading feature.  I&#39;m quite worried that starting with it disabled will result in slow rediscovery of several screwy bugs (as we have been doing for most of the ticket) from subtle interactions with older kernels.

Looking at your submit files, Mats is on the right track.  You want to add a HTCondor submit file requirement that OASIS is present and functioning.  I think that, if you start there, the auto-loading will do the heavy lifting for you.  If you don&#39;t want to add this manually to your files, then you can utilize the APPEND_REQ_VANILLA config file parameter to have this done automatically for your users.

Brian

by /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=bbockelm/CN=659869/CN=Brian Paul Bockelman</pre></div><div class='update_description'><i onclick="document.location='34009#1497064695'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T03:18:15+00:00">Jun 10, 2017 03:18 AM UTC</time><a class="anchor" name="1497064695">&nbsp;</a></div><pre>Mats,
Poking around in the mailing lists, I found an exchange between you and Dave, where you described an undocumented feature in singularity that allows the colon syntax for the --home argument, to give the inside and outside paths for the homedir mount. This may be part of what I am running into&#58; nobody user complaining that Could not identify basedir for home directory path&#58; /
Will try this and let you know how it goes. Right now, I just want to get things working without autoload, and then once that is good, see if I can go back and figure out if the problems I was seeing with autoload were with my scripts or with inconsistencies in the container environment on different sites. It is very difficult to debug in autoload, because one is stuck inside the container and cannot see how the binds are being made when the container is started.
-Richard

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497063717'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T03:01:57+00:00">Jun 10, 2017 03:01 AM UTC</time><a class="anchor" name="1497063717">&nbsp;</a></div><pre>-adding 2 attachments

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497063549'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T02:59:09+00:00">Jun 10, 2017 02:59 AM UTC</time> by <b>GLUEX</b><a class="anchor" name="1497063549">&nbsp;</a></div><pre>Hello Mats,

I am attaching two submit files, the first with singularity autoload, and
the second with autoload disabled.

-Richard

On Fri, Jun 9, 2017 at 9&#58;12 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1497057148'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T01:12:28+00:00">Jun 10, 2017 01:12 AM UTC</time><a class="anchor" name="1497057148">&nbsp;</a></div><pre>That directory should always be available. What are your job requirements? Try adding&#58;

HAS_CVMFS_oasis_opensciencegrid_org == True

Using that and the autoloading of the image should work fine. If not, can you show me your submit file?

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Mats Rynge 45</pre></div><div class='update_description'><i onclick="document.location='34009#1497056529'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-10T01:02:09+00:00">Jun 10, 2017 01:02 AM UTC</time> by <b>GLUEX</b><a class="anchor" name="1497056529">&nbsp;</a></div><pre>It is /cvmfs/oasis.opensciencegrid.org/gluex
-Richard

On Fri, Jun 9, 2017 at 7&#58;19 PM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1497050359'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T23:19:19+00:00">Jun 9, 2017 11:19 PM UTC</time><a class="anchor" name="1497050359">&nbsp;</a></div><pre>Richard,

Which /cvmfs repo is it that you are seeing being unmounted? We have a list of mounts we keep alive for users.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Mats Rynge 45</pre></div><div class='update_description'><i onclick="document.location='34009#1497042321'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T21:05:21+00:00">Jun 9, 2017 09:05 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497042321">&nbsp;</a></div><pre>The JIRA tickets are <a href='https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-2769' target='_blank' rel='nofollow'>https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-2769</a> and <a href='https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-2770.' target='_blank' rel='nofollow'>https&#58;//jira.opensciencegrid.org/browse/SOFTWARE-2770.</a>  Maybe they&#39;ll be processed quickly.</pre></div><div class='update_description'><i onclick="document.location='34009#1497040607'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T20:36:47+00:00">Jun 9, 2017 08:36 PM UTC</time><a class="anchor" name="1497040607">&nbsp;</a></div><pre>Yes, I have confirmed your diagnosis&#58; Centos 6 nodes cannot expose /cvmfs inside my container using the +SingularityBindCVMFS = True option in my submit file. Centos 7 nodes can. If I were able to tell it that I only want to see /cvmfs/oasis.opensciencegrid.org inside my container, it would work in both Centos6 and Centos7. There seems to be no option of this kind at the moment.

This leaves me with only one option that I can think of&#58; load everything my jobs might ever need from cvmfs into my container. If everything is inside, there is no need for /cvmfs. Of course this is going to make for a large image, in excess of 6GB.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497040053'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T20:27:33+00:00">Jun 9, 2017 08:27 PM UTC</time><a class="anchor" name="1497040053">&nbsp;</a></div><pre>I cannot invoke singularity myself, at least on any of the nodes that are running my jobs at present. The command &#34;singularity&#34; results in &#34;command not found&#34;, even though I have HAS_SINGULARITY in my job Requirements. I think this is interpreted to mean that &#34;jobs can start up inside a container on this node&#34;, not that &#34;you can start your own container on this node&#34;. It seems the method proposed by Derek on his blog does not work.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497039907'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T20:25:07+00:00">Jun 9, 2017 08:25 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497039907">&nbsp;</a></div><pre>Well I&#39;m not completely sure that this el6 mounting problem is your problem, but if it is, one thing you can do is stick with el7 worker nodes.  Otherwise I guess for now you can invoke singularity yourself, although in the future I expect that will change.

The singularity options that I recommend are

HERE=&#96;pwd&#96;
cd /cvmfs/singularity.opensciencegrid.org/rjones30/gluex&#58;latest
singularity exec --containall --bind /cvmfs --home $HERE&#58;/srv --pwd /srv . yourcommand

where yourcommand is your own script.  The reason for the cd is to prevent /cvmfs/singularity.opensciencegrid.org from being unmounted by the automounter, which is a problem with singularity-2.2.1.  ...

Uh oh, I just discovered that other repositories have the same problem&#58; that is, if you reference them outside of singularity to mount them, then only reference them inside of singularity, they can still get unmounted.   They actually keep working then inside of that singularity shell, but if another one comes along it cannot mount that repository again.  This is a problem only on el6 (although the ability to unmount the original image directory is on both el6 and el7).  The only thing I can think to solve this is to have the process outside of singularity not only do a one-time access of the repositories that need to be mounted, they need to hold on to a file descriptor in those repositories until singularity is finished.  I&#39;ll make a JIRA ticket about this.</pre></div><div class='update_description'><i onclick="document.location='34009#1497037833'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T19:50:33+00:00">Jun 9, 2017 07:50 PM UTC</time><a class="anchor" name="1497037833">&nbsp;</a></div><pre>What would you recommend that would get me going?

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497037742'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T19:49:02+00:00">Jun 9, 2017 07:49 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497037742">&nbsp;</a></div><pre>Yes that would download every time from docker hub, you don&#39;t want to do that in every job.

I found Derek&#39;s blog post.  I don&#39;t much like the fact that the OSG pilot provides an option to run singularity yourself, because at some point I think Singularity should be always be run for everyone.  Otherwise it defeats the purpose of isolation if users can specify code to run before singularity.

I think what there should be is a job submit option to list the cvmfs repositories that need to be mounted before running singularity.  oasis probably ought to be one of the ones that is accessed by default, if it isn&#39;t already.</pre></div><div class='update_description'><i onclick="document.location='34009#1497034963'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T19:02:43+00:00">Jun 9, 2017 07:02 PM UTC</time><a class="anchor" name="1497034963">&nbsp;</a></div><pre>Ok, so autoloading the singularity image is not working for the moment, if you need /cvmfs to be visible inside. I can work around this.
Are there instructions for building a singularity image based on what is present in /cvmfs/singularity.opensciencegrid.org ?
The instructions on Derek Weitzel&#39;s blog say to do the following to start a singularity image from within my script, eg.

singularity exec --bind &#96;pwd&#96;&#58;/srv  --pwd /srv docker&#58;//python&#58;latest python -V

But isn&#39;t this pulling the image from docker hub separately on every worker?
Is there a way to build it locally from what is already cached in /cvmfs/singularity.opensciencegrid.org ?

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1497034452'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T18:54:12+00:00">Jun 9, 2017 06:54 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1497034452">&nbsp;</a></div><pre>Richard,

It is a known problem on EL6 that it doesn&#39;t work to automount a cvmfs repository from inside of a singularity container; you have to try to access it first outside of the container.  That might be the issue.

Dave</pre></div><div class='update_description'><i onclick="document.location='34009#1497034040'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-09T18:47:20+00:00">Jun 9, 2017 06:47 PM UTC</time><a class="anchor" name="1497034040">&nbsp;</a></div><pre>I now have my docker container correctly propagating onto the /cvmfs/singularity.opensciencegrid.org/ area, and some of my jobs are running as expected, inside the container with /cvmfs mounted inside. However, some of the jobs start up without /cvmfs mounted. Can you check what is going wrong? I have attached a copy of my submit file.

Example of a node where the job runs correctly, where /cvmfs/oasis.opensciencegrid.org/gluex is showing up as expected&#58;
ligo-496773.0-c5419.red.hcc.unl.edu

Example of a node where the job starts up inside a container, but /cvmfs/oasis.opensciencegrid.org/gluex is not visible
cabinet-0-0-19.t2.ucsd.edu

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1496902903'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-08T06:21:43+00:00">Jun 8, 2017 06:21 AM UTC</time><a class="anchor" name="1496902903">&nbsp;</a></div><pre>Brian,

I just submitted a PR to opensciencegrid/cvmfs-singularity-sync for an updated docker_images.txt with a new entry for Gluex. The source is docker.io/rjones30/gluex which is automatically generated from the Dockerfile in rjones30/Gluexprod on github. As soon as this image shows up on /cvmfs/singularity.opensciencegrid.org can you let me know?

thankyou, -Richard J.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1496784353'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-06T21:25:53+00:00">Jun 6, 2017 09:25 PM UTC</time><a class="anchor" name="1496784353">&nbsp;</a></div><pre>I agree 7GB is on the larger side, but not unreasonable.  It&#39;s fairly close to the LIGO image in size, actually.

If you could post an image on DockerHub, we can sync to CVMFS.  The currently-supported version of Singularity (2.2.x) allows user code to run as root when building images - something we haven&#39;t been able to support.  This is another reason why we request pushing things through Docker.

If possible, I&#39;d request a build via Dockerfiles (instead of &#96;docker push&#96;) in order to help us understand the contents of the image.

Thanks Richard!  This all looks like a good direction!

by /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=bbockelm/CN=659869/CN=Brian Paul Bockelman</pre></div><div class='update_description'><i onclick="document.location='34009#1496778749'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-06T19:52:29+00:00">Jun 6, 2017 07:52 PM UTC</time><a class="anchor" name="1496778749">&nbsp;</a></div><pre>ps. Yes, I have a script that makes it. I give the script a versions.xml file of the release numbers of all packages, and it spits out a singularity container with everything built, loaded and ready to run.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1496778462'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-06T19:47:42+00:00">Jun 6, 2017 07:47 PM UTC</time><a class="anchor" name="1496778462">&nbsp;</a></div><pre>GlueX has a single collection of C++ libraries and programs used for Monte Carlo generation, simulation, reconstruction, and analysis. These packages have dependencies other standard libraries, eg ROOT, Geant4, cernlib, boost, xerces, and a number of Jefferson Lab shared packages for raw event decoding (evio), multitheaded analysis framework (jana), run conditions db (rcdb), calibration constants db(ccdb), and the like. These in turn have dependencies on many system-installed packages. Here is a breakdown of the total size of my container.

* amptools - 8.1M
* cernlib - 130M
* root - 933M
* geant4 - 1.5G
* ccdb - 102M
* rcdb - 31M
* sim_recon - 1.3G
* gluex_root_analysis - 3M
* xerces-c - 418M
* hdds - 40M
* hd_utilities - 1M
* jana - 461M
* evio - 9M
* hdgeant4 - 122M
* [remaining OS installed footprint in container] - 500MB

All of this adds up to 6.3GB, which I rounded up to 7GB in my earlier communication. But that is the breakdown. This footprint is negligible compared with the volume of data that might be generated based on this image on any given site during a production run. It is roughly the same size as our aggregate occupation on oasis during the last data challenge. If I wanted to, I could compress it into a still smaller space by, eg. stripping symbols out of the libraries, wiping the sources, etc, but there are conveniences to having these present in the image and I cannot think of a context in which 6.3GB would be considered &#34;large&#34;.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1496776450'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-06T19:14:10+00:00">Jun 6, 2017 07:14 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1496776450">&nbsp;</a></div><pre>Yes, we only support downloading docker containers.  We do this for many reasons, but one of them is that the tools surrounding docker containers are much, much better than those around singularity images.  Most of that is related to how widespread docker is used.

I agree singularity is what you want to use, but docker is just the container format, and a well understood one at that.

7GB sounds like a really huge image.  I curious how you made it?  Do you have a script that makes it?

-Derek

<font color='#7F7E6F'>&#62; On Jun 6, 2017, at 1&#58;05 PM, Open Science Grid FootPrints &#60;osg@....&#62; wrote&#58;</font>
<font color='#7F7E6F'>&#62;</font>
<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1496775911'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-06T19:05:11+00:00">Jun 6, 2017 07:05 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1496775911">&nbsp;</a></div><pre>Derek,

Thank you for this. I am more comfortable with singularity than I am with
docker, since it seems to better match what I am trying to do with my grid
work. I currently have everything built inside a large singularity image
(7GB). Is there some way for me to skip the docker export/import step, and
work directly with my singularity image? Or would you prefer for me to go
the route you describe and export first to docker, then have you pull my
files back into a new singularity container?

-Richard Jones

On Mon, Jun 5, 2017 at 11&#58;00 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font></pre></div><div class='update_description'><i onclick="document.location='34009#1496674806'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-05T15:00:06+00:00">Jun 5, 2017 03:00 PM UTC</time> by <b>Derek Weitzel</b><a class="anchor" name="1496674806">&nbsp;</a></div><pre>Hi Richard,

For singularity images, we have a process to distribute the images through CVMFS.  Here is the basic workflow&#58;

1. Push your image to docker hub.  It is easiest (in our experience) to make a docker image.
2. Open a pull request on this repo&#58; <a href='https&#58;//github.com/opensciencegrid/cvmfs-singularity-sync' target='_blank' rel='nofollow'>https&#58;//github.com/opensciencegrid/cvmfs-singularity-sync</a>, changing the docker_images.txt file to list your docker image.
3. Our service will then publish the docker image in a format that is usable by singularity.  Also, it will constantly keep the image updated with whatever is published on Docker Hub.

-Derek</pre></div><div class='update_description'><i onclick="document.location='34009#1496674325'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-05T14:52:05+00:00">Jun 5, 2017 02:52 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1496674325">&nbsp;</a></div><pre>I meant &#34;much more efficiently&#34;, not &#34;much more frequently&#34;.</pre></div><div class='update_description'><i onclick="document.location='34009#1496674260'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-05T14:51:00+00:00">Jun 5, 2017 02:51 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1496674260">&nbsp;</a></div><pre>Richard,

We&#39;ve got another solution for distributing singularity images.  We unpack the images into separate files in /cvmfs/singularity.opensciencegrid.org.  The administrators of that repository configure a docker image of your choice to upload, and you can update the image in docker whenever you want.  By sharing one repository we&#39;re able to take advantage of cvmfs&#39; de-deduplication to store and cache the images much more frequently.

I add Derek Weitzel & Brian Bockelman to the Cc to get you set up there.

Most people do not put their entire software stack in the image, so please keep that in mind.  Typically the image contains things that are standard operating system packages that they can install with rpm, then the application stack is published separately in cvmfs in an application-specific area.

Dave</pre></div><div class='update_description'><i onclick="document.location='34009#1496671871'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-05T14:11:11+00:00">Jun 5, 2017 02:11 PM UTC</time> by <b>GLUEX</b><a class="anchor" name="1496671871">&nbsp;</a></div><pre>Hello Dave,

It is a singularity image. This contains all of our Gluex software stack.
We may be a bit ahead of the curve on this, but linux containers are a very
efficient way to deliver a production environment in a site-independent
way. At the recent All-Hands meetings I learned that several large VOs are
preparing to move to singularity or other linux containers for their grid
production environments.

It sounds like cvmfs is not a great way to deliver linux containers to the
worker nodes. In this case, could you kick this ticket up to a higher
level, and ask the grid production experts how linux container images are
best delivered to production sites? Here are some possibilities that come
to mind.

1. Have the first job stage the image onto $OSG_APP and then have all of
the other jobs discover and access it there. This allows us to keep
everything from a production release inside one image, which is nice from
the standpoint of reproducibility of results and record keeping, but would
require careful housekeeping to prevent $OSG_APP from quickly filling up
with old images.
2. Build the entire production environment in a large container image
(~10GB), then extract the built software trees from the container into a
host filesystem, from where they can be copied onto oasis. Then build a
minimal singularity container (~1GB) that can mount these external
directories within the container and run as if they were all inside.
3. An extension of option 2, where even the base OS components are
extracted from the container into /cvmfs directories, until all that
remains inside the image is what is needed to bootstrap the container and
mount everything.

Option 3 could potentially produce a single one-size-fits-all minimal
container for, eg. Centos 7 of size ~500MB, that would never have to change
as software development takes place.

<div id='show_1958754568' class=''><button class="btn">Show More</button></div><div class='detail hidden' id='detail_1958754568'>-Richard Jones

On Mon, Jun 5, 2017 at 9&#58;02 AM, Open Science Grid FootPrints &#60;
osg@....&#62; wrote&#58;

<font color='#7F7E6F'>&#62; [Duplicate message snipped]</font>
</div><script type='text/javascript'>
        $('#show_1958754568').click(function() {
            $('#detail_1958754568').slideDown("normal");
            $('#show_1958754568').hide();
            $('#hide_1958754568').show();
        });
        $('#hide_1958754568').click(function() {
            $('#detail_1958754568').slideUp();
            $('#hide_1958754568').hide();
            $('#show_1958754568').show();
        });
        </script></pre></div><div class='update_description'><i onclick="document.location='34009#1496667719'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-05T13:01:59+00:00">Jun 5, 2017 01:01 PM UTC</time> by <b>Dave Dykstra</b><a class="anchor" name="1496667719">&nbsp;</a></div><pre>Here&#39;s the problem in the logs&#58;

Sun Jun  4 12&#58;58&#58;01 UTC 2017 starting vo update for gluex
rsync&#58; writefd_unbuffered failed to write 4 bytes to socket [sender]&#58; Broken pip
e (32)
rsync&#58; write failed on &#34;/cvmfs/oasis.opensciencegrid.org/gluex/centos7.img&#34;&#58; No
space left on device (28)
rsync error&#58; error in file IO (code 11) at receiver.c(322) [receiver=3.0.9]
rsync&#58; connection unexpectedly closed (31 bytes received so far) [sender]
rsync error&#58; error in rsync protocol data stream (code 12) at io.c(605) [sender=
3.0.9]

You were trying to publish a 7GB file, and that&#39;s much too large of a file for cvmfs.  What are you trying to do with the image?</pre></div><div class='update_description'><i onclick="document.location='34009#1496663707'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-05T11:55:07+00:00">Jun 5, 2017 11:55 AM UTC</time> by <b>Scott Teige</b><a class="anchor" name="1496663707">&nbsp;</a></div><pre>Assigning to Oasis developer. Please let me know if the problems persist.</pre></div><div class='update_description'><i onclick="document.location='34009#1496582296'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-04T13:18:16+00:00">Jun 4, 2017 01:18 PM UTC</time><a class="anchor" name="1496582296">&nbsp;</a></div><pre>ps. I waited 12 hours before posting this ticket.

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div><div class='update_description'><i onclick="document.location='34009#1496582239'; reset_anchor();" class="pull-right icon icon-share"></i><div class="header"><time datetime="2017-06-04T13:17:19+00:00">Jun 4, 2017 01:17 PM UTC</time> by <b>OSG-GOC</b><a class="anchor" name="1496582239">&nbsp;</a></div><pre>My osg-oasis-update commands on oasis-login are being accepted, and seem to complete successfully, but my content is never showing up on the /cvmfs filesystem. Can it be that the change of my DN from digicert to cilogon has not been propagated to the authorization files somewhere? On the /stage filesystem I see the update.details file contains my new DN and the latest timestamp when I did the update, but the update.details that is visible on the /cvmfs filesystem at my site still shows the old DN and an old timestamp from the last time I did an update back in 2014.
-Richard Jones

by /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Richard T. Jones 594</pre></div>
</div>
<script type="text/javascript">
function reset_anchor() {
    $("#updates .selected").removeClass("selected");
    var urls = document.location.toString().split('#'); 
    var anchor = urls[1];
    if(anchor) {
        $("a[name='"+anchor+"']").parents(".update_description").addClass("selected");
    }
}
function submitspam(ticket_id) {
    myret = confirm("Would you like to close this ticket as a security ticket, and submit the ticket content to akismet?");
    if(myret == true) {
        $.ajax("viewer/processspam?id="+ticket_id).done(function() {
            window.location.reload();
        });
    }
}

$(function() {
    reset_anchor();
    var ADDITIONAL_COOKIE_NAME = 'gocticket';
    var options = { path: '/', expires: 365};

    if(window.opener && window.opener.name == "gocticket_list") {
        v = $.cookie("closewindow");
        if(!v) {
            $("#closewindow").attr("checked", "checked"); //on by default
        } else {
            if(v == "checked") {
                $("#closewindow").attr("checked", "checked");
            }
        }
        $("#closewindow").click(function() {
            $.cookie("closewindow", $(this).attr('checked'), options);
        });
    } else {
        $("#closewindow_area").hide();
    }
    function updateTimeago() {
        $("time").timeago();
        setTimeout(updateTimeago, 30*1000);
    }
    updateTimeago();
    $(".description").focus(expand_description);
});
</script>
<hr/>
<footer>
<p>GOC Ticket Version 2.2 | <a href="https://ticket.opensciencegrid.org/goc/submit?app_issue_check=on&amp;app_issue_type=goc&amp;app_goc_url=https%3A%2F%2Fticket.opensciencegrid.org%3A443%2F34009">Report Bugs</a>
 | <a href="https://github.com/opensciencegrid/operations/blob/master/docs/privacy.md">Privacy Policy</a>
</p>

<p> <img align="top" src="images/tag_orange.png"/> Copyright 2018 The Trustees of Indiana University - Developed for Open Science Grid</p>
</footer>


</div><!--container-fluid-->
<script>
//used by searchbox
function parseValue(value) {
    var obj = new Object();
    var tokens = value.split("\t");
    obj.str = tokens[0];
    obj.count = tokens[1];
    return obj;
}

$(function() {
    //bootstrap-2.0.4 stuff
    $(".alert-message").alert();
    $(".dropdown-toggle").dropdown();
    $("span[rel='tooltip']").tooltip();
    $("a[rel=popover]").popover();

    //activate menu that user is currently on
    $("#menu_navigator").addClass("active"); 
    $("#submenu_").addClass("active"); 

    //translate zend validation error message to bootstrap
    $(".errors").addClass("alert").addClass("alert-error");

    //enable autocomplete for search box
    $("#search").autocomplete({
        source: function( request, response ) {
            $.ajax({
                url: "search/autocomplete",
                dataType: "text",
                data: {
                    //featureClass: "P",
                    //style: "full",
                    //maxRows: 12,
                    //name_startsWith: request.term
                    q: request.term
                },
                success: function( data ) {
                    response( $.map( data.split("\n"), function( item ) {
                        if(item == "") return null;
                        return {
                            value: item
                        }
                    }));
                }
            });
        },
        select: function(event, ui) {
            document.location = "search?q="+ui.item.value;
        }
    });
    
});
</script>


</body>
