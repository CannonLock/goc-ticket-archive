09/07/17 17:57:54 ******************************************************
09/07/17 17:57:54 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/07/17 17:57:54 ** /usr/sbin/condor_gridmanager
09/07/17 17:57:54 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/07/17 17:57:54 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/07/17 17:57:54 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/07/17 17:57:54 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/07/17 17:57:54 ** PID = 4837
09/07/17 17:57:54 ** Log last touched time unavailable (No such file or directory)
09/07/17 17:57:54 ******************************************************
09/07/17 17:57:54 Using config source: /etc/condor-ce/condor_config
09/07/17 17:57:54 Using local config sources: 
09/07/17 17:57:54    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/07/17 17:57:54    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/01-ce-auth.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/01-ce-router.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/01-common-auth.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/02-ce-slurm.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/03-managed-fork.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/05-ce-health.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/05-ce-view.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/50-osg-configure.conf
09/07/17 17:57:54    /etc/condor-ce/config.d/99-local.conf
09/07/17 17:57:54    /usr/share/condor-ce/condor_ce_router_defaults|
09/07/17 17:57:54 config Macros = 176, Sorted = 176, StringBytes = 14902, TablesBytes = 6568
09/07/17 17:57:54 CLASSAD_CACHING is ENABLED
09/07/17 17:57:54 Daemon Log is logging: D_ALWAYS D_ERROR
09/07/17 17:57:54 SharedPortEndpoint: waiting for connections to named socket 4789_089c_4
09/07/17 17:57:54 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_4>
09/07/17 17:57:54 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_4>
09/07/17 17:57:57 [4837] Found job 2.0 --- inserting
09/07/17 17:57:57 [4837] gahp server not up yet, delaying ping
09/07/17 17:57:57 [4837] (2.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/07/17 17:57:57 [4837] GAHP server pid = 4839
09/07/17 17:58:02 [4837] resource  is now up
09/07/17 17:58:02 [4837] (2.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/07/17 17:58:03 [4837] (2.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/07/17 17:58:07 [4837] (2.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/07/17 17:59:07 [4837] (2.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/07/17 17:59:08 [4837] (2.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/07/17 17:59:08 [4837] (2.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/07/17 17:59:13 [4837] No jobs left, shutting down
09/07/17 17:59:13 [4837] Got SIGTERM. Performing graceful shutdown.
09/07/17 17:59:13 [4837] **** condor_gridmanager (condor_GRIDMANAGER) pid 4837 EXITING WITH STATUS 0
09/07/17 20:30:54 ******************************************************
09/07/17 20:30:54 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/07/17 20:30:54 ** /usr/sbin/condor_gridmanager
09/07/17 20:30:54 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/07/17 20:30:54 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/07/17 20:30:54 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/07/17 20:30:54 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/07/17 20:30:54 ** PID = 5887
09/07/17 20:30:54 ** Log last touched 9/7 17:59:13
09/07/17 20:30:54 ******************************************************
09/07/17 20:30:54 Using config source: /etc/condor-ce/condor_config
09/07/17 20:30:54 Using local config sources: 
09/07/17 20:30:54    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/07/17 20:30:54    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/01-ce-auth.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/01-ce-router.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/01-common-auth.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/02-ce-slurm.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/03-managed-fork.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/05-ce-health.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/05-ce-view.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/50-osg-configure.conf
09/07/17 20:30:54    /etc/condor-ce/config.d/99-local.conf
09/07/17 20:30:54    /usr/share/condor-ce/condor_ce_router_defaults|
09/07/17 20:30:54 config Macros = 176, Sorted = 176, StringBytes = 14902, TablesBytes = 6568
09/07/17 20:30:54 CLASSAD_CACHING is ENABLED
09/07/17 20:30:54 Daemon Log is logging: D_ALWAYS D_ERROR
09/07/17 20:30:54 SharedPortEndpoint: waiting for connections to named socket 4789_089c_56
09/07/17 20:30:54 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_56>
09/07/17 20:30:54 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_56>
09/07/17 20:30:57 [5887] Found job 4.0 --- inserting
09/07/17 20:30:57 [5887] gahp server not up yet, delaying ping
09/07/17 20:30:57 [5887] (4.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/07/17 20:30:57 [5887] GAHP server pid = 5890
09/07/17 20:31:02 [5887] resource  is now up
09/07/17 20:31:02 [5887] (4.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/07/17 20:31:02 [5887] (4.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/07/17 20:31:07 [5887] (4.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/07/17 20:32:07 [5887] (4.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/07/17 20:32:08 [5887] (4.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/07/17 20:32:08 [5887] (4.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/07/17 20:32:13 [5887] No jobs left, shutting down
09/07/17 20:32:13 [5887] Got SIGTERM. Performing graceful shutdown.
09/07/17 20:32:13 [5887] **** condor_gridmanager (condor_GRIDMANAGER) pid 5887 EXITING WITH STATUS 0
09/07/17 20:57:15 ******************************************************
09/07/17 20:57:15 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/07/17 20:57:15 ** /usr/sbin/condor_gridmanager
09/07/17 20:57:15 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/07/17 20:57:15 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/07/17 20:57:15 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/07/17 20:57:15 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/07/17 20:57:15 ** PID = 6322
09/07/17 20:57:15 ** Log last touched 9/7 20:32:13
09/07/17 20:57:15 ******************************************************
09/07/17 20:57:15 Using config source: /etc/condor-ce/condor_config
09/07/17 20:57:15 Using local config sources: 
09/07/17 20:57:15    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/07/17 20:57:15    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/01-ce-auth.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/01-ce-router.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/01-common-auth.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/02-ce-slurm.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/03-managed-fork.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/05-ce-health.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/05-ce-view.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/50-osg-configure.conf
09/07/17 20:57:15    /etc/condor-ce/config.d/99-local.conf
09/07/17 20:57:15    /usr/share/condor-ce/condor_ce_router_defaults|
09/07/17 20:57:15 config Macros = 176, Sorted = 176, StringBytes = 14902, TablesBytes = 6568
09/07/17 20:57:15 CLASSAD_CACHING is ENABLED
09/07/17 20:57:15 Daemon Log is logging: D_ALWAYS D_ERROR
09/07/17 20:57:15 SharedPortEndpoint: waiting for connections to named socket 4789_089c_66
09/07/17 20:57:15 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_66>
09/07/17 20:57:15 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_66>
09/07/17 20:57:18 [6322] Found job 6.0 --- inserting
09/07/17 20:57:18 [6322] gahp server not up yet, delaying ping
09/07/17 20:57:18 [6322] (6.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/07/17 20:57:18 [6322] GAHP server pid = 6324
09/07/17 20:57:23 [6322] resource  is now up
09/07/17 20:57:24 [6322] (6.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/07/17 20:57:24 [6322] (6.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/07/17 20:57:29 [6322] (6.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/07/17 20:58:29 [6322] (6.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/07/17 20:58:29 [6322] (6.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/07/17 20:58:29 [6322] (6.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/07/17 20:58:34 [6322] No jobs left, shutting down
09/07/17 20:58:34 [6322] Got SIGTERM. Performing graceful shutdown.
09/07/17 20:58:34 [6322] **** condor_gridmanager (condor_GRIDMANAGER) pid 6322 EXITING WITH STATUS 0
09/07/17 20:59:04 ******************************************************
09/07/17 20:59:04 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/07/17 20:59:04 ** /usr/sbin/condor_gridmanager
09/07/17 20:59:04 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/07/17 20:59:04 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/07/17 20:59:04 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/07/17 20:59:04 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/07/17 20:59:04 ** PID = 6419
09/07/17 20:59:04 ** Log last touched 9/7 20:58:34
09/07/17 20:59:04 ******************************************************
09/07/17 20:59:04 Using config source: /etc/condor-ce/condor_config
09/07/17 20:59:04 Using local config sources: 
09/07/17 20:59:04    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/07/17 20:59:04    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/01-ce-auth.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/01-ce-router.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/01-common-auth.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/02-ce-slurm.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/03-managed-fork.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/05-ce-health.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/05-ce-view.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/50-osg-configure.conf
09/07/17 20:59:04    /etc/condor-ce/config.d/99-local.conf
09/07/17 20:59:04    /usr/share/condor-ce/condor_ce_router_defaults|
09/07/17 20:59:04 config Macros = 176, Sorted = 176, StringBytes = 14902, TablesBytes = 6568
09/07/17 20:59:04 CLASSAD_CACHING is ENABLED
09/07/17 20:59:04 Daemon Log is logging: D_ALWAYS D_ERROR
09/07/17 20:59:04 SharedPortEndpoint: waiting for connections to named socket 4789_089c_67
09/07/17 20:59:04 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_67>
09/07/17 20:59:04 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_67>
09/07/17 20:59:07 [6419] Found job 8.0 --- inserting
09/07/17 20:59:07 [6419] gahp server not up yet, delaying ping
09/07/17 20:59:07 [6419] (8.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/07/17 20:59:07 [6419] GAHP server pid = 6422
09/07/17 20:59:12 [6419] resource  is now up
09/07/17 20:59:12 [6419] (8.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/07/17 20:59:12 [6419] (8.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/07/17 20:59:17 [6419] (8.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/07/17 21:00:17 [6419] (8.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/07/17 21:00:17 [6419] (8.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/07/17 21:01:17 [6419] (8.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/07/17 21:01:17 [6419] (8.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/07/17 21:02:17 [6419] (8.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/07/17 21:02:17 [6419] (8.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/07/17 21:03:17 [6419] (8.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/07/17 21:03:17 [6419] (8.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/07/17 21:04:17 [6419] (8.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/07/17 21:04:17 [6419] (8.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/07/17 21:05:17 [6419] (8.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/07/17 21:05:17 [6419] (8.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/07/17 21:05:17 [6419] (8.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/07/17 21:05:22 [6419] No jobs left, shutting down
09/07/17 21:05:22 [6419] Got SIGTERM. Performing graceful shutdown.
09/07/17 21:05:22 [6419] **** condor_gridmanager (condor_GRIDMANAGER) pid 6419 EXITING WITH STATUS 0
09/08/17 09:53:56 ******************************************************
09/08/17 09:53:56 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 09:53:56 ** /usr/sbin/condor_gridmanager
09/08/17 09:53:56 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 09:53:56 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 09:53:56 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 09:53:56 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 09:53:56 ** PID = 10992
09/08/17 09:53:56 ** Log last touched 9/7 21:05:22
09/08/17 09:53:56 ******************************************************
09/08/17 09:53:56 Using config source: /etc/condor-ce/condor_config
09/08/17 09:53:56 Using local config sources: 
09/08/17 09:53:56    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 09:53:56    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 09:53:56    /etc/condor-ce/config.d/99-local.conf
09/08/17 09:53:56    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 09:53:56 config Macros = 176, Sorted = 176, StringBytes = 14904, TablesBytes = 6568
09/08/17 09:53:56 CLASSAD_CACHING is ENABLED
09/08/17 09:53:56 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 09:53:56 SharedPortEndpoint: waiting for connections to named socket 4789_089c_326
09/08/17 09:53:56 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_326>
09/08/17 09:53:56 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_326>
09/08/17 09:53:59 [10992] Found job 10.0 --- inserting
09/08/17 09:53:59 [10992] gahp server not up yet, delaying ping
09/08/17 09:53:59 [10992] (10.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 09:53:59 [10992] GAHP server pid = 10995
09/08/17 09:54:04 [10992] resource  is now up
09/08/17 09:54:04 [10992] (10.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 09:54:05 [10992] (10.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 09:54:09 [10992] (10.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 09:55:09 [10992] (10.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 09:55:09 [10992] (10.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 09:56:09 [10992] (10.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 09:56:09 [10992] (10.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 09:57:09 [10992] (10.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 09:57:09 [10992] (10.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 09:58:09 [10992] (10.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 09:58:09 [10992] (10.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 09:59:09 [10992] (10.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 09:59:09 [10992] (10.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 10:00:09 [10992] (10.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 10:00:10 [10992] (10.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 10:00:10 [10992] (10.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 10:00:15 [10992] No jobs left, shutting down
09/08/17 10:00:15 [10992] Got SIGTERM. Performing graceful shutdown.
09/08/17 10:00:15 [10992] **** condor_gridmanager (condor_GRIDMANAGER) pid 10992 EXITING WITH STATUS 0
09/08/17 10:42:40 ******************************************************
09/08/17 10:42:40 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 10:42:40 ** /usr/sbin/condor_gridmanager
09/08/17 10:42:40 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 10:42:40 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 10:42:40 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 10:42:40 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 10:42:40 ** PID = 11687
09/08/17 10:42:40 ** Log last touched 9/8 10:00:15
09/08/17 10:42:40 ******************************************************
09/08/17 10:42:40 Using config source: /etc/condor-ce/condor_config
09/08/17 10:42:40 Using local config sources: 
09/08/17 10:42:40    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 10:42:40    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 10:42:40    /etc/condor-ce/config.d/99-local.conf
09/08/17 10:42:40    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 10:42:40 config Macros = 176, Sorted = 176, StringBytes = 14904, TablesBytes = 6568
09/08/17 10:42:40 CLASSAD_CACHING is ENABLED
09/08/17 10:42:40 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 10:42:40 SharedPortEndpoint: waiting for connections to named socket 4789_089c_344
09/08/17 10:42:40 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_344>
09/08/17 10:42:40 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_344>
09/08/17 10:42:43 [11687] Found job 12.0 --- inserting
09/08/17 10:42:43 [11687] gahp server not up yet, delaying ping
09/08/17 10:42:43 [11687] (12.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 10:42:43 [11687] GAHP server pid = 11689
09/08/17 10:42:48 [11687] resource  is now up
09/08/17 10:42:48 [11687] (12.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 10:42:49 [11687] (12.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 10:42:53 [11687] (12.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 10:43:53 [11687] (12.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 10:43:53 [11687] (12.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 10:43:53 [11687] (12.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 10:43:58 [11687] No jobs left, shutting down
09/08/17 10:43:58 [11687] Got SIGTERM. Performing graceful shutdown.
09/08/17 10:43:58 [11687] **** condor_gridmanager (condor_GRIDMANAGER) pid 11687 EXITING WITH STATUS 0
09/08/17 11:02:19 ******************************************************
09/08/17 11:02:19 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 11:02:19 ** /usr/sbin/condor_gridmanager
09/08/17 11:02:19 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 11:02:19 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 11:02:19 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 11:02:19 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 11:02:19 ** PID = 12091
09/08/17 11:02:19 ** Log last touched 9/8 10:43:58
09/08/17 11:02:19 ******************************************************
09/08/17 11:02:19 Using config source: /etc/condor-ce/condor_config
09/08/17 11:02:19 Using local config sources: 
09/08/17 11:02:19    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 11:02:19    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 11:02:19    /etc/condor-ce/config.d/99-local.conf
09/08/17 11:02:19    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 11:02:19 config Macros = 176, Sorted = 176, StringBytes = 14904, TablesBytes = 6568
09/08/17 11:02:19 CLASSAD_CACHING is ENABLED
09/08/17 11:02:19 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 11:02:19 SharedPortEndpoint: waiting for connections to named socket 4789_089c_351
09/08/17 11:02:19 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_351>
09/08/17 11:02:19 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_351>
09/08/17 11:02:22 [12091] Found job 14.0 --- inserting
09/08/17 11:02:22 [12091] gahp server not up yet, delaying ping
09/08/17 11:02:22 [12091] (14.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 11:02:22 [12091] GAHP server pid = 12093
09/08/17 11:02:27 [12091] resource  is now up
09/08/17 11:02:27 [12091] (14.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 11:02:27 [12091] (14.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 11:02:32 [12091] (14.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 11:03:32 [12091] (14.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 11:03:33 [12091] (14.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 11:03:33 [12091] (14.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 11:03:38 [12091] No jobs left, shutting down
09/08/17 11:03:38 [12091] Got SIGTERM. Performing graceful shutdown.
09/08/17 11:03:38 [12091] **** condor_gridmanager (condor_GRIDMANAGER) pid 12091 EXITING WITH STATUS 0
09/08/17 11:25:42 ******************************************************
09/08/17 11:25:42 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 11:25:42 ** /usr/sbin/condor_gridmanager
09/08/17 11:25:42 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 11:25:42 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 11:25:42 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 11:25:42 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 11:25:42 ** PID = 12355
09/08/17 11:25:42 ** Log last touched 9/8 11:03:38
09/08/17 11:25:42 ******************************************************
09/08/17 11:25:42 Using config source: /etc/condor-ce/condor_config
09/08/17 11:25:42 Using local config sources: 
09/08/17 11:25:42    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 11:25:42    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 11:25:42    /etc/condor-ce/config.d/99-local.conf
09/08/17 11:25:42    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 11:25:42 config Macros = 176, Sorted = 176, StringBytes = 14904, TablesBytes = 6568
09/08/17 11:25:42 CLASSAD_CACHING is ENABLED
09/08/17 11:25:42 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 11:25:42 SharedPortEndpoint: waiting for connections to named socket 4789_089c_360
09/08/17 11:25:42 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_360>
09/08/17 11:25:42 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_360>
09/08/17 11:25:45 [12355] Found job 16.0 --- inserting
09/08/17 11:25:45 [12355] gahp server not up yet, delaying ping
09/08/17 11:25:45 [12355] (16.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 11:25:45 [12355] GAHP server pid = 12357
09/08/17 11:25:50 [12355] resource  is now up
09/08/17 11:25:50 [12355] (16.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 11:25:51 [12355] (16.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 11:25:55 [12355] (16.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 11:26:55 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 11:26:55 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 11:27:55 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:27:55 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:28:55 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:28:55 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:29:55 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:29:55 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:30:55 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:30:55 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:31:55 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:31:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:32:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:32:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:33:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:33:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:34:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:34:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:35:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:35:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:36:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:36:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:37:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:37:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:38:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:38:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:39:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:39:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:40:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:40:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:41:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:41:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:42:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:42:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:43:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:43:56 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:44:56 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:44:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:45:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:45:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:46:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:46:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:47:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:47:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:48:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:48:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:49:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:49:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:50:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:50:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:51:16 [12355] Found job 18.0 --- inserting
09/08/17 11:51:16 [12355] (18.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 11:51:21 [12355] (18.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 11:51:21 [12355] (18.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 11:51:26 [12355] (18.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 11:51:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:51:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:52:26 [12355] (18.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 11:52:27 [12355] (18.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 11:52:27 [12355] (18.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 11:52:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:52:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:53:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:53:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:54:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:54:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:55:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:55:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:56:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:56:57 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:57:57 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:57:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:58:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:58:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 11:59:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 11:59:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:00:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:00:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:01:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:01:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:02:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:02:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:03:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:03:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:04:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:04:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:05:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:05:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:06:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:06:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:07:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:07:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:08:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:08:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:09:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:09:58 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:10:58 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:10:59 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:11:59 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:11:59 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:12:59 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:12:59 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:13:59 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:13:59 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:14:59 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:14:59 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:15:59 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:15:59 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:16:59 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:17:00 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:18:00 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:18:00 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:19:00 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:19:00 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:20:00 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:20:00 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:21:00 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:21:00 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:22:00 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:22:00 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:23:00 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:23:00 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:24:00 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:24:01 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:25:01 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:25:01 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:26:01 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:26:01 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:27:01 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:27:01 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:28:01 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:28:01 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:29:01 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:29:01 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:30:01 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:30:02 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:31:02 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:31:02 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:32:02 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:32:02 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:33:02 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:33:02 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:34:02 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:34:02 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:35:02 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:35:02 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:36:02 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:36:02 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:37:02 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:37:03 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:38:03 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:38:03 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:39:03 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:39:03 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:40:03 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:40:03 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:41:03 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:41:03 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:42:03 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:42:03 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:43:03 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:43:04 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:44:04 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:44:04 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:45:04 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:45:04 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:46:04 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:46:04 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:47:04 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:47:04 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:48:04 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:48:04 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:49:04 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:49:04 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:50:04 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:50:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:51:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:51:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:52:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:52:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:53:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:53:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:54:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:54:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:55:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:55:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:56:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:56:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:57:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:57:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:58:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:58:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 12:59:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 12:59:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:00:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:00:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:01:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:01:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:02:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:02:05 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:03:05 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:03:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:04:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:04:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:05:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:05:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:06:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:06:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:07:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:07:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:08:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:08:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:09:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:09:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:10:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:10:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:11:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:11:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:12:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:12:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:13:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:13:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:14:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:14:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:15:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:15:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:16:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:16:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:17:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:17:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:18:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:18:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:19:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:19:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:20:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:20:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:21:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:21:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:22:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:22:06 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:23:06 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:23:07 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:24:07 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:24:07 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:25:07 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:25:07 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:26:07 [12355] (16.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2
09/08/17 13:26:07 [12355] (16.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 2
09/08/17 13:26:07 [12355] (16.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:26:12 [12355] No jobs left, shutting down
09/08/17 13:26:12 [12355] Got SIGTERM. Performing graceful shutdown.
09/08/17 13:26:12 [12355] **** condor_gridmanager (condor_GRIDMANAGER) pid 12355 EXITING WITH STATUS 0
09/08/17 13:46:19 ******************************************************
09/08/17 13:46:19 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 13:46:19 ** /usr/sbin/condor_gridmanager
09/08/17 13:46:19 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 13:46:19 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 13:46:19 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 13:46:19 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 13:46:19 ** PID = 16641
09/08/17 13:46:19 ** Log last touched 9/8 13:26:12
09/08/17 13:46:19 ******************************************************
09/08/17 13:46:19 Using config source: /etc/condor-ce/condor_config
09/08/17 13:46:19 Using local config sources: 
09/08/17 13:46:19    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 13:46:19    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 13:46:19    /etc/condor-ce/config.d/99-local.conf
09/08/17 13:46:19    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 13:46:19 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 13:46:19 CLASSAD_CACHING is ENABLED
09/08/17 13:46:19 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 13:46:19 SharedPortEndpoint: waiting for connections to named socket 4789_089c_409
09/08/17 13:46:19 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_409>
09/08/17 13:46:19 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_409>
09/08/17 13:46:22 [16641] Found job 62.0 --- inserting
09/08/17 13:46:22 [16641] Found job 61.0 --- inserting
09/08/17 13:46:22 [16641] gahp server not up yet, delaying ping
09/08/17 13:46:22 [16641] (62.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:22 [16641] GAHP server pid = 16671
09/08/17 13:46:22 [16641] (61.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:27 [16641] resource  is now up
09/08/17 13:46:27 [16641] (61.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:27 [16641] (62.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:27 [16641] (62.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:28 [16641] (61.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:32 [16641] Found job 72.0 --- inserting
09/08/17 13:46:32 [16641] Found job 71.0 --- inserting
09/08/17 13:46:32 [16641] Found job 70.0 --- inserting
09/08/17 13:46:32 [16641] Found job 69.0 --- inserting
09/08/17 13:46:32 [16641] Found job 68.0 --- inserting
09/08/17 13:46:32 [16641] Found job 67.0 --- inserting
09/08/17 13:46:32 [16641] Found job 66.0 --- inserting
09/08/17 13:46:32 [16641] Found job 65.0 --- inserting
09/08/17 13:46:32 [16641] Found job 64.0 --- inserting
09/08/17 13:46:32 [16641] Found job 100.0 --- inserting
09/08/17 13:46:32 [16641] Found job 63.0 --- inserting
09/08/17 13:46:32 [16641] Found job 99.0 --- inserting
09/08/17 13:46:32 [16641] Found job 98.0 --- inserting
09/08/17 13:46:32 [16641] Found job 97.0 --- inserting
09/08/17 13:46:32 [16641] Found job 96.0 --- inserting
09/08/17 13:46:32 [16641] Found job 95.0 --- inserting
09/08/17 13:46:32 [16641] Found job 94.0 --- inserting
09/08/17 13:46:32 [16641] Found job 93.0 --- inserting
09/08/17 13:46:32 [16641] Found job 92.0 --- inserting
09/08/17 13:46:32 [16641] Found job 91.0 --- inserting
09/08/17 13:46:32 [16641] Found job 90.0 --- inserting
09/08/17 13:46:32 [16641] Found job 89.0 --- inserting
09/08/17 13:46:32 [16641] Found job 88.0 --- inserting
09/08/17 13:46:32 [16641] Found job 87.0 --- inserting
09/08/17 13:46:32 [16641] Found job 86.0 --- inserting
09/08/17 13:46:32 [16641] Found job 85.0 --- inserting
09/08/17 13:46:32 [16641] Found job 84.0 --- inserting
09/08/17 13:46:32 [16641] Found job 83.0 --- inserting
09/08/17 13:46:32 [16641] Found job 82.0 --- inserting
09/08/17 13:46:32 [16641] Found job 81.0 --- inserting
09/08/17 13:46:32 [16641] Found job 80.0 --- inserting
09/08/17 13:46:32 [16641] Found job 79.0 --- inserting
09/08/17 13:46:32 [16641] Found job 78.0 --- inserting
09/08/17 13:46:32 [16641] Found job 77.0 --- inserting
09/08/17 13:46:32 [16641] Found job 76.0 --- inserting
09/08/17 13:46:32 [16641] Found job 75.0 --- inserting
09/08/17 13:46:32 [16641] Found job 74.0 --- inserting
09/08/17 13:46:32 [16641] Found job 73.0 --- inserting
09/08/17 13:46:32 [16641] (72.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (71.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (70.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (69.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (68.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (67.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (66.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (65.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (64.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (100.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (63.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (99.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (98.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (97.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (96.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (95.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (94.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (93.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (92.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (91.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (90.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (89.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (88.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (87.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (86.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (85.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (84.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (83.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (82.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (81.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (80.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (79.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (78.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (77.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (76.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (75.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (74.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (73.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:46:32 [16641] (61.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:32 [16641] (62.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:37 [16641] (63.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (64.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (65.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (66.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (67.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (68.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (69.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (70.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (71.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (72.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (73.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (74.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (75.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (76.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (77.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (78.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (79.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (80.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (81.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (82.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (83.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (84.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (85.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (86.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (87.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (88.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (89.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (90.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (91.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (92.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (93.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (94.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (95.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (96.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (97.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (98.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (99.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:37 [16641] (100.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:46:40 [16641] (68.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:42 [16641] (68.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:43 [16641] (74.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:43 [16641] (87.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:43 [16641] (96.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:43 [16641] (90.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (76.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (69.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (86.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (84.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (70.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (71.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (66.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (95.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (94.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (98.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (77.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (93.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (79.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (83.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (88.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (78.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (99.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (80.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (63.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (72.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (92.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (64.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (100.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:44 [16641] (89.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (75.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (97.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (85.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (67.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (81.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (91.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (65.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (73.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:45 [16641] (82.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 13:46:47 [16641] (63.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (64.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (65.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (66.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (67.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (69.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (70.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (71.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (72.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (73.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (74.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (75.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (76.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (77.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (78.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (79.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (80.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (81.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (82.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (83.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (84.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (85.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (86.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (87.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (88.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (89.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (90.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (91.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (92.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (93.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (94.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (95.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (96.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (97.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (98.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (99.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:46:47 [16641] (100.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 13:47:32 [16641] (61.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:32 [16641] (62.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:33 [16641] (62.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:33 [16641] (62.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:34 [16641] (61.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:38 [16641] (61.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:42 [16641] (68.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:42 [16641] (68.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:43 [16641] (68.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:47 [16641] (63.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (64.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (65.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (66.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (67.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (69.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (70.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (71.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (72.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (73.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (74.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (75.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (76.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (77.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (78.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (79.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (80.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (81.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (82.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (83.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (84.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (85.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (86.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (87.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (88.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (89.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (90.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (91.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (92.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (93.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (94.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (95.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (96.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (97.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (98.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (99.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (100.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 13:47:47 [16641] (63.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (70.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (73.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (67.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (75.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (76.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (77.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (66.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (72.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (83.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (79.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (80.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (82.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (84.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (87.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (91.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (85.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (93.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (92.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (90.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (95.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (88.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (98.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:47 [16641] (100.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:48 [16641] (63.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (66.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (67.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (70.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (72.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (73.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (75.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (76.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (77.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (79.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (80.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (82.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (83.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (84.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (85.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (87.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (88.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (90.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (91.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (92.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (93.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (95.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (98.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (100.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:48 [16641] (86.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:48 [16641] (78.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:48 [16641] (81.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:48 [16641] (97.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:48 [16641] (99.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:48 [16641] (96.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:48 [16641] (65.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:49 [16641] (94.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:49 [16641] (89.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:49 [16641] (74.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:49 [16641] (71.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:49 [16641] (69.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:49 [16641] (64.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 13:47:53 [16641] (64.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (65.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (69.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (71.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (74.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (78.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (81.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (86.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (89.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (94.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (96.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (97.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:53 [16641] (99.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 13:47:58 [16641] No jobs left, shutting down
09/08/17 13:47:58 [16641] Got SIGTERM. Performing graceful shutdown.
09/08/17 13:47:58 [16641] **** condor_gridmanager (condor_GRIDMANAGER) pid 16641 EXITING WITH STATUS 0
09/08/17 13:51:52 ******************************************************
09/08/17 13:51:52 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 13:51:52 ** /usr/sbin/condor_gridmanager
09/08/17 13:51:52 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 13:51:52 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 13:51:52 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 13:51:52 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 13:51:52 ** PID = 19543
09/08/17 13:51:52 ** Log last touched 9/8 13:47:58
09/08/17 13:51:52 ******************************************************
09/08/17 13:51:52 Using config source: /etc/condor-ce/condor_config
09/08/17 13:51:52 Using local config sources: 
09/08/17 13:51:52    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 13:51:52    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 13:51:52    /etc/condor-ce/config.d/99-local.conf
09/08/17 13:51:52    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 13:51:52 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 13:51:52 CLASSAD_CACHING is ENABLED
09/08/17 13:51:52 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 13:51:52 SharedPortEndpoint: waiting for connections to named socket 4789_089c_412
09/08/17 13:51:52 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_412>
09/08/17 13:51:52 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_412>
09/08/17 13:51:55 [19543] Found job 144.0 --- inserting
09/08/17 13:51:55 [19543] Found job 143.0 --- inserting
09/08/17 13:51:55 [19543] Found job 142.0 --- inserting
09/08/17 13:51:55 [19543] Found job 141.0 --- inserting
09/08/17 13:51:55 [19543] Found job 140.0 --- inserting
09/08/17 13:51:55 [19543] Found job 139.0 --- inserting
09/08/17 13:51:55 [19543] Found job 138.0 --- inserting
09/08/17 13:51:55 [19543] Found job 137.0 --- inserting
09/08/17 13:51:55 [19543] Found job 136.0 --- inserting
09/08/17 13:51:55 [19543] Found job 135.0 --- inserting
09/08/17 13:51:55 [19543] Found job 134.0 --- inserting
09/08/17 13:51:55 [19543] Found job 133.0 --- inserting
09/08/17 13:51:55 [19543] Found job 132.0 --- inserting
09/08/17 13:51:55 [19543] Found job 131.0 --- inserting
09/08/17 13:51:55 [19543] Found job 130.0 --- inserting
09/08/17 13:51:55 [19543] Found job 129.0 --- inserting
09/08/17 13:51:55 [19543] Found job 128.0 --- inserting
09/08/17 13:51:55 [19543] Found job 127.0 --- inserting
09/08/17 13:51:55 [19543] Found job 126.0 --- inserting
09/08/17 13:51:55 [19543] Found job 125.0 --- inserting
09/08/17 13:51:55 [19543] Found job 124.0 --- inserting
09/08/17 13:51:55 [19543] Found job 123.0 --- inserting
09/08/17 13:51:55 [19543] gahp server not up yet, delaying ping
09/08/17 13:51:55 [19543] (144.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] GAHP server pid = 19546
09/08/17 13:51:55 [19543] (143.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (142.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (141.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (140.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (139.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (138.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (137.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (136.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (135.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (134.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (133.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (132.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (131.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (130.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (129.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (128.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (127.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (126.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (125.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (124.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:51:55 [19543] (123.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 13:52:00 [19543] resource  is now up
09/08/17 13:52:00 [19543] (124.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:52:00 [19543] (125.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:52:00 [19543] (126.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:52:00 [19543] (127.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 13:52:00 [19543] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 13:56:53 ******************************************************
09/08/17 13:56:53 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 13:56:53 ** /usr/sbin/condor_gridmanager
09/08/17 13:56:53 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 13:56:53 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 13:56:53 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 13:56:53 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 13:56:53 ** PID = 19601
09/08/17 13:56:53 ** Log last touched 9/8 13:52:00
09/08/17 13:56:53 ******************************************************
09/08/17 13:56:53 Using config source: /etc/condor-ce/condor_config
09/08/17 13:56:53 Using local config sources: 
09/08/17 13:56:53    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 13:56:53    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 13:56:53    /etc/condor-ce/config.d/99-local.conf
09/08/17 13:56:53    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 13:56:53 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 13:56:53 CLASSAD_CACHING is ENABLED
09/08/17 13:56:53 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 13:56:53 SharedPortEndpoint: waiting for connections to named socket 4789_089c_414
09/08/17 13:56:53 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_414>
09/08/17 13:56:53 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_414>
09/08/17 13:56:56 [19601] Found job 144.0 --- inserting
09/08/17 13:56:56 [19601] Found job 143.0 --- inserting
09/08/17 13:56:56 [19601] Found job 142.0 --- inserting
09/08/17 13:56:56 [19601] Found job 141.0 --- inserting
09/08/17 13:56:56 [19601] Found job 140.0 --- inserting
09/08/17 13:56:56 [19601] Found job 139.0 --- inserting
09/08/17 13:56:56 [19601] Found job 138.0 --- inserting
09/08/17 13:56:56 [19601] Found job 137.0 --- inserting
09/08/17 13:56:56 [19601] Found job 136.0 --- inserting
09/08/17 13:56:56 [19601] Found job 135.0 --- inserting
09/08/17 13:56:56 [19601] Found job 134.0 --- inserting
09/08/17 13:56:56 [19601] Found job 133.0 --- inserting
09/08/17 13:56:56 [19601] Found job 132.0 --- inserting
09/08/17 13:56:56 [19601] Found job 131.0 --- inserting
09/08/17 13:56:56 [19601] Found job 130.0 --- inserting
09/08/17 13:56:56 [19601] Found job 129.0 --- inserting
09/08/17 13:56:56 [19601] Found job 128.0 --- inserting
09/08/17 13:56:56 [19601] Found job 127.0 --- inserting
09/08/17 13:56:56 [19601] Found job 126.0 --- inserting
09/08/17 13:56:56 [19601] Found job 125.0 --- inserting
09/08/17 13:56:56 [19601] Found job 124.0 --- inserting
09/08/17 13:56:56 [19601] Found job 123.0 --- inserting
09/08/17 13:56:56 [19601] gahp server not up yet, delaying ping
09/08/17 13:56:56 [19601] (144.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 13:56:56 [19601] GAHP server pid = 19604
09/08/17 13:56:56 [19601] (143.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 13:56:56 [19601] (142.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 13:56:56 [19601] (141.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 13:56:56 [19601] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:01:33 ******************************************************
09/08/17 14:01:33 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:01:33 ** /usr/sbin/condor_gridmanager
09/08/17 14:01:33 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:01:33 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:01:33 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:01:33 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:01:33 ** PID = 19675
09/08/17 14:01:33 ** Log last touched 9/8 13:56:56
09/08/17 14:01:33 ******************************************************
09/08/17 14:01:33 Using config source: /etc/condor-ce/condor_config
09/08/17 14:01:33 Using local config sources: 
09/08/17 14:01:33    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:01:33    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:01:33    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:01:33    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:01:33 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:01:33 CLASSAD_CACHING is ENABLED
09/08/17 14:01:33 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:01:33 SharedPortEndpoint: waiting for connections to named socket 4789_089c_417
09/08/17 14:01:33 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_417>
09/08/17 14:01:33 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_417>
09/08/17 14:01:36 [19675] Found job 144.0 --- inserting
09/08/17 14:01:36 [19675] Found job 143.0 --- inserting
09/08/17 14:01:36 [19675] Found job 142.0 --- inserting
09/08/17 14:01:36 [19675] Found job 141.0 --- inserting
09/08/17 14:01:36 [19675] Found job 140.0 --- inserting
09/08/17 14:01:36 [19675] Found job 139.0 --- inserting
09/08/17 14:01:36 [19675] Found job 138.0 --- inserting
09/08/17 14:01:36 [19675] Found job 137.0 --- inserting
09/08/17 14:01:36 [19675] Found job 136.0 --- inserting
09/08/17 14:01:36 [19675] Found job 135.0 --- inserting
09/08/17 14:01:36 [19675] Found job 134.0 --- inserting
09/08/17 14:01:36 [19675] Found job 133.0 --- inserting
09/08/17 14:01:36 [19675] Found job 132.0 --- inserting
09/08/17 14:01:36 [19675] Found job 131.0 --- inserting
09/08/17 14:01:36 [19675] Found job 130.0 --- inserting
09/08/17 14:01:36 [19675] Found job 129.0 --- inserting
09/08/17 14:01:36 [19675] Found job 128.0 --- inserting
09/08/17 14:01:36 [19675] Found job 127.0 --- inserting
09/08/17 14:01:36 [19675] Found job 126.0 --- inserting
09/08/17 14:01:36 [19675] Found job 125.0 --- inserting
09/08/17 14:01:36 [19675] Found job 124.0 --- inserting
09/08/17 14:01:36 [19675] Found job 123.0 --- inserting
09/08/17 14:01:36 [19675] gahp server not up yet, delaying ping
09/08/17 14:01:36 [19675] (144.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:36 [19675] GAHP server pid = 19679
09/08/17 14:01:36 [19675] (143.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:36 [19675] (142.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:36 [19675] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:01:38 ******************************************************
09/08/17 14:01:38 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:01:38 ** /usr/sbin/condor_gridmanager
09/08/17 14:01:38 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:01:38 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:01:38 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:01:38 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:01:38 ** PID = 19692
09/08/17 14:01:38 ** Log last touched 9/8 14:01:36
09/08/17 14:01:38 ******************************************************
09/08/17 14:01:38 Using config source: /etc/condor-ce/condor_config
09/08/17 14:01:38 Using local config sources: 
09/08/17 14:01:38    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:01:38    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:01:38    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:01:38    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:01:38 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:01:38 CLASSAD_CACHING is ENABLED
09/08/17 14:01:38 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:01:38 SharedPortEndpoint: waiting for connections to named socket 4789_089c_418
09/08/17 14:01:38 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_418>
09/08/17 14:01:38 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_418>
09/08/17 14:01:41 [19692] Found job 144.0 --- inserting
09/08/17 14:01:41 [19692] Found job 143.0 --- inserting
09/08/17 14:01:41 [19692] Found job 142.0 --- inserting
09/08/17 14:01:41 [19692] Found job 141.0 --- inserting
09/08/17 14:01:41 [19692] Found job 140.0 --- inserting
09/08/17 14:01:41 [19692] Found job 139.0 --- inserting
09/08/17 14:01:41 [19692] Found job 138.0 --- inserting
09/08/17 14:01:41 [19692] Found job 137.0 --- inserting
09/08/17 14:01:41 [19692] Found job 136.0 --- inserting
09/08/17 14:01:41 [19692] Found job 135.0 --- inserting
09/08/17 14:01:41 [19692] Found job 134.0 --- inserting
09/08/17 14:01:41 [19692] Found job 133.0 --- inserting
09/08/17 14:01:41 [19692] Found job 132.0 --- inserting
09/08/17 14:01:41 [19692] Found job 131.0 --- inserting
09/08/17 14:01:41 [19692] Found job 130.0 --- inserting
09/08/17 14:01:41 [19692] Found job 129.0 --- inserting
09/08/17 14:01:41 [19692] Found job 128.0 --- inserting
09/08/17 14:01:41 [19692] Found job 127.0 --- inserting
09/08/17 14:01:41 [19692] Found job 126.0 --- inserting
09/08/17 14:01:41 [19692] Found job 125.0 --- inserting
09/08/17 14:01:41 [19692] Found job 124.0 --- inserting
09/08/17 14:01:41 [19692] Found job 123.0 --- inserting
09/08/17 14:01:41 [19692] Found job 148.0 --- inserting
09/08/17 14:01:41 [19692] Found job 147.0 --- inserting
09/08/17 14:01:41 [19692] gahp server not up yet, delaying ping
09/08/17 14:01:41 [19692] (144.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:41 [19692] GAHP server pid = 19694
09/08/17 14:01:41 [19692] (143.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:41 [19692] (142.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:41 [19692] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:01:43 ******************************************************
09/08/17 14:01:43 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:01:43 ** /usr/sbin/condor_gridmanager
09/08/17 14:01:43 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:01:43 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:01:43 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:01:43 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:01:43 ** PID = 19707
09/08/17 14:01:43 ** Log last touched 9/8 14:01:41
09/08/17 14:01:43 ******************************************************
09/08/17 14:01:43 Using config source: /etc/condor-ce/condor_config
09/08/17 14:01:43 Using local config sources: 
09/08/17 14:01:43    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:01:43    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:01:43    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:01:43    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:01:43 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:01:43 CLASSAD_CACHING is ENABLED
09/08/17 14:01:43 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:01:43 SharedPortEndpoint: waiting for connections to named socket 4789_089c_419
09/08/17 14:01:43 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_419>
09/08/17 14:01:43 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_419>
09/08/17 14:01:46 [19707] Found job 144.0 --- inserting
09/08/17 14:01:46 [19707] Found job 143.0 --- inserting
09/08/17 14:01:46 [19707] Found job 142.0 --- inserting
09/08/17 14:01:46 [19707] Found job 141.0 --- inserting
09/08/17 14:01:46 [19707] Found job 140.0 --- inserting
09/08/17 14:01:46 [19707] Found job 139.0 --- inserting
09/08/17 14:01:46 [19707] Found job 138.0 --- inserting
09/08/17 14:01:46 [19707] Found job 137.0 --- inserting
09/08/17 14:01:46 [19707] Found job 136.0 --- inserting
09/08/17 14:01:46 [19707] Found job 135.0 --- inserting
09/08/17 14:01:46 [19707] Found job 134.0 --- inserting
09/08/17 14:01:46 [19707] Found job 133.0 --- inserting
09/08/17 14:01:46 [19707] Found job 132.0 --- inserting
09/08/17 14:01:46 [19707] Found job 131.0 --- inserting
09/08/17 14:01:46 [19707] Found job 130.0 --- inserting
09/08/17 14:01:46 [19707] Found job 129.0 --- inserting
09/08/17 14:01:46 [19707] Found job 128.0 --- inserting
09/08/17 14:01:46 [19707] Found job 127.0 --- inserting
09/08/17 14:01:46 [19707] Found job 126.0 --- inserting
09/08/17 14:01:46 [19707] Found job 125.0 --- inserting
09/08/17 14:01:46 [19707] Found job 124.0 --- inserting
09/08/17 14:01:46 [19707] Found job 123.0 --- inserting
09/08/17 14:01:46 [19707] Found job 148.0 --- inserting
09/08/17 14:01:46 [19707] Found job 147.0 --- inserting
09/08/17 14:01:46 [19707] gahp server not up yet, delaying ping
09/08/17 14:01:46 [19707] (144.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:46 [19707] GAHP server pid = 19709
09/08/17 14:01:46 [19707] (143.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:46 [19707] (142.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:01:46 [19707] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:03:24 ******************************************************
09/08/17 14:03:24 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:03:24 ** /usr/sbin/condor_gridmanager
09/08/17 14:03:24 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:03:24 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:03:24 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:03:24 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:03:24 ** PID = 19744
09/08/17 14:03:24 ** Log last touched 9/8 14:01:46
09/08/17 14:03:24 ******************************************************
09/08/17 14:03:24 Using config source: /etc/condor-ce/condor_config
09/08/17 14:03:24 Using local config sources: 
09/08/17 14:03:24    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:03:24    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:03:24    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:03:24    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:03:24 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:03:24 CLASSAD_CACHING is ENABLED
09/08/17 14:03:24 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:03:24 SharedPortEndpoint: waiting for connections to named socket 4789_089c_421
09/08/17 14:03:24 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_421>
09/08/17 14:03:24 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_421>
09/08/17 14:03:26 [19744] Found job 144.0 --- inserting
09/08/17 14:03:26 [19744] Found job 143.0 --- inserting
09/08/17 14:03:26 [19744] Found job 142.0 --- inserting
09/08/17 14:03:26 [19744] Found job 141.0 --- inserting
09/08/17 14:03:26 [19744] Found job 140.0 --- inserting
09/08/17 14:03:26 [19744] Found job 139.0 --- inserting
09/08/17 14:03:26 [19744] Found job 138.0 --- inserting
09/08/17 14:03:26 [19744] Found job 137.0 --- inserting
09/08/17 14:03:26 [19744] Found job 136.0 --- inserting
09/08/17 14:03:26 [19744] Found job 135.0 --- inserting
09/08/17 14:03:26 [19744] Found job 134.0 --- inserting
09/08/17 14:03:26 [19744] Found job 133.0 --- inserting
09/08/17 14:03:26 [19744] Found job 132.0 --- inserting
09/08/17 14:03:26 [19744] Found job 131.0 --- inserting
09/08/17 14:03:26 [19744] Found job 130.0 --- inserting
09/08/17 14:03:26 [19744] Found job 129.0 --- inserting
09/08/17 14:03:26 [19744] Found job 128.0 --- inserting
09/08/17 14:03:26 [19744] Found job 127.0 --- inserting
09/08/17 14:03:26 [19744] Found job 126.0 --- inserting
09/08/17 14:03:26 [19744] Found job 125.0 --- inserting
09/08/17 14:03:26 [19744] Found job 124.0 --- inserting
09/08/17 14:03:26 [19744] Found job 123.0 --- inserting
09/08/17 14:03:26 [19744] Found job 148.0 --- inserting
09/08/17 14:03:26 [19744] Found job 147.0 --- inserting
09/08/17 14:03:26 [19744] gahp server not up yet, delaying ping
09/08/17 14:03:26 [19744] (144.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] GAHP server pid = 19747
09/08/17 14:03:26 [19744] (143.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (142.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (141.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (140.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (139.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (138.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (137.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (136.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (135.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (134.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (133.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (132.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (131.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (130.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (129.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (128.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (127.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (126.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (125.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (124.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (123.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:03:26 [19744] (148.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 14:03:26 [19744] (147.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 14:03:28 [19744] (142.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (142.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:30 [19744] (139.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (127.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (132.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (135.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (139.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:30 [19744] (132.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:30 [19744] (144.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (133.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (131.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (136.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (128.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (134.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (140.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (130.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (141.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:30 [19744] (144.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:30 [19744] (128.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:30 [19744] (134.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:30 [19744] (140.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:30 [19744] (130.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] resource  is now up
09/08/17 14:03:31 [19744] (143.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (141.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (137.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (136.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (135.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (133.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (131.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (127.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (126.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (125.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (124.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (123.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (138.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (129.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (138.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (127.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (135.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (124.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (125.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (123.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (126.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (137.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (143.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:03:31 [19744] (124.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (126.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (123.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (137.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (133.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (131.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (136.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:31 [19744] (141.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:32 [19744] (129.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:32 [19744] (125.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:32 [19744] (143.0) doEvaluateState called: gmState GM_CANCEL, remoteState -1
09/08/17 14:03:36 [19744] No jobs left, shutting down
09/08/17 14:03:36 [19744] Got SIGTERM. Performing graceful shutdown.
09/08/17 14:03:36 [19744] **** condor_gridmanager (condor_GRIDMANAGER) pid 19744 EXITING WITH STATUS 0
09/08/17 14:04:04 ******************************************************
09/08/17 14:04:04 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:04:04 ** /usr/sbin/condor_gridmanager
09/08/17 14:04:04 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:04:04 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:04:04 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:04:04 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:04:04 ** PID = 21219
09/08/17 14:04:04 ** Log last touched 9/8 14:03:36
09/08/17 14:04:04 ******************************************************
09/08/17 14:04:04 Using config source: /etc/condor-ce/condor_config
09/08/17 14:04:04 Using local config sources: 
09/08/17 14:04:04    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:04:04    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:04:04    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:04:04    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:04:04 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:04:04 CLASSAD_CACHING is ENABLED
09/08/17 14:04:04 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:04:04 SharedPortEndpoint: waiting for connections to named socket 4789_089c_422
09/08/17 14:04:04 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_422>
09/08/17 14:04:04 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_422>
09/08/17 14:04:07 [21219] Found job 152.0 --- inserting
09/08/17 14:04:07 [21219] Found job 151.0 --- inserting
09/08/17 14:04:07 [21219] gahp server not up yet, delaying ping
09/08/17 14:04:07 [21219] (152.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 14:04:07 [21219] GAHP server pid = 21221
09/08/17 14:04:07 [21219] (151.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 14:04:12 [21219] resource  is now up
09/08/17 14:04:12 [21219] (151.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 14:04:12 [21219] (152.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 14:04:12 [21219] GAHP command 'RESULTS' failed
09/08/17 14:04:12 [21219] GAHP command 'RESULTS' failed
09/08/17 14:04:12 [21219] ERROR "Gahp Server (pid=21221) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:09:04 ******************************************************
09/08/17 14:09:04 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:09:04 ** /usr/sbin/condor_gridmanager
09/08/17 14:09:04 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:09:04 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:09:04 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:09:04 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:09:04 ** PID = 21269
09/08/17 14:09:04 ** Log last touched 9/8 14:04:12
09/08/17 14:09:04 ******************************************************
09/08/17 14:09:04 Using config source: /etc/condor-ce/condor_config
09/08/17 14:09:04 Using local config sources: 
09/08/17 14:09:04    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:09:04    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:09:04    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:09:04    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:09:04 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:09:04 CLASSAD_CACHING is ENABLED
09/08/17 14:09:04 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:09:04 SharedPortEndpoint: waiting for connections to named socket 4789_089c_424
09/08/17 14:09:04 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_424>
09/08/17 14:09:04 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_424>
09/08/17 14:09:07 [21269] Found job 152.0 --- inserting
09/08/17 14:09:07 [21269] Found job 151.0 --- inserting
09/08/17 14:09:07 [21269] gahp server not up yet, delaying ping
09/08/17 14:09:07 [21269] (152.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:09:07 [21269] GAHP server pid = 21271
09/08/17 14:09:07 [21269] (151.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:09:07 [21269] GAHP command 'RESULTS' failed
09/08/17 14:09:07 [21269] ERROR "Gahp Server (pid=21271) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:14:05 ******************************************************
09/08/17 14:14:05 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:14:05 ** /usr/sbin/condor_gridmanager
09/08/17 14:14:05 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:14:05 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:14:05 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:14:05 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:14:05 ** PID = 21504
09/08/17 14:14:05 ** Log last touched 9/8 14:09:07
09/08/17 14:14:05 ******************************************************
09/08/17 14:14:05 Using config source: /etc/condor-ce/condor_config
09/08/17 14:14:05 Using local config sources: 
09/08/17 14:14:05    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:14:05    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:14:05    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:14:05    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:14:05 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:14:05 CLASSAD_CACHING is ENABLED
09/08/17 14:14:05 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:14:05 SharedPortEndpoint: waiting for connections to named socket 4789_089c_427
09/08/17 14:14:05 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_427>
09/08/17 14:14:05 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_427>
09/08/17 14:14:08 [21504] Found job 152.0 --- inserting
09/08/17 14:14:08 [21504] Found job 151.0 --- inserting
09/08/17 14:14:08 [21504] gahp server not up yet, delaying ping
09/08/17 14:14:08 [21504] (152.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:14:08 [21504] GAHP server pid = 21507
09/08/17 14:14:08 [21504] (151.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:14:08 [21504] GAHP command 'RESULTS' failed
09/08/17 14:14:08 [21504] ERROR "Gahp Server (pid=21507) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:19:05 ******************************************************
09/08/17 14:19:05 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:19:05 ** /usr/sbin/condor_gridmanager
09/08/17 14:19:05 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:19:05 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:19:05 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:19:05 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:19:05 ** PID = 21559
09/08/17 14:19:05 ** Log last touched 9/8 14:14:08
09/08/17 14:19:05 ******************************************************
09/08/17 14:19:05 Using config source: /etc/condor-ce/condor_config
09/08/17 14:19:05 Using local config sources: 
09/08/17 14:19:05    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:19:05    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:19:05    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:19:05    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:19:05 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:19:05 CLASSAD_CACHING is ENABLED
09/08/17 14:19:05 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:19:05 SharedPortEndpoint: waiting for connections to named socket 4789_089c_430
09/08/17 14:19:05 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_430>
09/08/17 14:19:05 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_430>
09/08/17 14:19:08 [21559] Found job 152.0 --- inserting
09/08/17 14:19:08 [21559] Found job 151.0 --- inserting
09/08/17 14:19:08 [21559] gahp server not up yet, delaying ping
09/08/17 14:19:08 [21559] (152.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:19:08 [21559] GAHP server pid = 21562
09/08/17 14:19:08 [21559] (151.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:19:08 [21559] GAHP command 'RESULTS' failed
09/08/17 14:19:08 [21559] ERROR "Gahp Server (pid=21562) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:24:05 ******************************************************
09/08/17 14:24:05 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:24:05 ** /usr/sbin/condor_gridmanager
09/08/17 14:24:05 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:24:05 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:24:05 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:24:05 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:24:05 ** PID = 21608
09/08/17 14:24:05 ** Log last touched 9/8 14:19:08
09/08/17 14:24:05 ******************************************************
09/08/17 14:24:05 Using config source: /etc/condor-ce/condor_config
09/08/17 14:24:05 Using local config sources: 
09/08/17 14:24:05    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:24:05    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:24:05    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:24:05    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:24:05 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:24:05 CLASSAD_CACHING is ENABLED
09/08/17 14:24:05 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:24:05 SharedPortEndpoint: waiting for connections to named socket 4789_089c_432
09/08/17 14:24:05 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_432>
09/08/17 14:24:05 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_432>
09/08/17 14:24:08 [21608] Found job 152.0 --- inserting
09/08/17 14:24:08 [21608] Found job 151.0 --- inserting
09/08/17 14:24:08 [21608] gahp server not up yet, delaying ping
09/08/17 14:24:08 [21608] (152.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:24:08 [21608] GAHP server pid = 21611
09/08/17 14:24:08 [21608] (151.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:24:08 [21608] GAHP command 'RESULTS' failed
09/08/17 14:24:08 [21608] ERROR "Gahp Server (pid=21611) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:29:07 ******************************************************
09/08/17 14:29:07 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:29:07 ** /usr/sbin/condor_gridmanager
09/08/17 14:29:07 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:29:07 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:29:07 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:29:07 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:29:07 ** PID = 21655
09/08/17 14:29:07 ** Log last touched 9/8 14:24:08
09/08/17 14:29:07 ******************************************************
09/08/17 14:29:07 Using config source: /etc/condor-ce/condor_config
09/08/17 14:29:07 Using local config sources: 
09/08/17 14:29:07    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:29:07    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:29:07    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:29:07    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:29:07 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:29:07 CLASSAD_CACHING is ENABLED
09/08/17 14:29:07 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:29:07 SharedPortEndpoint: waiting for connections to named socket 4789_089c_435
09/08/17 14:29:07 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_435>
09/08/17 14:29:07 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_435>
09/08/17 14:29:10 [21655] Found job 152.0 --- inserting
09/08/17 14:29:10 [21655] Found job 151.0 --- inserting
09/08/17 14:29:10 [21655] gahp server not up yet, delaying ping
09/08/17 14:29:10 [21655] (152.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:29:10 [21655] GAHP server pid = 21658
09/08/17 14:29:10 [21655] (151.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:29:10 [21655] GAHP command 'RESULTS' failed
09/08/17 14:29:10 [21655] ERROR "Gahp Server (pid=21658) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:34:07 ******************************************************
09/08/17 14:34:07 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:34:07 ** /usr/sbin/condor_gridmanager
09/08/17 14:34:07 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:34:07 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:34:07 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:34:07 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:34:07 ** PID = 21716
09/08/17 14:34:07 ** Log last touched 9/8 14:29:10
09/08/17 14:34:07 ******************************************************
09/08/17 14:34:07 Using config source: /etc/condor-ce/condor_config
09/08/17 14:34:07 Using local config sources: 
09/08/17 14:34:07    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:34:07    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:34:07    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:34:07    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:34:07 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:34:07 CLASSAD_CACHING is ENABLED
09/08/17 14:34:07 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:34:07 SharedPortEndpoint: waiting for connections to named socket 4789_089c_438
09/08/17 14:34:07 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_438>
09/08/17 14:34:07 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_438>
09/08/17 14:34:10 [21716] Found job 152.0 --- inserting
09/08/17 14:34:10 [21716] Found job 151.0 --- inserting
09/08/17 14:34:10 [21716] gahp server not up yet, delaying ping
09/08/17 14:34:10 [21716] (152.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:34:10 [21716] GAHP server pid = 21719
09/08/17 14:34:10 [21716] (151.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:34:10 [21716] GAHP command 'RESULTS' failed
09/08/17 14:34:10 [21716] ERROR "Gahp Server (pid=21719) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:39:09 ******************************************************
09/08/17 14:39:09 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:39:09 ** /usr/sbin/condor_gridmanager
09/08/17 14:39:09 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:39:09 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:39:09 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:39:09 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:39:09 ** PID = 21766
09/08/17 14:39:09 ** Log last touched 9/8 14:34:10
09/08/17 14:39:09 ******************************************************
09/08/17 14:39:09 Using config source: /etc/condor-ce/condor_config
09/08/17 14:39:09 Using local config sources: 
09/08/17 14:39:09    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:39:09    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:39:09    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:39:09    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:39:09 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:39:09 CLASSAD_CACHING is ENABLED
09/08/17 14:39:09 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:39:09 SharedPortEndpoint: waiting for connections to named socket 4789_089c_440
09/08/17 14:39:09 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_440>
09/08/17 14:39:09 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_440>
09/08/17 14:39:11 [21766] Found job 152.0 --- inserting
09/08/17 14:39:11 [21766] Found job 151.0 --- inserting
09/08/17 14:39:11 [21766] gahp server not up yet, delaying ping
09/08/17 14:39:11 [21766] (152.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:39:11 [21766] GAHP server pid = 21769
09/08/17 14:39:11 [21766] (151.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:39:11 [21766] (152.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:39:11 [21766] (151.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:39:14 [21766] (151.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 14:39:14 [21766] (152.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 14:39:16 [21766] resource  is now up
09/08/17 14:39:16 [21766] (151.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 14:39:16 [21766] (152.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 14:40:16 [21766] (151.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 14:40:16 [21766] (152.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 14:40:16 [21766] (152.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 14:40:16 [21766] (152.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 14:40:17 [21766] (151.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 14:40:21 [21766] (151.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 14:40:26 [21766] No jobs left, shutting down
09/08/17 14:40:26 [21766] Got SIGTERM. Performing graceful shutdown.
09/08/17 14:40:26 [21766] **** condor_gridmanager (condor_GRIDMANAGER) pid 21766 EXITING WITH STATUS 0
09/08/17 14:42:33 ******************************************************
09/08/17 14:42:33 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:42:33 ** /usr/sbin/condor_gridmanager
09/08/17 14:42:33 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:42:33 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:42:33 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:42:33 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:42:33 ** PID = 21997
09/08/17 14:42:33 ** Log last touched 9/8 14:40:26
09/08/17 14:42:33 ******************************************************
09/08/17 14:42:33 Using config source: /etc/condor-ce/condor_config
09/08/17 14:42:33 Using local config sources: 
09/08/17 14:42:33    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:42:33    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:42:33    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:42:33    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:42:33 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:42:33 CLASSAD_CACHING is ENABLED
09/08/17 14:42:33 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:42:33 SharedPortEndpoint: waiting for connections to named socket 4789_089c_443
09/08/17 14:42:33 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_443>
09/08/17 14:42:33 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_443>
09/08/17 14:42:36 [21997] Found job 156.0 --- inserting
09/08/17 14:42:36 [21997] Found job 155.0 --- inserting
09/08/17 14:42:36 [21997] gahp server not up yet, delaying ping
09/08/17 14:42:36 [21997] (156.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 14:42:36 [21997] GAHP server pid = 22004
09/08/17 14:42:36 [21997] (155.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 14:42:41 [21997] resource  is now up
09/08/17 14:42:41 [21997] (155.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 14:42:41 [21997] (156.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 14:42:41 [21997] GAHP command 'RESULTS' failed
09/08/17 14:42:41 [21997] GAHP command 'RESULTS' failed
09/08/17 14:42:41 [21997] ERROR "Gahp Server (pid=22004) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:47:33 ******************************************************
09/08/17 14:47:33 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:47:33 ** /usr/sbin/condor_gridmanager
09/08/17 14:47:33 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:47:33 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:47:33 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:47:33 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:47:33 ** PID = 22188
09/08/17 14:47:33 ** Log last touched 9/8 14:42:41
09/08/17 14:47:33 ******************************************************
09/08/17 14:47:33 Using config source: /etc/condor-ce/condor_config
09/08/17 14:47:33 Using local config sources: 
09/08/17 14:47:33    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:47:33    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:47:33    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:47:33    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:47:33 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:47:33 CLASSAD_CACHING is ENABLED
09/08/17 14:47:33 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:47:33 SharedPortEndpoint: waiting for connections to named socket 4789_089c_445
09/08/17 14:47:33 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_445>
09/08/17 14:47:33 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_445>
09/08/17 14:47:36 [22188] Found job 156.0 --- inserting
09/08/17 14:47:36 [22188] Found job 155.0 --- inserting
09/08/17 14:47:36 [22188] gahp server not up yet, delaying ping
09/08/17 14:47:36 [22188] (156.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:47:36 [22188] GAHP server pid = 22192
09/08/17 14:47:36 [22188] (155.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:47:41 [22188] resource  is now up
09/08/17 14:47:41 [22188] (156.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:47:41 [22188] (155.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:52:37 [22188] (156.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:52:37 [22188] (156.0) blah_job_submit() failed: BLAH_JOB_SUBMIT timed out
09/08/17 14:52:37 [22188] (155.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:52:37 [22188] (155.0) blah_job_submit() failed: BLAH_JOB_SUBMIT timed out
09/08/17 14:52:38 [22188] (155.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState -1
09/08/17 14:52:38 [22188] (156.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState -1
09/08/17 14:52:43 [22188] (155.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 14:52:43 [22188] (156.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 14:52:48 [22188] No jobs left, shutting down
09/08/17 14:52:48 [22188] Got SIGTERM. Performing graceful shutdown.
09/08/17 14:52:48 [22188] **** condor_gridmanager (condor_GRIDMANAGER) pid 22188 EXITING WITH STATUS 0
09/08/17 14:54:20 ******************************************************
09/08/17 14:54:20 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:54:20 ** /usr/sbin/condor_gridmanager
09/08/17 14:54:20 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:54:20 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:54:20 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:54:20 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:54:20 ** PID = 22492
09/08/17 14:54:20 ** Log last touched 9/8 14:52:48
09/08/17 14:54:20 ******************************************************
09/08/17 14:54:20 Using config source: /etc/condor-ce/condor_config
09/08/17 14:54:20 Using local config sources: 
09/08/17 14:54:20    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:54:20    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:54:20    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:54:20    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:54:20 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:54:20 CLASSAD_CACHING is ENABLED
09/08/17 14:54:20 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:54:20 SharedPortEndpoint: waiting for connections to named socket 4789_089c_449
09/08/17 14:54:20 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_449>
09/08/17 14:54:20 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_449>
09/08/17 14:54:23 [22492] Failed to get expiration time of proxy /common/osg/condor2/154/0/cluster154.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 14:54:23 [22492] Found job 156.0 --- inserting
09/08/17 14:54:23 [22492] Failed to get expiration time of proxy /common/osg/condor2/153/0/cluster153.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 14:54:23 [22492] Found job 155.0 --- inserting
09/08/17 14:54:23 [22492] gahp server not up yet, delaying ping
09/08/17 14:54:23 [22492] (156.0) doEvaluateState called: gmState GM_HOLD, remoteState -1
09/08/17 14:54:23 [22492] (155.0) doEvaluateState called: gmState GM_HOLD, remoteState -1
09/08/17 14:54:28 [22492] gahp server not up yet, delaying ping
09/08/17 14:54:28 [22492] No jobs left, shutting down
09/08/17 14:54:28 [22492] Got SIGTERM. Performing graceful shutdown.
09/08/17 14:54:28 [22492] **** condor_gridmanager (condor_GRIDMANAGER) pid 22492 EXITING WITH STATUS 0
09/08/17 14:55:15 ******************************************************
09/08/17 14:55:15 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:55:15 ** /usr/sbin/condor_gridmanager
09/08/17 14:55:15 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:55:15 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:55:15 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:55:15 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:55:15 ** PID = 22531
09/08/17 14:55:15 ** Log last touched 9/8 14:54:28
09/08/17 14:55:15 ******************************************************
09/08/17 14:55:15 Using config source: /etc/condor-ce/condor_config
09/08/17 14:55:15 Using local config sources: 
09/08/17 14:55:15    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:55:15    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:55:15    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:55:15    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:55:15 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:55:15 CLASSAD_CACHING is ENABLED
09/08/17 14:55:15 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:55:15 SharedPortEndpoint: waiting for connections to named socket 4789_089c_450
09/08/17 14:55:15 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_450>
09/08/17 14:55:15 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_450>
09/08/17 14:55:17 [22531] Failed to get expiration time of proxy /common/osg/condor2/154/0/cluster154.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 14:55:17 [22531] Found job 156.0 --- inserting
09/08/17 14:55:17 [22531] Failed to get expiration time of proxy /common/osg/condor2/153/0/cluster153.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 14:55:17 [22531] Found job 155.0 --- inserting
09/08/17 14:55:17 [22531] gahp server not up yet, delaying ping
09/08/17 14:55:17 [22531] (156.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:55:17 [22531] GAHP server pid = 22534
09/08/17 14:55:17 [22531] (155.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:55:17 [22531] GAHP command 'RESULTS' failed
09/08/17 14:55:17 [22531] ERROR "Gahp Server (pid=22534) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 14:56:18 ******************************************************
09/08/17 14:56:18 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 14:56:18 ** /usr/sbin/condor_gridmanager
09/08/17 14:56:18 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 14:56:18 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 14:56:18 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 14:56:18 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 14:56:18 ** PID = 22607
09/08/17 14:56:18 ** Log last touched 9/8 14:55:17
09/08/17 14:56:18 ******************************************************
09/08/17 14:56:18 Using config source: /etc/condor-ce/condor_config
09/08/17 14:56:18 Using local config sources: 
09/08/17 14:56:18    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 14:56:18    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 14:56:18    /etc/condor-ce/config.d/99-local.conf
09/08/17 14:56:18    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 14:56:18 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 14:56:18 CLASSAD_CACHING is ENABLED
09/08/17 14:56:18 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 14:56:18 SharedPortEndpoint: waiting for connections to named socket 4789_089c_451
09/08/17 14:56:18 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_451>
09/08/17 14:56:18 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_451>
09/08/17 14:56:21 [22607] Failed to get expiration time of proxy /common/osg/condor2/154/0/cluster154.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 14:56:21 [22607] Found job 156.0 --- inserting
09/08/17 14:56:21 [22607] Failed to get expiration time of proxy /common/osg/condor2/153/0/cluster153.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 14:56:21 [22607] Found job 155.0 --- inserting
09/08/17 14:56:21 [22607] gahp server not up yet, delaying ping
09/08/17 14:56:21 [22607] (156.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:56:21 [22607] GAHP server pid = 22611
09/08/17 14:56:21 [22607] (155.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 14:56:21 [22607] (156.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:56:21 [22607] (156.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/154/0/cluster154.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 14:56:21 [22607] (155.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 14:56:21 [22607] (155.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/153/0/cluster153.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 14:56:26 [22607] resource  is now up
09/08/17 14:56:26 [22607] No jobs left, shutting down
09/08/17 14:56:26 [22607] Got SIGTERM. Performing graceful shutdown.
09/08/17 14:56:26 [22607] **** condor_gridmanager (condor_GRIDMANAGER) pid 22607 EXITING WITH STATUS 0
09/08/17 15:01:23 ******************************************************
09/08/17 15:01:23 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:01:23 ** /usr/sbin/condor_gridmanager
09/08/17 15:01:23 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:01:23 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:01:23 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:01:23 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:01:23 ** PID = 22924
09/08/17 15:01:23 ** Log last touched 9/8 14:56:26
09/08/17 15:01:23 ******************************************************
09/08/17 15:01:23 Using config source: /etc/condor-ce/condor_config
09/08/17 15:01:23 Using local config sources: 
09/08/17 15:01:23    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:01:23    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:01:23    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:01:23    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:01:23 config Macros = 176, Sorted = 176, StringBytes = 14912, TablesBytes = 6568
09/08/17 15:01:23 CLASSAD_CACHING is ENABLED
09/08/17 15:01:23 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:01:23 SharedPortEndpoint: waiting for connections to named socket 4789_089c_454
09/08/17 15:01:23 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_454>
09/08/17 15:01:23 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=4789_089c_454>
09/08/17 15:01:26 [22924] Found job 160.0 --- inserting
09/08/17 15:01:26 [22924] Found job 159.0 --- inserting
09/08/17 15:01:26 [22924] gahp server not up yet, delaying ping
09/08/17 15:01:26 [22924] (160.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:01:26 [22924] GAHP server pid = 22927
09/08/17 15:01:26 [22924] (159.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:01:31 [22924] resource  is now up
09/08/17 15:01:31 [22924] (159.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:01:31 [22924] (160.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:01:31 [22924] (159.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 15:01:31 [22924] (160.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 15:01:36 [22924] (159.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 15:01:36 [22924] (160.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 15:02:17 [22924] Got SIGTERM. Performing graceful shutdown.
09/08/17 15:02:17 [22924] **** condor_gridmanager (condor_GRIDMANAGER) pid 22924 EXITING WITH STATUS 0
09/08/17 15:02:27 ******************************************************
09/08/17 15:02:27 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:02:27 ** /usr/sbin/condor_gridmanager
09/08/17 15:02:27 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:02:27 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:02:27 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:02:27 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:02:27 ** PID = 23103
09/08/17 15:02:27 ** Log last touched 9/8 15:02:17
09/08/17 15:02:27 ******************************************************
09/08/17 15:02:27 Using config source: /etc/condor-ce/condor_config
09/08/17 15:02:27 Using local config sources: 
09/08/17 15:02:27    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:02:27    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:02:27    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:02:27    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:02:27 config Macros = 176, Sorted = 176, StringBytes = 14915, TablesBytes = 6568
09/08/17 15:02:27 CLASSAD_CACHING is ENABLED
09/08/17 15:02:27 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:02:27 SharedPortEndpoint: waiting for connections to named socket 23091_72c4_3
09/08/17 15:02:27 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_3>
09/08/17 15:02:27 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_3>
09/08/17 15:02:30 [23103] Found job 160.0 --- inserting
09/08/17 15:02:30 [23103] Found job 159.0 --- inserting
09/08/17 15:02:30 [23103] gahp server not up yet, delaying ping
09/08/17 15:02:30 [23103] (160.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:02:30 [23103] GAHP server pid = 23110
09/08/17 15:02:30 [23103] (159.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:02:35 [23103] resource  is now up
09/08/17 15:02:35 [23103] (160.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:02:35 [23103] (159.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:03:30 [23103] (160.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:03:30 [23103] (159.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:03:31 [23103] (160.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 15:03:32 [23103] (160.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:03:32 [23103] (159.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 15:03:37 [23103] (159.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:03:42 [23103] No jobs left, shutting down
09/08/17 15:03:42 [23103] Got SIGTERM. Performing graceful shutdown.
09/08/17 15:03:42 [23103] **** condor_gridmanager (condor_GRIDMANAGER) pid 23103 EXITING WITH STATUS 0
09/08/17 15:08:05 ******************************************************
09/08/17 15:08:05 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:08:05 ** /usr/sbin/condor_gridmanager
09/08/17 15:08:05 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:08:05 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:08:05 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:08:05 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:08:05 ** PID = 23294
09/08/17 15:08:05 ** Log last touched 9/8 15:03:42
09/08/17 15:08:05 ******************************************************
09/08/17 15:08:05 Using config source: /etc/condor-ce/condor_config
09/08/17 15:08:05 Using local config sources: 
09/08/17 15:08:05    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:08:05    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:08:05    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:08:05    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:08:05 config Macros = 176, Sorted = 176, StringBytes = 14915, TablesBytes = 6568
09/08/17 15:08:05 CLASSAD_CACHING is ENABLED
09/08/17 15:08:05 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:08:05 SharedPortEndpoint: waiting for connections to named socket 23091_72c4_5
09/08/17 15:08:05 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_5>
09/08/17 15:08:05 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_5>
09/08/17 15:08:08 [23294] Found job 164.0 --- inserting
09/08/17 15:08:08 [23294] Found job 163.0 --- inserting
09/08/17 15:08:08 [23294] gahp server not up yet, delaying ping
09/08/17 15:08:08 [23294] (164.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:08:08 [23294] GAHP server pid = 23304
09/08/17 15:08:08 [23294] (163.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:08:13 [23294] resource  is now up
09/08/17 15:08:13 [23294] (163.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:08:13 [23294] (164.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:08:13 [23294] GAHP command 'RESULTS' failed
09/08/17 15:08:13 [23294] ERROR "Gahp Server (pid=23304) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 15:10:44 ******************************************************
09/08/17 15:10:44 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:10:44 ** /usr/sbin/condor_gridmanager
09/08/17 15:10:44 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:10:44 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:10:44 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:10:44 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:10:44 ** PID = 23444
09/08/17 15:10:44 ** Log last touched 9/8 15:08:13
09/08/17 15:10:44 ******************************************************
09/08/17 15:10:44 Using config source: /etc/condor-ce/condor_config
09/08/17 15:10:44 Using local config sources: 
09/08/17 15:10:44    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:10:44    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:10:44    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:10:44    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:10:44 config Macros = 176, Sorted = 176, StringBytes = 14915, TablesBytes = 6568
09/08/17 15:10:44 CLASSAD_CACHING is ENABLED
09/08/17 15:10:44 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:10:44 SharedPortEndpoint: waiting for connections to named socket 23091_72c4_7
09/08/17 15:10:44 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_7>
09/08/17 15:10:44 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_7>
09/08/17 15:10:47 [23444] Found job 164.0 --- inserting
09/08/17 15:10:47 [23444] Found job 163.0 --- inserting
09/08/17 15:10:47 [23444] gahp server not up yet, delaying ping
09/08/17 15:10:47 [23444] (164.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:10:47 [23444] GAHP server pid = 23451
09/08/17 15:10:47 [23444] (163.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:10:47 [23444] GAHP command 'RESULTS' failed
09/08/17 15:10:47 [23444] ERROR "Gahp Server (pid=23451) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 15:10:49 ******************************************************
09/08/17 15:10:49 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:10:49 ** /usr/sbin/condor_gridmanager
09/08/17 15:10:49 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:10:49 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:10:49 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:10:49 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:10:49 ** PID = 23468
09/08/17 15:10:49 ** Log last touched 9/8 15:10:47
09/08/17 15:10:49 ******************************************************
09/08/17 15:10:49 Using config source: /etc/condor-ce/condor_config
09/08/17 15:10:49 Using local config sources: 
09/08/17 15:10:49    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:10:49    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:10:49    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:10:49    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:10:49 config Macros = 176, Sorted = 176, StringBytes = 14915, TablesBytes = 6568
09/08/17 15:10:49 CLASSAD_CACHING is ENABLED
09/08/17 15:10:49 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:10:49 SharedPortEndpoint: waiting for connections to named socket 23091_72c4_8
09/08/17 15:10:49 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_8>
09/08/17 15:10:49 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_8>
09/08/17 15:10:52 [23468] Found job 164.0 --- inserting
09/08/17 15:10:52 [23468] Found job 163.0 --- inserting
09/08/17 15:10:52 [23468] gahp server not up yet, delaying ping
09/08/17 15:10:52 [23468] (164.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:10:52 [23468] GAHP server pid = 23478
09/08/17 15:10:52 [23468] (163.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:10:52 [23468] (163.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 15:10:52 [23468] (164.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 15:10:57 [23468] resource  is now up
09/08/17 15:10:57 [23468] Found job 167.0 --- inserting
09/08/17 15:10:57 [23468] Found job 168.0 --- inserting
09/08/17 15:10:57 [23468] (167.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:10:57 [23468] (168.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:10:57 [23468] (163.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 15:10:57 [23468] (164.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 15:11:02 [23468] (168.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:11:02 [23468] (167.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:11:02 [23468] (168.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 15:11:03 [23468] (167.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 15:11:07 [23468] (168.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 15:11:07 [23468] (167.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 15:11:57 [23468] (163.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:11:57 [23468] (164.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:11:58 [23468] (163.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 15:11:58 [23468] (163.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:11:59 [23468] (164.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 15:12:03 [23468] (164.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:12:07 [23468] (168.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 15:12:07 [23468] (167.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 15:12:07 [23468] (168.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 15:12:08 [23468] (168.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:12:08 [23468] (167.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 15:12:13 [23468] (167.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:12:18 [23468] No jobs left, shutting down
09/08/17 15:12:18 [23468] Got SIGTERM. Performing graceful shutdown.
09/08/17 15:12:18 [23468] **** condor_gridmanager (condor_GRIDMANAGER) pid 23468 EXITING WITH STATUS 0
09/08/17 15:16:06 ******************************************************
09/08/17 15:16:06 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:16:06 ** /usr/sbin/condor_gridmanager
09/08/17 15:16:06 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:16:06 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:16:06 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:16:06 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:16:06 ** PID = 23866
09/08/17 15:16:06 ** Log last touched 9/8 15:12:18
09/08/17 15:16:06 ******************************************************
09/08/17 15:16:06 Using config source: /etc/condor-ce/condor_config
09/08/17 15:16:06 Using local config sources: 
09/08/17 15:16:06    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:16:06    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:16:06    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:16:06    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:16:06 config Macros = 176, Sorted = 176, StringBytes = 14915, TablesBytes = 6568
09/08/17 15:16:06 CLASSAD_CACHING is ENABLED
09/08/17 15:16:06 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:16:06 SharedPortEndpoint: waiting for connections to named socket 23091_72c4_11
09/08/17 15:16:06 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_11>
09/08/17 15:16:06 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_11>
09/08/17 15:16:08 [23866] Found job 172.0 --- inserting
09/08/17 15:16:08 [23866] Found job 171.0 --- inserting
09/08/17 15:16:08 [23866] gahp server not up yet, delaying ping
09/08/17 15:16:08 [23866] (172.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:16:08 [23866] GAHP server pid = 23868
09/08/17 15:16:08 [23866] (171.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 15:16:13 [23866] resource  is now up
09/08/17 15:16:13 [23866] (171.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:16:13 [23866] (172.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 15:16:13 [23866] GAHP command 'RESULTS' failed
09/08/17 15:16:13 [23866] GAHP command 'RESULTS' failed
09/08/17 15:16:13 [23866] ERROR "Gahp Server (pid=23868) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 15:21:06 ******************************************************
09/08/17 15:21:06 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:21:06 ** /usr/sbin/condor_gridmanager
09/08/17 15:21:06 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:21:06 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:21:06 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:21:06 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:21:06 ** PID = 23993
09/08/17 15:21:06 ** Log last touched 9/8 15:16:13
09/08/17 15:21:06 ******************************************************
09/08/17 15:21:06 Using config source: /etc/condor-ce/condor_config
09/08/17 15:21:06 Using local config sources: 
09/08/17 15:21:06    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:21:06    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:21:06    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:21:06    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:21:06 config Macros = 176, Sorted = 176, StringBytes = 14915, TablesBytes = 6568
09/08/17 15:21:06 CLASSAD_CACHING is ENABLED
09/08/17 15:21:06 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:21:06 SharedPortEndpoint: waiting for connections to named socket 23091_72c4_14
09/08/17 15:21:06 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_14>
09/08/17 15:21:06 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_14>
09/08/17 15:21:09 [23993] Found job 172.0 --- inserting
09/08/17 15:21:09 [23993] Found job 171.0 --- inserting
09/08/17 15:21:09 [23993] gahp server not up yet, delaying ping
09/08/17 15:21:09 [23993] (172.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:21:09 [23993] GAHP server pid = 23995
09/08/17 15:21:09 [23993] (171.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:21:09 [23993] GAHP command 'RESULTS' failed
09/08/17 15:21:09 [23993] ERROR "Gahp Server (pid=23995) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 15:26:06 ******************************************************
09/08/17 15:26:06 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:26:06 ** /usr/sbin/condor_gridmanager
09/08/17 15:26:06 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:26:06 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:26:06 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:26:06 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:26:06 ** PID = 24082
09/08/17 15:26:06 ** Log last touched 9/8 15:21:09
09/08/17 15:26:06 ******************************************************
09/08/17 15:26:06 Using config source: /etc/condor-ce/condor_config
09/08/17 15:26:06 Using local config sources: 
09/08/17 15:26:06    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:26:06    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:26:06    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:26:06    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:26:06 config Macros = 176, Sorted = 176, StringBytes = 14915, TablesBytes = 6568
09/08/17 15:26:06 CLASSAD_CACHING is ENABLED
09/08/17 15:26:06 Daemon Log is logging: D_ALWAYS D_ERROR
09/08/17 15:26:06 SharedPortEndpoint: waiting for connections to named socket 23091_72c4_16
09/08/17 15:26:06 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_16>
09/08/17 15:26:06 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=23091_72c4_16>
09/08/17 15:26:09 [24082] Found job 172.0 --- inserting
09/08/17 15:26:09 [24082] Found job 171.0 --- inserting
09/08/17 15:26:09 [24082] gahp server not up yet, delaying ping
09/08/17 15:26:09 [24082] (172.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:26:09 [24082] GAHP server pid = 24085
09/08/17 15:26:09 [24082] (171.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:26:09 [24082] GAHP command 'RESULTS' failed
09/08/17 15:26:09 [24082] ERROR "Gahp Server (pid=24085) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 15:29:50 Result of reading /etc/issue:  \S
 
09/08/17 15:29:50 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 15:29:50 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 15:29:50 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 15:29:50 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 15:29:50 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 15:29:50 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 15:29:50 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 15:29:50 ******************************************************
09/08/17 15:29:50 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 15:29:50 ** /usr/sbin/condor_gridmanager
09/08/17 15:29:50 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 15:29:50 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 15:29:50 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 15:29:50 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 15:29:50 ** PID = 24213
09/08/17 15:29:50 ** Log last touched 9/8 15:26:09
09/08/17 15:29:50 ******************************************************
09/08/17 15:29:50 Using config source: /etc/condor-ce/condor_config
09/08/17 15:29:50 Using local config sources: 
09/08/17 15:29:50    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 15:29:50    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 15:29:50    /etc/condor-ce/config.d/99-local.conf
09/08/17 15:29:50    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 15:29:50 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 15:29:50 CLASSAD_CACHING is ENABLED
09/08/17 15:29:50 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 15:29:50 SharedPortEndpoint: waiting for connections to named socket 24200_d6d1_3
09/08/17 15:29:50 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_3>
09/08/17 15:29:50 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_3>
09/08/17 15:29:50 Setting maximum accepts per cycle 8.
09/08/17 15:29:50 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 15:29:50 [24213] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 15:29:50 [24213] DaemonCore: No more children processes to reap.
09/08/17 15:29:50 [24213] DaemonCore: in SendAliveToParent()
09/08/17 15:29:50 [24213] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:50 [24213] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:50 [24213] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:50 [24213] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:50 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:50 [24213] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29224>
09/08/17 15:29:50 [24213] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 15:29:50 [24213] Checking proxies
09/08/17 15:29:53 [24213] Received ADD_JOBS signal
09/08/17 15:29:53 [24213] in doContactSchedd()
09/08/17 15:29:53 [24213] querying for new jobs
09/08/17 15:29:53 [24213] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 15:29:53 [24213] Using job type INFNBatch for job 171.0
09/08/17 15:29:53 [24213] (171.0) SetJobLeaseTimers()
09/08/17 15:29:53 [24213] Found job 171.0 --- inserting
09/08/17 15:29:53 [24213] Using job type INFNBatch for job 172.0
09/08/17 15:29:53 [24213] (172.0) SetJobLeaseTimers()
09/08/17 15:29:53 [24213] Found job 172.0 --- inserting
09/08/17 15:29:53 [24213] Fetched 2 new job ads from schedd
09/08/17 15:29:53 [24213] querying for removed/held jobs
09/08/17 15:29:53 [24213] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 15:29:53 [24213] Fetched 0 job ads from schedd
09/08/17 15:29:53 [24213] leaving doContactSchedd()
09/08/17 15:29:53 [24213] gahp server not up yet, delaying ping
09/08/17 15:29:53 [24213] *** UpdateLeases called
09/08/17 15:29:53 [24213]     Leases not supported, cancelling timer
09/08/17 15:29:53 [24213] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504909793
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29224>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 15:29:53 [24213] Trying to update collector <128.55.162.46:9619>
09/08/17 15:29:53 [24213] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 15:29:53 [24213] File descriptor limits: max 4096, safe 3277
09/08/17 15:29:53 [24213] (171.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:29:53 [24213] GAHP server pid = 24219
09/08/17 15:29:53 [24213] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 15:29:53 [24213] GAHP[24219] <- 'COMMANDS'
09/08/17 15:29:53 [24213] GAHP[24219] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 15:29:53 [24213] GAHP[24219] <- 'ASYNC_MODE_ON'
09/08/17 15:29:53 [24213] GAHP[24219] -> 'S' 'Async mode on'
09/08/17 15:29:53 [24213] (171.0) gm state change: GM_INIT -> GM_START
09/08/17 15:29:53 [24213] (171.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 15:29:53 [24213] (171.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 15:29:53 [24213] GAHP[24219] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#171.0#1504908962";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/170/0/cluster170.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/170/0/cluster170.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/170/0/cluster170.proc0.subproc0/test.sh"\ ]'
09/08/17 15:29:53 [24213] GAHP[24219] -> 'S'
09/08/17 15:29:53 [24213] (172.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 15:29:53 [24213] (172.0) gm state change: GM_INIT -> GM_START
09/08/17 15:29:53 [24213] (172.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 15:29:53 [24213] (172.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 15:29:53 [24213] GAHP[24219] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#172.0#1504908962";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/169/0/cluster169.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/169/0/cluster169.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/169/0/cluster169.proc0.subproc0/test.sh"\ ]'
09/08/17 15:29:53 [24213] GAHP[24219] -> 'S'
09/08/17 15:29:53 [24213] This process has a valid certificate & key
09/08/17 15:29:53 [24213] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:53 [24213] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:53 [24213] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:53 [24213] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 15:29:53 [24213] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 15:29:53 [24213] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 15:29:53 [24213] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:53 [24213] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:53 [24213] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:53 [24213] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 15:29:53 [24213] IPVERIFY: ip found is 1
09/08/17 15:29:54 [24213] GAHP[24219] <- 'RESULTS'
09/08/17 15:29:54 [24213] GAHP[24219] -> 'R'
09/08/17 15:29:54 [24213] GAHP[24219] -> 'S' '1'
09/08/17 15:29:54 [24213] GAHP[24219] -> '2' '0' 'No error' 'slurm/20170908/144157'
09/08/17 15:29:54 [24213] (171.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 15:29:54 [24213] (171.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 15:29:55 [24213] Evaluating staleness of remote job statuses.
09/08/17 15:29:55 [24213] GAHP[24219] <- 'RESULTS'
09/08/17 15:29:55 [24213] GAHP[24219] -> 'R'
09/08/17 15:29:55 [24213] GAHP[24219] -> 'S' '1'
09/08/17 15:29:55 [24213] GAHP[24219] -> '3' '0' 'No error' 'slurm/20170908/144158'
09/08/17 15:29:55 [24213] (172.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 15:29:55 [24213] (172.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 15:29:58 [24213] resource  is now up
09/08/17 15:29:58 [24213] in doContactSchedd()
09/08/17 15:29:58 [24213] querying for removed/held jobs
09/08/17 15:29:58 [24213] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 15:29:58 [24213] Fetched 0 job ads from schedd
09/08/17 15:29:58 [24213] Updating classad values for 171.0:
09/08/17 15:29:58 [24213]    DelegatedProxyExpiration = 1505103329
09/08/17 15:29:58 [24213]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#171.0#1504908962 slurm/20170908/144157"
09/08/17 15:29:58 [24213] Updating classad values for 172.0:
09/08/17 15:29:58 [24213]    DelegatedProxyExpiration = 1505103329
09/08/17 15:29:58 [24213]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#172.0#1504908962 slurm/20170908/144158"
09/08/17 15:29:58 [24213] leaving doContactSchedd()
09/08/17 15:29:58 [24213] (171.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 15:29:58 [24213] (171.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 15:29:58 [24213] (172.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 15:29:58 [24213] (172.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 15:30:50 [24213] Received CHECK_LEASES signal
09/08/17 15:30:50 [24213] in doContactSchedd()
09/08/17 15:30:50 [24213] querying for renewed leases
09/08/17 15:30:50 [24213] querying for removed/held jobs
09/08/17 15:30:50 [24213] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 15:30:50 [24213] Fetched 0 job ads from schedd
09/08/17 15:30:50 [24213] leaving doContactSchedd()
09/08/17 15:30:53 [24213] GAHP[24219] <- 'RESULTS'
09/08/17 15:30:53 [24213] GAHP[24219] -> 'S' '0'
09/08/17 15:30:55 [24213] Evaluating staleness of remote job statuses.
09/08/17 15:30:58 [24213] (171.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:30:58 [24213] (171.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 15:30:58 [24213] GAHP[24219] <- 'BLAH_JOB_STATUS 4 slurm/20170908/144157'
09/08/17 15:30:58 [24213] GAHP[24219] -> 'S'
09/08/17 15:30:58 [24213] (172.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 15:30:58 [24213] (172.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 15:30:58 [24213] GAHP[24219] <- 'BLAH_JOB_STATUS 5 slurm/20170908/144158'
09/08/17 15:30:58 [24213] GAHP[24219] -> 'S'
09/08/17 15:30:58 [24213] GAHP[24219] <- 'RESULTS'
09/08/17 15:30:58 [24213] GAHP[24219] -> 'R'
09/08/17 15:30:58 [24213] GAHP[24219] -> 'S' '1'
09/08/17 15:30:58 [24213] GAHP[24219] -> '5' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144158"; ImageSize = 0; WorkerNode = "mc1533"; RemoteUserCpu = 0 ]'
09/08/17 15:30:58 [24213] (172.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 15:30:58 [24213] (172.0) ***ProcessRemoteAd
09/08/17 15:30:58 [24213] (172.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 15:30:58 [24213] (172.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 15:30:58 [24213] (172.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 15:30:58 [24213] in doContactSchedd()
09/08/17 15:30:58 [24213] querying for removed/held jobs
09/08/17 15:30:58 [24213] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 15:30:58 [24213] Fetched 0 job ads from schedd
09/08/17 15:30:58 [24213] Updating classad values for 172.0:
09/08/17 15:30:58 [24213]    EnteredCurrentStatus = 1504909858
09/08/17 15:30:58 [24213]    ExitCode = 0
09/08/17 15:30:58 [24213]    GridJobStatus = "COMPLETED"
09/08/17 15:30:58 [24213]    ImageSize = 0
09/08/17 15:30:58 [24213]    JobStatus = 4
09/08/17 15:30:58 [24213]    LastRemoteStatusUpdate = 1504909858
09/08/17 15:30:58 [24213]    RemoteUserCpu = 0
09/08/17 15:30:58 [24213]    RemoteWallClockTime = 0.0
09/08/17 15:30:58 [24213] leaving doContactSchedd()
09/08/17 15:30:58 [24213] (172.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:30:58 [24213] (172.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 15:30:58 [24213] (172.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 15:30:58 [24213] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 15:30:58 [24213] (172.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 15:30:58 [24213] GAHP[24219] <- 'RESULTS'
09/08/17 15:30:58 [24213] GAHP[24219] -> 'R'
09/08/17 15:30:58 [24213] GAHP[24219] -> 'S' '1'
09/08/17 15:30:58 [24213] GAHP[24219] -> '4' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144157"; ImageSize = 0; WorkerNode = "mc1532"; RemoteUserCpu = 0 ]'
09/08/17 15:30:58 [24213] (171.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 15:30:58 [24213] (171.0) ***ProcessRemoteAd
09/08/17 15:30:58 [24213] (171.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 15:30:58 [24213] (171.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 15:30:58 [24213] (171.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 15:31:03 [24213] in doContactSchedd()
09/08/17 15:31:03 [24213] querying for removed/held jobs
09/08/17 15:31:03 [24213] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 15:31:03 [24213] Fetched 1 job ads from schedd
09/08/17 15:31:03 [24213] Updating classad values for 171.0:
09/08/17 15:31:03 [24213]    EnteredCurrentStatus = 1504909858
09/08/17 15:31:03 [24213]    ExitCode = 0
09/08/17 15:31:03 [24213]    GridJobStatus = "COMPLETED"
09/08/17 15:31:03 [24213]    ImageSize = 0
09/08/17 15:31:03 [24213]    JobStatus = 4
09/08/17 15:31:03 [24213]    LastRemoteStatusUpdate = 1504909858
09/08/17 15:31:03 [24213]    RemoteUserCpu = 0
09/08/17 15:31:03 [24213]    RemoteWallClockTime = 0.0
09/08/17 15:31:03 [24213] Updating classad values for 172.0:
09/08/17 15:31:03 [24213]    CurrentStatusUnknown = false
09/08/17 15:31:03 [24213]    GridJobId = undefined
09/08/17 15:31:03 [24213]    LastRemoteStatusUpdate = 0
09/08/17 15:31:03 [24213]    Managed = "ScheddDone"
09/08/17 15:31:03 [24213] Deleting job 172.0 from schedd
09/08/17 15:31:03 [24213] leaving doContactSchedd()
09/08/17 15:31:03 [24213] (171.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 15:31:03 [24213] (171.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 15:31:03 [24213] (171.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 15:31:03 [24213] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 15:31:03 [24213] (171.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 15:31:08 [24213] in doContactSchedd()
09/08/17 15:31:08 [24213] querying for removed/held jobs
09/08/17 15:31:08 [24213] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 15:31:08 [24213] Fetched 1 job ads from schedd
09/08/17 15:31:08 [24213] Updating classad values for 171.0:
09/08/17 15:31:08 [24213]    CurrentStatusUnknown = false
09/08/17 15:31:08 [24213]    GridJobId = undefined
09/08/17 15:31:08 [24213]    LastRemoteStatusUpdate = 0
09/08/17 15:31:08 [24213]    Managed = "ScheddDone"
09/08/17 15:31:08 [24213] Deleting job 171.0 from schedd
09/08/17 15:31:08 [24213] No jobs left, shutting down
09/08/17 15:31:08 [24213] leaving doContactSchedd()
09/08/17 15:31:08 [24213] Got SIGTERM. Performing graceful shutdown.
09/08/17 15:31:08 [24213] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 15:31:08 [24213] **** condor_gridmanager (condor_GRIDMANAGER) pid 24213 EXITING WITH STATUS 0
09/08/17 16:00:32 Result of reading /etc/issue:  \S
 
09/08/17 16:00:32 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:00:32 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:00:32 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:00:32 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:00:32 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:00:32 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:00:32 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:00:32 ******************************************************
09/08/17 16:00:32 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:00:32 ** /usr/sbin/condor_gridmanager
09/08/17 16:00:32 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:00:32 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:00:32 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:00:32 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:00:32 ** PID = 24760
09/08/17 16:00:32 ** Log last touched 9/8 15:31:08
09/08/17 16:00:32 ******************************************************
09/08/17 16:00:32 Using config source: /etc/condor-ce/condor_config
09/08/17 16:00:32 Using local config sources: 
09/08/17 16:00:32    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:00:32    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:00:32    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:00:32    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:00:32 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:00:32 CLASSAD_CACHING is ENABLED
09/08/17 16:00:32 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:00:32 SharedPortEndpoint: waiting for connections to named socket 24200_d6d1_14
09/08/17 16:00:32 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_14>
09/08/17 16:00:32 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_14>
09/08/17 16:00:32 Setting maximum accepts per cycle 8.
09/08/17 16:00:32 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:00:32 [24760] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:00:32 [24760] DaemonCore: No more children processes to reap.
09/08/17 16:00:32 [24760] DaemonCore: in SendAliveToParent()
09/08/17 16:00:32 [24760] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:32 [24760] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:32 [24760] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:32 [24760] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:32 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:32 [24760] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29224>
09/08/17 16:00:32 [24760] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:00:32 [24760] Checking proxies
09/08/17 16:00:35 [24760] Received ADD_JOBS signal
09/08/17 16:00:35 [24760] in doContactSchedd()
09/08/17 16:00:35 [24760] querying for new jobs
09/08/17 16:00:35 [24760] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 255.0
09/08/17 16:00:35 [24760] (255.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 255.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 218.0
09/08/17 16:00:35 [24760] (218.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 218.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 254.0
09/08/17 16:00:35 [24760] (254.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 254.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 217.0
09/08/17 16:00:35 [24760] (217.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 217.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 253.0
09/08/17 16:00:35 [24760] (253.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 253.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 216.0
09/08/17 16:00:35 [24760] (216.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 216.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 252.0
09/08/17 16:00:35 [24760] (252.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 252.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 215.0
09/08/17 16:00:35 [24760] (215.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 215.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 251.0
09/08/17 16:00:35 [24760] (251.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 251.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 250.0
09/08/17 16:00:35 [24760] (250.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 250.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 249.0
09/08/17 16:00:35 [24760] (249.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 249.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 248.0
09/08/17 16:00:35 [24760] (248.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 248.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 247.0
09/08/17 16:00:35 [24760] (247.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 247.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 246.0
09/08/17 16:00:35 [24760] (246.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 246.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 245.0
09/08/17 16:00:35 [24760] (245.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 245.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 244.0
09/08/17 16:00:35 [24760] (244.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 244.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 243.0
09/08/17 16:00:35 [24760] (243.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 243.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 242.0
09/08/17 16:00:35 [24760] (242.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 242.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 241.0
09/08/17 16:00:35 [24760] (241.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 241.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 240.0
09/08/17 16:00:35 [24760] (240.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 240.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 239.0
09/08/17 16:00:35 [24760] (239.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 239.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 238.0
09/08/17 16:00:35 [24760] (238.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 238.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 237.0
09/08/17 16:00:35 [24760] (237.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 237.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 236.0
09/08/17 16:00:35 [24760] (236.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 236.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 235.0
09/08/17 16:00:35 [24760] (235.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 235.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 234.0
09/08/17 16:00:35 [24760] (234.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 234.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 233.0
09/08/17 16:00:35 [24760] (233.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 233.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 232.0
09/08/17 16:00:35 [24760] (232.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 232.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 231.0
09/08/17 16:00:35 [24760] (231.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 231.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 230.0
09/08/17 16:00:35 [24760] (230.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 230.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 229.0
09/08/17 16:00:35 [24760] (229.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 229.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 228.0
09/08/17 16:00:35 [24760] (228.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 228.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 227.0
09/08/17 16:00:35 [24760] (227.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 227.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 226.0
09/08/17 16:00:35 [24760] (226.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 226.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 225.0
09/08/17 16:00:35 [24760] (225.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 225.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 224.0
09/08/17 16:00:35 [24760] (224.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 224.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 223.0
09/08/17 16:00:35 [24760] (223.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 223.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 222.0
09/08/17 16:00:35 [24760] (222.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 222.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 221.0
09/08/17 16:00:35 [24760] (221.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 221.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 220.0
09/08/17 16:00:35 [24760] (220.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 220.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 256.0
09/08/17 16:00:35 [24760] (256.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 256.0 --- inserting
09/08/17 16:00:35 [24760] Using job type INFNBatch for job 219.0
09/08/17 16:00:35 [24760] (219.0) SetJobLeaseTimers()
09/08/17 16:00:35 [24760] Found job 219.0 --- inserting
09/08/17 16:00:35 [24760] Fetched 42 new job ads from schedd
09/08/17 16:00:35 [24760] querying for removed/held jobs
09/08/17 16:00:35 [24760] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:00:35 [24760] Fetched 0 job ads from schedd
09/08/17 16:00:35 [24760] leaving doContactSchedd()
09/08/17 16:00:35 [24760] gahp server not up yet, delaying ping
09/08/17 16:00:35 [24760] *** UpdateLeases called
09/08/17 16:00:35 [24760]     Leases not supported, cancelling timer
09/08/17 16:00:35 [24760] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_14\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_14\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_14>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504911635
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29224>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:00:35 [24760] Trying to update collector <128.55.162.46:9619>
09/08/17 16:00:35 [24760] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:00:35 [24760] File descriptor limits: max 4096, safe 3277
09/08/17 16:00:35 [24760] (255.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] GAHP server pid = 24772
09/08/17 16:00:35 [24760] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:00:35 [24760] GAHP[24772] <- 'COMMANDS'
09/08/17 16:00:35 [24760] GAHP[24772] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:00:35 [24760] GAHP[24772] <- 'ASYNC_MODE_ON'
09/08/17 16:00:35 [24760] GAHP[24772] -> 'S' 'Async mode on'
09/08/17 16:00:35 [24760] (255.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (255.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (255.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (255.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (218.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (218.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (218.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (218.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (218.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (254.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (254.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (254.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (254.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (254.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (217.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (217.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (217.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (217.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (217.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (253.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (253.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (253.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (253.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (253.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] This process has a valid certificate & key
09/08/17 16:00:35 [24760] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:00:35 [24760] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:00:35 [24760] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:00:35 [24760] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:00:35 [24760] IPVERIFY: ip found is 1
09/08/17 16:00:35 [24760] (216.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (216.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (216.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (216.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (216.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (252.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (252.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (252.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (252.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (252.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (215.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (215.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (215.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (215.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (215.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (251.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (251.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (251.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (251.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (251.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (250.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (250.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (250.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (250.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (250.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (249.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (249.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (249.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (249.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (249.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (248.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (248.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (248.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (248.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (248.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (247.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (247.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (247.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (247.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (247.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (246.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (246.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (246.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (246.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (246.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (245.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (245.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (245.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (245.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (245.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (244.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (244.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (244.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (244.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (244.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (243.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (243.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (243.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (243.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (243.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (242.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (242.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (242.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (242.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (242.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (241.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (241.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (241.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (241.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (241.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (240.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (240.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (240.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (240.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (240.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (239.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (239.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (239.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (239.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (239.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (238.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (238.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (238.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (238.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (238.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (237.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (237.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (237.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (237.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (237.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (236.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (236.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (236.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (236.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (236.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (235.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (235.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (235.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (235.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (235.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (234.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (234.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (234.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (234.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (234.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (233.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (233.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (233.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (233.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (233.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (232.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (232.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (232.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (232.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (232.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (231.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (231.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (231.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (231.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (231.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (230.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (230.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (230.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (230.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (230.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (229.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (229.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (229.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (229.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (229.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (228.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (228.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (228.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (228.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (228.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (227.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (227.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (227.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (227.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (227.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (226.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (226.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (226.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (226.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (226.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (225.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (225.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (225.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (225.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (225.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (224.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (224.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (224.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (224.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (224.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (223.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (223.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (223.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (223.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (223.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (222.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (222.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (222.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (222.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (222.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (221.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (221.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (221.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (221.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (221.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (220.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (220.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (220.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (220.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (220.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (256.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (256.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (256.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (256.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (256.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:35 [24760] (219.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:00:35 [24760] (219.0) gm state change: GM_INIT -> GM_START
09/08/17 16:00:35 [24760] (219.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:00:35 [24760] (219.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:00:35 [24760] (219.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:00:37 [24760] Evaluating staleness of remote job statuses.
09/08/17 16:00:40 [24760] resource  is now up
09/08/17 16:00:40 [24760] in doContactSchedd()
09/08/17 16:00:40 [24760] querying for removed/held jobs
09/08/17 16:00:40 [24760] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:00:40 [24760] Fetched 0 job ads from schedd
09/08/17 16:00:40 [24760] Updating classad values for 252.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#252.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 253.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#253.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 254.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 255.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 256.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#256.0#1504911628"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 215.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#215.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 216.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#216.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 217.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#217.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 218.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#218.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 219.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#219.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 220.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#220.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 221.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#221.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 222.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#222.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 223.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#223.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 224.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#224.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 225.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#225.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 226.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#226.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 227.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#227.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 228.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#228.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 229.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#229.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 230.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#230.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 231.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#231.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 232.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#232.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 233.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#233.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 234.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#234.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 235.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#235.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 236.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#236.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 237.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#237.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 238.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#238.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 239.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#239.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 240.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#240.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 241.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#241.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 242.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#242.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 243.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#243.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 244.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#244.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 245.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#245.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 246.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#246.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 247.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#247.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 248.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#248.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 249.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#249.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 250.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#250.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] Updating classad values for 251.0:
09/08/17 16:00:40 [24760]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#251.0#1504911627"
09/08/17 16:00:40 [24760]    LastRemoteStatusUpdate = 1504911635
09/08/17 16:00:40 [24760] leaving doContactSchedd()
09/08/17 16:00:40 [24760] (252.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:00:40 [24760] (252.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:00:40 [24760] (252.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:00:40 [24760] GAHP[24772] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#252.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/174/0/cluster174.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/174/0/cluster174.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/174/0/cluster174.proc0.subproc0/test.sh"\ ]'
09/08/17 16:00:40 [24760] GAHP[24772] -> 'S'
09/08/17 16:00:40 [24760] (253.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:00:40 [24760] (253.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:00:40 [24760] (253.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:00:40 [24760] GAHP[24772] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#253.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/test.sh"\ ]'
09/08/17 16:00:40 [24760] GAHP[24772] -> 'S'
09/08/17 16:00:40 [24760] (254.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:00:40 [24760] (254.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:00:40 [24760] (254.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:00:40 [24760] GAHP[24772] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/test.sh"\ ]'
09/08/17 16:00:40 [24760] GAHP[24772] -> 'S'
09/08/17 16:00:40 [24760] (255.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:00:40 [24760] (255.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:00:40 [24760] (255.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:00:40 [24760] GAHP[24772] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/test.sh"\ ]'
09/08/17 16:00:40 [24760] GAHP[24772] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:00:40 [24760] GAHP[24772] -> EOF
09/08/17 16:00:40 [24760] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:05:32 Result of reading /etc/issue:  \S
 
09/08/17 16:05:32 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:05:32 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:05:32 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:05:32 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:05:32 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:05:32 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:05:32 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:05:32 ******************************************************
09/08/17 16:05:32 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:05:32 ** /usr/sbin/condor_gridmanager
09/08/17 16:05:32 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:05:32 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:05:32 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:05:32 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:05:32 ** PID = 25135
09/08/17 16:05:32 ** Log last touched 9/8 16:00:40
09/08/17 16:05:32 ******************************************************
09/08/17 16:05:32 Using config source: /etc/condor-ce/condor_config
09/08/17 16:05:32 Using local config sources: 
09/08/17 16:05:32    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:05:32    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:05:32    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:05:32    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:05:32 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:05:32 CLASSAD_CACHING is ENABLED
09/08/17 16:05:32 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:05:32 SharedPortEndpoint: waiting for connections to named socket 24200_d6d1_16
09/08/17 16:05:32 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_16>
09/08/17 16:05:32 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_16>
09/08/17 16:05:32 Setting maximum accepts per cycle 8.
09/08/17 16:05:32 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:05:32 [25135] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:05:32 [25135] DaemonCore: No more children processes to reap.
09/08/17 16:05:32 [25135] DaemonCore: in SendAliveToParent()
09/08/17 16:05:32 [25135] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: ip found is 1
09/08/17 16:05:32 [25135] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: ip found is 1
09/08/17 16:05:32 [25135] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: ip found is 1
09/08/17 16:05:32 [25135] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:05:32 [25135] IPVERIFY: ip found is 1
09/08/17 16:05:32 [25135] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29224>
09/08/17 16:05:32 [25135] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:05:32 [25135] Checking proxies
09/08/17 16:05:35 [25135] Received ADD_JOBS signal
09/08/17 16:05:35 [25135] in doContactSchedd()
09/08/17 16:05:35 [25135] querying for new jobs
09/08/17 16:05:35 [25135] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 255.0
09/08/17 16:05:35 [25135] (255.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 255.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 218.0
09/08/17 16:05:35 [25135] (218.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 218.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 254.0
09/08/17 16:05:35 [25135] (254.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 254.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 217.0
09/08/17 16:05:35 [25135] (217.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 217.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 253.0
09/08/17 16:05:35 [25135] (253.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 253.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 216.0
09/08/17 16:05:35 [25135] (216.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 216.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 252.0
09/08/17 16:05:35 [25135] (252.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 252.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 215.0
09/08/17 16:05:35 [25135] (215.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 215.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 251.0
09/08/17 16:05:35 [25135] (251.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 251.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 250.0
09/08/17 16:05:35 [25135] (250.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 250.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 249.0
09/08/17 16:05:35 [25135] (249.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 249.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 248.0
09/08/17 16:05:35 [25135] (248.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 248.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 247.0
09/08/17 16:05:35 [25135] (247.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 247.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 246.0
09/08/17 16:05:35 [25135] (246.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 246.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 245.0
09/08/17 16:05:35 [25135] (245.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 245.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 244.0
09/08/17 16:05:35 [25135] (244.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 244.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 243.0
09/08/17 16:05:35 [25135] (243.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 243.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 242.0
09/08/17 16:05:35 [25135] (242.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 242.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 241.0
09/08/17 16:05:35 [25135] (241.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 241.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 240.0
09/08/17 16:05:35 [25135] (240.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 240.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 239.0
09/08/17 16:05:35 [25135] (239.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 239.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 238.0
09/08/17 16:05:35 [25135] (238.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 238.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 237.0
09/08/17 16:05:35 [25135] (237.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 237.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 236.0
09/08/17 16:05:35 [25135] (236.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 236.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 235.0
09/08/17 16:05:35 [25135] (235.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 235.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 234.0
09/08/17 16:05:35 [25135] (234.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 234.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 233.0
09/08/17 16:05:35 [25135] (233.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 233.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 232.0
09/08/17 16:05:35 [25135] (232.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 232.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 231.0
09/08/17 16:05:35 [25135] (231.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 231.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 230.0
09/08/17 16:05:35 [25135] (230.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 230.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 229.0
09/08/17 16:05:35 [25135] (229.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 229.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 228.0
09/08/17 16:05:35 [25135] (228.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 228.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 227.0
09/08/17 16:05:35 [25135] (227.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 227.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 226.0
09/08/17 16:05:35 [25135] (226.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 226.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 225.0
09/08/17 16:05:35 [25135] (225.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 225.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 224.0
09/08/17 16:05:35 [25135] (224.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 224.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 223.0
09/08/17 16:05:35 [25135] (223.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 223.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 222.0
09/08/17 16:05:35 [25135] (222.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 222.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 221.0
09/08/17 16:05:35 [25135] (221.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 221.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 220.0
09/08/17 16:05:35 [25135] (220.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 220.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 256.0
09/08/17 16:05:35 [25135] (256.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 256.0 --- inserting
09/08/17 16:05:35 [25135] Using job type INFNBatch for job 219.0
09/08/17 16:05:35 [25135] (219.0) SetJobLeaseTimers()
09/08/17 16:05:35 [25135] Found job 219.0 --- inserting
09/08/17 16:05:35 [25135] Fetched 42 new job ads from schedd
09/08/17 16:05:35 [25135] querying for removed/held jobs
09/08/17 16:05:35 [25135] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:05:35 [25135] Fetched 0 job ads from schedd
09/08/17 16:05:35 [25135] leaving doContactSchedd()
09/08/17 16:05:35 [25135] gahp server not up yet, delaying ping
09/08/17 16:05:35 [25135] *** UpdateLeases called
09/08/17 16:05:35 [25135]     Leases not supported, cancelling timer
09/08/17 16:05:35 [25135] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_16\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_16\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_16>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504911935
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29224>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:05:35 [25135] Trying to update collector <128.55.162.46:9619>
09/08/17 16:05:35 [25135] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:05:35 [25135] File descriptor limits: max 4096, safe 3277
09/08/17 16:05:35 [25135] (255.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:05:35 [25135] GAHP server pid = 25143
09/08/17 16:05:35 [25135] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:05:35 [25135] GAHP[25143] <- 'COMMANDS'
09/08/17 16:05:35 [25135] GAHP[25143] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:05:35 [25135] GAHP[25143] <- 'ASYNC_MODE_ON'
09/08/17 16:05:35 [25135] GAHP[25143] -> 'S' 'Async mode on'
09/08/17 16:05:35 [25135] (255.0) gm state change: GM_INIT -> GM_START
09/08/17 16:05:35 [25135] (255.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:05:35 [25135] (255.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:05:35 [25135] GAHP[25143] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/test.sh"\ ]'
09/08/17 16:05:35 [25135] GAHP[25143] -> 'S'
09/08/17 16:05:35 [25135] (218.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:05:35 [25135] (218.0) gm state change: GM_INIT -> GM_START
09/08/17 16:05:35 [25135] (218.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:05:35 [25135] (218.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:05:35 [25135] GAHP[25143] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#218.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/test.sh"\ ]'
09/08/17 16:05:35 [25135] GAHP[25143] -> 'S'
09/08/17 16:05:35 [25135] (254.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:05:35 [25135] (254.0) gm state change: GM_INIT -> GM_START
09/08/17 16:05:35 [25135] (254.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:05:35 [25135] (254.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:05:35 [25135] GAHP[25143] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/test.sh"\ ]'
09/08/17 16:05:35 [25135] GAHP[25143] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:05:35 [25135] GAHP[25143] -> EOF
09/08/17 16:05:35 [25135] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:10:32 Result of reading /etc/issue:  \S
 
09/08/17 16:10:32 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:10:32 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:10:32 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:10:32 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:10:32 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:10:32 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:10:32 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:10:32 ******************************************************
09/08/17 16:10:32 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:10:32 ** /usr/sbin/condor_gridmanager
09/08/17 16:10:32 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:10:32 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:10:32 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:10:32 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:10:32 ** PID = 25318
09/08/17 16:10:32 ** Log last touched 9/8 16:05:35
09/08/17 16:10:32 ******************************************************
09/08/17 16:10:32 Using config source: /etc/condor-ce/condor_config
09/08/17 16:10:32 Using local config sources: 
09/08/17 16:10:32    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:10:32    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:10:32    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:10:32    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:10:32 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:10:32 CLASSAD_CACHING is ENABLED
09/08/17 16:10:32 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:10:32 SharedPortEndpoint: waiting for connections to named socket 24200_d6d1_19
09/08/17 16:10:32 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_19>
09/08/17 16:10:32 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_19>
09/08/17 16:10:32 Setting maximum accepts per cycle 8.
09/08/17 16:10:32 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:10:32 [25318] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:10:32 [25318] DaemonCore: No more children processes to reap.
09/08/17 16:10:32 [25318] DaemonCore: in SendAliveToParent()
09/08/17 16:10:32 [25318] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: ip found is 1
09/08/17 16:10:32 [25318] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: ip found is 1
09/08/17 16:10:32 [25318] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: ip found is 1
09/08/17 16:10:32 [25318] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:10:32 [25318] IPVERIFY: ip found is 1
09/08/17 16:10:32 [25318] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29224>
09/08/17 16:10:32 [25318] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:10:32 [25318] Checking proxies
09/08/17 16:10:35 [25318] Received ADD_JOBS signal
09/08/17 16:10:35 [25318] in doContactSchedd()
09/08/17 16:10:35 [25318] querying for new jobs
09/08/17 16:10:35 [25318] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 255.0
09/08/17 16:10:35 [25318] (255.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 255.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 218.0
09/08/17 16:10:35 [25318] (218.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 218.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 254.0
09/08/17 16:10:35 [25318] (254.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 254.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 217.0
09/08/17 16:10:35 [25318] (217.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 217.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 253.0
09/08/17 16:10:35 [25318] (253.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 253.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 216.0
09/08/17 16:10:35 [25318] (216.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 216.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 252.0
09/08/17 16:10:35 [25318] (252.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 252.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 215.0
09/08/17 16:10:35 [25318] (215.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 215.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 251.0
09/08/17 16:10:35 [25318] (251.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 251.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 250.0
09/08/17 16:10:35 [25318] (250.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 250.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 249.0
09/08/17 16:10:35 [25318] (249.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 249.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 248.0
09/08/17 16:10:35 [25318] (248.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 248.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 247.0
09/08/17 16:10:35 [25318] (247.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 247.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 246.0
09/08/17 16:10:35 [25318] (246.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 246.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 245.0
09/08/17 16:10:35 [25318] (245.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 245.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 244.0
09/08/17 16:10:35 [25318] (244.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 244.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 243.0
09/08/17 16:10:35 [25318] (243.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 243.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 242.0
09/08/17 16:10:35 [25318] (242.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 242.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 241.0
09/08/17 16:10:35 [25318] (241.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 241.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 240.0
09/08/17 16:10:35 [25318] (240.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 240.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 239.0
09/08/17 16:10:35 [25318] (239.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 239.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 238.0
09/08/17 16:10:35 [25318] (238.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 238.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 237.0
09/08/17 16:10:35 [25318] (237.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 237.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 236.0
09/08/17 16:10:35 [25318] (236.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 236.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 235.0
09/08/17 16:10:35 [25318] (235.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 235.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 234.0
09/08/17 16:10:35 [25318] (234.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 234.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 233.0
09/08/17 16:10:35 [25318] (233.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 233.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 232.0
09/08/17 16:10:35 [25318] (232.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 232.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 231.0
09/08/17 16:10:35 [25318] (231.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 231.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 230.0
09/08/17 16:10:35 [25318] (230.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 230.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 229.0
09/08/17 16:10:35 [25318] (229.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 229.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 228.0
09/08/17 16:10:35 [25318] (228.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 228.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 227.0
09/08/17 16:10:35 [25318] (227.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 227.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 226.0
09/08/17 16:10:35 [25318] (226.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 226.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 225.0
09/08/17 16:10:35 [25318] (225.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 225.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 224.0
09/08/17 16:10:35 [25318] (224.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 224.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 223.0
09/08/17 16:10:35 [25318] (223.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 223.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 222.0
09/08/17 16:10:35 [25318] (222.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 222.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 221.0
09/08/17 16:10:35 [25318] (221.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 221.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 220.0
09/08/17 16:10:35 [25318] (220.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 220.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 256.0
09/08/17 16:10:35 [25318] (256.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 256.0 --- inserting
09/08/17 16:10:35 [25318] Using job type INFNBatch for job 219.0
09/08/17 16:10:35 [25318] (219.0) SetJobLeaseTimers()
09/08/17 16:10:35 [25318] Found job 219.0 --- inserting
09/08/17 16:10:35 [25318] Fetched 42 new job ads from schedd
09/08/17 16:10:35 [25318] querying for removed/held jobs
09/08/17 16:10:35 [25318] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:10:35 [25318] Fetched 0 job ads from schedd
09/08/17 16:10:35 [25318] leaving doContactSchedd()
09/08/17 16:10:35 [25318] gahp server not up yet, delaying ping
09/08/17 16:10:35 [25318] *** UpdateLeases called
09/08/17 16:10:35 [25318]     Leases not supported, cancelling timer
09/08/17 16:10:35 [25318] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_19\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_19\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_19>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504912235
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29224>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:10:35 [25318] Trying to update collector <128.55.162.46:9619>
09/08/17 16:10:35 [25318] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:10:35 [25318] File descriptor limits: max 4096, safe 3277
09/08/17 16:10:35 [25318] (255.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:10:35 [25318] GAHP server pid = 25320
09/08/17 16:10:35 [25318] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:10:35 [25318] GAHP[25320] <- 'COMMANDS'
09/08/17 16:10:35 [25318] GAHP[25320] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:10:35 [25318] GAHP[25320] <- 'ASYNC_MODE_ON'
09/08/17 16:10:35 [25318] GAHP[25320] -> 'S' 'Async mode on'
09/08/17 16:10:35 [25318] (255.0) gm state change: GM_INIT -> GM_START
09/08/17 16:10:35 [25318] (255.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:10:35 [25318] (255.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:10:35 [25318] GAHP[25320] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/test.sh"\ ]'
09/08/17 16:10:35 [25318] GAHP[25320] -> 'S'
09/08/17 16:10:35 [25318] (218.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:10:35 [25318] (218.0) gm state change: GM_INIT -> GM_START
09/08/17 16:10:35 [25318] (218.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:10:35 [25318] (218.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:10:35 [25318] GAHP[25320] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#218.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/test.sh"\ ]'
09/08/17 16:10:35 [25318] GAHP[25320] -> 'S'
09/08/17 16:10:35 [25318] (254.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:10:35 [25318] (254.0) gm state change: GM_INIT -> GM_START
09/08/17 16:10:35 [25318] (254.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:10:35 [25318] (254.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:10:35 [25318] GAHP[25320] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/test.sh"\ ]'
09/08/17 16:10:35 [25318] GAHP[25320] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:10:35 [25318] GAHP[25320] -> EOF
09/08/17 16:10:35 [25318] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:15:32 Result of reading /etc/issue:  \S
 
09/08/17 16:15:32 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:15:32 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:15:32 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:15:32 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:15:32 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:15:32 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:15:32 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:15:32 ******************************************************
09/08/17 16:15:32 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:15:32 ** /usr/sbin/condor_gridmanager
09/08/17 16:15:32 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:15:32 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:15:32 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:15:32 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:15:32 ** PID = 25392
09/08/17 16:15:32 ** Log last touched 9/8 16:10:35
09/08/17 16:15:32 ******************************************************
09/08/17 16:15:32 Using config source: /etc/condor-ce/condor_config
09/08/17 16:15:32 Using local config sources: 
09/08/17 16:15:32    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:15:32    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:15:32    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:15:32    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:15:32 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:15:32 CLASSAD_CACHING is ENABLED
09/08/17 16:15:32 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:15:32 SharedPortEndpoint: waiting for connections to named socket 24200_d6d1_22
09/08/17 16:15:32 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_22>
09/08/17 16:15:32 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_22>
09/08/17 16:15:32 Setting maximum accepts per cycle 8.
09/08/17 16:15:32 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:15:32 [25392] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:15:32 [25392] DaemonCore: No more children processes to reap.
09/08/17 16:15:32 [25392] DaemonCore: in SendAliveToParent()
09/08/17 16:15:32 [25392] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:32 [25392] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:32 [25392] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:32 [25392] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:32 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:32 [25392] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29224>
09/08/17 16:15:32 [25392] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:15:32 [25392] Checking proxies
09/08/17 16:15:35 [25392] Received ADD_JOBS signal
09/08/17 16:15:35 [25392] in doContactSchedd()
09/08/17 16:15:35 [25392] querying for new jobs
09/08/17 16:15:35 [25392] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 255.0
09/08/17 16:15:35 [25392] (255.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 255.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 218.0
09/08/17 16:15:35 [25392] (218.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 218.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 254.0
09/08/17 16:15:35 [25392] (254.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 254.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 217.0
09/08/17 16:15:35 [25392] (217.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 217.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 253.0
09/08/17 16:15:35 [25392] (253.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 253.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 216.0
09/08/17 16:15:35 [25392] (216.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 216.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 252.0
09/08/17 16:15:35 [25392] (252.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 252.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 215.0
09/08/17 16:15:35 [25392] (215.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 215.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 251.0
09/08/17 16:15:35 [25392] (251.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 251.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 250.0
09/08/17 16:15:35 [25392] (250.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 250.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 249.0
09/08/17 16:15:35 [25392] (249.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 249.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 248.0
09/08/17 16:15:35 [25392] (248.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 248.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 247.0
09/08/17 16:15:35 [25392] (247.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 247.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 246.0
09/08/17 16:15:35 [25392] (246.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 246.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 245.0
09/08/17 16:15:35 [25392] (245.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 245.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 244.0
09/08/17 16:15:35 [25392] (244.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 244.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 243.0
09/08/17 16:15:35 [25392] (243.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 243.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 242.0
09/08/17 16:15:35 [25392] (242.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 242.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 241.0
09/08/17 16:15:35 [25392] (241.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 241.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 240.0
09/08/17 16:15:35 [25392] (240.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 240.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 239.0
09/08/17 16:15:35 [25392] (239.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 239.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 238.0
09/08/17 16:15:35 [25392] (238.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 238.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 237.0
09/08/17 16:15:35 [25392] (237.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 237.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 236.0
09/08/17 16:15:35 [25392] (236.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 236.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 235.0
09/08/17 16:15:35 [25392] (235.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 235.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 234.0
09/08/17 16:15:35 [25392] (234.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 234.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 233.0
09/08/17 16:15:35 [25392] (233.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 233.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 232.0
09/08/17 16:15:35 [25392] (232.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 232.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 231.0
09/08/17 16:15:35 [25392] (231.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 231.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 230.0
09/08/17 16:15:35 [25392] (230.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 230.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 229.0
09/08/17 16:15:35 [25392] (229.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 229.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 228.0
09/08/17 16:15:35 [25392] (228.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 228.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 227.0
09/08/17 16:15:35 [25392] (227.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 227.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 226.0
09/08/17 16:15:35 [25392] (226.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 226.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 225.0
09/08/17 16:15:35 [25392] (225.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 225.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 224.0
09/08/17 16:15:35 [25392] (224.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 224.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 223.0
09/08/17 16:15:35 [25392] (223.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 223.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 222.0
09/08/17 16:15:35 [25392] (222.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 222.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 221.0
09/08/17 16:15:35 [25392] (221.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 221.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 220.0
09/08/17 16:15:35 [25392] (220.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 220.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 256.0
09/08/17 16:15:35 [25392] (256.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 256.0 --- inserting
09/08/17 16:15:35 [25392] Using job type INFNBatch for job 219.0
09/08/17 16:15:35 [25392] (219.0) SetJobLeaseTimers()
09/08/17 16:15:35 [25392] Found job 219.0 --- inserting
09/08/17 16:15:35 [25392] Fetched 42 new job ads from schedd
09/08/17 16:15:35 [25392] querying for removed/held jobs
09/08/17 16:15:35 [25392] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:15:35 [25392] Fetched 0 job ads from schedd
09/08/17 16:15:35 [25392] leaving doContactSchedd()
09/08/17 16:15:35 [25392] gahp server not up yet, delaying ping
09/08/17 16:15:35 [25392] *** UpdateLeases called
09/08/17 16:15:35 [25392]     Leases not supported, cancelling timer
09/08/17 16:15:35 [25392] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_22\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"24200_d6d1_22\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=24200_d6d1_22>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504912535
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29224>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:15:35 [25392] Trying to update collector <128.55.162.46:9619>
09/08/17 16:15:35 [25392] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:15:35 [25392] File descriptor limits: max 4096, safe 3277
09/08/17 16:15:35 [25392] (255.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:15:35 [25392] GAHP server pid = 25394
09/08/17 16:15:35 [25392] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:15:35 [25392] GAHP[25394] <- 'COMMANDS'
09/08/17 16:15:35 [25392] GAHP[25394] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:15:35 [25392] GAHP[25394] <- 'ASYNC_MODE_ON'
09/08/17 16:15:35 [25392] GAHP[25394] -> 'S' 'Async mode on'
09/08/17 16:15:35 [25392] (255.0) gm state change: GM_INIT -> GM_START
09/08/17 16:15:35 [25392] (255.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:15:35 [25392] (255.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:15:35 [25392] GAHP[25394] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/test.sh"\ ]'
09/08/17 16:15:35 [25392] GAHP[25394] -> 'S'
09/08/17 16:15:35 [25392] (218.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:15:35 [25392] (218.0) gm state change: GM_INIT -> GM_START
09/08/17 16:15:35 [25392] (218.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:15:35 [25392] (218.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:15:35 [25392] GAHP[25394] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#218.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/test.sh"\ ]'
09/08/17 16:15:35 [25392] GAHP[25394] -> 'S'
09/08/17 16:15:35 [25392] (254.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:15:35 [25392] (254.0) gm state change: GM_INIT -> GM_START
09/08/17 16:15:35 [25392] (254.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:15:35 [25392] (254.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:15:35 [25392] GAHP[25394] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/test.sh"\ ]'
09/08/17 16:15:35 [25392] GAHP[25394] -> 'S'
09/08/17 16:15:35 [25392] (217.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:15:35 [25392] (217.0) gm state change: GM_INIT -> GM_START
09/08/17 16:15:35 [25392] (217.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:15:35 [25392] (217.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:15:35 [25392] GAHP[25394] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#217.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0/test.sh"\ ]'
09/08/17 16:15:35 [25392] GAHP[25394] -> 'S'
09/08/17 16:15:35 [25392] (253.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:15:35 [25392] (253.0) gm state change: GM_INIT -> GM_START
09/08/17 16:15:35 [25392] (253.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:15:35 [25392] (253.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:15:35 [25392] GAHP[25394] <- 'BLAH_JOB_SUBMIT 6 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#253.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/test.sh"\ ]'
09/08/17 16:15:35 [25392] GAHP[25394] -> 'S'
09/08/17 16:15:35 [25392] This process has a valid certificate & key
09/08/17 16:15:35 [25392] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:15:35 [25392] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:15:35 [25392] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:15:35 [25392] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:15:35 [25392] IPVERIFY: ip found is 1
09/08/17 16:15:35 [25392] (216.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:15:35 [25392] (216.0) gm state change: GM_INIT -> GM_START
09/08/17 16:15:35 [25392] (216.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:15:35 [25392] (216.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:15:35 [25392] GAHP[25394] <- 'BLAH_JOB_SUBMIT 7 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#216.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0/test.sh"\ ]'
09/08/17 16:15:35 [25392] GAHP[25394] (stderr) -> Assertion mutex->level > 0 failed in file globus_module.c at line 1147
09/08/17 16:15:35 [25392] GAHP[25394] -> EOF
09/08/17 16:15:35 [25392] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:18:36 Result of reading /etc/issue:  \S
 
09/08/17 16:18:36 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:18:36 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:18:36 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:18:36 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:18:36 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:18:36 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:18:36 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:18:36 ******************************************************
09/08/17 16:18:36 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:18:36 ** /usr/sbin/condor_gridmanager
09/08/17 16:18:36 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:18:36 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:18:36 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:18:36 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:18:36 ** PID = 25572
09/08/17 16:18:36 ** Log last touched 9/8 16:15:35
09/08/17 16:18:36 ******************************************************
09/08/17 16:18:36 Using config source: /etc/condor-ce/condor_config
09/08/17 16:18:36 Using local config sources: 
09/08/17 16:18:36    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:18:36    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:18:36    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:18:36    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:18:36 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:18:36 CLASSAD_CACHING is ENABLED
09/08/17 16:18:36 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:18:36 SharedPortEndpoint: waiting for connections to named socket 25559_60a8_3
09/08/17 16:18:37 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_3>
09/08/17 16:18:37 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_3>
09/08/17 16:18:37 Setting maximum accepts per cycle 8.
09/08/17 16:18:37 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:18:37 [25572] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:18:37 [25572] DaemonCore: No more children processes to reap.
09/08/17 16:18:37 [25572] DaemonCore: in SendAliveToParent()
09/08/17 16:18:37 [25572] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:37 [25572] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:37 [25572] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:37 [25572] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:37 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:37 [25572] Completed DC_CHILDALIVE to daemon at <128.55.162.46:13716>
09/08/17 16:18:37 [25572] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:18:37 [25572] Checking proxies
09/08/17 16:18:39 [25572] Received ADD_JOBS signal
09/08/17 16:18:39 [25572] in doContactSchedd()
09/08/17 16:18:39 [25572] querying for new jobs
09/08/17 16:18:39 [25572] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 255.0
09/08/17 16:18:39 [25572] (255.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 255.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 218.0
09/08/17 16:18:39 [25572] (218.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 218.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 254.0
09/08/17 16:18:39 [25572] (254.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 254.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 217.0
09/08/17 16:18:39 [25572] (217.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 217.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 253.0
09/08/17 16:18:39 [25572] (253.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 253.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 216.0
09/08/17 16:18:39 [25572] (216.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 216.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 252.0
09/08/17 16:18:39 [25572] (252.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 252.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 215.0
09/08/17 16:18:39 [25572] (215.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 215.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 251.0
09/08/17 16:18:39 [25572] (251.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 251.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 250.0
09/08/17 16:18:39 [25572] (250.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 250.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 249.0
09/08/17 16:18:39 [25572] (249.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 249.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 248.0
09/08/17 16:18:39 [25572] (248.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 248.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 247.0
09/08/17 16:18:39 [25572] (247.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 247.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 246.0
09/08/17 16:18:39 [25572] (246.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 246.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 245.0
09/08/17 16:18:39 [25572] (245.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 245.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 244.0
09/08/17 16:18:39 [25572] (244.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 244.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 243.0
09/08/17 16:18:39 [25572] (243.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 243.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 242.0
09/08/17 16:18:39 [25572] (242.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 242.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 241.0
09/08/17 16:18:39 [25572] (241.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 241.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 240.0
09/08/17 16:18:39 [25572] (240.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 240.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 239.0
09/08/17 16:18:39 [25572] (239.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 239.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 238.0
09/08/17 16:18:39 [25572] (238.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 238.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 237.0
09/08/17 16:18:39 [25572] (237.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 237.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 236.0
09/08/17 16:18:39 [25572] (236.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 236.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 235.0
09/08/17 16:18:39 [25572] (235.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 235.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 234.0
09/08/17 16:18:39 [25572] (234.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 234.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 233.0
09/08/17 16:18:39 [25572] (233.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 233.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 232.0
09/08/17 16:18:39 [25572] (232.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 232.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 231.0
09/08/17 16:18:39 [25572] (231.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 231.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 230.0
09/08/17 16:18:39 [25572] (230.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 230.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 229.0
09/08/17 16:18:39 [25572] (229.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 229.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 228.0
09/08/17 16:18:39 [25572] (228.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 228.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 227.0
09/08/17 16:18:39 [25572] (227.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 227.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 226.0
09/08/17 16:18:39 [25572] (226.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 226.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 225.0
09/08/17 16:18:39 [25572] (225.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 225.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 224.0
09/08/17 16:18:39 [25572] (224.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 224.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 223.0
09/08/17 16:18:39 [25572] (223.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 223.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 222.0
09/08/17 16:18:39 [25572] (222.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 222.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 221.0
09/08/17 16:18:39 [25572] (221.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 221.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 220.0
09/08/17 16:18:39 [25572] (220.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 220.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 256.0
09/08/17 16:18:39 [25572] (256.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 256.0 --- inserting
09/08/17 16:18:39 [25572] Using job type INFNBatch for job 219.0
09/08/17 16:18:39 [25572] (219.0) SetJobLeaseTimers()
09/08/17 16:18:39 [25572] Found job 219.0 --- inserting
09/08/17 16:18:39 [25572] Fetched 42 new job ads from schedd
09/08/17 16:18:39 [25572] querying for removed/held jobs
09/08/17 16:18:39 [25572] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:18:39 [25572] Fetched 0 job ads from schedd
09/08/17 16:18:39 [25572] leaving doContactSchedd()
09/08/17 16:18:39 [25572] gahp server not up yet, delaying ping
09/08/17 16:18:39 [25572] *** UpdateLeases called
09/08/17 16:18:39 [25572]     Leases not supported, cancelling timer
09/08/17 16:18:39 [25572] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504912719
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:13716>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:18:39 [25572] Trying to update collector <128.55.162.46:9619>
09/08/17 16:18:39 [25572] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:18:39 [25572] File descriptor limits: max 4096, safe 3277
09/08/17 16:18:39 [25572] (255.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:18:39 [25572] GAHP server pid = 25578
09/08/17 16:18:39 [25572] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:18:39 [25572] GAHP[25578] <- 'COMMANDS'
09/08/17 16:18:39 [25572] GAHP[25578] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:18:39 [25572] GAHP[25578] <- 'ASYNC_MODE_ON'
09/08/17 16:18:39 [25572] GAHP[25578] -> 'S' 'Async mode on'
09/08/17 16:18:39 [25572] (255.0) gm state change: GM_INIT -> GM_START
09/08/17 16:18:39 [25572] (255.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:18:39 [25572] (255.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:18:39 [25572] GAHP[25578] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/test.sh"\ ]'
09/08/17 16:18:39 [25572] GAHP[25578] -> 'S'
09/08/17 16:18:39 [25572] (218.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:18:39 [25572] (218.0) gm state change: GM_INIT -> GM_START
09/08/17 16:18:39 [25572] (218.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:18:39 [25572] (218.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:18:39 [25572] GAHP[25578] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#218.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/test.sh"\ ]'
09/08/17 16:18:39 [25572] GAHP[25578] -> 'S'
09/08/17 16:18:39 [25572] (254.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:18:39 [25572] (254.0) gm state change: GM_INIT -> GM_START
09/08/17 16:18:39 [25572] (254.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:18:39 [25572] (254.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:18:39 [25572] GAHP[25578] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/test.sh"\ ]'
09/08/17 16:18:39 [25572] GAHP[25578] -> 'S'
09/08/17 16:18:39 [25572] (217.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:18:39 [25572] (217.0) gm state change: GM_INIT -> GM_START
09/08/17 16:18:39 [25572] (217.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:18:39 [25572] (217.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:18:39 [25572] GAHP[25578] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#217.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0/test.sh"\ ]'
09/08/17 16:18:39 [25572] GAHP[25578] -> 'S'
09/08/17 16:18:39 [25572] (253.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:18:39 [25572] (253.0) gm state change: GM_INIT -> GM_START
09/08/17 16:18:39 [25572] (253.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:18:39 [25572] (253.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:18:39 [25572] GAHP[25578] <- 'BLAH_JOB_SUBMIT 6 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#253.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/test.sh"\ ]'
09/08/17 16:18:39 [25572] GAHP[25578] -> 'S'
09/08/17 16:18:39 [25572] This process has a valid certificate & key
09/08/17 16:18:39 [25572] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:18:39 [25572] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:18:39 [25572] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:18:39 [25572] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:18:39 [25572] IPVERIFY: ip found is 1
09/08/17 16:18:39 [25572] (216.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:18:39 [25572] (216.0) gm state change: GM_INIT -> GM_START
09/08/17 16:18:39 [25572] (216.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:18:39 [25572] (216.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:18:39 [25572] GAHP[25578] <- 'BLAH_JOB_SUBMIT 7 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#216.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0/test.sh"\ ]'
09/08/17 16:18:39 [25572] GAHP[25578] (stderr) -> Assertion mutex->level > 0 failed in file globus_module.c at line 1147
09/08/17 16:18:39 [25572] GAHP[25578] -> EOF
09/08/17 16:18:39 [25572] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:20:46 Result of reading /etc/issue:  \S
 
09/08/17 16:20:46 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:20:46 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:20:46 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:20:46 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:20:46 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:20:46 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:20:46 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:20:46 ******************************************************
09/08/17 16:20:46 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:20:46 ** /usr/sbin/condor_gridmanager
09/08/17 16:20:46 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:20:46 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:20:46 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:20:46 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:20:46 ** PID = 25622
09/08/17 16:20:46 ** Log last touched 9/8 16:18:39
09/08/17 16:20:46 ******************************************************
09/08/17 16:20:46 Using config source: /etc/condor-ce/condor_config
09/08/17 16:20:46 Using local config sources: 
09/08/17 16:20:46    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:20:46    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:20:46    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:20:46    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:20:46 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:20:46 CLASSAD_CACHING is ENABLED
09/08/17 16:20:46 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:20:46 SharedPortEndpoint: waiting for connections to named socket 25559_60a8_4
09/08/17 16:20:46 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_4>
09/08/17 16:20:46 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_4>
09/08/17 16:20:46 Setting maximum accepts per cycle 8.
09/08/17 16:20:46 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:20:46 [25622] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:20:46 [25622] DaemonCore: No more children processes to reap.
09/08/17 16:20:46 [25622] DaemonCore: in SendAliveToParent()
09/08/17 16:20:46 [25622] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: ip found is 1
09/08/17 16:20:46 [25622] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: ip found is 1
09/08/17 16:20:46 [25622] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: ip found is 1
09/08/17 16:20:46 [25622] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:20:46 [25622] IPVERIFY: ip found is 1
09/08/17 16:20:46 [25622] Completed DC_CHILDALIVE to daemon at <128.55.162.46:13716>
09/08/17 16:20:46 [25622] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:20:46 [25622] Checking proxies
09/08/17 16:20:48 [25622] Received REMOVE_JOBS signal
09/08/17 16:20:48 [25622] in doContactSchedd()
09/08/17 16:20:48 [25622] querying for new jobs
09/08/17 16:20:48 [25622] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 255.0
09/08/17 16:20:48 [25622] (255.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 255.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 218.0
09/08/17 16:20:48 [25622] (218.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 218.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 254.0
09/08/17 16:20:48 [25622] (254.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 254.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 217.0
09/08/17 16:20:48 [25622] (217.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 217.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 253.0
09/08/17 16:20:48 [25622] (253.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 253.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 216.0
09/08/17 16:20:48 [25622] (216.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 216.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 252.0
09/08/17 16:20:48 [25622] (252.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 252.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 215.0
09/08/17 16:20:48 [25622] (215.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 215.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 251.0
09/08/17 16:20:48 [25622] (251.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 251.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 250.0
09/08/17 16:20:48 [25622] (250.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 250.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 249.0
09/08/17 16:20:48 [25622] (249.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 249.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 248.0
09/08/17 16:20:48 [25622] (248.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 248.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 247.0
09/08/17 16:20:48 [25622] (247.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 247.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 246.0
09/08/17 16:20:48 [25622] (246.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 246.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 245.0
09/08/17 16:20:48 [25622] (245.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 245.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 244.0
09/08/17 16:20:48 [25622] (244.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 244.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 243.0
09/08/17 16:20:48 [25622] (243.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 243.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 242.0
09/08/17 16:20:48 [25622] (242.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 242.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 241.0
09/08/17 16:20:48 [25622] (241.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 241.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 240.0
09/08/17 16:20:48 [25622] (240.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 240.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 239.0
09/08/17 16:20:48 [25622] (239.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 239.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 238.0
09/08/17 16:20:48 [25622] (238.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 238.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 237.0
09/08/17 16:20:48 [25622] (237.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 237.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 236.0
09/08/17 16:20:48 [25622] (236.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 236.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 235.0
09/08/17 16:20:48 [25622] (235.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 235.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 234.0
09/08/17 16:20:48 [25622] (234.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 234.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 233.0
09/08/17 16:20:48 [25622] (233.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 233.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 232.0
09/08/17 16:20:48 [25622] (232.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 232.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 231.0
09/08/17 16:20:48 [25622] (231.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 231.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 230.0
09/08/17 16:20:48 [25622] (230.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 230.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 229.0
09/08/17 16:20:48 [25622] (229.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 229.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 228.0
09/08/17 16:20:48 [25622] (228.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 228.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 227.0
09/08/17 16:20:48 [25622] (227.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 227.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 226.0
09/08/17 16:20:48 [25622] (226.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 226.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 225.0
09/08/17 16:20:48 [25622] (225.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 225.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 224.0
09/08/17 16:20:48 [25622] (224.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 224.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 223.0
09/08/17 16:20:48 [25622] (223.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 223.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 222.0
09/08/17 16:20:48 [25622] (222.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 222.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 221.0
09/08/17 16:20:48 [25622] (221.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 221.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 220.0
09/08/17 16:20:48 [25622] (220.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 220.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 256.0
09/08/17 16:20:48 [25622] (256.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 256.0 --- inserting
09/08/17 16:20:48 [25622] Using job type INFNBatch for job 219.0
09/08/17 16:20:48 [25622] (219.0) SetJobLeaseTimers()
09/08/17 16:20:48 [25622] Found job 219.0 --- inserting
09/08/17 16:20:48 [25622] Fetched 42 new job ads from schedd
09/08/17 16:20:48 [25622] querying for removed/held jobs
09/08/17 16:20:48 [25622] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:20:48 [25622] Fetched 42 job ads from schedd
09/08/17 16:20:48 [25622] leaving doContactSchedd()
09/08/17 16:20:48 [25622] gahp server not up yet, delaying ping
09/08/17 16:20:48 [25622] *** UpdateLeases called
09/08/17 16:20:48 [25622]     Leases not supported, cancelling timer
09/08/17 16:20:48 [25622] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_4\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_4\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_4>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504912848
IdleJobs = 0
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:13716>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:20:48 [25622] Trying to update collector <128.55.162.46:9619>
09/08/17 16:20:48 [25622] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:20:48 [25622] File descriptor limits: max 4096, safe 3277
09/08/17 16:20:48 [25622] (255.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:20:48 [25622] GAHP server pid = 25624
09/08/17 16:20:48 [25622] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:20:48 [25622] GAHP[25624] <- 'COMMANDS'
09/08/17 16:20:48 [25622] GAHP[25624] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:20:48 [25622] GAHP[25624] <- 'ASYNC_MODE_ON'
09/08/17 16:20:48 [25622] GAHP[25624] -> 'S' 'Async mode on'
09/08/17 16:20:48 [25622] (255.0) gm state change: GM_INIT -> GM_START
09/08/17 16:20:48 [25622] (255.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:20:48 [25622] (255.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:20:48 [25622] GAHP[25624] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/test.sh"\ ]'
09/08/17 16:20:48 [25622] GAHP[25624] -> 'S'
09/08/17 16:20:48 [25622] (218.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:20:48 [25622] (218.0) gm state change: GM_INIT -> GM_START
09/08/17 16:20:48 [25622] (218.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:20:48 [25622] (218.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:20:48 [25622] GAHP[25624] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#218.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/test.sh"\ ]'
09/08/17 16:20:48 [25622] GAHP[25624] -> 'S'
09/08/17 16:20:48 [25622] (254.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:20:48 [25622] (254.0) gm state change: GM_INIT -> GM_START
09/08/17 16:20:48 [25622] (254.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:20:48 [25622] (254.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:20:48 [25622] GAHP[25624] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/test.sh"\ ]'
09/08/17 16:20:48 [25622] GAHP[25624] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:20:48 [25622] GAHP[25624] -> EOF
09/08/17 16:20:48 [25622] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:23:25 Result of reading /etc/issue:  \S
 
09/08/17 16:23:25 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:23:25 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:23:25 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:23:25 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:23:25 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:23:25 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:23:25 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:23:25 ******************************************************
09/08/17 16:23:25 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:23:25 ** /usr/sbin/condor_gridmanager
09/08/17 16:23:25 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:23:25 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:23:25 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:23:25 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:23:25 ** PID = 25697
09/08/17 16:23:25 ** Log last touched 9/8 16:20:48
09/08/17 16:23:25 ******************************************************
09/08/17 16:23:25 Using config source: /etc/condor-ce/condor_config
09/08/17 16:23:25 Using local config sources: 
09/08/17 16:23:25    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:23:25    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:23:25    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:23:25    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:23:25 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:23:25 CLASSAD_CACHING is ENABLED
09/08/17 16:23:25 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:23:25 SharedPortEndpoint: waiting for connections to named socket 25559_60a8_6
09/08/17 16:23:25 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_6>
09/08/17 16:23:25 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_6>
09/08/17 16:23:25 Setting maximum accepts per cycle 8.
09/08/17 16:23:25 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:23:25 [25697] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:23:25 [25697] DaemonCore: No more children processes to reap.
09/08/17 16:23:25 [25697] DaemonCore: in SendAliveToParent()
09/08/17 16:23:25 [25697] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:25 [25697] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:25 [25697] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:25 [25697] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:25 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:25 [25697] Completed DC_CHILDALIVE to daemon at <128.55.162.46:13716>
09/08/17 16:23:25 [25697] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:23:25 [25697] Checking proxies
09/08/17 16:23:28 [25697] Received ADD_JOBS signal
09/08/17 16:23:28 [25697] in doContactSchedd()
09/08/17 16:23:28 [25697] querying for new jobs
09/08/17 16:23:28 [25697] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 255.0
09/08/17 16:23:28 [25697] (255.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 255.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 218.0
09/08/17 16:23:28 [25697] (218.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 218.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 254.0
09/08/17 16:23:28 [25697] (254.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 254.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 217.0
09/08/17 16:23:28 [25697] (217.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/194/0/cluster194.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 217.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 253.0
09/08/17 16:23:28 [25697] (253.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/192/0/cluster192.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 253.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 216.0
09/08/17 16:23:28 [25697] (216.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/185/0/cluster185.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 216.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 252.0
09/08/17 16:23:28 [25697] (252.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/174/0/cluster174.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 252.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 215.0
09/08/17 16:23:28 [25697] (215.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/176/0/cluster176.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 215.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 251.0
09/08/17 16:23:28 [25697] (251.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/183/0/cluster183.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 251.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 250.0
09/08/17 16:23:28 [25697] (250.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/191/0/cluster191.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 250.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 249.0
09/08/17 16:23:28 [25697] (249.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/173/0/cluster173.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 249.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 248.0
09/08/17 16:23:28 [25697] (248.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/182/0/cluster182.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 248.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 247.0
09/08/17 16:23:28 [25697] (247.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/209/0/cluster209.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 247.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 246.0
09/08/17 16:23:28 [25697] (246.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/190/0/cluster190.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 246.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 245.0
09/08/17 16:23:28 [25697] (245.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/181/0/cluster181.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 245.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 244.0
09/08/17 16:23:28 [25697] (244.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/208/0/cluster208.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 244.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 243.0
09/08/17 16:23:28 [25697] (243.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/180/0/cluster180.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 243.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 242.0
09/08/17 16:23:28 [25697] (242.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/207/0/cluster207.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 242.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 241.0
09/08/17 16:23:28 [25697] (241.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/206/0/cluster206.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 241.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 240.0
09/08/17 16:23:28 [25697] (240.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/205/0/cluster205.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 240.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 239.0
09/08/17 16:23:28 [25697] (239.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/214/0/cluster214.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 239.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 238.0
09/08/17 16:23:28 [25697] (238.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/204/0/cluster204.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 238.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 237.0
09/08/17 16:23:28 [25697] (237.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/213/0/cluster213.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 237.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 236.0
09/08/17 16:23:28 [25697] (236.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/203/0/cluster203.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 236.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 235.0
09/08/17 16:23:28 [25697] (235.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/212/0/cluster212.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 235.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 234.0
09/08/17 16:23:28 [25697] (234.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/202/0/cluster202.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 234.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 233.0
09/08/17 16:23:28 [25697] (233.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/211/0/cluster211.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 233.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 232.0
09/08/17 16:23:28 [25697] (232.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/201/0/cluster201.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 232.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 231.0
09/08/17 16:23:28 [25697] (231.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/210/0/cluster210.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 231.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 230.0
09/08/17 16:23:28 [25697] (230.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/200/0/cluster200.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 230.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 229.0
09/08/17 16:23:28 [25697] (229.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/199/0/cluster199.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 229.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 228.0
09/08/17 16:23:28 [25697] (228.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/189/0/cluster189.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 228.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 227.0
09/08/17 16:23:28 [25697] (227.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/198/0/cluster198.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 227.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 226.0
09/08/17 16:23:28 [25697] (226.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/197/0/cluster197.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 226.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 225.0
09/08/17 16:23:28 [25697] (225.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/188/0/cluster188.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 225.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 224.0
09/08/17 16:23:28 [25697] (224.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/179/0/cluster179.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 224.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 223.0
09/08/17 16:23:28 [25697] (223.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/196/0/cluster196.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 223.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 222.0
09/08/17 16:23:28 [25697] (222.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/187/0/cluster187.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 222.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 221.0
09/08/17 16:23:28 [25697] (221.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/178/0/cluster178.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 221.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 220.0
09/08/17 16:23:28 [25697] (220.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/195/0/cluster195.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 220.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 256.0
09/08/17 16:23:28 [25697] (256.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/193/0/cluster193.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 256.0 --- inserting
09/08/17 16:23:28 [25697] Using job type INFNBatch for job 219.0
09/08/17 16:23:28 [25697] (219.0) SetJobLeaseTimers()
09/08/17 16:23:28 [25697] Failed to get expiration time of proxy /common/osg/condor2/186/0/cluster186.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:23:28 [25697] Found job 219.0 --- inserting
09/08/17 16:23:28 [25697] Fetched 42 new job ads from schedd
09/08/17 16:23:28 [25697] querying for removed/held jobs
09/08/17 16:23:28 [25697] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:23:28 [25697] Fetched 42 job ads from schedd
09/08/17 16:23:28 [25697] leaving doContactSchedd()
09/08/17 16:23:28 [25697] gahp server not up yet, delaying ping
09/08/17 16:23:28 [25697] *** UpdateLeases called
09/08/17 16:23:28 [25697]     Leases not supported, cancelling timer
09/08/17 16:23:28 [25697] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_6\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_6\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_6>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504913008
IdleJobs = 0
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:13716>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:23:28 [25697] Trying to update collector <128.55.162.46:9619>
09/08/17 16:23:28 [25697] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:23:28 [25697] File descriptor limits: max 4096, safe 3277
09/08/17 16:23:28 [25697] (255.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] GAHP server pid = 25701
09/08/17 16:23:28 [25697] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:23:28 [25697] GAHP[25701] <- 'COMMANDS'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:23:28 [25697] GAHP[25701] <- 'ASYNC_MODE_ON'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S' 'Async mode on'
09/08/17 16:23:28 [25697] (255.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (255.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (255.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#255.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/175/0/cluster175.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (218.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (218.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (218.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (218.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#218.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/177/0/cluster177.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (254.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (254.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (254.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (254.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#254.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/184/0/cluster184.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'R'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (217.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (217.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (217.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (217.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#217.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/194/0/cluster194.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (253.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (253.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (253.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (253.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 6 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#253.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/192/0/cluster192.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] This process has a valid certificate & key
09/08/17 16:23:28 [25697] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:23:28 [25697] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:23:28 [25697] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:23:28 [25697] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:28 [25697] IPVERIFY: ip found is 1
09/08/17 16:23:28 [25697] (216.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (216.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (216.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (216.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 7 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#216.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/185/0/cluster185.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (252.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (252.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (252.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (252.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 8 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#252.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/174/0/cluster174.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/174/0/cluster174.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/174/0/cluster174.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (215.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (215.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (215.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (215.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 9 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#215.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/176/0/cluster176.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/176/0/cluster176.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/176/0/cluster176.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (251.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (251.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (251.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (251.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 10 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#251.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/183/0/cluster183.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/183/0/cluster183.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/183/0/cluster183.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (250.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (250.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (250.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (250.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 11 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#250.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/191/0/cluster191.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/191/0/cluster191.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/191/0/cluster191.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (249.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (249.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (249.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (249.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 12 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#249.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/173/0/cluster173.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/173/0/cluster173.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/173/0/cluster173.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (248.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (248.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (248.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (248.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 13 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#248.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/182/0/cluster182.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/182/0/cluster182.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/182/0/cluster182.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (247.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (247.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (247.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (247.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 14 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#247.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/209/0/cluster209.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/209/0/cluster209.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/209/0/cluster209.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (246.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (246.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (246.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (246.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 15 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#246.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/190/0/cluster190.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/190/0/cluster190.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/190/0/cluster190.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (245.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (245.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (245.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (245.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 16 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#245.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/181/0/cluster181.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/181/0/cluster181.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/181/0/cluster181.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (244.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (244.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (244.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (244.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 17 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#244.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/208/0/cluster208.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/208/0/cluster208.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/208/0/cluster208.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (243.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (243.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (243.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (243.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 18 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#243.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/180/0/cluster180.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/180/0/cluster180.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/180/0/cluster180.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (242.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (242.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (242.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (242.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 19 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#242.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/207/0/cluster207.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/207/0/cluster207.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/207/0/cluster207.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (241.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (241.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (241.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (241.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 20 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#241.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/206/0/cluster206.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/206/0/cluster206.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/206/0/cluster206.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (240.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (240.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (240.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (240.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 21 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#240.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/205/0/cluster205.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/205/0/cluster205.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/205/0/cluster205.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (239.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (239.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (239.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (239.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 22 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#239.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/214/0/cluster214.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/214/0/cluster214.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/214/0/cluster214.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (238.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (238.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (238.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (238.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 23 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#238.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/204/0/cluster204.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/204/0/cluster204.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/204/0/cluster204.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (237.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (237.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (237.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (237.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 24 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#237.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/213/0/cluster213.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/213/0/cluster213.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/213/0/cluster213.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (236.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (236.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (236.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (236.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 25 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#236.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/203/0/cluster203.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/203/0/cluster203.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/203/0/cluster203.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (235.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (235.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (235.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (235.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 26 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#235.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/212/0/cluster212.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/212/0/cluster212.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/212/0/cluster212.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (234.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (234.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (234.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (234.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 27 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#234.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/202/0/cluster202.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/202/0/cluster202.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/202/0/cluster202.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (233.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (233.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (233.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (233.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 28 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#233.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/211/0/cluster211.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/211/0/cluster211.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/211/0/cluster211.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (232.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (232.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (232.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (232.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 29 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#232.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/201/0/cluster201.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/201/0/cluster201.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/201/0/cluster201.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (231.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (231.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (231.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (231.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 30 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#231.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/210/0/cluster210.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/210/0/cluster210.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/210/0/cluster210.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (230.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (230.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (230.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (230.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 31 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#230.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/200/0/cluster200.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/200/0/cluster200.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/200/0/cluster200.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (229.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (229.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (229.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (229.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 32 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#229.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/199/0/cluster199.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/199/0/cluster199.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/199/0/cluster199.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (228.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (228.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (228.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (228.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 33 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#228.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/189/0/cluster189.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/189/0/cluster189.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/189/0/cluster189.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (227.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (227.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (227.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (227.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 34 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#227.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/198/0/cluster198.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/198/0/cluster198.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/198/0/cluster198.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (226.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (226.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (226.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (226.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 35 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#226.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/197/0/cluster197.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/197/0/cluster197.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/197/0/cluster197.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (225.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (225.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (225.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (225.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 36 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#225.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/188/0/cluster188.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/188/0/cluster188.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/188/0/cluster188.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (224.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (224.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (224.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (224.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 37 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#224.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/179/0/cluster179.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/179/0/cluster179.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/179/0/cluster179.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (223.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (223.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (223.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (223.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 38 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#223.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/196/0/cluster196.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/196/0/cluster196.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/196/0/cluster196.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (222.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (222.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (222.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (222.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 39 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#222.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/187/0/cluster187.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/187/0/cluster187.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/187/0/cluster187.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (221.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (221.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (221.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (221.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 40 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#221.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/178/0/cluster178.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/178/0/cluster178.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/178/0/cluster178.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (220.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (220.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (220.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (220.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 41 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#220.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/195/0/cluster195.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/195/0/cluster195.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/195/0/cluster195.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (256.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (256.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (256.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (256.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 42 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#256.0#1504911628";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/193/0/cluster193.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/193/0/cluster193.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/193/0/cluster193.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] (219.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:23:28 [25697] (219.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:28 [25697] (219.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:23:28 [25697] (219.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:28 [25697] GAHP[25701] <- 'BLAH_JOB_SUBMIT 43 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#219.0#1504911627";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/186/0/cluster186.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/186/0/cluster186.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/186/0/cluster186.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S'
09/08/17 16:23:28 [25697] GAHP[25701] <- 'RESULTS'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S' '41'
09/08/17 16:23:28 [25697] GAHP[25701] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '3' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '4' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '5' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/194/0/cluster194.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '6' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/192/0/cluster192.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '7' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/185/0/cluster185.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '8' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/174/0/cluster174.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '9' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/176/0/cluster176.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '10' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/183/0/cluster183.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '11' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/191/0/cluster191.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '12' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/173/0/cluster173.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '13' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/182/0/cluster182.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '14' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/209/0/cluster209.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '15' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/190/0/cluster190.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '16' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/181/0/cluster181.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '17' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/208/0/cluster208.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '18' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/180/0/cluster180.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '19' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/207/0/cluster207.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '20' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/206/0/cluster206.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '21' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/205/0/cluster205.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '22' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/214/0/cluster214.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '23' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/204/0/cluster204.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '24' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/213/0/cluster213.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '25' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/203/0/cluster203.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '26' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/212/0/cluster212.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '27' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/202/0/cluster202.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '28' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/211/0/cluster211.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '29' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/201/0/cluster201.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '30' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/210/0/cluster210.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '31' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/200/0/cluster200.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '32' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/199/0/cluster199.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '33' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/189/0/cluster189.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '34' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/198/0/cluster198.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '35' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/197/0/cluster197.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '36' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/188/0/cluster188.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '37' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/179/0/cluster179.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '38' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/196/0/cluster196.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '39' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/187/0/cluster187.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '40' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/178/0/cluster178.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '41' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/195/0/cluster195.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] -> '42' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/193/0/cluster193.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] GAHP[25701] <- 'RESULTS'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'R'
09/08/17 16:23:28 [25697] GAHP[25701] -> 'S' '1'
09/08/17 16:23:28 [25697] GAHP[25701] -> '43' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/186/0/cluster186.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:23:28 [25697] (255.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (255.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/175/0/cluster175.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (255.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (255.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (218.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (218.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/177/0/cluster177.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (218.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (218.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (254.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (254.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/184/0/cluster184.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (254.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (254.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (217.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (217.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/194/0/cluster194.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (217.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (217.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (253.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (253.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/192/0/cluster192.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (253.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (253.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (216.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (216.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/185/0/cluster185.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (216.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (216.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (252.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (252.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/174/0/cluster174.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (252.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (252.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (215.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (215.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/176/0/cluster176.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (215.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (215.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (251.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (251.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/183/0/cluster183.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (251.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (251.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (250.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (250.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/191/0/cluster191.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (250.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (250.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (249.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (249.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/173/0/cluster173.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (249.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (249.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (248.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (248.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/182/0/cluster182.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (248.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (248.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (247.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (247.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/209/0/cluster209.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (247.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (247.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (246.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (246.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/190/0/cluster190.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (246.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (246.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (245.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (245.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/181/0/cluster181.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (245.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (245.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (244.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (244.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/208/0/cluster208.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (244.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (244.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (243.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (243.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/180/0/cluster180.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (243.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (243.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (242.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (242.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/207/0/cluster207.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (242.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (242.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (241.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (241.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/206/0/cluster206.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (241.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (241.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (240.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (240.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/205/0/cluster205.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (240.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (240.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (239.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (239.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/214/0/cluster214.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (239.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (239.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (238.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (238.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/204/0/cluster204.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (238.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (238.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (237.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (237.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/213/0/cluster213.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (237.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (237.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (236.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (236.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/203/0/cluster203.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (236.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (236.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (235.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (235.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/212/0/cluster212.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (235.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (235.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (234.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (234.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/202/0/cluster202.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (234.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (234.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (233.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (233.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/211/0/cluster211.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (233.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (233.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (232.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (232.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/201/0/cluster201.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (232.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (232.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (231.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (231.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/210/0/cluster210.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (231.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (231.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (230.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (230.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/200/0/cluster200.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (230.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (230.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (229.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (229.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/199/0/cluster199.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (229.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (229.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (228.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (228.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/189/0/cluster189.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (228.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (228.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (227.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (227.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/198/0/cluster198.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (227.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (227.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (226.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (226.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/197/0/cluster197.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (226.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (226.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (225.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (225.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/188/0/cluster188.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (225.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (225.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (224.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (224.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/179/0/cluster179.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (224.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (224.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (223.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (223.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/196/0/cluster196.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (223.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (223.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (222.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (222.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/187/0/cluster187.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (222.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (222.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (221.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (221.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/178/0/cluster178.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (221.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (221.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (220.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (220.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/195/0/cluster195.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (220.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (220.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (256.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (256.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/193/0/cluster193.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (256.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (256.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:28 [25697] (219.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:23:28 [25697] (219.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/186/0/cluster186.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:23:28 [25697] (219.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:23:28 [25697] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:23:28 [25697] (219.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:23:30 [25697] Evaluating staleness of remote job statuses.
09/08/17 16:23:33 [25697] resource  is now up
09/08/17 16:23:33 [25697] in doContactSchedd()
09/08/17 16:23:33 [25697] querying for removed/held jobs
09/08/17 16:23:33 [25697] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:23:33 [25697] Fetched 42 job ads from schedd
09/08/17 16:23:33 [25697] Updating classad values for 252.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 253.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 254.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 255.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 256.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 215.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 216.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 217.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 218.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 219.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 220.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 221.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 222.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 223.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 224.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 225.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 226.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 227.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 228.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 229.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 230.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 231.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 232.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 233.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 234.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 235.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 236.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 237.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 238.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 239.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 240.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 241.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 242.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 243.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 244.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 245.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 246.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 247.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 248.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 249.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 250.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Updating classad values for 251.0:
09/08/17 16:23:33 [25697]    CurrentStatusUnknown = false
09/08/17 16:23:33 [25697]    GridJobId = undefined
09/08/17 16:23:33 [25697]    LastRemoteStatusUpdate = 0
09/08/17 16:23:33 [25697]    Managed = "ScheddDone"
09/08/17 16:23:33 [25697] Deleting job 252.0 from schedd
09/08/17 16:23:33 [25697] Deleting job 253.0 from schedd
09/08/17 16:23:33 [25697] Deleting job 254.0 from schedd
09/08/17 16:23:33 [25697] Deleting job 255.0 from schedd
09/08/17 16:23:33 [25697] Deleting job 256.0 from schedd
09/08/17 16:23:33 [25697] Deleting job 215.0 from schedd
09/08/17 16:23:33 [25697] Deleting job 216.0 from schedd
09/08/17 16:23:33 [25697] Deleting job 217.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 218.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 219.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 220.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 221.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 222.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 223.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 224.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 225.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 226.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 227.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 228.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 229.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 230.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 231.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 232.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 233.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 234.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 235.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 236.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 237.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 238.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 239.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 240.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 241.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 242.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 243.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 244.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 245.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 246.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 247.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 248.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 249.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 250.0 from schedd
09/08/17 16:23:34 [25697] Deleting job 251.0 from schedd
09/08/17 16:23:34 [25697] No jobs left, shutting down
09/08/17 16:23:34 [25697] leaving doContactSchedd()
09/08/17 16:23:34 [25697] Got SIGTERM. Performing graceful shutdown.
09/08/17 16:23:34 [25697] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 16:23:34 [25697] **** condor_gridmanager (condor_GRIDMANAGER) pid 25697 EXITING WITH STATUS 0
09/08/17 16:23:36 Result of reading /etc/issue:  \S
 
09/08/17 16:23:36 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:23:36 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:23:36 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:23:36 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:23:36 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:23:36 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:23:36 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:23:36 ******************************************************
09/08/17 16:23:36 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:23:36 ** /usr/sbin/condor_gridmanager
09/08/17 16:23:36 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:23:36 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:23:36 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:23:36 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:23:36 ** PID = 25754
09/08/17 16:23:36 ** Log last touched 9/8 16:23:34
09/08/17 16:23:36 ******************************************************
09/08/17 16:23:36 Using config source: /etc/condor-ce/condor_config
09/08/17 16:23:36 Using local config sources: 
09/08/17 16:23:36    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:23:36    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:23:36    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:23:36    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:23:36 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:23:36 CLASSAD_CACHING is ENABLED
09/08/17 16:23:36 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:23:36 SharedPortEndpoint: waiting for connections to named socket 25559_60a8_7
09/08/17 16:23:36 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_7>
09/08/17 16:23:36 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_7>
09/08/17 16:23:36 Setting maximum accepts per cycle 8.
09/08/17 16:23:36 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:23:36 [25754] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:23:36 [25754] DaemonCore: No more children processes to reap.
09/08/17 16:23:36 [25754] DaemonCore: in SendAliveToParent()
09/08/17 16:23:36 [25754] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:36 [25754] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:36 [25754] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:36 [25754] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:36 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:36 [25754] Completed DC_CHILDALIVE to daemon at <128.55.162.46:13716>
09/08/17 16:23:36 [25754] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:23:36 [25754] Checking proxies
09/08/17 16:23:39 [25754] Received ADD_JOBS signal
09/08/17 16:23:39 [25754] in doContactSchedd()
09/08/17 16:23:39 [25754] querying for new jobs
09/08/17 16:23:39 [25754] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:23:39 [25754] Using job type INFNBatch for job 260.0
09/08/17 16:23:39 [25754] (260.0) SetJobLeaseTimers()
09/08/17 16:23:39 [25754] Found job 260.0 --- inserting
09/08/17 16:23:39 [25754] Using job type INFNBatch for job 259.0
09/08/17 16:23:39 [25754] (259.0) SetJobLeaseTimers()
09/08/17 16:23:39 [25754] Found job 259.0 --- inserting
09/08/17 16:23:39 [25754] Fetched 2 new job ads from schedd
09/08/17 16:23:39 [25754] querying for removed/held jobs
09/08/17 16:23:39 [25754] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:23:39 [25754] Fetched 0 job ads from schedd
09/08/17 16:23:39 [25754] leaving doContactSchedd()
09/08/17 16:23:39 [25754] gahp server not up yet, delaying ping
09/08/17 16:23:39 [25754] *** UpdateLeases called
09/08/17 16:23:39 [25754]     Leases not supported, cancelling timer
09/08/17 16:23:39 [25754] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_7\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_7\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_7>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504913019
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:13716>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:23:39 [25754] Trying to update collector <128.55.162.46:9619>
09/08/17 16:23:39 [25754] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:23:39 [25754] File descriptor limits: max 4096, safe 3277
09/08/17 16:23:39 [25754] (260.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:23:39 [25754] GAHP server pid = 25756
09/08/17 16:23:39 [25754] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:23:39 [25754] GAHP[25756] <- 'COMMANDS'
09/08/17 16:23:39 [25754] GAHP[25756] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:23:39 [25754] GAHP[25756] <- 'ASYNC_MODE_ON'
09/08/17 16:23:39 [25754] GAHP[25756] -> 'S' 'Async mode on'
09/08/17 16:23:39 [25754] (260.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:39 [25754] (260.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:23:39 [25754] (260.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:23:39 [25754] (260.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:23:39 [25754] (259.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:23:39 [25754] (259.0) gm state change: GM_INIT -> GM_START
09/08/17 16:23:39 [25754] (259.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:23:39 [25754] (259.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:23:39 [25754] (259.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:23:39 [25754] This process has a valid certificate & key
09/08/17 16:23:39 [25754] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:39 [25754] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:39 [25754] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:39 [25754] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:23:39 [25754] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:23:39 [25754] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:23:39 [25754] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:39 [25754] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:39 [25754] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:39 [25754] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:23:39 [25754] IPVERIFY: ip found is 1
09/08/17 16:23:41 [25754] Evaluating staleness of remote job statuses.
09/08/17 16:23:44 [25754] resource  is now up
09/08/17 16:23:44 [25754] in doContactSchedd()
09/08/17 16:23:44 [25754] querying for removed/held jobs
09/08/17 16:23:44 [25754] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:23:44 [25754] Fetched 0 job ads from schedd
09/08/17 16:23:44 [25754] Updating classad values for 259.0:
09/08/17 16:23:44 [25754]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#259.0#1504913010"
09/08/17 16:23:44 [25754]    LastRemoteStatusUpdate = 1504913019
09/08/17 16:23:44 [25754] Updating classad values for 260.0:
09/08/17 16:23:44 [25754]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#260.0#1504913010"
09/08/17 16:23:44 [25754]    LastRemoteStatusUpdate = 1504913019
09/08/17 16:23:44 [25754] leaving doContactSchedd()
09/08/17 16:23:44 [25754] (259.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:23:44 [25754] (259.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:23:44 [25754] (259.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:44 [25754] GAHP[25756] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#259.0#1504913010";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:44 [25754] GAHP[25756] -> 'S'
09/08/17 16:23:44 [25754] (260.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:23:44 [25754] (260.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:23:44 [25754] (260.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:23:44 [25754] GAHP[25756] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#260.0#1504913010";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0/test.sh"\ ]'
09/08/17 16:23:44 [25754] GAHP[25756] -> 'S'
09/08/17 16:23:44 [25754] GAHP[25756] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:23:44 [25754] GAHP[25756] <- 'RESULTS'
09/08/17 16:23:44 [25754] GAHP[25756] -> EOF
09/08/17 16:23:44 [25754] GAHP command 'RESULTS' failed
09/08/17 16:23:44 [25754] DaemonCore: No more children processes to reap.
09/08/17 16:23:44 [25754] GAHP[25756] <- 'RESULTS'
09/08/17 16:23:44 [25754] GAHP[25756] -> EOF
09/08/17 16:23:44 [25754] GAHP command 'RESULTS' failed
09/08/17 16:23:44 [25754] ERROR "Gahp Server (pid=25756) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:28:36 Result of reading /etc/issue:  \S
 
09/08/17 16:28:36 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:28:36 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:28:36 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:28:36 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:28:36 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:28:36 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:28:36 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:28:36 ******************************************************
09/08/17 16:28:36 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:28:36 ** /usr/sbin/condor_gridmanager
09/08/17 16:28:36 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:28:36 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:28:36 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:28:36 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:28:36 ** PID = 25830
09/08/17 16:28:36 ** Log last touched 9/8 16:23:44
09/08/17 16:28:36 ******************************************************
09/08/17 16:28:36 Using config source: /etc/condor-ce/condor_config
09/08/17 16:28:36 Using local config sources: 
09/08/17 16:28:36    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:28:36    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:28:36    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:28:36    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:28:36 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:28:36 CLASSAD_CACHING is ENABLED
09/08/17 16:28:36 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:28:36 SharedPortEndpoint: waiting for connections to named socket 25559_60a8_10
09/08/17 16:28:36 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_10>
09/08/17 16:28:36 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_10>
09/08/17 16:28:36 Setting maximum accepts per cycle 8.
09/08/17 16:28:36 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:28:36 [25830] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:28:36 [25830] DaemonCore: No more children processes to reap.
09/08/17 16:28:36 [25830] DaemonCore: in SendAliveToParent()
09/08/17 16:28:36 [25830] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:36 [25830] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:36 [25830] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:36 [25830] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:36 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:36 [25830] Completed DC_CHILDALIVE to daemon at <128.55.162.46:13716>
09/08/17 16:28:36 [25830] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:28:36 [25830] Checking proxies
09/08/17 16:28:39 [25830] Received ADD_JOBS signal
09/08/17 16:28:39 [25830] in doContactSchedd()
09/08/17 16:28:39 [25830] querying for new jobs
09/08/17 16:28:39 [25830] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:28:39 [25830] Using job type INFNBatch for job 260.0
09/08/17 16:28:39 [25830] (260.0) SetJobLeaseTimers()
09/08/17 16:28:39 [25830] Found job 260.0 --- inserting
09/08/17 16:28:39 [25830] Using job type INFNBatch for job 259.0
09/08/17 16:28:39 [25830] (259.0) SetJobLeaseTimers()
09/08/17 16:28:39 [25830] Found job 259.0 --- inserting
09/08/17 16:28:39 [25830] Fetched 2 new job ads from schedd
09/08/17 16:28:39 [25830] querying for removed/held jobs
09/08/17 16:28:39 [25830] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:28:39 [25830] Fetched 0 job ads from schedd
09/08/17 16:28:39 [25830] leaving doContactSchedd()
09/08/17 16:28:39 [25830] gahp server not up yet, delaying ping
09/08/17 16:28:39 [25830] *** UpdateLeases called
09/08/17 16:28:39 [25830]     Leases not supported, cancelling timer
09/08/17 16:28:39 [25830] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_10\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25559_60a8_10\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25559_60a8_10>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504913319
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:13716>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:28:39 [25830] Trying to update collector <128.55.162.46:9619>
09/08/17 16:28:39 [25830] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:28:39 [25830] File descriptor limits: max 4096, safe 3277
09/08/17 16:28:39 [25830] (260.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:28:39 [25830] GAHP server pid = 25833
09/08/17 16:28:39 [25830] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:28:39 [25830] GAHP[25833] <- 'COMMANDS'
09/08/17 16:28:39 [25830] GAHP[25833] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:28:39 [25830] GAHP[25833] <- 'ASYNC_MODE_ON'
09/08/17 16:28:39 [25830] GAHP[25833] -> 'S' 'Async mode on'
09/08/17 16:28:39 [25830] (260.0) gm state change: GM_INIT -> GM_START
09/08/17 16:28:39 [25830] (260.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:28:39 [25830] (260.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:28:39 [25830] GAHP[25833] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#260.0#1504913010";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0/test.sh"\ ]'
09/08/17 16:28:39 [25830] GAHP[25833] -> 'S'
09/08/17 16:28:39 [25830] (259.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:28:39 [25830] (259.0) gm state change: GM_INIT -> GM_START
09/08/17 16:28:39 [25830] (259.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:28:39 [25830] (259.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:28:39 [25830] GAHP[25833] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#259.0#1504913010";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0/test.sh"\ ]'
09/08/17 16:28:39 [25830] GAHP[25833] -> 'S'
09/08/17 16:28:39 [25830] GAHP[25833] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:28:39 [25830] This process has a valid certificate & key
09/08/17 16:28:39 [25830] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:28:39 [25830] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:28:39 [25830] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:28:39 [25830] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:28:39 [25830] IPVERIFY: ip found is 1
09/08/17 16:28:39 [25830] DaemonCore: No more children processes to reap.
09/08/17 16:28:39 [25830] GAHP[25833] <- 'RESULTS'
09/08/17 16:28:39 [25830] GAHP[25833] -> EOF
09/08/17 16:28:39 [25830] GAHP command 'RESULTS' failed
09/08/17 16:28:39 [25830] ERROR "Gahp Server (pid=25833) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:31:37 Result of reading /etc/issue:  \S
 
09/08/17 16:31:37 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:31:37 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:31:37 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:31:37 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:31:37 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:31:37 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:31:37 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:31:37 ******************************************************
09/08/17 16:31:37 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:31:37 ** /usr/sbin/condor_gridmanager
09/08/17 16:31:37 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:31:37 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:31:37 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:31:37 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:31:37 ** PID = 25950
09/08/17 16:31:37 ** Log last touched 9/8 16:28:39
09/08/17 16:31:37 ******************************************************
09/08/17 16:31:37 Using config source: /etc/condor-ce/condor_config
09/08/17 16:31:37 Using local config sources: 
09/08/17 16:31:37    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:31:37    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:31:37    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:31:37    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:31:37 config Macros = 177, Sorted = 177, StringBytes = 14928, TablesBytes = 6604
09/08/17 16:31:37 CLASSAD_CACHING is ENABLED
09/08/17 16:31:37 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:31:37 SharedPortEndpoint: waiting for connections to named socket 25937_b678_3
09/08/17 16:31:37 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_3>
09/08/17 16:31:37 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_3>
09/08/17 16:31:37 Setting maximum accepts per cycle 8.
09/08/17 16:31:37 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:31:37 [25950] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:31:37 [25950] DaemonCore: No more children processes to reap.
09/08/17 16:31:37 [25950] DaemonCore: in SendAliveToParent()
09/08/17 16:31:37 [25950] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:37 [25950] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:37 [25950] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:37 [25950] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:37 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:37 [25950] Completed DC_CHILDALIVE to daemon at <128.55.162.46:44734>
09/08/17 16:31:37 [25950] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:31:37 [25950] Checking proxies
09/08/17 16:31:40 [25950] Received ADD_JOBS signal
09/08/17 16:31:40 [25950] in doContactSchedd()
09/08/17 16:31:40 [25950] querying for new jobs
09/08/17 16:31:40 [25950] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:31:40 [25950] Using job type INFNBatch for job 259.0
09/08/17 16:31:40 [25950] (259.0) SetJobLeaseTimers()
09/08/17 16:31:40 [25950] Found job 259.0 --- inserting
09/08/17 16:31:40 [25950] Using job type INFNBatch for job 260.0
09/08/17 16:31:40 [25950] (260.0) SetJobLeaseTimers()
09/08/17 16:31:40 [25950] Found job 260.0 --- inserting
09/08/17 16:31:40 [25950] Fetched 2 new job ads from schedd
09/08/17 16:31:40 [25950] querying for removed/held jobs
09/08/17 16:31:40 [25950] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:31:40 [25950] Fetched 0 job ads from schedd
09/08/17 16:31:40 [25950] leaving doContactSchedd()
09/08/17 16:31:40 [25950] gahp server not up yet, delaying ping
09/08/17 16:31:40 [25950] *** UpdateLeases called
09/08/17 16:31:40 [25950]     Leases not supported, cancelling timer
09/08/17 16:31:40 [25950] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25937_b678_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25937_b678_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504913500
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:44734>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:31:40 [25950] Trying to update collector <128.55.162.46:9619>
09/08/17 16:31:40 [25950] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:31:40 [25950] File descriptor limits: max 4096, safe 3277
09/08/17 16:31:40 [25950] (259.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:31:40 [25950] GAHP server pid = 25956
09/08/17 16:31:40 [25950] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:31:40 [25950] GAHP[25956] <- 'COMMANDS'
09/08/17 16:31:40 [25950] GAHP[25956] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:31:40 [25950] GAHP[25956] <- 'ASYNC_MODE_ON'
09/08/17 16:31:40 [25950] GAHP[25956] -> 'S' 'Async mode on'
09/08/17 16:31:40 [25950] (259.0) gm state change: GM_INIT -> GM_START
09/08/17 16:31:40 [25950] (259.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:31:40 [25950] (259.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:31:40 [25950] GAHP[25956] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#259.0#1504913010";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/257/0/cluster257.proc0.subproc0/test.sh"\ ]'
09/08/17 16:31:40 [25950] GAHP[25956] -> 'S'
09/08/17 16:31:40 [25950] (260.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:31:40 [25950] (260.0) gm state change: GM_INIT -> GM_START
09/08/17 16:31:40 [25950] (260.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:31:40 [25950] (260.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:31:40 [25950] GAHP[25956] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#260.0#1504913010";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/258/0/cluster258.proc0.subproc0/test.sh"\ ]'
09/08/17 16:31:40 [25950] GAHP[25956] -> 'S'
09/08/17 16:31:40 [25950] This process has a valid certificate & key
09/08/17 16:31:40 [25950] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:40 [25950] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:40 [25950] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:40 [25950] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:31:40 [25950] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:31:40 [25950] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:31:40 [25950] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:40 [25950] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:40 [25950] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:40 [25950] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:31:40 [25950] IPVERIFY: ip found is 1
09/08/17 16:31:41 [25950] GAHP[25956] <- 'RESULTS'
09/08/17 16:31:41 [25950] GAHP[25956] -> 'R'
09/08/17 16:31:41 [25950] GAHP[25956] -> 'S' '1'
09/08/17 16:31:41 [25950] GAHP[25956] -> '2' '0' 'No error' 'slurm/20170908/144160'
09/08/17 16:31:41 [25950] (259.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:31:41 [25950] (259.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 16:31:42 [25950] Evaluating staleness of remote job statuses.
09/08/17 16:31:42 [25950] GAHP[25956] <- 'RESULTS'
09/08/17 16:31:42 [25950] GAHP[25956] -> 'R'
09/08/17 16:31:42 [25950] GAHP[25956] -> 'S' '1'
09/08/17 16:31:42 [25950] GAHP[25956] -> '3' '0' 'No error' 'slurm/20170908/144161'
09/08/17 16:31:42 [25950] (260.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:31:42 [25950] (260.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 16:31:45 [25950] resource  is now up
09/08/17 16:31:45 [25950] in doContactSchedd()
09/08/17 16:31:45 [25950] querying for removed/held jobs
09/08/17 16:31:45 [25950] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:31:45 [25950] Fetched 0 job ads from schedd
09/08/17 16:31:45 [25950] Updating classad values for 259.0:
09/08/17 16:31:45 [25950]    DelegatedProxyExpiration = 1505103329
09/08/17 16:31:45 [25950]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#259.0#1504913010 slurm/20170908/144160"
09/08/17 16:31:45 [25950] Updating classad values for 260.0:
09/08/17 16:31:45 [25950]    DelegatedProxyExpiration = 1505103329
09/08/17 16:31:45 [25950]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#260.0#1504913010 slurm/20170908/144161"
09/08/17 16:31:45 [25950] leaving doContactSchedd()
09/08/17 16:31:45 [25950] (259.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 16:31:45 [25950] (259.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 16:31:45 [25950] (260.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 16:31:45 [25950] (260.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 16:32:37 [25950] Received CHECK_LEASES signal
09/08/17 16:32:37 [25950] in doContactSchedd()
09/08/17 16:32:37 [25950] querying for renewed leases
09/08/17 16:32:37 [25950] querying for removed/held jobs
09/08/17 16:32:37 [25950] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:32:37 [25950] Fetched 0 job ads from schedd
09/08/17 16:32:37 [25950] leaving doContactSchedd()
09/08/17 16:32:40 [25950] GAHP[25956] <- 'RESULTS'
09/08/17 16:32:40 [25950] GAHP[25956] -> 'S' '0'
09/08/17 16:32:42 [25950] Evaluating staleness of remote job statuses.
09/08/17 16:32:45 [25950] (259.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 16:32:45 [25950] (259.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 16:32:45 [25950] GAHP[25956] <- 'BLAH_JOB_STATUS 4 slurm/20170908/144160'
09/08/17 16:32:45 [25950] GAHP[25956] -> 'S'
09/08/17 16:32:45 [25950] (260.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 16:32:45 [25950] (260.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 16:32:45 [25950] GAHP[25956] <- 'BLAH_JOB_STATUS 5 slurm/20170908/144161'
09/08/17 16:32:45 [25950] GAHP[25956] -> 'S'
09/08/17 16:32:46 [25950] GAHP[25956] <- 'RESULTS'
09/08/17 16:32:46 [25950] GAHP[25956] -> 'R'
09/08/17 16:32:46 [25950] GAHP[25956] -> 'S' '1'
09/08/17 16:32:46 [25950] GAHP[25956] -> '5' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144161"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 16:32:46 [25950] (260.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 16:32:46 [25950] (260.0) ***ProcessRemoteAd
09/08/17 16:32:46 [25950] (260.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 16:32:46 [25950] (260.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 16:32:46 [25950] (260.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 16:32:46 [25950] in doContactSchedd()
09/08/17 16:32:46 [25950] querying for removed/held jobs
09/08/17 16:32:46 [25950] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:32:46 [25950] Fetched 0 job ads from schedd
09/08/17 16:32:46 [25950] Updating classad values for 260.0:
09/08/17 16:32:46 [25950]    EnteredCurrentStatus = 1504913566
09/08/17 16:32:46 [25950]    ExitCode = 0
09/08/17 16:32:46 [25950]    GridJobStatus = "COMPLETED"
09/08/17 16:32:46 [25950]    ImageSize = 0
09/08/17 16:32:46 [25950]    JobStatus = 4
09/08/17 16:32:46 [25950]    LastRemoteStatusUpdate = 1504913566
09/08/17 16:32:46 [25950]    RemoteUserCpu = 0
09/08/17 16:32:46 [25950]    RemoteWallClockTime = 0.0
09/08/17 16:32:46 [25950] leaving doContactSchedd()
09/08/17 16:32:46 [25950] (260.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 16:32:46 [25950] (260.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 16:32:46 [25950] (260.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 16:32:46 [25950] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:32:46 [25950] (260.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:32:46 [25950] GAHP[25956] <- 'RESULTS'
09/08/17 16:32:46 [25950] GAHP[25956] -> 'R'
09/08/17 16:32:46 [25950] GAHP[25956] -> 'S' '1'
09/08/17 16:32:46 [25950] GAHP[25956] -> '4' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144160"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 16:32:46 [25950] (259.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 16:32:46 [25950] (259.0) ***ProcessRemoteAd
09/08/17 16:32:46 [25950] (259.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 16:32:46 [25950] (259.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 16:32:46 [25950] (259.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 16:32:51 [25950] in doContactSchedd()
09/08/17 16:32:51 [25950] querying for removed/held jobs
09/08/17 16:32:51 [25950] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:32:51 [25950] Fetched 1 job ads from schedd
09/08/17 16:32:51 [25950] Updating classad values for 259.0:
09/08/17 16:32:51 [25950]    EnteredCurrentStatus = 1504913566
09/08/17 16:32:51 [25950]    ExitCode = 0
09/08/17 16:32:51 [25950]    GridJobStatus = "COMPLETED"
09/08/17 16:32:51 [25950]    ImageSize = 0
09/08/17 16:32:51 [25950]    JobStatus = 4
09/08/17 16:32:51 [25950]    LastRemoteStatusUpdate = 1504913566
09/08/17 16:32:51 [25950]    RemoteUserCpu = 0
09/08/17 16:32:51 [25950]    RemoteWallClockTime = 0.0
09/08/17 16:32:51 [25950] Updating classad values for 260.0:
09/08/17 16:32:51 [25950]    CurrentStatusUnknown = false
09/08/17 16:32:51 [25950]    GridJobId = undefined
09/08/17 16:32:51 [25950]    LastRemoteStatusUpdate = 0
09/08/17 16:32:51 [25950]    Managed = "ScheddDone"
09/08/17 16:32:51 [25950] Deleting job 260.0 from schedd
09/08/17 16:32:51 [25950] leaving doContactSchedd()
09/08/17 16:32:51 [25950] (259.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 16:32:51 [25950] (259.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 16:32:51 [25950] (259.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 16:32:51 [25950] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:32:52 [25950] (259.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:32:56 [25950] in doContactSchedd()
09/08/17 16:32:56 [25950] querying for removed/held jobs
09/08/17 16:32:56 [25950] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:32:56 [25950] Fetched 1 job ads from schedd
09/08/17 16:32:56 [25950] Updating classad values for 259.0:
09/08/17 16:32:56 [25950]    CurrentStatusUnknown = false
09/08/17 16:32:56 [25950]    GridJobId = undefined
09/08/17 16:32:56 [25950]    LastRemoteStatusUpdate = 0
09/08/17 16:32:56 [25950]    Managed = "ScheddDone"
09/08/17 16:32:56 [25950] Deleting job 259.0 from schedd
09/08/17 16:32:56 [25950] No jobs left, shutting down
09/08/17 16:32:56 [25950] leaving doContactSchedd()
09/08/17 16:32:56 [25950] Got SIGTERM. Performing graceful shutdown.
09/08/17 16:32:56 [25950] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 16:32:56 [25950] **** condor_gridmanager (condor_GRIDMANAGER) pid 25950 EXITING WITH STATUS 0
09/08/17 16:35:06 Result of reading /etc/issue:  \S
 
09/08/17 16:35:06 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:35:06 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:35:06 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:35:06 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:35:06 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:35:06 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:35:06 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:35:06 ******************************************************
09/08/17 16:35:06 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:35:06 ** /usr/sbin/condor_gridmanager
09/08/17 16:35:06 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:35:06 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:35:06 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:35:06 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:35:06 ** PID = 26302
09/08/17 16:35:06 ** Log last touched 9/8 16:32:56
09/08/17 16:35:06 ******************************************************
09/08/17 16:35:06 Using config source: /etc/condor-ce/condor_config
09/08/17 16:35:06 Using local config sources: 
09/08/17 16:35:06    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:35:06    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:35:06    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:35:06    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:35:06 config Macros = 177, Sorted = 177, StringBytes = 14928, TablesBytes = 6604
09/08/17 16:35:06 CLASSAD_CACHING is ENABLED
09/08/17 16:35:06 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:35:06 SharedPortEndpoint: waiting for connections to named socket 25937_b678_5
09/08/17 16:35:06 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_5>
09/08/17 16:35:06 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_5>
09/08/17 16:35:06 Setting maximum accepts per cycle 8.
09/08/17 16:35:06 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:35:06 [26302] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:35:06 [26302] DaemonCore: No more children processes to reap.
09/08/17 16:35:06 [26302] DaemonCore: in SendAliveToParent()
09/08/17 16:35:06 [26302] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:06 [26302] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:06 [26302] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:06 [26302] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:06 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:06 [26302] Completed DC_CHILDALIVE to daemon at <128.55.162.46:44734>
09/08/17 16:35:06 [26302] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:35:06 [26302] Checking proxies
09/08/17 16:35:09 [26302] Received ADD_JOBS signal
09/08/17 16:35:09 [26302] in doContactSchedd()
09/08/17 16:35:09 [26302] querying for new jobs
09/08/17 16:35:09 [26302] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 327.0
09/08/17 16:35:09 [26302] (327.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 327.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 326.0
09/08/17 16:35:09 [26302] (326.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 326.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 325.0
09/08/17 16:35:09 [26302] (325.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 325.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 324.0
09/08/17 16:35:09 [26302] (324.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 324.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 323.0
09/08/17 16:35:09 [26302] (323.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 323.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 322.0
09/08/17 16:35:09 [26302] (322.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 322.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 321.0
09/08/17 16:35:09 [26302] (321.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 321.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 320.0
09/08/17 16:35:09 [26302] (320.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 320.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 319.0
09/08/17 16:35:09 [26302] (319.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 319.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 318.0
09/08/17 16:35:09 [26302] (318.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 318.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 317.0
09/08/17 16:35:09 [26302] (317.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 317.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 316.0
09/08/17 16:35:09 [26302] (316.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 316.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 315.0
09/08/17 16:35:09 [26302] (315.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 315.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 314.0
09/08/17 16:35:09 [26302] (314.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 314.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 313.0
09/08/17 16:35:09 [26302] (313.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 313.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 312.0
09/08/17 16:35:09 [26302] (312.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 312.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 311.0
09/08/17 16:35:09 [26302] (311.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 311.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 310.0
09/08/17 16:35:09 [26302] (310.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 310.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 309.0
09/08/17 16:35:09 [26302] (309.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 309.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 308.0
09/08/17 16:35:09 [26302] (308.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 308.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 307.0
09/08/17 16:35:09 [26302] (307.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 307.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 306.0
09/08/17 16:35:09 [26302] (306.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 306.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 305.0
09/08/17 16:35:09 [26302] (305.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 305.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 304.0
09/08/17 16:35:09 [26302] (304.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 304.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 303.0
09/08/17 16:35:09 [26302] (303.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 303.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 332.0
09/08/17 16:35:09 [26302] (332.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 332.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 331.0
09/08/17 16:35:09 [26302] (331.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 331.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 330.0
09/08/17 16:35:09 [26302] (330.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 330.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 329.0
09/08/17 16:35:09 [26302] (329.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 329.0 --- inserting
09/08/17 16:35:09 [26302] Using job type INFNBatch for job 328.0
09/08/17 16:35:09 [26302] (328.0) SetJobLeaseTimers()
09/08/17 16:35:09 [26302] Found job 328.0 --- inserting
09/08/17 16:35:09 [26302] Fetched 30 new job ads from schedd
09/08/17 16:35:09 [26302] querying for removed/held jobs
09/08/17 16:35:09 [26302] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:35:09 [26302] Fetched 0 job ads from schedd
09/08/17 16:35:09 [26302] leaving doContactSchedd()
09/08/17 16:35:09 [26302] gahp server not up yet, delaying ping
09/08/17 16:35:09 [26302] *** UpdateLeases called
09/08/17 16:35:09 [26302]     Leases not supported, cancelling timer
09/08/17 16:35:09 [26302] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25937_b678_5\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25937_b678_5\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_5>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504913709
IdleJobs = 30
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:44734>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 30
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:35:09 [26302] Trying to update collector <128.55.162.46:9619>
09/08/17 16:35:09 [26302] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:35:09 [26302] File descriptor limits: max 4096, safe 3277
09/08/17 16:35:09 [26302] (327.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] GAHP server pid = 26309
09/08/17 16:35:09 [26302] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:35:09 [26302] GAHP[26309] <- 'COMMANDS'
09/08/17 16:35:09 [26302] GAHP[26309] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:35:09 [26302] GAHP[26309] <- 'ASYNC_MODE_ON'
09/08/17 16:35:09 [26302] GAHP[26309] -> 'S' 'Async mode on'
09/08/17 16:35:09 [26302] (327.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (327.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (327.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (327.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (326.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (326.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (326.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (326.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (326.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (325.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (325.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (325.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (325.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (325.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (324.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (324.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (324.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (324.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (324.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (323.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (323.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (323.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (323.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (323.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] This process has a valid certificate & key
09/08/17 16:35:09 [26302] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:35:09 [26302] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:35:09 [26302] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:35:09 [26302] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:09 [26302] IPVERIFY: ip found is 1
09/08/17 16:35:09 [26302] (322.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (322.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (322.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (322.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (322.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (321.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (321.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (321.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (321.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (321.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (320.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (320.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (320.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (320.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (320.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (319.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (319.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (319.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (319.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (319.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (318.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (318.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (318.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (318.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (318.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (317.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (317.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (317.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (317.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (317.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (316.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (316.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (316.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (316.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (316.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (315.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (315.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (315.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (315.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (315.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (314.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (314.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (314.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (314.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (314.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (313.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (313.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (313.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (313.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (313.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (312.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (312.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (312.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (312.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (312.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (311.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (311.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (311.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (311.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (311.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (310.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (310.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (310.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (310.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (310.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (309.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (309.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (309.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (309.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (309.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (308.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (308.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (308.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (308.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (308.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (307.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (307.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (307.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (307.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (307.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (306.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (306.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (306.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (306.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (306.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (305.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (305.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (305.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (305.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (305.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (304.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (304.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (304.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (304.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (304.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (303.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (303.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (303.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (303.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (303.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (332.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (332.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (332.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (332.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (332.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (331.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (331.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (331.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (331.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (331.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (330.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (330.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (330.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (330.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (330.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (329.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (329.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (329.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (329.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (329.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:09 [26302] (328.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:35:09 [26302] (328.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:09 [26302] (328.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:35:09 [26302] (328.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:35:09 [26302] (328.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:35:11 [26302] Evaluating staleness of remote job statuses.
09/08/17 16:35:14 [26302] resource  is now up
09/08/17 16:35:14 [26302] in doContactSchedd()
09/08/17 16:35:14 [26302] querying for removed/held jobs
09/08/17 16:35:14 [26302] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:35:14 [26302] Fetched 0 job ads from schedd
09/08/17 16:35:14 [26302] Updating classad values for 315.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#315.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 316.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#316.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 317.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#317.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 318.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#318.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 319.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#319.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 320.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#320.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 321.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#321.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 322.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#322.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 323.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#323.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 324.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#324.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 325.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#325.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 326.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#326.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 327.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#327.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 328.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#328.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 329.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#329.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 330.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#330.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 331.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#331.0#1504913703"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 332.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#332.0#1504913703"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 303.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#303.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 304.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#304.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 305.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#305.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 306.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#306.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 307.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#307.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 308.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#308.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 309.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#309.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 310.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#310.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 311.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#311.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 312.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#312.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 313.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#313.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] Updating classad values for 314.0:
09/08/17 16:35:14 [26302]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#314.0#1504913702"
09/08/17 16:35:14 [26302]    LastRemoteStatusUpdate = 1504913709
09/08/17 16:35:14 [26302] leaving doContactSchedd()
09/08/17 16:35:14 [26302] (315.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:35:14 [26302] (315.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:35:14 [26302] (315.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:35:14 [26302] GAHP[26309] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#315.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/288/0/cluster288.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/288/0/cluster288.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/288/0/cluster288.proc0.subproc0/test.sh"\ ]'
09/08/17 16:35:14 [26302] GAHP[26309] -> 'S'
09/08/17 16:35:14 [26302] (316.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:35:14 [26302] (316.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:35:14 [26302] (316.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:35:14 [26302] GAHP[26309] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#316.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/279/0/cluster279.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/279/0/cluster279.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/279/0/cluster279.proc0.subproc0/test.sh"\ ]'
09/08/17 16:35:14 [26302] GAHP[26309] -> 'S'
09/08/17 16:35:14 [26302] (317.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:35:14 [26302] (317.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:35:14 [26302] (317.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:35:14 [26302] GAHP[26309] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#317.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/289/0/cluster289.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/289/0/cluster289.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/289/0/cluster289.proc0.subproc0/test.sh"\ ]'
09/08/17 16:35:14 [26302] GAHP[26309] -> 'S'
09/08/17 16:35:14 [26302] (318.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:35:14 [26302] (318.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:35:14 [26302] (318.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:35:14 [26302] GAHP[26309] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#318.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/270/0/cluster270.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/270/0/cluster270.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/270/0/cluster270.proc0.subproc0/test.sh"\ ]'
09/08/17 16:35:14 [26302] GAHP[26309] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:35:14 [26302] GAHP[26309] -> EOF
09/08/17 16:35:14 [26302] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:35:16 Result of reading /etc/issue:  \S
 
09/08/17 16:35:16 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:35:16 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:35:16 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:35:16 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:35:16 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:35:16 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:35:16 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:35:16 ******************************************************
09/08/17 16:35:16 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:35:16 ** /usr/sbin/condor_gridmanager
09/08/17 16:35:16 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:35:16 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:35:16 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:35:16 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:35:16 ** PID = 26328
09/08/17 16:35:16 ** Log last touched 9/8 16:35:14
09/08/17 16:35:16 ******************************************************
09/08/17 16:35:16 Using config source: /etc/condor-ce/condor_config
09/08/17 16:35:16 Using local config sources: 
09/08/17 16:35:16    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:35:16    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:35:16    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:35:16    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:35:16 config Macros = 177, Sorted = 177, StringBytes = 14928, TablesBytes = 6604
09/08/17 16:35:16 CLASSAD_CACHING is ENABLED
09/08/17 16:35:16 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:35:16 SharedPortEndpoint: waiting for connections to named socket 25937_b678_6
09/08/17 16:35:16 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_6>
09/08/17 16:35:16 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_6>
09/08/17 16:35:16 Setting maximum accepts per cycle 8.
09/08/17 16:35:16 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:35:16 [26328] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:35:16 [26328] DaemonCore: No more children processes to reap.
09/08/17 16:35:16 [26328] DaemonCore: in SendAliveToParent()
09/08/17 16:35:16 [26328] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: ip found is 1
09/08/17 16:35:16 [26328] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: ip found is 1
09/08/17 16:35:16 [26328] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: ip found is 1
09/08/17 16:35:16 [26328] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:35:16 [26328] IPVERIFY: ip found is 1
09/08/17 16:35:16 [26328] Completed DC_CHILDALIVE to daemon at <128.55.162.46:44734>
09/08/17 16:35:16 [26328] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:35:16 [26328] Checking proxies
09/08/17 16:35:19 [26328] Received ADD_JOBS signal
09/08/17 16:35:19 [26328] in doContactSchedd()
09/08/17 16:35:19 [26328] querying for new jobs
09/08/17 16:35:19 [26328] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 327.0
09/08/17 16:35:19 [26328] (327.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 327.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 326.0
09/08/17 16:35:19 [26328] (326.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 326.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 325.0
09/08/17 16:35:19 [26328] (325.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 325.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 324.0
09/08/17 16:35:19 [26328] (324.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 324.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 323.0
09/08/17 16:35:19 [26328] (323.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 323.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 322.0
09/08/17 16:35:19 [26328] (322.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 322.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 321.0
09/08/17 16:35:19 [26328] (321.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 321.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 320.0
09/08/17 16:35:19 [26328] (320.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 320.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 319.0
09/08/17 16:35:19 [26328] (319.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 319.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 318.0
09/08/17 16:35:19 [26328] (318.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 318.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 317.0
09/08/17 16:35:19 [26328] (317.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 317.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 316.0
09/08/17 16:35:19 [26328] (316.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 316.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 315.0
09/08/17 16:35:19 [26328] (315.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 315.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 314.0
09/08/17 16:35:19 [26328] (314.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 314.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 313.0
09/08/17 16:35:19 [26328] (313.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 313.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 312.0
09/08/17 16:35:19 [26328] (312.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 312.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 311.0
09/08/17 16:35:19 [26328] (311.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 311.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 310.0
09/08/17 16:35:19 [26328] (310.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 310.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 309.0
09/08/17 16:35:19 [26328] (309.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 309.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 308.0
09/08/17 16:35:19 [26328] (308.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 308.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 344.0
09/08/17 16:35:19 [26328] (344.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 344.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 307.0
09/08/17 16:35:19 [26328] (307.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 307.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 343.0
09/08/17 16:35:19 [26328] (343.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 343.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 306.0
09/08/17 16:35:19 [26328] (306.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 306.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 342.0
09/08/17 16:35:19 [26328] (342.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 342.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 305.0
09/08/17 16:35:19 [26328] (305.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 305.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 341.0
09/08/17 16:35:19 [26328] (341.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 341.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 304.0
09/08/17 16:35:19 [26328] (304.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 304.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 340.0
09/08/17 16:35:19 [26328] (340.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 340.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 303.0
09/08/17 16:35:19 [26328] (303.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 303.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 339.0
09/08/17 16:35:19 [26328] (339.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 339.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 338.0
09/08/17 16:35:19 [26328] (338.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 338.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 337.0
09/08/17 16:35:19 [26328] (337.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 337.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 336.0
09/08/17 16:35:19 [26328] (336.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 336.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 335.0
09/08/17 16:35:19 [26328] (335.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 335.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 334.0
09/08/17 16:35:19 [26328] (334.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 334.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 333.0
09/08/17 16:35:19 [26328] (333.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 333.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 332.0
09/08/17 16:35:19 [26328] (332.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 332.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 331.0
09/08/17 16:35:19 [26328] (331.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 331.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 330.0
09/08/17 16:35:19 [26328] (330.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 330.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 329.0
09/08/17 16:35:19 [26328] (329.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 329.0 --- inserting
09/08/17 16:35:19 [26328] Using job type INFNBatch for job 328.0
09/08/17 16:35:19 [26328] (328.0) SetJobLeaseTimers()
09/08/17 16:35:19 [26328] Found job 328.0 --- inserting
09/08/17 16:35:19 [26328] Fetched 42 new job ads from schedd
09/08/17 16:35:19 [26328] querying for removed/held jobs
09/08/17 16:35:19 [26328] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:35:19 [26328] Fetched 0 job ads from schedd
09/08/17 16:35:19 [26328] leaving doContactSchedd()
09/08/17 16:35:19 [26328] gahp server not up yet, delaying ping
09/08/17 16:35:19 [26328] *** UpdateLeases called
09/08/17 16:35:19 [26328]     Leases not supported, cancelling timer
09/08/17 16:35:19 [26328] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25937_b678_6\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"25937_b678_6\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=25937_b678_6>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504913719
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:44734>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:35:19 [26328] Trying to update collector <128.55.162.46:9619>
09/08/17 16:35:19 [26328] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:35:19 [26328] File descriptor limits: max 4096, safe 3277
09/08/17 16:35:19 [26328] (327.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:35:19 [26328] GAHP server pid = 26332
09/08/17 16:35:19 [26328] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:35:19 [26328] GAHP[26332] <- 'COMMANDS'
09/08/17 16:35:19 [26328] GAHP[26332] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:35:19 [26328] GAHP[26332] <- 'ASYNC_MODE_ON'
09/08/17 16:35:19 [26328] GAHP[26332] -> 'S' 'Async mode on'
09/08/17 16:35:19 [26328] (327.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:19 [26328] (327.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:35:19 [26328] (327.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:35:19 [26328] GAHP[26332] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#327.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/test.sh"\ ]'
09/08/17 16:35:19 [26328] GAHP[26332] -> 'S'
09/08/17 16:35:19 [26328] (326.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:35:19 [26328] (326.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:19 [26328] (326.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:35:19 [26328] (326.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:35:19 [26328] GAHP[26332] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#326.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/test.sh"\ ]'
09/08/17 16:35:19 [26328] GAHP[26332] -> 'S'
09/08/17 16:35:19 [26328] (325.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:35:19 [26328] (325.0) gm state change: GM_INIT -> GM_START
09/08/17 16:35:19 [26328] (325.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:35:19 [26328] (325.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:35:19 [26328] GAHP[26332] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#325.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/test.sh"\ ]'
09/08/17 16:35:19 [26328] GAHP[26332] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:35:19 [26328] GAHP[26332] -> EOF
09/08/17 16:35:19 [26328] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:39:20 Result of reading /etc/issue:  \S
 
09/08/17 16:39:20 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:39:20 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:39:20 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:39:20 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:39:20 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:39:20 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:39:20 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:39:20 ******************************************************
09/08/17 16:39:20 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:39:20 ** /usr/sbin/condor_gridmanager
09/08/17 16:39:20 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:39:20 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:39:20 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:39:20 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:39:20 ** PID = 26674
09/08/17 16:39:20 ** Log last touched 9/8 16:35:19
09/08/17 16:39:20 ******************************************************
09/08/17 16:39:20 Using config source: /etc/condor-ce/condor_config
09/08/17 16:39:20 Using local config sources: 
09/08/17 16:39:20    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:39:20    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:39:20    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:39:20    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:39:20 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:39:20 CLASSAD_CACHING is ENABLED
09/08/17 16:39:20 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:39:20 SharedPortEndpoint: waiting for connections to named socket 26662_8a75_3
09/08/17 16:39:20 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26662_8a75_3>
09/08/17 16:39:20 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26662_8a75_3>
09/08/17 16:39:20 Setting maximum accepts per cycle 8.
09/08/17 16:39:20 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:39:20 [26674] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:39:20 [26674] DaemonCore: No more children processes to reap.
09/08/17 16:39:20 [26674] DaemonCore: in SendAliveToParent()
09/08/17 16:39:20 [26674] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: ip found is 1
09/08/17 16:39:20 [26674] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: ip found is 1
09/08/17 16:39:20 [26674] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: ip found is 1
09/08/17 16:39:20 [26674] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:39:20 [26674] IPVERIFY: ip found is 1
09/08/17 16:39:20 [26674] Completed DC_CHILDALIVE to daemon at <128.55.162.46:18839>
09/08/17 16:39:20 [26674] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:39:20 [26674] Checking proxies
09/08/17 16:39:23 [26674] Received ADD_JOBS signal
09/08/17 16:39:23 [26674] in doContactSchedd()
09/08/17 16:39:23 [26674] querying for new jobs
09/08/17 16:39:23 [26674] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 327.0
09/08/17 16:39:23 [26674] (327.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 327.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 326.0
09/08/17 16:39:23 [26674] (326.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 326.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 325.0
09/08/17 16:39:23 [26674] (325.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 325.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 324.0
09/08/17 16:39:23 [26674] (324.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 324.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 323.0
09/08/17 16:39:23 [26674] (323.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 323.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 322.0
09/08/17 16:39:23 [26674] (322.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 322.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 321.0
09/08/17 16:39:23 [26674] (321.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 321.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 320.0
09/08/17 16:39:23 [26674] (320.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 320.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 319.0
09/08/17 16:39:23 [26674] (319.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 319.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 318.0
09/08/17 16:39:23 [26674] (318.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 318.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 317.0
09/08/17 16:39:23 [26674] (317.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 317.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 316.0
09/08/17 16:39:23 [26674] (316.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 316.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 315.0
09/08/17 16:39:23 [26674] (315.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 315.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 314.0
09/08/17 16:39:23 [26674] (314.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 314.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 313.0
09/08/17 16:39:23 [26674] (313.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 313.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 312.0
09/08/17 16:39:23 [26674] (312.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 312.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 311.0
09/08/17 16:39:23 [26674] (311.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 311.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 310.0
09/08/17 16:39:23 [26674] (310.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 310.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 309.0
09/08/17 16:39:23 [26674] (309.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 309.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 308.0
09/08/17 16:39:23 [26674] (308.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 308.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 344.0
09/08/17 16:39:23 [26674] (344.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 344.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 307.0
09/08/17 16:39:23 [26674] (307.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 307.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 343.0
09/08/17 16:39:23 [26674] (343.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 343.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 306.0
09/08/17 16:39:23 [26674] (306.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 306.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 342.0
09/08/17 16:39:23 [26674] (342.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 342.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 305.0
09/08/17 16:39:23 [26674] (305.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 305.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 341.0
09/08/17 16:39:23 [26674] (341.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 341.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 304.0
09/08/17 16:39:23 [26674] (304.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 304.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 340.0
09/08/17 16:39:23 [26674] (340.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 340.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 303.0
09/08/17 16:39:23 [26674] (303.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 303.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 339.0
09/08/17 16:39:23 [26674] (339.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 339.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 338.0
09/08/17 16:39:23 [26674] (338.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 338.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 337.0
09/08/17 16:39:23 [26674] (337.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 337.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 336.0
09/08/17 16:39:23 [26674] (336.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 336.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 335.0
09/08/17 16:39:23 [26674] (335.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 335.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 334.0
09/08/17 16:39:23 [26674] (334.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 334.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 333.0
09/08/17 16:39:23 [26674] (333.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 333.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 332.0
09/08/17 16:39:23 [26674] (332.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 332.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 331.0
09/08/17 16:39:23 [26674] (331.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 331.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 330.0
09/08/17 16:39:23 [26674] (330.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 330.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 329.0
09/08/17 16:39:23 [26674] (329.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 329.0 --- inserting
09/08/17 16:39:23 [26674] Using job type INFNBatch for job 328.0
09/08/17 16:39:23 [26674] (328.0) SetJobLeaseTimers()
09/08/17 16:39:23 [26674] Found job 328.0 --- inserting
09/08/17 16:39:23 [26674] Fetched 42 new job ads from schedd
09/08/17 16:39:23 [26674] querying for removed/held jobs
09/08/17 16:39:23 [26674] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:39:23 [26674] Fetched 0 job ads from schedd
09/08/17 16:39:23 [26674] leaving doContactSchedd()
09/08/17 16:39:23 [26674] gahp server not up yet, delaying ping
09/08/17 16:39:23 [26674] *** UpdateLeases called
09/08/17 16:39:23 [26674]     Leases not supported, cancelling timer
09/08/17 16:39:23 [26674] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26662_8a75_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26662_8a75_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26662_8a75_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504913963
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:18839>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:39:23 [26674] Trying to update collector <128.55.162.46:9619>
09/08/17 16:39:23 [26674] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:39:23 [26674] File descriptor limits: max 4096, safe 3277
09/08/17 16:39:23 [26674] (327.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:39:23 [26674] GAHP server pid = 26681
09/08/17 16:39:23 [26674] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:39:23 [26674] GAHP[26681] <- 'COMMANDS'
09/08/17 16:39:23 [26674] GAHP[26681] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:39:23 [26674] GAHP[26681] <- 'ASYNC_MODE_ON'
09/08/17 16:39:23 [26674] GAHP[26681] -> 'S' 'Async mode on'
09/08/17 16:39:23 [26674] (327.0) gm state change: GM_INIT -> GM_START
09/08/17 16:39:23 [26674] (327.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:39:23 [26674] (327.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:39:23 [26674] GAHP[26681] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#327.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/test.sh"\ ]'
09/08/17 16:39:23 [26674] GAHP[26681] -> 'S'
09/08/17 16:39:23 [26674] (326.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:39:23 [26674] (326.0) gm state change: GM_INIT -> GM_START
09/08/17 16:39:23 [26674] (326.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:39:23 [26674] (326.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:39:23 [26674] GAHP[26681] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#326.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/test.sh"\ ]'
09/08/17 16:39:23 [26674] GAHP[26681] -> 'S'
09/08/17 16:39:23 [26674] (325.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:39:23 [26674] (325.0) gm state change: GM_INIT -> GM_START
09/08/17 16:39:23 [26674] (325.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:39:23 [26674] (325.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:39:23 [26674] GAHP[26681] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#325.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/test.sh"\ ]'
09/08/17 16:39:23 [26674] GAHP[26681] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:39:23 [26674] GAHP[26681] -> EOF
09/08/17 16:39:23 [26674] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:40:21 Result of reading /etc/issue:  \S
 
09/08/17 16:40:21 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:40:21 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:40:21 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:40:21 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:40:21 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:40:21 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:40:21 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:40:21 ******************************************************
09/08/17 16:40:21 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:40:21 ** /usr/sbin/condor_gridmanager
09/08/17 16:40:21 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:40:21 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:40:21 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:40:21 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:40:21 ** PID = 26782
09/08/17 16:40:21 ** Log last touched 9/8 16:39:23
09/08/17 16:40:21 ******************************************************
09/08/17 16:40:21 Using config source: /etc/condor-ce/condor_config
09/08/17 16:40:21 Using local config sources: 
09/08/17 16:40:21    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:40:21    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:40:21    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:40:21    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:40:21 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:40:21 CLASSAD_CACHING is ENABLED
09/08/17 16:40:21 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:40:21 SharedPortEndpoint: waiting for connections to named socket 26772_0bba_3
09/08/17 16:40:21 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26772_0bba_3>
09/08/17 16:40:21 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26772_0bba_3>
09/08/17 16:40:21 Setting maximum accepts per cycle 8.
09/08/17 16:40:21 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:40:21 [26782] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:40:21 [26782] DaemonCore: No more children processes to reap.
09/08/17 16:40:21 [26782] DaemonCore: in SendAliveToParent()
09/08/17 16:40:21 [26782] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: ip found is 1
09/08/17 16:40:21 [26782] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: ip found is 1
09/08/17 16:40:21 [26782] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: ip found is 1
09/08/17 16:40:21 [26782] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:40:21 [26782] IPVERIFY: ip found is 1
09/08/17 16:40:21 [26782] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29799>
09/08/17 16:40:21 [26782] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:40:21 [26782] Checking proxies
09/08/17 16:40:24 [26782] Received ADD_JOBS signal
09/08/17 16:40:24 [26782] in doContactSchedd()
09/08/17 16:40:24 [26782] querying for new jobs
09/08/17 16:40:24 [26782] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 327.0
09/08/17 16:40:24 [26782] (327.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 327.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 326.0
09/08/17 16:40:24 [26782] (326.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 326.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 325.0
09/08/17 16:40:24 [26782] (325.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 325.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 324.0
09/08/17 16:40:24 [26782] (324.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 324.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 323.0
09/08/17 16:40:24 [26782] (323.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 323.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 322.0
09/08/17 16:40:24 [26782] (322.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 322.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 321.0
09/08/17 16:40:24 [26782] (321.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 321.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 320.0
09/08/17 16:40:24 [26782] (320.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 320.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 319.0
09/08/17 16:40:24 [26782] (319.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 319.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 318.0
09/08/17 16:40:24 [26782] (318.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 318.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 317.0
09/08/17 16:40:24 [26782] (317.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 317.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 316.0
09/08/17 16:40:24 [26782] (316.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 316.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 315.0
09/08/17 16:40:24 [26782] (315.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 315.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 314.0
09/08/17 16:40:24 [26782] (314.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 314.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 313.0
09/08/17 16:40:24 [26782] (313.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 313.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 312.0
09/08/17 16:40:24 [26782] (312.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 312.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 311.0
09/08/17 16:40:24 [26782] (311.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 311.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 310.0
09/08/17 16:40:24 [26782] (310.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 310.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 309.0
09/08/17 16:40:24 [26782] (309.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 309.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 308.0
09/08/17 16:40:24 [26782] (308.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 308.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 344.0
09/08/17 16:40:24 [26782] (344.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 344.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 307.0
09/08/17 16:40:24 [26782] (307.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 307.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 343.0
09/08/17 16:40:24 [26782] (343.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 343.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 306.0
09/08/17 16:40:24 [26782] (306.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 306.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 342.0
09/08/17 16:40:24 [26782] (342.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 342.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 305.0
09/08/17 16:40:24 [26782] (305.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 305.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 341.0
09/08/17 16:40:24 [26782] (341.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 341.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 304.0
09/08/17 16:40:24 [26782] (304.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 304.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 340.0
09/08/17 16:40:24 [26782] (340.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 340.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 303.0
09/08/17 16:40:24 [26782] (303.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 303.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 339.0
09/08/17 16:40:24 [26782] (339.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 339.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 338.0
09/08/17 16:40:24 [26782] (338.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 338.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 337.0
09/08/17 16:40:24 [26782] (337.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 337.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 336.0
09/08/17 16:40:24 [26782] (336.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 336.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 335.0
09/08/17 16:40:24 [26782] (335.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 335.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 334.0
09/08/17 16:40:24 [26782] (334.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 334.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 333.0
09/08/17 16:40:24 [26782] (333.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 333.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 332.0
09/08/17 16:40:24 [26782] (332.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 332.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 331.0
09/08/17 16:40:24 [26782] (331.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 331.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 330.0
09/08/17 16:40:24 [26782] (330.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 330.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 329.0
09/08/17 16:40:24 [26782] (329.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 329.0 --- inserting
09/08/17 16:40:24 [26782] Using job type INFNBatch for job 328.0
09/08/17 16:40:24 [26782] (328.0) SetJobLeaseTimers()
09/08/17 16:40:24 [26782] Found job 328.0 --- inserting
09/08/17 16:40:24 [26782] Fetched 42 new job ads from schedd
09/08/17 16:40:24 [26782] querying for removed/held jobs
09/08/17 16:40:24 [26782] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:40:24 [26782] Fetched 0 job ads from schedd
09/08/17 16:40:24 [26782] leaving doContactSchedd()
09/08/17 16:40:24 [26782] gahp server not up yet, delaying ping
09/08/17 16:40:24 [26782] *** UpdateLeases called
09/08/17 16:40:24 [26782]     Leases not supported, cancelling timer
09/08/17 16:40:24 [26782] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26772_0bba_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26772_0bba_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26772_0bba_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914024
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29799>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:40:24 [26782] Trying to update collector <128.55.162.46:9619>
09/08/17 16:40:24 [26782] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:40:24 [26782] File descriptor limits: max 4096, safe 3277
09/08/17 16:40:24 [26782] (327.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:40:24 [26782] GAHP server pid = 26790
09/08/17 16:40:24 [26782] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:40:24 [26782] GAHP[26790] <- 'COMMANDS'
09/08/17 16:40:24 [26782] GAHP[26790] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:40:24 [26782] GAHP[26790] <- 'ASYNC_MODE_ON'
09/08/17 16:40:24 [26782] GAHP[26790] -> 'S' 'Async mode on'
09/08/17 16:40:24 [26782] (327.0) gm state change: GM_INIT -> GM_START
09/08/17 16:40:24 [26782] (327.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:40:24 [26782] (327.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:40:24 [26782] GAHP[26790] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#327.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/test.sh"\ ]'
09/08/17 16:40:24 [26782] GAHP[26790] -> 'S'
09/08/17 16:40:24 [26782] (326.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:40:24 [26782] (326.0) gm state change: GM_INIT -> GM_START
09/08/17 16:40:24 [26782] (326.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:40:24 [26782] (326.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:40:24 [26782] GAHP[26790] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#326.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/test.sh"\ ]'
09/08/17 16:40:24 [26782] GAHP[26790] -> 'S'
09/08/17 16:40:24 [26782] (325.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:40:24 [26782] (325.0) gm state change: GM_INIT -> GM_START
09/08/17 16:40:24 [26782] (325.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:40:24 [26782] (325.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:40:24 [26782] GAHP[26790] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#325.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/test.sh"\ ]'
09/08/17 16:40:24 [26782] GAHP[26790] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:40:24 [26782] GAHP[26790] -> EOF
09/08/17 16:40:24 [26782] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:43:16 Result of reading /etc/issue:  \S
 
09/08/17 16:43:16 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:43:16 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:43:16 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:43:16 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:43:16 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:43:16 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:43:16 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:43:16 ******************************************************
09/08/17 16:43:16 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:43:16 ** /usr/sbin/condor_gridmanager
09/08/17 16:43:16 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:43:16 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:43:16 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:43:16 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:43:16 ** PID = 26901
09/08/17 16:43:16 ** Log last touched 9/8 16:40:24
09/08/17 16:43:16 ******************************************************
09/08/17 16:43:16 Using config source: /etc/condor-ce/condor_config
09/08/17 16:43:16 Using local config sources: 
09/08/17 16:43:16    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:43:16    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:43:16    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:43:16    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:43:16 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:43:16 CLASSAD_CACHING is ENABLED
09/08/17 16:43:16 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:43:16 SharedPortEndpoint: waiting for connections to named socket 26883_415e_3
09/08/17 16:43:16 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26883_415e_3>
09/08/17 16:43:16 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26883_415e_3>
09/08/17 16:43:16 Setting maximum accepts per cycle 8.
09/08/17 16:43:16 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:43:16 [26901] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:43:16 [26901] DaemonCore: No more children processes to reap.
09/08/17 16:43:16 [26901] DaemonCore: in SendAliveToParent()
09/08/17 16:43:16 [26901] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: ip found is 1
09/08/17 16:43:16 [26901] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: ip found is 1
09/08/17 16:43:16 [26901] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: ip found is 1
09/08/17 16:43:16 [26901] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:43:16 [26901] IPVERIFY: ip found is 1
09/08/17 16:43:16 [26901] Completed DC_CHILDALIVE to daemon at <128.55.162.46:37237>
09/08/17 16:43:16 [26901] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:43:16 [26901] Checking proxies
09/08/17 16:43:19 [26901] Received ADD_JOBS signal
09/08/17 16:43:19 [26901] in doContactSchedd()
09/08/17 16:43:19 [26901] querying for new jobs
09/08/17 16:43:19 [26901] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 327.0
09/08/17 16:43:19 [26901] (327.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 327.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 326.0
09/08/17 16:43:19 [26901] (326.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 326.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 325.0
09/08/17 16:43:19 [26901] (325.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 325.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 324.0
09/08/17 16:43:19 [26901] (324.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 324.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 323.0
09/08/17 16:43:19 [26901] (323.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 323.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 322.0
09/08/17 16:43:19 [26901] (322.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 322.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 321.0
09/08/17 16:43:19 [26901] (321.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 321.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 320.0
09/08/17 16:43:19 [26901] (320.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 320.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 319.0
09/08/17 16:43:19 [26901] (319.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 319.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 318.0
09/08/17 16:43:19 [26901] (318.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 318.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 317.0
09/08/17 16:43:19 [26901] (317.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 317.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 316.0
09/08/17 16:43:19 [26901] (316.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 316.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 315.0
09/08/17 16:43:19 [26901] (315.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 315.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 314.0
09/08/17 16:43:19 [26901] (314.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 314.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 313.0
09/08/17 16:43:19 [26901] (313.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 313.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 312.0
09/08/17 16:43:19 [26901] (312.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 312.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 311.0
09/08/17 16:43:19 [26901] (311.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 311.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 310.0
09/08/17 16:43:19 [26901] (310.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 310.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 309.0
09/08/17 16:43:19 [26901] (309.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 309.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 308.0
09/08/17 16:43:19 [26901] (308.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 308.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 344.0
09/08/17 16:43:19 [26901] (344.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 344.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 307.0
09/08/17 16:43:19 [26901] (307.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 307.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 343.0
09/08/17 16:43:19 [26901] (343.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 343.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 306.0
09/08/17 16:43:19 [26901] (306.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 306.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 342.0
09/08/17 16:43:19 [26901] (342.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 342.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 305.0
09/08/17 16:43:19 [26901] (305.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 305.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 341.0
09/08/17 16:43:19 [26901] (341.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 341.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 304.0
09/08/17 16:43:19 [26901] (304.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 304.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 340.0
09/08/17 16:43:19 [26901] (340.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 340.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 303.0
09/08/17 16:43:19 [26901] (303.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 303.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 339.0
09/08/17 16:43:19 [26901] (339.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 339.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 338.0
09/08/17 16:43:19 [26901] (338.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 338.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 337.0
09/08/17 16:43:19 [26901] (337.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 337.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 336.0
09/08/17 16:43:19 [26901] (336.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 336.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 335.0
09/08/17 16:43:19 [26901] (335.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 335.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 334.0
09/08/17 16:43:19 [26901] (334.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 334.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 333.0
09/08/17 16:43:19 [26901] (333.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 333.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 332.0
09/08/17 16:43:19 [26901] (332.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 332.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 331.0
09/08/17 16:43:19 [26901] (331.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 331.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 330.0
09/08/17 16:43:19 [26901] (330.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 330.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 329.0
09/08/17 16:43:19 [26901] (329.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 329.0 --- inserting
09/08/17 16:43:19 [26901] Using job type INFNBatch for job 328.0
09/08/17 16:43:19 [26901] (328.0) SetJobLeaseTimers()
09/08/17 16:43:19 [26901] Found job 328.0 --- inserting
09/08/17 16:43:19 [26901] Fetched 42 new job ads from schedd
09/08/17 16:43:19 [26901] querying for removed/held jobs
09/08/17 16:43:19 [26901] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:43:19 [26901] Fetched 0 job ads from schedd
09/08/17 16:43:19 [26901] leaving doContactSchedd()
09/08/17 16:43:19 [26901] gahp server not up yet, delaying ping
09/08/17 16:43:19 [26901] *** UpdateLeases called
09/08/17 16:43:19 [26901]     Leases not supported, cancelling timer
09/08/17 16:43:19 [26901] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26883_415e_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26883_415e_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26883_415e_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914199
IdleJobs = 42
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:37237>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:43:19 [26901] Trying to update collector <128.55.162.46:9619>
09/08/17 16:43:19 [26901] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:43:19 [26901] File descriptor limits: max 4096, safe 3277
09/08/17 16:43:19 [26901] (327.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:43:19 [26901] GAHP server pid = 26909
09/08/17 16:43:19 [26901] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:43:19 [26901] GAHP[26909] <- 'COMMANDS'
09/08/17 16:43:19 [26901] GAHP[26909] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:43:19 [26901] GAHP[26909] <- 'ASYNC_MODE_ON'
09/08/17 16:43:19 [26901] GAHP[26909] -> 'S' 'Async mode on'
09/08/17 16:43:19 [26901] (327.0) gm state change: GM_INIT -> GM_START
09/08/17 16:43:19 [26901] (327.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:43:19 [26901] (327.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:43:19 [26901] GAHP[26909] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#327.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/test.sh"\ ]'
09/08/17 16:43:19 [26901] GAHP[26909] -> 'S'
09/08/17 16:43:19 [26901] (326.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:43:19 [26901] (326.0) gm state change: GM_INIT -> GM_START
09/08/17 16:43:19 [26901] (326.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:43:19 [26901] (326.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:43:19 [26901] GAHP[26909] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#326.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/test.sh"\ ]'
09/08/17 16:43:19 [26901] GAHP[26909] -> 'S'
09/08/17 16:43:19 [26901] (325.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:43:19 [26901] (325.0) gm state change: GM_INIT -> GM_START
09/08/17 16:43:19 [26901] (325.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:43:19 [26901] (325.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:43:19 [26901] GAHP[26909] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#325.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/test.sh"\ ]'
09/08/17 16:43:19 [26901] GAHP[26909] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:43:19 [26901] GAHP[26909] -> EOF
09/08/17 16:43:19 [26901] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:45:01 Result of reading /etc/issue:  \S
 
09/08/17 16:45:01 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:45:01 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:45:01 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:45:01 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:45:01 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:45:01 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:45:01 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:45:01 ******************************************************
09/08/17 16:45:01 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:45:01 ** /usr/sbin/condor_gridmanager
09/08/17 16:45:01 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:45:01 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:45:01 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:45:01 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:45:01 ** PID = 26949
09/08/17 16:45:01 ** Log last touched 9/8 16:43:19
09/08/17 16:45:01 ******************************************************
09/08/17 16:45:01 Using config source: /etc/condor-ce/condor_config
09/08/17 16:45:01 Using local config sources: 
09/08/17 16:45:01    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:45:01    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:45:01    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:45:01    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:45:01 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:45:01 CLASSAD_CACHING is ENABLED
09/08/17 16:45:01 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:45:01 SharedPortEndpoint: waiting for connections to named socket 26883_415e_4
09/08/17 16:45:01 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26883_415e_4>
09/08/17 16:45:01 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26883_415e_4>
09/08/17 16:45:01 Setting maximum accepts per cycle 8.
09/08/17 16:45:01 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:45:01 [26949] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:45:01 [26949] DaemonCore: No more children processes to reap.
09/08/17 16:45:01 [26949] DaemonCore: in SendAliveToParent()
09/08/17 16:45:01 [26949] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: ip found is 1
09/08/17 16:45:01 [26949] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: ip found is 1
09/08/17 16:45:01 [26949] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: ip found is 1
09/08/17 16:45:01 [26949] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:45:01 [26949] IPVERIFY: ip found is 1
09/08/17 16:45:01 [26949] Completed DC_CHILDALIVE to daemon at <128.55.162.46:37237>
09/08/17 16:45:01 [26949] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:45:01 [26949] Checking proxies
09/08/17 16:45:03 [26949] Received REMOVE_JOBS signal
09/08/17 16:45:03 [26949] in doContactSchedd()
09/08/17 16:45:03 [26949] querying for new jobs
09/08/17 16:45:03 [26949] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 327.0
09/08/17 16:45:03 [26949] (327.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 327.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 326.0
09/08/17 16:45:03 [26949] (326.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 326.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 325.0
09/08/17 16:45:03 [26949] (325.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 325.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 324.0
09/08/17 16:45:03 [26949] (324.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 324.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 323.0
09/08/17 16:45:03 [26949] (323.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 323.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 322.0
09/08/17 16:45:03 [26949] (322.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 322.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 321.0
09/08/17 16:45:03 [26949] (321.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 321.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 320.0
09/08/17 16:45:03 [26949] (320.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 320.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 319.0
09/08/17 16:45:03 [26949] (319.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 319.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 318.0
09/08/17 16:45:03 [26949] (318.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 318.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 317.0
09/08/17 16:45:03 [26949] (317.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 317.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 316.0
09/08/17 16:45:03 [26949] (316.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 316.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 315.0
09/08/17 16:45:03 [26949] (315.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 315.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 314.0
09/08/17 16:45:03 [26949] (314.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 314.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 313.0
09/08/17 16:45:03 [26949] (313.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 313.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 312.0
09/08/17 16:45:03 [26949] (312.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 312.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 311.0
09/08/17 16:45:03 [26949] (311.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 311.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 310.0
09/08/17 16:45:03 [26949] (310.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 310.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 309.0
09/08/17 16:45:03 [26949] (309.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 309.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 308.0
09/08/17 16:45:03 [26949] (308.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 308.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 344.0
09/08/17 16:45:03 [26949] (344.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 344.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 307.0
09/08/17 16:45:03 [26949] (307.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 307.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 343.0
09/08/17 16:45:03 [26949] (343.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 343.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 306.0
09/08/17 16:45:03 [26949] (306.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 306.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 342.0
09/08/17 16:45:03 [26949] (342.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 342.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 305.0
09/08/17 16:45:03 [26949] (305.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 305.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 341.0
09/08/17 16:45:03 [26949] (341.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 341.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 304.0
09/08/17 16:45:03 [26949] (304.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 304.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 340.0
09/08/17 16:45:03 [26949] (340.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 340.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 303.0
09/08/17 16:45:03 [26949] (303.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 303.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 339.0
09/08/17 16:45:03 [26949] (339.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 339.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 338.0
09/08/17 16:45:03 [26949] (338.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 338.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 337.0
09/08/17 16:45:03 [26949] (337.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 337.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 336.0
09/08/17 16:45:03 [26949] (336.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 336.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 335.0
09/08/17 16:45:03 [26949] (335.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 335.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 334.0
09/08/17 16:45:03 [26949] (334.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 334.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 333.0
09/08/17 16:45:03 [26949] (333.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 333.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 332.0
09/08/17 16:45:03 [26949] (332.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 332.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 331.0
09/08/17 16:45:03 [26949] (331.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 331.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 330.0
09/08/17 16:45:03 [26949] (330.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 330.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 329.0
09/08/17 16:45:03 [26949] (329.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 329.0 --- inserting
09/08/17 16:45:03 [26949] Using job type INFNBatch for job 328.0
09/08/17 16:45:03 [26949] (328.0) SetJobLeaseTimers()
09/08/17 16:45:03 [26949] Found job 328.0 --- inserting
09/08/17 16:45:03 [26949] Fetched 42 new job ads from schedd
09/08/17 16:45:03 [26949] querying for removed/held jobs
09/08/17 16:45:03 [26949] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:45:03 [26949] Fetched 42 job ads from schedd
09/08/17 16:45:03 [26949] leaving doContactSchedd()
09/08/17 16:45:03 [26949] gahp server not up yet, delaying ping
09/08/17 16:45:03 [26949] *** UpdateLeases called
09/08/17 16:45:03 [26949]     Leases not supported, cancelling timer
09/08/17 16:45:03 [26949] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26883_415e_4\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"26883_415e_4\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=26883_415e_4>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914303
IdleJobs = 0
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:37237>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:45:03 [26949] Trying to update collector <128.55.162.46:9619>
09/08/17 16:45:03 [26949] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:45:03 [26949] File descriptor limits: max 4096, safe 3277
09/08/17 16:45:03 [26949] (327.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:45:03 [26949] GAHP server pid = 26954
09/08/17 16:45:03 [26949] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:45:03 [26949] GAHP[26954] <- 'COMMANDS'
09/08/17 16:45:03 [26949] GAHP[26954] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:45:03 [26949] GAHP[26954] <- 'ASYNC_MODE_ON'
09/08/17 16:45:03 [26949] GAHP[26954] -> 'S' 'Async mode on'
09/08/17 16:45:03 [26949] (327.0) gm state change: GM_INIT -> GM_START
09/08/17 16:45:03 [26949] (327.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:45:03 [26949] (327.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:45:03 [26949] GAHP[26954] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#327.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/test.sh"\ ]'
09/08/17 16:45:03 [26949] GAHP[26954] -> 'S'
09/08/17 16:45:03 [26949] (326.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:45:03 [26949] (326.0) gm state change: GM_INIT -> GM_START
09/08/17 16:45:03 [26949] (326.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:45:03 [26949] (326.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:45:03 [26949] GAHP[26954] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#326.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/test.sh"\ ]'
09/08/17 16:45:03 [26949] GAHP[26954] -> 'S'
09/08/17 16:45:03 [26949] (325.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:45:03 [26949] (325.0) gm state change: GM_INIT -> GM_START
09/08/17 16:45:03 [26949] (325.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:45:03 [26949] (325.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:45:03 [26949] GAHP[26954] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#325.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/test.sh"\ ]'
09/08/17 16:45:03 [26949] GAHP[26954] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:45:03 [26949] GAHP[26954] -> EOF
09/08/17 16:45:03 [26949] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:46:20 Result of reading /etc/issue:  \S
 
09/08/17 16:46:20 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:46:20 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:46:20 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:46:20 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:46:20 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:46:20 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:46:20 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:46:20 ******************************************************
09/08/17 16:46:20 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:46:20 ** /usr/sbin/condor_gridmanager
09/08/17 16:46:20 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:46:20 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:46:20 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:46:20 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:46:20 ** PID = 27066
09/08/17 16:46:20 ** Log last touched 9/8 16:45:03
09/08/17 16:46:20 ******************************************************
09/08/17 16:46:20 Using config source: /etc/condor-ce/condor_config
09/08/17 16:46:20 Using local config sources: 
09/08/17 16:46:20    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:46:20    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:46:20    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:46:20    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:46:20 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:46:20 CLASSAD_CACHING is ENABLED
09/08/17 16:46:20 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:46:20 SharedPortEndpoint: waiting for connections to named socket 27047_cee5_3
09/08/17 16:46:20 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27047_cee5_3>
09/08/17 16:46:20 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27047_cee5_3>
09/08/17 16:46:20 Setting maximum accepts per cycle 8.
09/08/17 16:46:20 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:46:20 [27066] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:46:20 [27066] DaemonCore: No more children processes to reap.
09/08/17 16:46:20 [27066] DaemonCore: in SendAliveToParent()
09/08/17 16:46:20 [27066] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:20 [27066] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:20 [27066] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:20 [27066] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:20 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:20 [27066] Completed DC_CHILDALIVE to daemon at <128.55.162.46:43712>
09/08/17 16:46:20 [27066] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:46:20 [27066] Checking proxies
09/08/17 16:46:23 [27066] Received ADD_JOBS signal
09/08/17 16:46:23 [27066] in doContactSchedd()
09/08/17 16:46:23 [27066] querying for new jobs
09/08/17 16:46:23 [27066] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 338.0
09/08/17 16:46:23 [27066] (338.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/298/0/cluster298.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 338.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 337.0
09/08/17 16:46:23 [27066] (337.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/297/0/cluster297.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 337.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 336.0
09/08/17 16:46:23 [27066] (336.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/296/0/cluster296.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 336.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 335.0
09/08/17 16:46:23 [27066] (335.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/295/0/cluster295.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 335.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 334.0
09/08/17 16:46:23 [27066] (334.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/294/0/cluster294.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 334.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 333.0
09/08/17 16:46:23 [27066] (333.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/293/0/cluster293.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 333.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 332.0
09/08/17 16:46:23 [27066] (332.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/283/0/cluster283.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 332.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 331.0
09/08/17 16:46:23 [27066] (331.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/274/0/cluster274.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 331.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 330.0
09/08/17 16:46:23 [27066] (330.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/265/0/cluster265.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 330.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 329.0
09/08/17 16:46:23 [27066] (329.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/282/0/cluster282.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 329.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 328.0
09/08/17 16:46:23 [27066] (328.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/273/0/cluster273.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 328.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 327.0
09/08/17 16:46:23 [27066] (327.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 327.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 326.0
09/08/17 16:46:23 [27066] (326.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 326.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 325.0
09/08/17 16:46:23 [27066] (325.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 325.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 324.0
09/08/17 16:46:23 [27066] (324.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/272/0/cluster272.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 324.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 323.0
09/08/17 16:46:23 [27066] (323.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/263/0/cluster263.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 323.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 322.0
09/08/17 16:46:23 [27066] (322.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/290/0/cluster290.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 322.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 321.0
09/08/17 16:46:23 [27066] (321.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/280/0/cluster280.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 321.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 320.0
09/08/17 16:46:23 [27066] (320.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/262/0/cluster262.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 320.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 319.0
09/08/17 16:46:23 [27066] (319.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/271/0/cluster271.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 319.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 318.0
09/08/17 16:46:23 [27066] (318.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/270/0/cluster270.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 318.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 317.0
09/08/17 16:46:23 [27066] (317.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/289/0/cluster289.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 317.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 316.0
09/08/17 16:46:23 [27066] (316.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/279/0/cluster279.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 316.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 315.0
09/08/17 16:46:23 [27066] (315.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/288/0/cluster288.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 315.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 314.0
09/08/17 16:46:23 [27066] (314.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/278/0/cluster278.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 314.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 313.0
09/08/17 16:46:23 [27066] (313.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/269/0/cluster269.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 313.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 312.0
09/08/17 16:46:23 [27066] (312.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/287/0/cluster287.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 312.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 311.0
09/08/17 16:46:23 [27066] (311.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/277/0/cluster277.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 311.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 310.0
09/08/17 16:46:23 [27066] (310.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/268/0/cluster268.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 310.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 309.0
09/08/17 16:46:23 [27066] (309.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/286/0/cluster286.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 309.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 308.0
09/08/17 16:46:23 [27066] (308.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/276/0/cluster276.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 308.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 307.0
09/08/17 16:46:23 [27066] (307.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/267/0/cluster267.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 307.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 306.0
09/08/17 16:46:23 [27066] (306.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/285/0/cluster285.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 306.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 305.0
09/08/17 16:46:23 [27066] (305.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/284/0/cluster284.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 305.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 304.0
09/08/17 16:46:23 [27066] (304.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/275/0/cluster275.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 304.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 303.0
09/08/17 16:46:23 [27066] (303.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/266/0/cluster266.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 303.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 344.0
09/08/17 16:46:23 [27066] (344.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/292/0/cluster292.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 344.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 343.0
09/08/17 16:46:23 [27066] (343.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/261/0/cluster261.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 343.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 342.0
09/08/17 16:46:23 [27066] (342.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/302/0/cluster302.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 342.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 341.0
09/08/17 16:46:23 [27066] (341.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/301/0/cluster301.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 341.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 340.0
09/08/17 16:46:23 [27066] (340.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/300/0/cluster300.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 340.0 --- inserting
09/08/17 16:46:23 [27066] Using job type INFNBatch for job 339.0
09/08/17 16:46:23 [27066] (339.0) SetJobLeaseTimers()
09/08/17 16:46:23 [27066] Failed to get expiration time of proxy /common/osg/condor2/299/0/cluster299.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:46:23 [27066] Found job 339.0 --- inserting
09/08/17 16:46:23 [27066] Fetched 42 new job ads from schedd
09/08/17 16:46:23 [27066] querying for removed/held jobs
09/08/17 16:46:23 [27066] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:46:23 [27066] Fetched 42 job ads from schedd
09/08/17 16:46:23 [27066] leaving doContactSchedd()
09/08/17 16:46:23 [27066] gahp server not up yet, delaying ping
09/08/17 16:46:23 [27066] *** UpdateLeases called
09/08/17 16:46:23 [27066]     Leases not supported, cancelling timer
09/08/17 16:46:23 [27066] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27047_cee5_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27047_cee5_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27047_cee5_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914383
IdleJobs = 0
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:43712>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 42
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:46:23 [27066] Trying to update collector <128.55.162.46:9619>
09/08/17 16:46:23 [27066] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:46:23 [27066] File descriptor limits: max 4096, safe 3277
09/08/17 16:46:23 [27066] (338.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] GAHP server pid = 27072
09/08/17 16:46:23 [27066] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:46:23 [27066] GAHP[27072] <- 'COMMANDS'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:46:23 [27066] GAHP[27072] <- 'ASYNC_MODE_ON'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S' 'Async mode on'
09/08/17 16:46:23 [27066] (338.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (338.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (338.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (338.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (337.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (337.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (337.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (337.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (337.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (336.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (336.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (336.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (336.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (336.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (335.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (335.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (335.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (335.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (335.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (334.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (334.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (334.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (334.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (334.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (333.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (333.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (333.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (333.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (333.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (332.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (332.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (332.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (332.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#332.0#1504913703";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/283/0/cluster283.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/283/0/cluster283.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/283/0/cluster283.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (331.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (331.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (331.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (331.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#331.0#1504913703";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/274/0/cluster274.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/274/0/cluster274.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/274/0/cluster274.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'R'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] This process has a valid certificate & key
09/08/17 16:46:23 [27066] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:46:23 [27066] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:46:23 [27066] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:46:23 [27066] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:46:23 [27066] IPVERIFY: ip found is 1
09/08/17 16:46:23 [27066] (330.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (330.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (330.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (330.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#330.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/265/0/cluster265.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/265/0/cluster265.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/265/0/cluster265.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (329.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (329.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (329.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (329.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#329.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/282/0/cluster282.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/282/0/cluster282.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/282/0/cluster282.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (328.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (328.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (328.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (328.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 6 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#328.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/273/0/cluster273.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/273/0/cluster273.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/273/0/cluster273.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (327.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (327.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (327.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (327.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 7 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#327.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/264/0/cluster264.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (326.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (326.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (326.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (326.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 8 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#326.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/291/0/cluster291.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (325.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (325.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (325.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (325.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 9 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#325.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/281/0/cluster281.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (324.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (324.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (324.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (324.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 10 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#324.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/272/0/cluster272.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/272/0/cluster272.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/272/0/cluster272.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (323.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (323.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (323.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (323.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 11 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#323.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/263/0/cluster263.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/263/0/cluster263.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/263/0/cluster263.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (322.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (322.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (322.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (322.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 12 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#322.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/290/0/cluster290.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/290/0/cluster290.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/290/0/cluster290.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (321.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (321.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (321.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (321.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 13 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#321.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/280/0/cluster280.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/280/0/cluster280.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/280/0/cluster280.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (320.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (320.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (320.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (320.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 14 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#320.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/262/0/cluster262.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/262/0/cluster262.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/262/0/cluster262.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (319.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (319.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (319.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (319.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 15 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#319.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/271/0/cluster271.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/271/0/cluster271.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/271/0/cluster271.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (318.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (318.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (318.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (318.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 16 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#318.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/270/0/cluster270.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/270/0/cluster270.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/270/0/cluster270.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (317.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (317.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (317.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (317.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 17 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#317.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/289/0/cluster289.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/289/0/cluster289.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/289/0/cluster289.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (316.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (316.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (316.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (316.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 18 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#316.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/279/0/cluster279.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/279/0/cluster279.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/279/0/cluster279.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (315.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (315.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (315.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (315.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 19 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#315.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/288/0/cluster288.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/288/0/cluster288.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/288/0/cluster288.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (314.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (314.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (314.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (314.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 20 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#314.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/278/0/cluster278.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/278/0/cluster278.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/278/0/cluster278.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (313.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (313.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (313.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (313.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 21 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#313.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/269/0/cluster269.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/269/0/cluster269.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/269/0/cluster269.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (312.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (312.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (312.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (312.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 22 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#312.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/287/0/cluster287.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/287/0/cluster287.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/287/0/cluster287.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (311.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (311.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (311.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (311.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 23 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#311.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/277/0/cluster277.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/277/0/cluster277.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/277/0/cluster277.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (310.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (310.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (310.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (310.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 24 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#310.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/268/0/cluster268.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/268/0/cluster268.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/268/0/cluster268.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (309.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (309.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (309.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (309.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 25 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#309.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/286/0/cluster286.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/286/0/cluster286.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/286/0/cluster286.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (308.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (308.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (308.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (308.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 26 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#308.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/276/0/cluster276.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/276/0/cluster276.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/276/0/cluster276.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (307.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (307.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (307.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (307.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 27 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#307.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/267/0/cluster267.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/267/0/cluster267.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/267/0/cluster267.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (306.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (306.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (306.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (306.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 28 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#306.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/285/0/cluster285.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/285/0/cluster285.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/285/0/cluster285.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (305.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (305.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (305.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (305.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 29 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#305.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/284/0/cluster284.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/284/0/cluster284.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/284/0/cluster284.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (304.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (304.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (304.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (304.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 30 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#304.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/275/0/cluster275.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/275/0/cluster275.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/275/0/cluster275.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (303.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:46:23 [27066] (303.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (303.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:46:23 [27066] (303.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:46:23 [27066] GAHP[27072] <- 'BLAH_JOB_SUBMIT 31 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#303.0#1504913702";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/266/0/cluster266.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/266/0/cluster266.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/266/0/cluster266.proc0.subproc0/test.sh"\ ]'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S'
09/08/17 16:46:23 [27066] (344.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (344.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (344.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (344.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (344.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (343.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (343.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (343.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (343.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (343.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (342.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (342.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (342.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (342.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (342.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (341.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (341.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (341.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (341.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (341.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (340.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (340.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (340.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (340.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (340.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] (339.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:46:23 [27066] (339.0) gm state change: GM_INIT -> GM_START
09/08/17 16:46:23 [27066] (339.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:46:23 [27066] (339.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:46:23 [27066] (339.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:46:23 [27066] GAHP[27072] <- 'RESULTS'
09/08/17 16:46:23 [27066] GAHP[27072] -> 'S' '30'
09/08/17 16:46:23 [27066] GAHP[27072] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/283/0/cluster283.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '3' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/274/0/cluster274.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '4' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/265/0/cluster265.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '5' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/282/0/cluster282.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '6' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/273/0/cluster273.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '7' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '8' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '9' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '10' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/272/0/cluster272.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '11' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/263/0/cluster263.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '12' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/290/0/cluster290.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '13' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/280/0/cluster280.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '14' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/262/0/cluster262.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '15' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/271/0/cluster271.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '16' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/270/0/cluster270.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '17' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/289/0/cluster289.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '18' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/279/0/cluster279.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '19' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/288/0/cluster288.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '20' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/278/0/cluster278.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '21' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/269/0/cluster269.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '22' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/287/0/cluster287.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '23' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/277/0/cluster277.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '24' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/268/0/cluster268.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '25' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/286/0/cluster286.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '26' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/276/0/cluster276.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '27' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/267/0/cluster267.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '28' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/285/0/cluster285.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '29' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/284/0/cluster284.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '30' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/275/0/cluster275.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] GAHP[27072] -> '31' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/266/0/cluster266.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:46:23 [27066] (332.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (332.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/283/0/cluster283.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (332.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (332.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (331.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (331.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/274/0/cluster274.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (331.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (331.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (330.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (330.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/265/0/cluster265.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (330.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (330.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (329.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (329.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/282/0/cluster282.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (329.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (329.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (328.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (328.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/273/0/cluster273.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (328.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (328.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (327.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (327.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/264/0/cluster264.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (327.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (327.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (326.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (326.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/291/0/cluster291.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (326.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (326.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (325.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (325.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/281/0/cluster281.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (325.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (325.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (324.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (324.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/272/0/cluster272.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (324.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (324.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (323.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (323.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/263/0/cluster263.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (323.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:23 [27066] (323.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:23 [27066] (322.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:23 [27066] (322.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/290/0/cluster290.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:23 [27066] (322.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:23 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (322.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (321.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (321.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/280/0/cluster280.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (321.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (321.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (320.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (320.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/262/0/cluster262.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (320.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (320.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (319.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (319.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/271/0/cluster271.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (319.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (319.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (318.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (318.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/270/0/cluster270.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (318.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (318.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (317.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (317.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/289/0/cluster289.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (317.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (317.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (316.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (316.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/279/0/cluster279.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (316.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (316.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (315.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (315.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/288/0/cluster288.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (315.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (315.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (314.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (314.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/278/0/cluster278.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (314.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (314.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (313.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (313.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/269/0/cluster269.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (313.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (313.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (312.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (312.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/287/0/cluster287.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (312.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (312.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (311.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (311.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/277/0/cluster277.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (311.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (311.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (310.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (310.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/268/0/cluster268.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (310.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (310.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (309.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (309.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/286/0/cluster286.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (309.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (309.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (308.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (308.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/276/0/cluster276.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (308.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (308.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (307.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (307.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/267/0/cluster267.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (307.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (307.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (306.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (306.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/285/0/cluster285.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (306.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (306.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (305.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (305.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/284/0/cluster284.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (305.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (305.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (304.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (304.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/275/0/cluster275.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (304.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (304.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:24 [27066] (303.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:46:24 [27066] (303.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/266/0/cluster266.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:46:24 [27066] (303.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:46:24 [27066] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:46:24 [27066] (303.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:46:25 [27066] Evaluating staleness of remote job statuses.
09/08/17 16:46:28 [27066] resource  is now up
09/08/17 16:46:28 [27066] in doContactSchedd()
09/08/17 16:46:28 [27066] querying for removed/held jobs
09/08/17 16:46:28 [27066] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:46:28 [27066] Fetched 42 job ads from schedd
09/08/17 16:46:28 [27066] Updating classad values for 315.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 316.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 317.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 318.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 319.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 320.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 321.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 322.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 323.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 324.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 325.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 326.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 327.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 328.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 329.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 330.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 331.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 332.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 333.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 334.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 335.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 336.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 337.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 338.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 339.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 340.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 341.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 342.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 343.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 344.0:
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 303.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 304.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 305.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 306.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 307.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 308.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 309.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 310.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 311.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 312.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 313.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Updating classad values for 314.0:
09/08/17 16:46:28 [27066]    CurrentStatusUnknown = false
09/08/17 16:46:28 [27066]    GridJobId = undefined
09/08/17 16:46:28 [27066]    LastRemoteStatusUpdate = 0
09/08/17 16:46:28 [27066]    Managed = "ScheddDone"
09/08/17 16:46:28 [27066] Deleting job 315.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 316.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 317.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 318.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 319.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 320.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 321.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 322.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 323.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 324.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 325.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 326.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 327.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 328.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 329.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 330.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 331.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 332.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 333.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 334.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 335.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 336.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 337.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 338.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 339.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 340.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 341.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 342.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 343.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 344.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 303.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 304.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 305.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 306.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 307.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 308.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 309.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 310.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 311.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 312.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 313.0 from schedd
09/08/17 16:46:28 [27066] Deleting job 314.0 from schedd
09/08/17 16:46:28 [27066] No jobs left, shutting down
09/08/17 16:46:28 [27066] leaving doContactSchedd()
09/08/17 16:46:28 [27066] Got SIGTERM. Performing graceful shutdown.
09/08/17 16:46:28 [27066] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 16:46:28 [27066] **** condor_gridmanager (condor_GRIDMANAGER) pid 27066 EXITING WITH STATUS 0
09/08/17 16:47:09 Result of reading /etc/issue:  \S
 
09/08/17 16:47:09 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:47:09 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:47:09 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:47:09 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:47:09 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:47:09 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:47:09 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:47:09 ******************************************************
09/08/17 16:47:09 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:47:09 ** /usr/sbin/condor_gridmanager
09/08/17 16:47:09 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:47:09 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:47:09 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:47:09 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:47:09 ** PID = 27131
09/08/17 16:47:09 ** Log last touched 9/8 16:46:28
09/08/17 16:47:09 ******************************************************
09/08/17 16:47:09 Using config source: /etc/condor-ce/condor_config
09/08/17 16:47:09 Using local config sources: 
09/08/17 16:47:09    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:47:09    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:47:09    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:47:09    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:47:09 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:47:09 CLASSAD_CACHING is ENABLED
09/08/17 16:47:09 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:47:09 SharedPortEndpoint: waiting for connections to named socket 27047_cee5_4
09/08/17 16:47:09 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27047_cee5_4>
09/08/17 16:47:09 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27047_cee5_4>
09/08/17 16:47:09 Setting maximum accepts per cycle 8.
09/08/17 16:47:09 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:47:09 [27131] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:47:09 [27131] DaemonCore: No more children processes to reap.
09/08/17 16:47:09 [27131] DaemonCore: in SendAliveToParent()
09/08/17 16:47:09 [27131] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:09 [27131] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:09 [27131] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:09 [27131] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:09 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:09 [27131] Completed DC_CHILDALIVE to daemon at <128.55.162.46:43712>
09/08/17 16:47:09 [27131] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:47:09 [27131] Checking proxies
09/08/17 16:47:12 [27131] Received ADD_JOBS signal
09/08/17 16:47:12 [27131] in doContactSchedd()
09/08/17 16:47:12 [27131] querying for new jobs
09/08/17 16:47:12 [27131] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:47:12 [27131] Using job type INFNBatch for job 348.0
09/08/17 16:47:12 [27131] (348.0) SetJobLeaseTimers()
09/08/17 16:47:12 [27131] Found job 348.0 --- inserting
09/08/17 16:47:12 [27131] Using job type INFNBatch for job 347.0
09/08/17 16:47:12 [27131] (347.0) SetJobLeaseTimers()
09/08/17 16:47:12 [27131] Found job 347.0 --- inserting
09/08/17 16:47:12 [27131] Fetched 2 new job ads from schedd
09/08/17 16:47:12 [27131] querying for removed/held jobs
09/08/17 16:47:12 [27131] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:47:12 [27131] Fetched 0 job ads from schedd
09/08/17 16:47:12 [27131] leaving doContactSchedd()
09/08/17 16:47:12 [27131] gahp server not up yet, delaying ping
09/08/17 16:47:12 [27131] *** UpdateLeases called
09/08/17 16:47:12 [27131]     Leases not supported, cancelling timer
09/08/17 16:47:12 [27131] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27047_cee5_4\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27047_cee5_4\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27047_cee5_4>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914432
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:43712>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:47:12 [27131] Trying to update collector <128.55.162.46:9619>
09/08/17 16:47:12 [27131] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:47:12 [27131] File descriptor limits: max 4096, safe 3277
09/08/17 16:47:12 [27131] (348.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:47:12 [27131] GAHP server pid = 27136
09/08/17 16:47:12 [27131] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:47:12 [27131] GAHP[27136] <- 'COMMANDS'
09/08/17 16:47:12 [27131] GAHP[27136] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:47:12 [27131] GAHP[27136] <- 'ASYNC_MODE_ON'
09/08/17 16:47:12 [27131] GAHP[27136] -> 'S' 'Async mode on'
09/08/17 16:47:12 [27131] (348.0) gm state change: GM_INIT -> GM_START
09/08/17 16:47:12 [27131] (348.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:47:12 [27131] (348.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:47:12 [27131] (348.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:47:12 [27131] (347.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:47:12 [27131] (347.0) gm state change: GM_INIT -> GM_START
09/08/17 16:47:12 [27131] (347.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:47:12 [27131] (347.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:47:12 [27131] (347.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:47:12 [27131] This process has a valid certificate & key
09/08/17 16:47:12 [27131] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:12 [27131] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:12 [27131] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:12 [27131] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:47:12 [27131] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:47:12 [27131] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:47:12 [27131] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:12 [27131] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:12 [27131] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:12 [27131] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:47:12 [27131] IPVERIFY: ip found is 1
09/08/17 16:47:14 [27131] Evaluating staleness of remote job statuses.
09/08/17 16:47:17 [27131] resource  is now up
09/08/17 16:47:17 [27131] in doContactSchedd()
09/08/17 16:47:17 [27131] querying for removed/held jobs
09/08/17 16:47:17 [27131] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:47:17 [27131] Fetched 0 job ads from schedd
09/08/17 16:47:17 [27131] Updating classad values for 347.0:
09/08/17 16:47:17 [27131]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#347.0#1504914425"
09/08/17 16:47:17 [27131]    LastRemoteStatusUpdate = 1504914432
09/08/17 16:47:17 [27131] Updating classad values for 348.0:
09/08/17 16:47:17 [27131]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#348.0#1504914425"
09/08/17 16:47:17 [27131]    LastRemoteStatusUpdate = 1504914432
09/08/17 16:47:17 [27131] leaving doContactSchedd()
09/08/17 16:47:17 [27131] (347.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:47:17 [27131] (347.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:47:17 [27131] (347.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:47:17 [27131] GAHP[27136] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#347.0#1504914425";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/345/0/cluster345.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/345/0/cluster345.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/345/0/cluster345.proc0.subproc0/test.sh"\ ]'
09/08/17 16:47:17 [27131] GAHP[27136] -> 'S'
09/08/17 16:47:17 [27131] (348.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:47:17 [27131] (348.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:47:17 [27131] (348.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:47:17 [27131] GAHP[27136] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#348.0#1504914425";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/346/0/cluster346.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/346/0/cluster346.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/346/0/cluster346.proc0.subproc0/test.sh"\ ]'
09/08/17 16:47:17 [27131] GAHP[27136] -> 'S'
09/08/17 16:47:17 [27131] GAHP[27136] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:47:17 [27131] DaemonCore: No more children processes to reap.
09/08/17 16:47:17 [27131] GAHP[27136] <- 'RESULTS'
09/08/17 16:47:17 [27131] GAHP[27136] -> EOF
09/08/17 16:47:17 [27131] GAHP command 'RESULTS' failed
09/08/17 16:47:17 [27131] ERROR "Gahp Server (pid=27136) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:48:25 Result of reading /etc/issue:  \S
 
09/08/17 16:48:25 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:48:25 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:48:25 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:48:25 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:48:25 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:48:25 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:48:25 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:48:25 ******************************************************
09/08/17 16:48:25 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:48:25 ** /usr/sbin/condor_gridmanager
09/08/17 16:48:25 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:48:25 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:48:25 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:48:25 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:48:25 ** PID = 27236
09/08/17 16:48:25 ** Log last touched 9/8 16:47:17
09/08/17 16:48:25 ******************************************************
09/08/17 16:48:25 Using config source: /etc/condor-ce/condor_config
09/08/17 16:48:25 Using local config sources: 
09/08/17 16:48:25    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:48:25    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:48:25    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:48:25    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:48:25 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:48:25 CLASSAD_CACHING is ENABLED
09/08/17 16:48:25 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:48:25 SharedPortEndpoint: waiting for connections to named socket 27223_d0ec_3
09/08/17 16:48:25 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_3>
09/08/17 16:48:25 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_3>
09/08/17 16:48:25 Setting maximum accepts per cycle 8.
09/08/17 16:48:25 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:48:25 [27236] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:48:25 [27236] DaemonCore: No more children processes to reap.
09/08/17 16:48:25 [27236] DaemonCore: in SendAliveToParent()
09/08/17 16:48:25 [27236] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:25 [27236] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:25 [27236] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:25 [27236] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:25 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:25 [27236] Completed DC_CHILDALIVE to daemon at <128.55.162.46:14728>
09/08/17 16:48:25 [27236] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:48:25 [27236] Checking proxies
09/08/17 16:48:28 [27236] Received ADD_JOBS signal
09/08/17 16:48:28 [27236] in doContactSchedd()
09/08/17 16:48:28 [27236] querying for new jobs
09/08/17 16:48:28 [27236] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:48:28 [27236] Using job type INFNBatch for job 347.0
09/08/17 16:48:28 [27236] (347.0) SetJobLeaseTimers()
09/08/17 16:48:28 [27236] Found job 347.0 --- inserting
09/08/17 16:48:28 [27236] Using job type INFNBatch for job 348.0
09/08/17 16:48:28 [27236] (348.0) SetJobLeaseTimers()
09/08/17 16:48:28 [27236] Found job 348.0 --- inserting
09/08/17 16:48:28 [27236] Fetched 2 new job ads from schedd
09/08/17 16:48:28 [27236] querying for removed/held jobs
09/08/17 16:48:28 [27236] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:48:28 [27236] Fetched 0 job ads from schedd
09/08/17 16:48:28 [27236] leaving doContactSchedd()
09/08/17 16:48:28 [27236] gahp server not up yet, delaying ping
09/08/17 16:48:28 [27236] *** UpdateLeases called
09/08/17 16:48:28 [27236]     Leases not supported, cancelling timer
09/08/17 16:48:28 [27236] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27223_d0ec_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27223_d0ec_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914508
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:14728>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:48:28 [27236] Trying to update collector <128.55.162.46:9619>
09/08/17 16:48:28 [27236] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:48:28 [27236] File descriptor limits: max 4096, safe 3277
09/08/17 16:48:28 [27236] (347.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:48:28 [27236] GAHP server pid = 27245
09/08/17 16:48:28 [27236] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:48:28 [27236] GAHP[27245] <- 'COMMANDS'
09/08/17 16:48:28 [27236] GAHP[27245] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:48:28 [27236] GAHP[27245] <- 'ASYNC_MODE_ON'
09/08/17 16:48:28 [27236] GAHP[27245] -> 'S' 'Async mode on'
09/08/17 16:48:28 [27236] (347.0) gm state change: GM_INIT -> GM_START
09/08/17 16:48:28 [27236] (347.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:48:28 [27236] (347.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:48:28 [27236] GAHP[27245] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#347.0#1504914425";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/345/0/cluster345.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/345/0/cluster345.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/345/0/cluster345.proc0.subproc0/test.sh"\ ]'
09/08/17 16:48:28 [27236] GAHP[27245] -> 'S'
09/08/17 16:48:28 [27236] (348.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:48:28 [27236] (348.0) gm state change: GM_INIT -> GM_START
09/08/17 16:48:28 [27236] (348.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:48:28 [27236] (348.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:48:28 [27236] GAHP[27245] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#348.0#1504914425";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/346/0/cluster346.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/346/0/cluster346.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/346/0/cluster346.proc0.subproc0/test.sh"\ ]'
09/08/17 16:48:28 [27236] GAHP[27245] -> 'S'
09/08/17 16:48:28 [27236] This process has a valid certificate & key
09/08/17 16:48:28 [27236] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:48:28 [27236] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:48:28 [27236] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:48:28 [27236] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:48:28 [27236] IPVERIFY: ip found is 1
09/08/17 16:48:28 [27236] GAHP[27245] <- 'RESULTS'
09/08/17 16:48:28 [27236] GAHP[27245] -> 'R'
09/08/17 16:48:28 [27236] GAHP[27245] -> 'S' '1'
09/08/17 16:48:28 [27236] GAHP[27245] -> '2' '0' 'No error' 'slurm/20170908/144164'
09/08/17 16:48:28 [27236] (347.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:48:28 [27236] (347.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 16:48:29 [27236] GAHP[27245] <- 'RESULTS'
09/08/17 16:48:29 [27236] GAHP[27245] -> 'R'
09/08/17 16:48:29 [27236] GAHP[27245] -> 'S' '1'
09/08/17 16:48:29 [27236] GAHP[27245] -> '3' '0' 'No error' 'slurm/20170908/144163'
09/08/17 16:48:29 [27236] (348.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:48:29 [27236] (348.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 16:48:30 [27236] Evaluating staleness of remote job statuses.
09/08/17 16:48:33 [27236] resource  is now up
09/08/17 16:48:33 [27236] in doContactSchedd()
09/08/17 16:48:33 [27236] querying for removed/held jobs
09/08/17 16:48:33 [27236] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:48:33 [27236] Fetched 0 job ads from schedd
09/08/17 16:48:33 [27236] Updating classad values for 347.0:
09/08/17 16:48:33 [27236]    DelegatedProxyExpiration = 1505345153
09/08/17 16:48:33 [27236]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#347.0#1504914425 slurm/20170908/144164"
09/08/17 16:48:33 [27236] Updating classad values for 348.0:
09/08/17 16:48:33 [27236]    DelegatedProxyExpiration = 1505345153
09/08/17 16:48:33 [27236]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#348.0#1504914425 slurm/20170908/144163"
09/08/17 16:48:33 [27236] leaving doContactSchedd()
09/08/17 16:48:33 [27236] (347.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 16:48:33 [27236] (347.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 16:48:33 [27236] (348.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 16:48:33 [27236] (348.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 16:49:25 [27236] Received CHECK_LEASES signal
09/08/17 16:49:25 [27236] in doContactSchedd()
09/08/17 16:49:25 [27236] querying for renewed leases
09/08/17 16:49:25 [27236] querying for removed/held jobs
09/08/17 16:49:25 [27236] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:49:25 [27236] Fetched 0 job ads from schedd
09/08/17 16:49:25 [27236] leaving doContactSchedd()
09/08/17 16:49:28 [27236] GAHP[27245] <- 'RESULTS'
09/08/17 16:49:28 [27236] GAHP[27245] -> 'S' '0'
09/08/17 16:49:30 [27236] Evaluating staleness of remote job statuses.
09/08/17 16:49:33 [27236] (347.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 16:49:33 [27236] (347.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 16:49:33 [27236] GAHP[27245] <- 'BLAH_JOB_STATUS 4 slurm/20170908/144164'
09/08/17 16:49:33 [27236] GAHP[27245] -> 'S'
09/08/17 16:49:33 [27236] (348.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 16:49:33 [27236] (348.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 16:49:33 [27236] GAHP[27245] <- 'BLAH_JOB_STATUS 5 slurm/20170908/144163'
09/08/17 16:49:33 [27236] GAHP[27245] -> 'S'
09/08/17 16:49:33 [27236] GAHP[27245] <- 'RESULTS'
09/08/17 16:49:33 [27236] GAHP[27245] -> 'R'
09/08/17 16:49:33 [27236] GAHP[27245] -> 'S' '1'
09/08/17 16:49:33 [27236] GAHP[27245] -> '4' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144164"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 16:49:33 [27236] (347.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 16:49:33 [27236] (347.0) ***ProcessRemoteAd
09/08/17 16:49:33 [27236] (347.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 16:49:33 [27236] (347.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 16:49:33 [27236] (347.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 16:49:33 [27236] in doContactSchedd()
09/08/17 16:49:33 [27236] querying for removed/held jobs
09/08/17 16:49:33 [27236] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:49:33 [27236] Fetched 0 job ads from schedd
09/08/17 16:49:33 [27236] Updating classad values for 347.0:
09/08/17 16:49:33 [27236]    EnteredCurrentStatus = 1504914573
09/08/17 16:49:33 [27236]    ExitCode = 0
09/08/17 16:49:33 [27236]    GridJobStatus = "COMPLETED"
09/08/17 16:49:33 [27236]    ImageSize = 0
09/08/17 16:49:33 [27236]    JobStatus = 4
09/08/17 16:49:33 [27236]    LastRemoteStatusUpdate = 1504914573
09/08/17 16:49:33 [27236]    RemoteUserCpu = 0
09/08/17 16:49:33 [27236]    RemoteWallClockTime = 0.0
09/08/17 16:49:33 [27236] leaving doContactSchedd()
09/08/17 16:49:33 [27236] (347.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 16:49:33 [27236] (347.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 16:49:33 [27236] (347.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 16:49:33 [27236] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:49:33 [27236] (347.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:49:35 [27236] GAHP[27245] <- 'RESULTS'
09/08/17 16:49:35 [27236] GAHP[27245] -> 'R'
09/08/17 16:49:35 [27236] GAHP[27245] -> 'S' '1'
09/08/17 16:49:35 [27236] GAHP[27245] -> '5' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144163"; ImageSize = 0; WorkerNode = "mc1529"; RemoteUserCpu = 0 ]'
09/08/17 16:49:35 [27236] (348.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 16:49:35 [27236] (348.0) ***ProcessRemoteAd
09/08/17 16:49:35 [27236] (348.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 16:49:35 [27236] (348.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 16:49:35 [27236] (348.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 16:49:38 [27236] in doContactSchedd()
09/08/17 16:49:38 [27236] querying for removed/held jobs
09/08/17 16:49:38 [27236] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:49:38 [27236] Fetched 1 job ads from schedd
09/08/17 16:49:38 [27236] Updating classad values for 347.0:
09/08/17 16:49:38 [27236]    CurrentStatusUnknown = false
09/08/17 16:49:38 [27236]    GridJobId = undefined
09/08/17 16:49:38 [27236]    LastRemoteStatusUpdate = 0
09/08/17 16:49:38 [27236]    Managed = "ScheddDone"
09/08/17 16:49:38 [27236] Updating classad values for 348.0:
09/08/17 16:49:38 [27236]    EnteredCurrentStatus = 1504914575
09/08/17 16:49:38 [27236]    ExitCode = 0
09/08/17 16:49:38 [27236]    GridJobStatus = "COMPLETED"
09/08/17 16:49:38 [27236]    ImageSize = 0
09/08/17 16:49:38 [27236]    JobStatus = 4
09/08/17 16:49:38 [27236]    LastRemoteStatusUpdate = 1504914575
09/08/17 16:49:38 [27236]    RemoteUserCpu = 0
09/08/17 16:49:38 [27236]    RemoteWallClockTime = 0.0
09/08/17 16:49:38 [27236] Deleting job 347.0 from schedd
09/08/17 16:49:38 [27236] leaving doContactSchedd()
09/08/17 16:49:38 [27236] (348.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 16:49:38 [27236] (348.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 16:49:38 [27236] (348.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 16:49:38 [27236] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:49:38 [27236] (348.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:49:43 [27236] in doContactSchedd()
09/08/17 16:49:43 [27236] querying for removed/held jobs
09/08/17 16:49:43 [27236] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:49:43 [27236] Fetched 1 job ads from schedd
09/08/17 16:49:43 [27236] Updating classad values for 348.0:
09/08/17 16:49:43 [27236]    CurrentStatusUnknown = false
09/08/17 16:49:43 [27236]    GridJobId = undefined
09/08/17 16:49:43 [27236]    LastRemoteStatusUpdate = 0
09/08/17 16:49:43 [27236]    Managed = "ScheddDone"
09/08/17 16:49:43 [27236] Deleting job 348.0 from schedd
09/08/17 16:49:43 [27236] No jobs left, shutting down
09/08/17 16:49:43 [27236] leaving doContactSchedd()
09/08/17 16:49:43 [27236] Got SIGTERM. Performing graceful shutdown.
09/08/17 16:49:43 [27236] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 16:49:43 [27236] **** condor_gridmanager (condor_GRIDMANAGER) pid 27236 EXITING WITH STATUS 0
09/08/17 16:55:45 Result of reading /etc/issue:  \S
 
09/08/17 16:55:45 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:55:45 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:55:45 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:55:45 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:55:45 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:55:45 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:55:45 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:55:45 ******************************************************
09/08/17 16:55:45 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:55:45 ** /usr/sbin/condor_gridmanager
09/08/17 16:55:45 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:55:45 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:55:45 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:55:45 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:55:45 ** PID = 27505
09/08/17 16:55:45 ** Log last touched 9/8 16:49:43
09/08/17 16:55:45 ******************************************************
09/08/17 16:55:45 Using config source: /etc/condor-ce/condor_config
09/08/17 16:55:45 Using local config sources: 
09/08/17 16:55:45    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:55:45    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:55:45    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:55:45    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:55:45 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:55:45 CLASSAD_CACHING is ENABLED
09/08/17 16:55:45 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:55:45 SharedPortEndpoint: waiting for connections to named socket 27223_d0ec_6
09/08/17 16:55:45 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_6>
09/08/17 16:55:45 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_6>
09/08/17 16:55:45 Setting maximum accepts per cycle 8.
09/08/17 16:55:45 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:55:45 [27505] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:55:45 [27505] DaemonCore: No more children processes to reap.
09/08/17 16:55:45 [27505] DaemonCore: in SendAliveToParent()
09/08/17 16:55:45 [27505] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:45 [27505] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:45 [27505] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:45 [27505] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:45 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:45 [27505] Completed DC_CHILDALIVE to daemon at <128.55.162.46:14728>
09/08/17 16:55:45 [27505] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:55:45 [27505] Checking proxies
09/08/17 16:55:48 [27505] Received ADD_JOBS signal
09/08/17 16:55:48 [27505] in doContactSchedd()
09/08/17 16:55:48 [27505] querying for new jobs
09/08/17 16:55:48 [27505] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:55:48 [27505] Using job type INFNBatch for job 353.0
09/08/17 16:55:48 [27505] (353.0) SetJobLeaseTimers()
09/08/17 16:55:48 [27505] Found job 353.0 --- inserting
09/08/17 16:55:48 [27505] Using job type INFNBatch for job 352.0
09/08/17 16:55:48 [27505] (352.0) SetJobLeaseTimers()
09/08/17 16:55:48 [27505] Found job 352.0 --- inserting
09/08/17 16:55:48 [27505] Fetched 2 new job ads from schedd
09/08/17 16:55:48 [27505] querying for removed/held jobs
09/08/17 16:55:48 [27505] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:55:48 [27505] Fetched 0 job ads from schedd
09/08/17 16:55:48 [27505] leaving doContactSchedd()
09/08/17 16:55:48 [27505] gahp server not up yet, delaying ping
09/08/17 16:55:48 [27505] *** UpdateLeases called
09/08/17 16:55:48 [27505]     Leases not supported, cancelling timer
09/08/17 16:55:48 [27505] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27223_d0ec_6\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27223_d0ec_6\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_6>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914948
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:14728>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:55:48 [27505] Trying to update collector <128.55.162.46:9619>
09/08/17 16:55:48 [27505] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:55:48 [27505] File descriptor limits: max 4096, safe 3277
09/08/17 16:55:48 [27505] (353.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:55:48 [27505] GAHP server pid = 27508
09/08/17 16:55:48 [27505] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:55:48 [27505] GAHP[27508] <- 'COMMANDS'
09/08/17 16:55:48 [27505] GAHP[27508] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:55:48 [27505] GAHP[27508] <- 'ASYNC_MODE_ON'
09/08/17 16:55:48 [27505] GAHP[27508] -> 'S' 'Async mode on'
09/08/17 16:55:48 [27505] (353.0) gm state change: GM_INIT -> GM_START
09/08/17 16:55:48 [27505] (353.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:55:48 [27505] (353.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:55:48 [27505] (353.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:55:48 [27505] (352.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:55:48 [27505] (352.0) gm state change: GM_INIT -> GM_START
09/08/17 16:55:48 [27505] (352.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:55:48 [27505] (352.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:55:48 [27505] (352.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:55:48 [27505] This process has a valid certificate & key
09/08/17 16:55:48 [27505] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:48 [27505] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:48 [27505] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:48 [27505] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:55:48 [27505] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:55:48 [27505] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:55:48 [27505] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:48 [27505] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:48 [27505] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:48 [27505] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:48 [27505] IPVERIFY: ip found is 1
09/08/17 16:55:50 [27505] Evaluating staleness of remote job statuses.
09/08/17 16:55:53 [27505] resource  is now up
09/08/17 16:55:53 [27505] in doContactSchedd()
09/08/17 16:55:53 [27505] querying for removed/held jobs
09/08/17 16:55:53 [27505] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:55:53 [27505] Fetched 0 job ads from schedd
09/08/17 16:55:53 [27505] Updating classad values for 352.0:
09/08/17 16:55:53 [27505]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#352.0#1504914940"
09/08/17 16:55:53 [27505]    LastRemoteStatusUpdate = 1504914948
09/08/17 16:55:53 [27505] Updating classad values for 353.0:
09/08/17 16:55:53 [27505]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#353.0#1504914940"
09/08/17 16:55:53 [27505]    LastRemoteStatusUpdate = 1504914948
09/08/17 16:55:53 [27505] leaving doContactSchedd()
09/08/17 16:55:53 [27505] (352.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:55:53 [27505] (352.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:55:53 [27505] (352.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:55:53 [27505] GAHP[27508] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#352.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/test.sh"\ ]'
09/08/17 16:55:53 [27505] GAHP[27508] -> 'S'
09/08/17 16:55:53 [27505] (353.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:55:53 [27505] (353.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:55:53 [27505] (353.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:55:53 [27505] GAHP[27508] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#353.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/test.sh"\ ]'
09/08/17 16:55:53 [27505] GAHP[27508] -> 'S'
09/08/17 16:55:53 [27505] GAHP[27508] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:55:53 [27505] GAHP[27508] <- 'RESULTS'
09/08/17 16:55:53 [27505] GAHP[27508] -> EOF
09/08/17 16:55:53 [27505] GAHP command 'RESULTS' failed
09/08/17 16:55:53 [27505] DaemonCore: No more children processes to reap.
09/08/17 16:55:53 [27505] GAHP[27508] <- 'RESULTS'
09/08/17 16:55:53 [27505] GAHP[27508] -> EOF
09/08/17 16:55:53 [27505] GAHP command 'RESULTS' failed
09/08/17 16:55:53 [27505] ERROR "Gahp Server (pid=27508) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:55:55 Result of reading /etc/issue:  \S
 
09/08/17 16:55:55 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:55:55 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:55:55 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:55:55 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:55:55 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:55:55 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:55:55 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:55:55 ******************************************************
09/08/17 16:55:55 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:55:55 ** /usr/sbin/condor_gridmanager
09/08/17 16:55:55 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:55:55 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:55:55 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:55:55 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:55:55 ** PID = 27521
09/08/17 16:55:55 ** Log last touched 9/8 16:55:53
09/08/17 16:55:55 ******************************************************
09/08/17 16:55:55 Using config source: /etc/condor-ce/condor_config
09/08/17 16:55:55 Using local config sources: 
09/08/17 16:55:55    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:55:55    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:55:55    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:55:55    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:55:55 config Macros = 177, Sorted = 177, StringBytes = 14929, TablesBytes = 6604
09/08/17 16:55:55 CLASSAD_CACHING is ENABLED
09/08/17 16:55:55 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:55:55 SharedPortEndpoint: waiting for connections to named socket 27223_d0ec_7
09/08/17 16:55:55 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_7>
09/08/17 16:55:55 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_7>
09/08/17 16:55:55 Setting maximum accepts per cycle 8.
09/08/17 16:55:55 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:55:55 [27521] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:55:55 [27521] DaemonCore: No more children processes to reap.
09/08/17 16:55:55 [27521] DaemonCore: in SendAliveToParent()
09/08/17 16:55:55 [27521] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:55 [27521] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:55 [27521] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:55 [27521] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:55 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:55 [27521] Completed DC_CHILDALIVE to daemon at <128.55.162.46:14728>
09/08/17 16:55:55 [27521] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:55:55 [27521] Checking proxies
09/08/17 16:55:58 [27521] Received ADD_JOBS signal
09/08/17 16:55:58 [27521] in doContactSchedd()
09/08/17 16:55:58 [27521] querying for new jobs
09/08/17 16:55:58 [27521] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:55:58 [27521] Using job type INFNBatch for job 353.0
09/08/17 16:55:58 [27521] (353.0) SetJobLeaseTimers()
09/08/17 16:55:58 [27521] Found job 353.0 --- inserting
09/08/17 16:55:58 [27521] Using job type INFNBatch for job 352.0
09/08/17 16:55:58 [27521] (352.0) SetJobLeaseTimers()
09/08/17 16:55:58 [27521] Found job 352.0 --- inserting
09/08/17 16:55:58 [27521] Using job type INFNBatch for job 354.0
09/08/17 16:55:58 [27521] (354.0) SetJobLeaseTimers()
09/08/17 16:55:58 [27521] Found job 354.0 --- inserting
09/08/17 16:55:58 [27521] Fetched 3 new job ads from schedd
09/08/17 16:55:58 [27521] querying for removed/held jobs
09/08/17 16:55:58 [27521] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:55:58 [27521] Fetched 0 job ads from schedd
09/08/17 16:55:58 [27521] leaving doContactSchedd()
09/08/17 16:55:58 [27521] gahp server not up yet, delaying ping
09/08/17 16:55:58 [27521] *** UpdateLeases called
09/08/17 16:55:58 [27521]     Leases not supported, cancelling timer
09/08/17 16:55:58 [27521] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27223_d0ec_7\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27223_d0ec_7\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27223_d0ec_7>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504914958
IdleJobs = 3
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:14728>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 3
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:55:58 [27521] Trying to update collector <128.55.162.46:9619>
09/08/17 16:55:58 [27521] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:55:58 [27521] File descriptor limits: max 4096, safe 3277
09/08/17 16:55:58 [27521] (353.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:55:58 [27521] GAHP server pid = 27525
09/08/17 16:55:58 [27521] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:55:58 [27521] GAHP[27525] <- 'COMMANDS'
09/08/17 16:55:58 [27521] GAHP[27525] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:55:58 [27521] GAHP[27525] <- 'ASYNC_MODE_ON'
09/08/17 16:55:58 [27521] GAHP[27525] -> 'S' 'Async mode on'
09/08/17 16:55:58 [27521] (353.0) gm state change: GM_INIT -> GM_START
09/08/17 16:55:58 [27521] (353.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:55:58 [27521] (353.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:55:58 [27521] GAHP[27525] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#353.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/test.sh"\ ]'
09/08/17 16:55:58 [27521] GAHP[27525] -> 'S'
09/08/17 16:55:58 [27521] (352.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:55:58 [27521] (352.0) gm state change: GM_INIT -> GM_START
09/08/17 16:55:58 [27521] (352.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:55:58 [27521] (352.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:55:58 [27521] GAHP[27525] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#352.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/test.sh"\ ]'
09/08/17 16:55:58 [27521] GAHP[27525] -> 'S'
09/08/17 16:55:58 [27521] (354.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:55:58 [27521] (354.0) gm state change: GM_INIT -> GM_START
09/08/17 16:55:58 [27521] (354.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:55:58 [27521] (354.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:55:58 [27521] (354.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:55:58 [27521] GAHP[27525] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:55:58 [27521] GAHP[27525] <- 'RESULTS'
09/08/17 16:55:58 [27521] GAHP[27525] -> EOF
09/08/17 16:55:58 [27521] GAHP command 'RESULTS' failed
09/08/17 16:55:58 [27521] DaemonCore: No more children processes to reap.
09/08/17 16:55:58 [27521] GAHP[27525] <- 'RESULTS'
09/08/17 16:55:58 [27521] GAHP[27525] -> EOF
09/08/17 16:55:58 [27521] GAHP command 'RESULTS' failed
09/08/17 16:55:58 [27521] This process has a valid certificate & key
09/08/17 16:55:58 [27521] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:55:58 [27521] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:55:58 [27521] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:55:58 [27521] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:55:58 [27521] IPVERIFY: ip found is 1
09/08/17 16:55:58 [27521] ERROR "Gahp Server (pid=27525) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:57:07 Result of reading /etc/issue:  \S
 
09/08/17 16:57:07 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:57:07 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:57:07 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:57:07 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:57:07 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:57:07 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:57:07 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:57:07 ******************************************************
09/08/17 16:57:07 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:57:07 ** /usr/sbin/condor_gridmanager
09/08/17 16:57:07 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:57:07 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:57:07 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:57:07 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:57:07 ** PID = 27620
09/08/17 16:57:07 ** Log last touched 9/8 16:55:58
09/08/17 16:57:07 ******************************************************
09/08/17 16:57:07 Using config source: /etc/condor-ce/condor_config
09/08/17 16:57:07 Using local config sources: 
09/08/17 16:57:07    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:57:07    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:57:07    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:57:07    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:57:07 config Macros = 177, Sorted = 177, StringBytes = 14927, TablesBytes = 6604
09/08/17 16:57:07 CLASSAD_CACHING is ENABLED
09/08/17 16:57:07 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:57:07 SharedPortEndpoint: waiting for connections to named socket 27607_60fc_3
09/08/17 16:57:07 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27607_60fc_3>
09/08/17 16:57:07 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27607_60fc_3>
09/08/17 16:57:07 Setting maximum accepts per cycle 8.
09/08/17 16:57:07 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:57:07 [27620] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:57:07 [27620] DaemonCore: No more children processes to reap.
09/08/17 16:57:07 [27620] DaemonCore: in SendAliveToParent()
09/08/17 16:57:07 [27620] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: ip found is 1
09/08/17 16:57:07 [27620] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: ip found is 1
09/08/17 16:57:07 [27620] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: ip found is 1
09/08/17 16:57:07 [27620] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:07 [27620] IPVERIFY: ip found is 1
09/08/17 16:57:07 [27620] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29811>
09/08/17 16:57:07 [27620] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:57:07 [27620] Checking proxies
09/08/17 16:57:10 [27620] Received ADD_JOBS signal
09/08/17 16:57:10 [27620] in doContactSchedd()
09/08/17 16:57:10 [27620] querying for new jobs
09/08/17 16:57:10 [27620] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:57:10 [27620] Using job type INFNBatch for job 353.0
09/08/17 16:57:10 [27620] (353.0) SetJobLeaseTimers()
09/08/17 16:57:10 [27620] Found job 353.0 --- inserting
09/08/17 16:57:10 [27620] Using job type INFNBatch for job 352.0
09/08/17 16:57:10 [27620] (352.0) SetJobLeaseTimers()
09/08/17 16:57:10 [27620] Found job 352.0 --- inserting
09/08/17 16:57:10 [27620] Using job type INFNBatch for job 354.0
09/08/17 16:57:10 [27620] (354.0) SetJobLeaseTimers()
09/08/17 16:57:10 [27620] Found job 354.0 --- inserting
09/08/17 16:57:10 [27620] Fetched 3 new job ads from schedd
09/08/17 16:57:10 [27620] querying for removed/held jobs
09/08/17 16:57:10 [27620] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:57:10 [27620] Fetched 0 job ads from schedd
09/08/17 16:57:10 [27620] leaving doContactSchedd()
09/08/17 16:57:10 [27620] gahp server not up yet, delaying ping
09/08/17 16:57:10 [27620] *** UpdateLeases called
09/08/17 16:57:10 [27620]     Leases not supported, cancelling timer
09/08/17 16:57:10 [27620] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27607_60fc_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27607_60fc_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27607_60fc_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504915030
IdleJobs = 3
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29811>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 3
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:57:10 [27620] Trying to update collector <128.55.162.46:9619>
09/08/17 16:57:10 [27620] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:57:10 [27620] File descriptor limits: max 4096, safe 3277
09/08/17 16:57:10 [27620] (353.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:57:10 [27620] GAHP server pid = 27622
09/08/17 16:57:10 [27620] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:57:10 [27620] GAHP[27622] <- 'COMMANDS'
09/08/17 16:57:10 [27620] GAHP[27622] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:57:10 [27620] GAHP[27622] <- 'ASYNC_MODE_ON'
09/08/17 16:57:10 [27620] GAHP[27622] -> 'S' 'Async mode on'
09/08/17 16:57:10 [27620] (353.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:10 [27620] (353.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:57:10 [27620] (353.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:57:10 [27620] GAHP[27622] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#353.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/test.sh"\ ]'
09/08/17 16:57:10 [27620] GAHP[27622] -> 'S'
09/08/17 16:57:10 [27620] (352.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:57:10 [27620] (352.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:10 [27620] (352.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:57:10 [27620] (352.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:57:10 [27620] GAHP[27622] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#352.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/test.sh"\ ]'
09/08/17 16:57:10 [27620] GAHP[27622] -> 'S'
09/08/17 16:57:10 [27620] (354.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:57:10 [27620] (354.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:10 [27620] (354.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:57:10 [27620] (354.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:57:10 [27620] (354.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:57:10 [27620] GAHP[27622] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:57:10 [27620] GAHP[27622] <- 'RESULTS'
09/08/17 16:57:10 [27620] GAHP[27622] -> EOF
09/08/17 16:57:10 [27620] GAHP command 'RESULTS' failed
09/08/17 16:57:10 [27620] DaemonCore: No more children processes to reap.
09/08/17 16:57:10 [27620] GAHP[27622] <- 'RESULTS'
09/08/17 16:57:10 [27620] GAHP[27622] -> EOF
09/08/17 16:57:10 [27620] GAHP command 'RESULTS' failed
09/08/17 16:57:10 [27620] ERROR "Gahp Server (pid=27622) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:57:25 Result of reading /etc/issue:  \S
 
09/08/17 16:57:25 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:57:25 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:57:25 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:57:25 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:57:25 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:57:25 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:57:25 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:57:25 ******************************************************
09/08/17 16:57:25 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:57:25 ** /usr/sbin/condor_gridmanager
09/08/17 16:57:25 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:57:25 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:57:25 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:57:25 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:57:25 ** PID = 27643
09/08/17 16:57:25 ** Log last touched 9/8 16:57:10
09/08/17 16:57:25 ******************************************************
09/08/17 16:57:25 Using config source: /etc/condor-ce/condor_config
09/08/17 16:57:25 Using local config sources: 
09/08/17 16:57:25    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:57:25    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:57:25    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:57:25    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:57:25 config Macros = 177, Sorted = 177, StringBytes = 14927, TablesBytes = 6604
09/08/17 16:57:25 CLASSAD_CACHING is ENABLED
09/08/17 16:57:25 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:57:25 SharedPortEndpoint: waiting for connections to named socket 27607_60fc_4
09/08/17 16:57:25 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27607_60fc_4>
09/08/17 16:57:25 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27607_60fc_4>
09/08/17 16:57:25 Setting maximum accepts per cycle 8.
09/08/17 16:57:25 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:57:25 [27643] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:57:25 [27643] DaemonCore: No more children processes to reap.
09/08/17 16:57:25 [27643] DaemonCore: in SendAliveToParent()
09/08/17 16:57:25 [27643] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:25 [27643] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:25 [27643] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:25 [27643] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:25 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:25 [27643] Completed DC_CHILDALIVE to daemon at <128.55.162.46:29811>
09/08/17 16:57:25 [27643] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:57:25 [27643] Checking proxies
09/08/17 16:57:27 [27643] Received REMOVE_JOBS signal
09/08/17 16:57:27 [27643] in doContactSchedd()
09/08/17 16:57:27 [27643] querying for new jobs
09/08/17 16:57:27 [27643] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:57:27 [27643] Using job type INFNBatch for job 353.0
09/08/17 16:57:27 [27643] (353.0) SetJobLeaseTimers()
09/08/17 16:57:27 [27643] Found job 353.0 --- inserting
09/08/17 16:57:27 [27643] Using job type INFNBatch for job 352.0
09/08/17 16:57:27 [27643] (352.0) SetJobLeaseTimers()
09/08/17 16:57:27 [27643] Found job 352.0 --- inserting
09/08/17 16:57:27 [27643] Using job type INFNBatch for job 354.0
09/08/17 16:57:27 [27643] (354.0) SetJobLeaseTimers()
09/08/17 16:57:27 [27643] Found job 354.0 --- inserting
09/08/17 16:57:27 [27643] Fetched 3 new job ads from schedd
09/08/17 16:57:27 [27643] querying for removed/held jobs
09/08/17 16:57:27 [27643] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:57:27 [27643] Fetched 3 job ads from schedd
09/08/17 16:57:27 [27643] leaving doContactSchedd()
09/08/17 16:57:27 [27643] gahp server not up yet, delaying ping
09/08/17 16:57:27 [27643] *** UpdateLeases called
09/08/17 16:57:27 [27643]     Leases not supported, cancelling timer
09/08/17 16:57:27 [27643] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27607_60fc_4\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27607_60fc_4\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27607_60fc_4>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504915047
IdleJobs = 0
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:29811>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 3
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:57:27 [27643] Trying to update collector <128.55.162.46:9619>
09/08/17 16:57:27 [27643] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:57:27 [27643] File descriptor limits: max 4096, safe 3277
09/08/17 16:57:27 [27643] (353.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:57:27 [27643] GAHP server pid = 27645
09/08/17 16:57:27 [27643] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:57:27 [27643] GAHP[27645] <- 'COMMANDS'
09/08/17 16:57:27 [27643] GAHP[27645] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:57:27 [27643] GAHP[27645] <- 'ASYNC_MODE_ON'
09/08/17 16:57:27 [27643] GAHP[27645] -> 'S' 'Async mode on'
09/08/17 16:57:27 [27643] (353.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:27 [27643] (353.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:57:27 [27643] (353.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:57:27 [27643] GAHP[27645] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#353.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/test.sh"\ ]'
09/08/17 16:57:27 [27643] GAHP[27645] -> 'S'
09/08/17 16:57:27 [27643] (352.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:57:27 [27643] (352.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:27 [27643] (352.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:57:27 [27643] (352.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:57:27 [27643] GAHP[27645] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#352.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/test.sh"\ ]'
09/08/17 16:57:27 [27643] GAHP[27645] -> 'S'
09/08/17 16:57:27 [27643] (354.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:57:27 [27643] (354.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:27 [27643] (354.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:57:27 [27643] (354.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:57:27 [27643] (354.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:57:27 [27643] GAHP[27645] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 16:57:27 [27643] This process has a valid certificate & key
09/08/17 16:57:27 [27643] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:57:27 [27643] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:57:27 [27643] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:57:27 [27643] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:27 [27643] IPVERIFY: ip found is 1
09/08/17 16:57:27 [27643] DaemonCore: No more children processes to reap.
09/08/17 16:57:27 [27643] GAHP[27645] <- 'RESULTS'
09/08/17 16:57:27 [27643] GAHP[27645] -> EOF
09/08/17 16:57:27 [27643] GAHP command 'RESULTS' failed
09/08/17 16:57:27 [27643] ERROR "Gahp Server (pid=27645) died due to signal 6 (Aborted) unexpectedly" at line 354 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 16:57:46 Result of reading /etc/issue:  \S
 
09/08/17 16:57:46 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:57:46 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:57:46 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:57:46 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:57:46 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:57:46 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:57:46 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:57:46 ******************************************************
09/08/17 16:57:46 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:57:46 ** /usr/sbin/condor_gridmanager
09/08/17 16:57:46 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:57:46 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:57:46 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:57:46 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:57:46 ** PID = 27732
09/08/17 16:57:46 ** Log last touched 9/8 16:57:27
09/08/17 16:57:46 ******************************************************
09/08/17 16:57:46 Using config source: /etc/condor-ce/condor_config
09/08/17 16:57:46 Using local config sources: 
09/08/17 16:57:46    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:57:46    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:57:46    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:57:46    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:57:46 config Macros = 177, Sorted = 177, StringBytes = 14928, TablesBytes = 6604
09/08/17 16:57:46 CLASSAD_CACHING is ENABLED
09/08/17 16:57:46 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:57:46 SharedPortEndpoint: waiting for connections to named socket 27714_c520_3
09/08/17 16:57:46 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27714_c520_3>
09/08/17 16:57:46 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27714_c520_3>
09/08/17 16:57:46 Setting maximum accepts per cycle 8.
09/08/17 16:57:46 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:57:46 [27732] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:57:46 [27732] DaemonCore: No more children processes to reap.
09/08/17 16:57:46 [27732] DaemonCore: in SendAliveToParent()
09/08/17 16:57:46 [27732] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:46 [27732] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:46 [27732] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:46 [27732] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:46 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:46 [27732] Completed DC_CHILDALIVE to daemon at <128.55.162.46:38933>
09/08/17 16:57:46 [27732] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:57:46 [27732] Checking proxies
09/08/17 16:57:49 [27732] Received ADD_JOBS signal
09/08/17 16:57:49 [27732] in doContactSchedd()
09/08/17 16:57:49 [27732] querying for new jobs
09/08/17 16:57:49 [27732] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:57:49 [27732] Using job type INFNBatch for job 353.0
09/08/17 16:57:49 [27732] (353.0) SetJobLeaseTimers()
09/08/17 16:57:49 [27732] Failed to get expiration time of proxy /common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:57:49 [27732] Found job 353.0 --- inserting
09/08/17 16:57:49 [27732] Using job type INFNBatch for job 352.0
09/08/17 16:57:49 [27732] (352.0) SetJobLeaseTimers()
09/08/17 16:57:49 [27732] Failed to get expiration time of proxy /common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:57:49 [27732] Found job 352.0 --- inserting
09/08/17 16:57:49 [27732] Using job type INFNBatch for job 354.0
09/08/17 16:57:49 [27732] (354.0) SetJobLeaseTimers()
09/08/17 16:57:49 [27732] Failed to get expiration time of proxy /common/osg/condor2/349/0/cluster349.proc0.subproc0/x509up_u49514: unable to read proxy file
09/08/17 16:57:49 [27732] Found job 354.0 --- inserting
09/08/17 16:57:49 [27732] Fetched 3 new job ads from schedd
09/08/17 16:57:49 [27732] querying for removed/held jobs
09/08/17 16:57:49 [27732] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:57:49 [27732] Fetched 3 job ads from schedd
09/08/17 16:57:49 [27732] leaving doContactSchedd()
09/08/17 16:57:49 [27732] gahp server not up yet, delaying ping
09/08/17 16:57:49 [27732] *** UpdateLeases called
09/08/17 16:57:49 [27732]     Leases not supported, cancelling timer
09/08/17 16:57:49 [27732] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27714_c520_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27714_c520_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27714_c520_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504915069
IdleJobs = 0
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:38933>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 3
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:57:49 [27732] Trying to update collector <128.55.162.46:9619>
09/08/17 16:57:49 [27732] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:57:49 [27732] File descriptor limits: max 4096, safe 3277
09/08/17 16:57:49 [27732] (353.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:57:49 [27732] GAHP server pid = 27738
09/08/17 16:57:49 [27732] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:57:49 [27732] GAHP[27738] <- 'COMMANDS'
09/08/17 16:57:49 [27732] GAHP[27738] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:57:49 [27732] GAHP[27738] <- 'ASYNC_MODE_ON'
09/08/17 16:57:49 [27732] GAHP[27738] -> 'S' 'Async mode on'
09/08/17 16:57:49 [27732] (353.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:49 [27732] (353.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:57:49 [27732] (353.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:57:49 [27732] GAHP[27738] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#353.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/351/0/cluster351.proc0.subproc0/test.sh"\ ]'
09/08/17 16:57:49 [27732] GAHP[27738] -> 'S'
09/08/17 16:57:49 [27732] (352.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 16:57:49 [27732] (352.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:49 [27732] (352.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 16:57:49 [27732] (352.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:57:49 [27732] GAHP[27738] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#352.0#1504914940";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/350/0/cluster350.proc0.subproc0/test.sh"\ ]'
09/08/17 16:57:49 [27732] GAHP[27738] -> 'S'
09/08/17 16:57:49 [27732] (354.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:57:49 [27732] (354.0) gm state change: GM_INIT -> GM_START
09/08/17 16:57:49 [27732] (354.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:57:49 [27732] (354.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:57:49 [27732] (354.0) gm state change: GM_UNSUBMITTED -> GM_DELETE
09/08/17 16:57:49 [27732] GAHP[27738] <- 'RESULTS'
09/08/17 16:57:49 [27732] GAHP[27738] -> 'R'
09/08/17 16:57:49 [27732] GAHP[27738] -> 'S' '2'
09/08/17 16:57:49 [27732] GAHP[27738] -> '3' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:57:49 [27732] GAHP[27738] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)' 'N/A'
09/08/17 16:57:49 [27732] (352.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:57:49 [27732] (352.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/350/0/cluster350.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:57:49 [27732] (352.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:57:49 [27732] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:57:49 [27732] (352.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:57:49 [27732] (353.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 16:57:49 [27732] (353.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/common/osg/condor2/351/0/cluster351.proc0.subproc0/x509up_u49514.lmt): errno=2, No such file or directory)
09/08/17 16:57:49 [27732] (353.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
09/08/17 16:57:49 [27732] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 16:57:49 [27732] (353.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 16:57:49 [27732] This process has a valid certificate & key
09/08/17 16:57:49 [27732] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:49 [27732] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:49 [27732] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:49 [27732] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:57:49 [27732] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:57:49 [27732] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:57:49 [27732] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:49 [27732] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:49 [27732] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:49 [27732] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:57:49 [27732] IPVERIFY: ip found is 1
09/08/17 16:57:51 [27732] Evaluating staleness of remote job statuses.
09/08/17 16:57:54 [27732] resource  is now up
09/08/17 16:57:54 [27732] in doContactSchedd()
09/08/17 16:57:54 [27732] querying for removed/held jobs
09/08/17 16:57:54 [27732] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:57:54 [27732] Fetched 3 job ads from schedd
09/08/17 16:57:54 [27732] Updating classad values for 352.0:
09/08/17 16:57:54 [27732]    CurrentStatusUnknown = false
09/08/17 16:57:54 [27732]    GridJobId = undefined
09/08/17 16:57:54 [27732]    LastRemoteStatusUpdate = 0
09/08/17 16:57:54 [27732]    Managed = "ScheddDone"
09/08/17 16:57:54 [27732] Updating classad values for 353.0:
09/08/17 16:57:54 [27732]    CurrentStatusUnknown = false
09/08/17 16:57:54 [27732]    GridJobId = undefined
09/08/17 16:57:54 [27732]    LastRemoteStatusUpdate = 0
09/08/17 16:57:54 [27732]    Managed = "ScheddDone"
09/08/17 16:57:54 [27732] Updating classad values for 354.0:
09/08/17 16:57:54 [27732]    Managed = "ScheddDone"
09/08/17 16:57:54 [27732] Deleting job 352.0 from schedd
09/08/17 16:57:54 [27732] Deleting job 353.0 from schedd
09/08/17 16:57:54 [27732] Deleting job 354.0 from schedd
09/08/17 16:57:54 [27732] No jobs left, shutting down
09/08/17 16:57:54 [27732] leaving doContactSchedd()
09/08/17 16:57:54 [27732] Got SIGTERM. Performing graceful shutdown.
09/08/17 16:57:54 [27732] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 16:57:54 [27732] **** condor_gridmanager (condor_GRIDMANAGER) pid 27732 EXITING WITH STATUS 0
09/08/17 16:59:03 Result of reading /etc/issue:  \S
 
09/08/17 16:59:03 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 16:59:03 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 16:59:03 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 16:59:03 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 16:59:03 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 16:59:03 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 16:59:03 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 16:59:03 ******************************************************
09/08/17 16:59:03 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 16:59:03 ** /usr/sbin/condor_gridmanager
09/08/17 16:59:03 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 16:59:03 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 16:59:03 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 16:59:03 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 16:59:03 ** PID = 27850
09/08/17 16:59:03 ** Log last touched 9/8 16:57:54
09/08/17 16:59:03 ******************************************************
09/08/17 16:59:03 Using config source: /etc/condor-ce/condor_config
09/08/17 16:59:03 Using local config sources: 
09/08/17 16:59:03    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 16:59:03    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 16:59:03    /etc/condor-ce/config.d/99-local.conf
09/08/17 16:59:03    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 16:59:03 config Macros = 177, Sorted = 177, StringBytes = 14928, TablesBytes = 6604
09/08/17 16:59:03 CLASSAD_CACHING is ENABLED
09/08/17 16:59:03 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 16:59:03 SharedPortEndpoint: waiting for connections to named socket 27823_9204_3
09/08/17 16:59:03 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27823_9204_3>
09/08/17 16:59:03 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27823_9204_3>
09/08/17 16:59:03 Setting maximum accepts per cycle 8.
09/08/17 16:59:03 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:59:03 [27850] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 16:59:03 [27850] DaemonCore: No more children processes to reap.
09/08/17 16:59:03 [27850] DaemonCore: in SendAliveToParent()
09/08/17 16:59:03 [27850] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:03 [27850] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:03 [27850] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:03 [27850] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:03 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:03 [27850] Completed DC_CHILDALIVE to daemon at <128.55.162.46:3912>
09/08/17 16:59:03 [27850] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 16:59:03 [27850] Checking proxies
09/08/17 16:59:06 [27850] Received ADD_JOBS signal
09/08/17 16:59:06 [27850] in doContactSchedd()
09/08/17 16:59:06 [27850] querying for new jobs
09/08/17 16:59:06 [27850] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 16:59:06 [27850] Using job type INFNBatch for job 356.0
09/08/17 16:59:06 [27850] (356.0) SetJobLeaseTimers()
09/08/17 16:59:06 [27850] Found job 356.0 --- inserting
09/08/17 16:59:06 [27850] Fetched 1 new job ads from schedd
09/08/17 16:59:06 [27850] querying for removed/held jobs
09/08/17 16:59:06 [27850] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:59:06 [27850] Fetched 0 job ads from schedd
09/08/17 16:59:06 [27850] leaving doContactSchedd()
09/08/17 16:59:06 [27850] gahp server not up yet, delaying ping
09/08/17 16:59:06 [27850] *** UpdateLeases called
09/08/17 16:59:06 [27850]     Leases not supported, cancelling timer
09/08/17 16:59:06 [27850] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27823_9204_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27823_9204_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27823_9204_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504915146
IdleJobs = 1
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:3912>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 1
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 16:59:06 [27850] Trying to update collector <128.55.162.46:9619>
09/08/17 16:59:06 [27850] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 16:59:06 [27850] File descriptor limits: max 4096, safe 3277
09/08/17 16:59:06 [27850] (356.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 16:59:06 [27850] GAHP server pid = 27855
09/08/17 16:59:06 [27850] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 16:59:06 [27850] GAHP[27855] <- 'COMMANDS'
09/08/17 16:59:06 [27850] GAHP[27855] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 16:59:06 [27850] GAHP[27855] <- 'ASYNC_MODE_ON'
09/08/17 16:59:06 [27850] GAHP[27855] -> 'S' 'Async mode on'
09/08/17 16:59:06 [27850] (356.0) gm state change: GM_INIT -> GM_START
09/08/17 16:59:06 [27850] (356.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 16:59:06 [27850] (356.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 16:59:06 [27850] (356.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 16:59:06 [27850] This process has a valid certificate & key
09/08/17 16:59:06 [27850] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:06 [27850] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:06 [27850] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:06 [27850] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 16:59:06 [27850] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 16:59:06 [27850] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 16:59:06 [27850] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:06 [27850] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:06 [27850] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:06 [27850] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 16:59:06 [27850] IPVERIFY: ip found is 1
09/08/17 16:59:08 [27850] Evaluating staleness of remote job statuses.
09/08/17 16:59:11 [27850] resource  is now up
09/08/17 16:59:11 [27850] in doContactSchedd()
09/08/17 16:59:11 [27850] querying for removed/held jobs
09/08/17 16:59:11 [27850] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:59:11 [27850] Fetched 0 job ads from schedd
09/08/17 16:59:11 [27850] Updating classad values for 356.0:
09/08/17 16:59:11 [27850]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#356.0#1504915142"
09/08/17 16:59:11 [27850]    LastRemoteStatusUpdate = 1504915146
09/08/17 16:59:11 [27850] leaving doContactSchedd()
09/08/17 16:59:11 [27850] (356.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 16:59:11 [27850] (356.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 16:59:11 [27850] (356.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 16:59:11 [27850] GAHP[27855] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#356.0#1504915142";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/355/0/cluster355.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/355/0/cluster355.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/355/0/cluster355.proc0.subproc0/test.sh"\ ]'
09/08/17 16:59:11 [27850] GAHP[27855] -> 'S'
09/08/17 16:59:11 [27850] GAHP[27855] <- 'RESULTS'
09/08/17 16:59:11 [27850] GAHP[27855] -> 'R'
09/08/17 16:59:11 [27850] GAHP[27855] -> 'S' '1'
09/08/17 16:59:11 [27850] GAHP[27855] -> '2' '0' 'No error' 'slurm/20170908/144165'
09/08/17 16:59:11 [27850] (356.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 16:59:11 [27850] (356.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 16:59:16 [27850] in doContactSchedd()
09/08/17 16:59:16 [27850] querying for removed/held jobs
09/08/17 16:59:16 [27850] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 16:59:16 [27850] Fetched 0 job ads from schedd
09/08/17 16:59:16 [27850] Updating classad values for 356.0:
09/08/17 16:59:16 [27850]    DelegatedProxyExpiration = 1505345153
09/08/17 16:59:16 [27850]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#356.0#1504915142 slurm/20170908/144165"
09/08/17 16:59:16 [27850] leaving doContactSchedd()
09/08/17 16:59:16 [27850] (356.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 16:59:16 [27850] (356.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:00:03 [27850] Received CHECK_LEASES signal
09/08/17 17:00:03 [27850] in doContactSchedd()
09/08/17 17:00:03 [27850] querying for renewed leases
09/08/17 17:00:03 [27850] querying for removed/held jobs
09/08/17 17:00:03 [27850] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:00:03 [27850] Fetched 0 job ads from schedd
09/08/17 17:00:03 [27850] leaving doContactSchedd()
09/08/17 17:00:06 [27850] GAHP[27855] <- 'RESULTS'
09/08/17 17:00:06 [27850] GAHP[27855] -> 'S' '0'
09/08/17 17:00:08 [27850] Evaluating staleness of remote job statuses.
09/08/17 17:00:16 [27850] (356.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:00:16 [27850] (356.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:00:16 [27850] GAHP[27855] <- 'BLAH_JOB_STATUS 3 slurm/20170908/144165'
09/08/17 17:00:16 [27850] GAHP[27855] -> 'S'
09/08/17 17:00:16 [27850] GAHP[27855] <- 'RESULTS'
09/08/17 17:00:16 [27850] GAHP[27855] -> 'R'
09/08/17 17:00:16 [27850] GAHP[27855] -> 'S' '1'
09/08/17 17:00:16 [27850] GAHP[27855] -> '3' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144165"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:00:16 [27850] (356.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:00:16 [27850] (356.0) ***ProcessRemoteAd
09/08/17 17:00:16 [27850] (356.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:00:16 [27850] (356.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:00:16 [27850] (356.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:00:16 [27850] in doContactSchedd()
09/08/17 17:00:16 [27850] querying for removed/held jobs
09/08/17 17:00:16 [27850] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:00:16 [27850] Fetched 0 job ads from schedd
09/08/17 17:00:16 [27850] Updating classad values for 356.0:
09/08/17 17:00:16 [27850]    EnteredCurrentStatus = 1504915216
09/08/17 17:00:16 [27850]    ExitCode = 0
09/08/17 17:00:16 [27850]    GridJobStatus = "COMPLETED"
09/08/17 17:00:16 [27850]    ImageSize = 0
09/08/17 17:00:16 [27850]    JobStatus = 4
09/08/17 17:00:16 [27850]    LastRemoteStatusUpdate = 1504915216
09/08/17 17:00:16 [27850]    RemoteUserCpu = 0
09/08/17 17:00:16 [27850]    RemoteWallClockTime = 0.0
09/08/17 17:00:16 [27850] leaving doContactSchedd()
09/08/17 17:00:16 [27850] (356.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:00:16 [27850] (356.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:00:16 [27850] (356.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:00:16 [27850] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:00:17 [27850] (356.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:00:21 [27850] in doContactSchedd()
09/08/17 17:00:21 [27850] querying for removed/held jobs
09/08/17 17:00:21 [27850] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:00:21 [27850] Fetched 1 job ads from schedd
09/08/17 17:00:21 [27850] Updating classad values for 356.0:
09/08/17 17:00:21 [27850]    CurrentStatusUnknown = false
09/08/17 17:00:21 [27850]    GridJobId = undefined
09/08/17 17:00:21 [27850]    LastRemoteStatusUpdate = 0
09/08/17 17:00:21 [27850]    Managed = "ScheddDone"
09/08/17 17:00:21 [27850] Deleting job 356.0 from schedd
09/08/17 17:00:21 [27850] No jobs left, shutting down
09/08/17 17:00:21 [27850] leaving doContactSchedd()
09/08/17 17:00:21 [27850] Got SIGTERM. Performing graceful shutdown.
09/08/17 17:00:21 [27850] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 17:00:21 [27850] **** condor_gridmanager (condor_GRIDMANAGER) pid 27850 EXITING WITH STATUS 0
09/08/17 17:01:36 Result of reading /etc/issue:  \S
 
09/08/17 17:01:36 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 17:01:36 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 17:01:36 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 17:01:36 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 17:01:36 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 17:01:36 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 17:01:36 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 17:01:36 ******************************************************
09/08/17 17:01:36 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 17:01:36 ** /usr/sbin/condor_gridmanager
09/08/17 17:01:36 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 17:01:36 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 17:01:36 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 17:01:36 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 17:01:36 ** PID = 28045
09/08/17 17:01:36 ** Log last touched 9/8 17:00:21
09/08/17 17:01:36 ******************************************************
09/08/17 17:01:36 Using config source: /etc/condor-ce/condor_config
09/08/17 17:01:36 Using local config sources: 
09/08/17 17:01:36    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 17:01:36    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 17:01:36    /etc/condor-ce/config.d/99-local.conf
09/08/17 17:01:36    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 17:01:36 config Macros = 177, Sorted = 177, StringBytes = 14928, TablesBytes = 6604
09/08/17 17:01:36 CLASSAD_CACHING is ENABLED
09/08/17 17:01:36 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 17:01:36 SharedPortEndpoint: waiting for connections to named socket 27823_9204_4
09/08/17 17:01:36 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27823_9204_4>
09/08/17 17:01:36 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27823_9204_4>
09/08/17 17:01:36 Setting maximum accepts per cycle 8.
09/08/17 17:01:36 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 17:01:36 [28045] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 17:01:36 [28045] DaemonCore: No more children processes to reap.
09/08/17 17:01:36 [28045] DaemonCore: in SendAliveToParent()
09/08/17 17:01:36 [28045] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:36 [28045] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:36 [28045] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:36 [28045] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:36 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:36 [28045] Completed DC_CHILDALIVE to daemon at <128.55.162.46:3912>
09/08/17 17:01:36 [28045] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 17:01:36 [28045] Checking proxies
09/08/17 17:01:39 [28045] Received ADD_JOBS signal
09/08/17 17:01:39 [28045] in doContactSchedd()
09/08/17 17:01:39 [28045] querying for new jobs
09/08/17 17:01:39 [28045] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 17:01:39 [28045] Using job type INFNBatch for job 366.0
09/08/17 17:01:39 [28045] (366.0) SetJobLeaseTimers()
09/08/17 17:01:39 [28045] Found job 366.0 --- inserting
09/08/17 17:01:39 [28045] Using job type INFNBatch for job 367.0
09/08/17 17:01:39 [28045] (367.0) SetJobLeaseTimers()
09/08/17 17:01:39 [28045] Found job 367.0 --- inserting
09/08/17 17:01:39 [28045] Using job type INFNBatch for job 368.0
09/08/17 17:01:39 [28045] (368.0) SetJobLeaseTimers()
09/08/17 17:01:39 [28045] Found job 368.0 --- inserting
09/08/17 17:01:39 [28045] Using job type INFNBatch for job 363.0
09/08/17 17:01:39 [28045] (363.0) SetJobLeaseTimers()
09/08/17 17:01:39 [28045] Found job 363.0 --- inserting
09/08/17 17:01:39 [28045] Using job type INFNBatch for job 364.0
09/08/17 17:01:39 [28045] (364.0) SetJobLeaseTimers()
09/08/17 17:01:39 [28045] Found job 364.0 --- inserting
09/08/17 17:01:39 [28045] Using job type INFNBatch for job 365.0
09/08/17 17:01:39 [28045] (365.0) SetJobLeaseTimers()
09/08/17 17:01:39 [28045] Found job 365.0 --- inserting
09/08/17 17:01:39 [28045] Fetched 6 new job ads from schedd
09/08/17 17:01:39 [28045] querying for removed/held jobs
09/08/17 17:01:39 [28045] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:01:39 [28045] Fetched 0 job ads from schedd
09/08/17 17:01:39 [28045] leaving doContactSchedd()
09/08/17 17:01:39 [28045] gahp server not up yet, delaying ping
09/08/17 17:01:39 [28045] *** UpdateLeases called
09/08/17 17:01:39 [28045]     Leases not supported, cancelling timer
09/08/17 17:01:39 [28045] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27823_9204_4\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"27823_9204_4\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=27823_9204_4>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504915299
IdleJobs = 6
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:3912>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 6
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 17:01:39 [28045] Trying to update collector <128.55.162.46:9619>
09/08/17 17:01:39 [28045] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 17:01:39 [28045] File descriptor limits: max 4096, safe 3277
09/08/17 17:01:39 [28045] (366.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:01:39 [28045] GAHP server pid = 28052
09/08/17 17:01:39 [28045] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 17:01:39 [28045] GAHP[28052] <- 'COMMANDS'
09/08/17 17:01:39 [28045] GAHP[28052] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 17:01:39 [28045] GAHP[28052] <- 'ASYNC_MODE_ON'
09/08/17 17:01:39 [28045] GAHP[28052] -> 'S' 'Async mode on'
09/08/17 17:01:39 [28045] (366.0) gm state change: GM_INIT -> GM_START
09/08/17 17:01:39 [28045] (366.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:01:39 [28045] (366.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:01:39 [28045] (366.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:01:39 [28045] (367.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:01:39 [28045] (367.0) gm state change: GM_INIT -> GM_START
09/08/17 17:01:39 [28045] (367.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:01:39 [28045] (367.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:01:39 [28045] (367.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:01:39 [28045] (368.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:01:39 [28045] (368.0) gm state change: GM_INIT -> GM_START
09/08/17 17:01:39 [28045] (368.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:01:39 [28045] (368.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:01:39 [28045] (368.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:01:39 [28045] (363.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:01:39 [28045] (363.0) gm state change: GM_INIT -> GM_START
09/08/17 17:01:39 [28045] (363.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:01:39 [28045] (363.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:01:39 [28045] (363.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:01:39 [28045] (364.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:01:39 [28045] (364.0) gm state change: GM_INIT -> GM_START
09/08/17 17:01:39 [28045] (364.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:01:39 [28045] (364.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:01:39 [28045] (364.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:01:39 [28045] This process has a valid certificate & key
09/08/17 17:01:39 [28045] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 17:01:39 [28045] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 17:01:39 [28045] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 17:01:39 [28045] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:01:39 [28045] IPVERIFY: ip found is 1
09/08/17 17:01:39 [28045] (365.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:01:39 [28045] (365.0) gm state change: GM_INIT -> GM_START
09/08/17 17:01:39 [28045] (365.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:01:39 [28045] (365.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:01:39 [28045] (365.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:01:41 [28045] Evaluating staleness of remote job statuses.
09/08/17 17:01:44 [28045] resource  is now up
09/08/17 17:01:44 [28045] in doContactSchedd()
09/08/17 17:01:44 [28045] querying for removed/held jobs
09/08/17 17:01:44 [28045] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:01:44 [28045] Fetched 0 job ads from schedd
09/08/17 17:01:44 [28045] Updating classad values for 363.0:
09/08/17 17:01:44 [28045]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#363.0#1504915292"
09/08/17 17:01:44 [28045]    LastRemoteStatusUpdate = 1504915299
09/08/17 17:01:44 [28045] Updating classad values for 364.0:
09/08/17 17:01:44 [28045]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#364.0#1504915292"
09/08/17 17:01:44 [28045]    LastRemoteStatusUpdate = 1504915299
09/08/17 17:01:44 [28045] Updating classad values for 365.0:
09/08/17 17:01:44 [28045]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#365.0#1504915292"
09/08/17 17:01:44 [28045]    LastRemoteStatusUpdate = 1504915299
09/08/17 17:01:44 [28045] Updating classad values for 366.0:
09/08/17 17:01:44 [28045]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#366.0#1504915293"
09/08/17 17:01:44 [28045]    LastRemoteStatusUpdate = 1504915299
09/08/17 17:01:44 [28045] Updating classad values for 367.0:
09/08/17 17:01:44 [28045]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#367.0#1504915293"
09/08/17 17:01:44 [28045]    LastRemoteStatusUpdate = 1504915299
09/08/17 17:01:44 [28045] Updating classad values for 368.0:
09/08/17 17:01:44 [28045]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#368.0#1504915293"
09/08/17 17:01:44 [28045]    LastRemoteStatusUpdate = 1504915299
09/08/17 17:01:44 [28045] leaving doContactSchedd()
09/08/17 17:01:44 [28045] (363.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:01:44 [28045] (363.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:01:44 [28045] (363.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:01:44 [28045] GAHP[28052] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#363.0#1504915292";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/359/0/cluster359.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/359/0/cluster359.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/359/0/cluster359.proc0.subproc0/test.sh"\ ]'
09/08/17 17:01:44 [28045] GAHP[28052] -> 'S'
09/08/17 17:01:44 [28045] (364.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:01:44 [28045] (364.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:01:44 [28045] (364.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:01:44 [28045] GAHP[28052] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#364.0#1504915292";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/360/0/cluster360.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/360/0/cluster360.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/360/0/cluster360.proc0.subproc0/test.sh"\ ]'
09/08/17 17:01:44 [28045] GAHP[28052] -> 'S'
09/08/17 17:01:44 [28045] (365.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:01:44 [28045] (365.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:01:44 [28045] (365.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:01:44 [28045] GAHP[28052] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#365.0#1504915292";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/361/0/cluster361.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/361/0/cluster361.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/361/0/cluster361.proc0.subproc0/test.sh"\ ]'
09/08/17 17:01:44 [28045] GAHP[28052] -> 'S'
09/08/17 17:01:44 [28045] (366.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:01:44 [28045] (366.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:01:44 [28045] (366.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:01:44 [28045] GAHP[28052] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#366.0#1504915293";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/362/0/cluster362.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/362/0/cluster362.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/362/0/cluster362.proc0.subproc0/test.sh"\ ]'
09/08/17 17:01:44 [28045] GAHP[28052] (stderr) -> Assertion 0 && "globus_hashtable_lookup bad parms" failed in file globus_hashtable.c at line 433
09/08/17 17:01:44 [28045] GAHP[28052] -> EOF
09/08/17 17:01:44 [28045] ERROR "Bad BLAH_JOB_SUBMIT Request: Empty response" at line 2620 in file /builddir/build/BUILD/condor-8.4.11/src/condor_gridmanager/gahp-client.cpp
09/08/17 17:03:37 Result of reading /etc/issue:  \S
 
09/08/17 17:03:37 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 17:03:37 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 17:03:37 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 17:03:37 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 17:03:37 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 17:03:37 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 17:03:37 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 17:03:37 ******************************************************
09/08/17 17:03:37 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 17:03:37 ** /usr/sbin/condor_gridmanager
09/08/17 17:03:37 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 17:03:37 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 17:03:37 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 17:03:37 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 17:03:37 ** PID = 28171
09/08/17 17:03:37 ** Log last touched 9/8 17:01:44
09/08/17 17:03:37 ******************************************************
09/08/17 17:03:37 Using config source: /etc/condor-ce/condor_config
09/08/17 17:03:37 Using local config sources: 
09/08/17 17:03:37    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 17:03:37    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 17:03:37    /etc/condor-ce/config.d/99-local.conf
09/08/17 17:03:37    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 17:03:37 config Macros = 177, Sorted = 177, StringBytes = 14927, TablesBytes = 6604
09/08/17 17:03:37 CLASSAD_CACHING is ENABLED
09/08/17 17:03:37 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 17:03:37 SharedPortEndpoint: waiting for connections to named socket 28160_0472_3
09/08/17 17:03:37 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=28160_0472_3>
09/08/17 17:03:37 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=28160_0472_3>
09/08/17 17:03:37 Setting maximum accepts per cycle 8.
09/08/17 17:03:37 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 17:03:37 [28171] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 17:03:37 [28171] DaemonCore: No more children processes to reap.
09/08/17 17:03:37 [28171] DaemonCore: in SendAliveToParent()
09/08/17 17:03:37 [28171] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:37 [28171] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:37 [28171] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:37 [28171] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:37 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:37 [28171] Completed DC_CHILDALIVE to daemon at <128.55.162.46:54131>
09/08/17 17:03:37 [28171] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 17:03:37 [28171] Checking proxies
09/08/17 17:03:40 [28171] Received ADD_JOBS signal
09/08/17 17:03:40 [28171] in doContactSchedd()
09/08/17 17:03:40 [28171] querying for new jobs
09/08/17 17:03:40 [28171] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 17:03:40 [28171] Using job type INFNBatch for job 366.0
09/08/17 17:03:40 [28171] (366.0) SetJobLeaseTimers()
09/08/17 17:03:40 [28171] Found job 366.0 --- inserting
09/08/17 17:03:40 [28171] Using job type INFNBatch for job 367.0
09/08/17 17:03:40 [28171] (367.0) SetJobLeaseTimers()
09/08/17 17:03:40 [28171] Found job 367.0 --- inserting
09/08/17 17:03:40 [28171] Using job type INFNBatch for job 368.0
09/08/17 17:03:40 [28171] (368.0) SetJobLeaseTimers()
09/08/17 17:03:40 [28171] Found job 368.0 --- inserting
09/08/17 17:03:40 [28171] Using job type INFNBatch for job 363.0
09/08/17 17:03:40 [28171] (363.0) SetJobLeaseTimers()
09/08/17 17:03:40 [28171] Found job 363.0 --- inserting
09/08/17 17:03:40 [28171] Using job type INFNBatch for job 364.0
09/08/17 17:03:40 [28171] (364.0) SetJobLeaseTimers()
09/08/17 17:03:40 [28171] Found job 364.0 --- inserting
09/08/17 17:03:40 [28171] Using job type INFNBatch for job 365.0
09/08/17 17:03:40 [28171] (365.0) SetJobLeaseTimers()
09/08/17 17:03:40 [28171] Found job 365.0 --- inserting
09/08/17 17:03:40 [28171] Fetched 6 new job ads from schedd
09/08/17 17:03:40 [28171] querying for removed/held jobs
09/08/17 17:03:40 [28171] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:03:40 [28171] Fetched 0 job ads from schedd
09/08/17 17:03:40 [28171] leaving doContactSchedd()
09/08/17 17:03:40 [28171] gahp server not up yet, delaying ping
09/08/17 17:03:40 [28171] *** UpdateLeases called
09/08/17 17:03:40 [28171]     Leases not supported, cancelling timer
09/08/17 17:03:40 [28171] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"28160_0472_3\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"28160_0472_3\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=28160_0472_3>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504915420
IdleJobs = 6
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:54131>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 6
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 17:03:40 [28171] Trying to update collector <128.55.162.46:9619>
09/08/17 17:03:40 [28171] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 17:03:40 [28171] File descriptor limits: max 4096, safe 3277
09/08/17 17:03:40 [28171] (366.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 17:03:40 [28171] GAHP server pid = 28177
09/08/17 17:03:40 [28171] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 17:03:40 [28171] GAHP[28177] <- 'COMMANDS'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 17:03:40 [28171] GAHP[28177] <- 'ASYNC_MODE_ON'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S' 'Async mode on'
09/08/17 17:03:40 [28171] (366.0) gm state change: GM_INIT -> GM_START
09/08/17 17:03:40 [28171] (366.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 17:03:40 [28171] (366.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:03:40 [28171] GAHP[28177] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#366.0#1504915293";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/362/0/cluster362.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/362/0/cluster362.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/362/0/cluster362.proc0.subproc0/test.sh"\ ]'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S'
09/08/17 17:03:40 [28171] (367.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 17:03:40 [28171] (367.0) gm state change: GM_INIT -> GM_START
09/08/17 17:03:40 [28171] (367.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 17:03:40 [28171] (367.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:03:40 [28171] GAHP[28177] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#367.0#1504915293";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/357/0/cluster357.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/357/0/cluster357.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/357/0/cluster357.proc0.subproc0/test.sh"\ ]'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S'
09/08/17 17:03:40 [28171] (368.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 17:03:40 [28171] (368.0) gm state change: GM_INIT -> GM_START
09/08/17 17:03:40 [28171] (368.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 17:03:40 [28171] (368.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:03:40 [28171] GAHP[28177] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#368.0#1504915293";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/358/0/cluster358.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/358/0/cluster358.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/358/0/cluster358.proc0.subproc0/test.sh"\ ]'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S'
09/08/17 17:03:40 [28171] (363.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 17:03:40 [28171] (363.0) gm state change: GM_INIT -> GM_START
09/08/17 17:03:40 [28171] (363.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 17:03:40 [28171] (363.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:03:40 [28171] GAHP[28177] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#363.0#1504915292";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/359/0/cluster359.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/359/0/cluster359.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/359/0/cluster359.proc0.subproc0/test.sh"\ ]'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S'
09/08/17 17:03:40 [28171] (364.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 17:03:40 [28171] (364.0) gm state change: GM_INIT -> GM_START
09/08/17 17:03:40 [28171] (364.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 17:03:40 [28171] (364.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:03:40 [28171] GAHP[28177] <- 'BLAH_JOB_SUBMIT 6 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#364.0#1504915292";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/360/0/cluster360.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/360/0/cluster360.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/360/0/cluster360.proc0.subproc0/test.sh"\ ]'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S'
09/08/17 17:03:40 [28171] This process has a valid certificate & key
09/08/17 17:03:40 [28171] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 17:03:40 [28171] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 17:03:40 [28171] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 17:03:40 [28171] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:03:40 [28171] IPVERIFY: ip found is 1
09/08/17 17:03:40 [28171] (365.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/08/17 17:03:40 [28171] (365.0) gm state change: GM_INIT -> GM_START
09/08/17 17:03:40 [28171] (365.0) gm state change: GM_START -> GM_TRANSFER_INPUT
09/08/17 17:03:40 [28171] (365.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:03:40 [28171] GAHP[28177] <- 'BLAH_JOB_SUBMIT 7 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#365.0#1504915292";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/361/0/cluster361.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/361/0/cluster361.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/361/0/cluster361.proc0.subproc0/test.sh"\ ]'
09/08/17 17:03:40 [28171] GAHP[28177] -> 'S'
09/08/17 17:03:41 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:03:41 [28171] GAHP[28177] -> 'R'
09/08/17 17:03:41 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:03:41 [28171] GAHP[28177] -> '4' '0' 'No error' 'slurm/20170908/144169'
09/08/17 17:03:41 [28171] (368.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 17:03:41 [28171] (368.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:03:41 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:03:41 [28171] GAHP[28177] -> 'R'
09/08/17 17:03:41 [28171] GAHP[28177] -> 'S' '3'
09/08/17 17:03:41 [28171] GAHP[28177] -> '3' '0' 'No error' 'slurm/20170908/144167'
09/08/17 17:03:41 [28171] GAHP[28177] -> '2' '0' 'No error' 'slurm/20170908/144168'
09/08/17 17:03:41 [28171] GAHP[28177] -> '5' '0' 'No error' 'slurm/20170908/144166'
09/08/17 17:03:41 [28171] (367.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 17:03:41 [28171] (367.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:03:41 [28171] (366.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 17:03:41 [28171] (366.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:03:41 [28171] (363.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 17:03:41 [28171] (363.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:03:41 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:03:41 [28171] GAHP[28177] -> 'R'
09/08/17 17:03:41 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:03:41 [28171] GAHP[28177] -> '7' '0' 'No error' 'slurm/20170908/144171'
09/08/17 17:03:41 [28171] (365.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 17:03:41 [28171] (365.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:03:42 [28171] Evaluating staleness of remote job statuses.
09/08/17 17:03:42 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:03:42 [28171] GAHP[28177] -> 'R'
09/08/17 17:03:42 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:03:42 [28171] GAHP[28177] -> '6' '0' 'No error' 'slurm/20170908/144170'
09/08/17 17:03:42 [28171] (364.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/08/17 17:03:42 [28171] (364.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:03:45 [28171] resource  is now up
09/08/17 17:03:45 [28171] in doContactSchedd()
09/08/17 17:03:45 [28171] querying for removed/held jobs
09/08/17 17:03:45 [28171] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:03:45 [28171] Fetched 0 job ads from schedd
09/08/17 17:03:45 [28171] Updating classad values for 363.0:
09/08/17 17:03:45 [28171]    DelegatedProxyExpiration = 1505345153
09/08/17 17:03:45 [28171]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#363.0#1504915292 slurm/20170908/144166"
09/08/17 17:03:45 [28171] Updating classad values for 364.0:
09/08/17 17:03:45 [28171]    DelegatedProxyExpiration = 1505345153
09/08/17 17:03:45 [28171]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#364.0#1504915292 slurm/20170908/144170"
09/08/17 17:03:45 [28171] Updating classad values for 365.0:
09/08/17 17:03:45 [28171]    DelegatedProxyExpiration = 1505345153
09/08/17 17:03:45 [28171]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#365.0#1504915292 slurm/20170908/144171"
09/08/17 17:03:45 [28171] Updating classad values for 366.0:
09/08/17 17:03:45 [28171]    DelegatedProxyExpiration = 1505345153
09/08/17 17:03:45 [28171]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#366.0#1504915293 slurm/20170908/144168"
09/08/17 17:03:45 [28171] Updating classad values for 367.0:
09/08/17 17:03:45 [28171]    DelegatedProxyExpiration = 1505345153
09/08/17 17:03:45 [28171]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#367.0#1504915293 slurm/20170908/144167"
09/08/17 17:03:45 [28171] Updating classad values for 368.0:
09/08/17 17:03:45 [28171]    DelegatedProxyExpiration = 1505345153
09/08/17 17:03:45 [28171]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#368.0#1504915293 slurm/20170908/144169"
09/08/17 17:03:45 [28171] leaving doContactSchedd()
09/08/17 17:03:45 [28171] (363.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 17:03:45 [28171] (363.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:03:45 [28171] (364.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 17:03:45 [28171] (364.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:03:45 [28171] (365.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 17:03:45 [28171] (365.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:03:45 [28171] (366.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 17:03:45 [28171] (366.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:03:45 [28171] (367.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 17:03:45 [28171] (367.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:03:45 [28171] (368.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState -1
09/08/17 17:03:45 [28171] (368.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:04:37 [28171] Received CHECK_LEASES signal
09/08/17 17:04:37 [28171] in doContactSchedd()
09/08/17 17:04:37 [28171] querying for renewed leases
09/08/17 17:04:37 [28171] querying for removed/held jobs
09/08/17 17:04:37 [28171] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:04:37 [28171] Fetched 0 job ads from schedd
09/08/17 17:04:37 [28171] leaving doContactSchedd()
09/08/17 17:04:40 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:04:40 [28171] GAHP[28177] -> 'S' '0'
09/08/17 17:04:42 [28171] Evaluating staleness of remote job statuses.
09/08/17 17:04:45 [28171] (363.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 17:04:45 [28171] (363.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:04:45 [28171] GAHP[28177] <- 'BLAH_JOB_STATUS 8 slurm/20170908/144166'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'S'
09/08/17 17:04:45 [28171] (364.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 17:04:45 [28171] (364.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:04:45 [28171] GAHP[28177] <- 'BLAH_JOB_STATUS 9 slurm/20170908/144170'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'S'
09/08/17 17:04:45 [28171] (365.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 17:04:45 [28171] (365.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:04:45 [28171] GAHP[28177] <- 'BLAH_JOB_STATUS 10 slurm/20170908/144171'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'S'
09/08/17 17:04:45 [28171] (366.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 17:04:45 [28171] (366.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:04:45 [28171] GAHP[28177] <- 'BLAH_JOB_STATUS 11 slurm/20170908/144168'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'S'
09/08/17 17:04:45 [28171] (367.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 17:04:45 [28171] (367.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:04:45 [28171] GAHP[28177] <- 'BLAH_JOB_STATUS 12 slurm/20170908/144167'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'S'
09/08/17 17:04:45 [28171] (368.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState -1
09/08/17 17:04:45 [28171] (368.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:04:45 [28171] GAHP[28177] <- 'BLAH_JOB_STATUS 13 slurm/20170908/144169'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'S'
09/08/17 17:04:45 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'R'
09/08/17 17:04:45 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:04:45 [28171] GAHP[28177] -> '12' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144167"; ImageSize = 0; WorkerNode = "mc1529"; RemoteUserCpu = 0 ]'
09/08/17 17:04:45 [28171] (367.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 17:04:45 [28171] (367.0) ***ProcessRemoteAd
09/08/17 17:04:45 [28171] (367.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:04:45 [28171] (367.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:04:45 [28171] (367.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:04:45 [28171] in doContactSchedd()
09/08/17 17:04:45 [28171] querying for removed/held jobs
09/08/17 17:04:45 [28171] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:04:45 [28171] Fetched 0 job ads from schedd
09/08/17 17:04:45 [28171] Updating classad values for 367.0:
09/08/17 17:04:45 [28171]    EnteredCurrentStatus = 1504915485
09/08/17 17:04:45 [28171]    ExitCode = 0
09/08/17 17:04:45 [28171]    GridJobStatus = "COMPLETED"
09/08/17 17:04:45 [28171]    ImageSize = 0
09/08/17 17:04:45 [28171]    JobStatus = 4
09/08/17 17:04:45 [28171]    LastRemoteStatusUpdate = 1504915485
09/08/17 17:04:45 [28171]    RemoteUserCpu = 0
09/08/17 17:04:45 [28171]    RemoteWallClockTime = 0.0
09/08/17 17:04:45 [28171] leaving doContactSchedd()
09/08/17 17:04:45 [28171] (367.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:04:45 [28171] (367.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:04:45 [28171] (367.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:04:45 [28171] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:04:45 [28171] (367.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:04:46 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'R'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:04:46 [28171] GAHP[28177] -> '13' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144169"; ImageSize = 0; WorkerNode = "mc1529"; RemoteUserCpu = 0 ]'
09/08/17 17:04:46 [28171] (368.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 17:04:46 [28171] (368.0) ***ProcessRemoteAd
09/08/17 17:04:46 [28171] (368.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:04:46 [28171] (368.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:04:46 [28171] (368.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:04:46 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'R'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:04:46 [28171] GAHP[28177] -> '9' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144170"; ImageSize = 0; WorkerNode = "mc1530"; RemoteUserCpu = 0 ]'
09/08/17 17:04:46 [28171] (364.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 17:04:46 [28171] (364.0) ***ProcessRemoteAd
09/08/17 17:04:46 [28171] (364.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:04:46 [28171] (364.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:04:46 [28171] (364.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:04:46 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'R'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:04:46 [28171] GAHP[28177] -> '11' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144168"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:04:46 [28171] (366.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 17:04:46 [28171] (366.0) ***ProcessRemoteAd
09/08/17 17:04:46 [28171] (366.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:04:46 [28171] (366.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:04:46 [28171] (366.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:04:46 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'R'
09/08/17 17:04:46 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:04:46 [28171] GAHP[28177] -> '8' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144166"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:04:46 [28171] (363.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 17:04:46 [28171] (363.0) ***ProcessRemoteAd
09/08/17 17:04:46 [28171] (363.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:04:46 [28171] (363.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:04:46 [28171] (363.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:04:47 [28171] GAHP[28177] <- 'RESULTS'
09/08/17 17:04:47 [28171] GAHP[28177] -> 'R'
09/08/17 17:04:47 [28171] GAHP[28177] -> 'S' '1'
09/08/17 17:04:47 [28171] GAHP[28177] -> '10' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144171"; ImageSize = 0; WorkerNode = "mc1532"; RemoteUserCpu = 0 ]'
09/08/17 17:04:47 [28171] (365.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState -1
09/08/17 17:04:47 [28171] (365.0) ***ProcessRemoteAd
09/08/17 17:04:47 [28171] (365.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:04:47 [28171] (365.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:04:47 [28171] (365.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:04:50 [28171] in doContactSchedd()
09/08/17 17:04:50 [28171] querying for removed/held jobs
09/08/17 17:04:50 [28171] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:04:50 [28171] Fetched 1 job ads from schedd
09/08/17 17:04:50 [28171] Updating classad values for 363.0:
09/08/17 17:04:50 [28171]    EnteredCurrentStatus = 1504915486
09/08/17 17:04:50 [28171]    ExitCode = 0
09/08/17 17:04:50 [28171]    GridJobStatus = "COMPLETED"
09/08/17 17:04:50 [28171]    ImageSize = 0
09/08/17 17:04:50 [28171]    JobStatus = 4
09/08/17 17:04:50 [28171]    LastRemoteStatusUpdate = 1504915486
09/08/17 17:04:50 [28171]    RemoteUserCpu = 0
09/08/17 17:04:50 [28171]    RemoteWallClockTime = 0.0
09/08/17 17:04:50 [28171] Updating classad values for 364.0:
09/08/17 17:04:50 [28171]    EnteredCurrentStatus = 1504915486
09/08/17 17:04:50 [28171]    ExitCode = 0
09/08/17 17:04:50 [28171]    GridJobStatus = "COMPLETED"
09/08/17 17:04:50 [28171]    ImageSize = 0
09/08/17 17:04:50 [28171]    JobStatus = 4
09/08/17 17:04:50 [28171]    LastRemoteStatusUpdate = 1504915486
09/08/17 17:04:50 [28171]    RemoteUserCpu = 0
09/08/17 17:04:50 [28171]    RemoteWallClockTime = 0.0
09/08/17 17:04:50 [28171] Updating classad values for 365.0:
09/08/17 17:04:50 [28171]    EnteredCurrentStatus = 1504915487
09/08/17 17:04:50 [28171]    ExitCode = 0
09/08/17 17:04:50 [28171]    GridJobStatus = "COMPLETED"
09/08/17 17:04:50 [28171]    ImageSize = 0
09/08/17 17:04:50 [28171]    JobStatus = 4
09/08/17 17:04:50 [28171]    LastRemoteStatusUpdate = 1504915487
09/08/17 17:04:50 [28171]    RemoteUserCpu = 0
09/08/17 17:04:50 [28171]    RemoteWallClockTime = 0.0
09/08/17 17:04:50 [28171] Updating classad values for 366.0:
09/08/17 17:04:50 [28171]    EnteredCurrentStatus = 1504915486
09/08/17 17:04:50 [28171]    ExitCode = 0
09/08/17 17:04:50 [28171]    GridJobStatus = "COMPLETED"
09/08/17 17:04:50 [28171]    ImageSize = 0
09/08/17 17:04:50 [28171]    JobStatus = 4
09/08/17 17:04:50 [28171]    LastRemoteStatusUpdate = 1504915486
09/08/17 17:04:50 [28171]    RemoteUserCpu = 0
09/08/17 17:04:50 [28171]    RemoteWallClockTime = 0.0
09/08/17 17:04:50 [28171] Updating classad values for 367.0:
09/08/17 17:04:50 [28171]    CurrentStatusUnknown = false
09/08/17 17:04:50 [28171]    GridJobId = undefined
09/08/17 17:04:50 [28171]    LastRemoteStatusUpdate = 0
09/08/17 17:04:50 [28171]    Managed = "ScheddDone"
09/08/17 17:04:50 [28171] Updating classad values for 368.0:
09/08/17 17:04:50 [28171]    EnteredCurrentStatus = 1504915486
09/08/17 17:04:50 [28171]    ExitCode = 0
09/08/17 17:04:50 [28171]    GridJobStatus = "COMPLETED"
09/08/17 17:04:50 [28171]    ImageSize = 0
09/08/17 17:04:50 [28171]    JobStatus = 4
09/08/17 17:04:50 [28171]    LastRemoteStatusUpdate = 1504915486
09/08/17 17:04:50 [28171]    RemoteUserCpu = 0
09/08/17 17:04:50 [28171]    RemoteWallClockTime = 0.0
09/08/17 17:04:50 [28171] Deleting job 367.0 from schedd
09/08/17 17:04:50 [28171] leaving doContactSchedd()
09/08/17 17:04:50 [28171] (363.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:04:50 [28171] (363.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:04:50 [28171] (363.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:04:50 [28171] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:04:50 [28171] (363.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:04:50 [28171] (364.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:04:50 [28171] (364.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:04:50 [28171] (364.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:04:50 [28171] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:04:50 [28171] (364.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:04:50 [28171] (365.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:04:50 [28171] (365.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:04:50 [28171] (365.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:04:50 [28171] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:04:50 [28171] (365.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:04:50 [28171] (366.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:04:50 [28171] (366.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:04:50 [28171] (366.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:04:50 [28171] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:04:50 [28171] (366.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:04:50 [28171] (368.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:04:50 [28171] (368.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:04:50 [28171] (368.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:04:50 [28171] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:04:50 [28171] (368.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:04:55 [28171] in doContactSchedd()
09/08/17 17:04:55 [28171] querying for removed/held jobs
09/08/17 17:04:55 [28171] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:04:55 [28171] Fetched 5 job ads from schedd
09/08/17 17:04:55 [28171] Updating classad values for 363.0:
09/08/17 17:04:55 [28171]    CurrentStatusUnknown = false
09/08/17 17:04:55 [28171]    GridJobId = undefined
09/08/17 17:04:55 [28171]    LastRemoteStatusUpdate = 0
09/08/17 17:04:55 [28171]    Managed = "ScheddDone"
09/08/17 17:04:55 [28171] Updating classad values for 364.0:
09/08/17 17:04:55 [28171]    CurrentStatusUnknown = false
09/08/17 17:04:55 [28171]    GridJobId = undefined
09/08/17 17:04:55 [28171]    LastRemoteStatusUpdate = 0
09/08/17 17:04:55 [28171]    Managed = "ScheddDone"
09/08/17 17:04:55 [28171] Updating classad values for 365.0:
09/08/17 17:04:55 [28171]    CurrentStatusUnknown = false
09/08/17 17:04:55 [28171]    GridJobId = undefined
09/08/17 17:04:55 [28171]    LastRemoteStatusUpdate = 0
09/08/17 17:04:55 [28171]    Managed = "ScheddDone"
09/08/17 17:04:55 [28171] Updating classad values for 366.0:
09/08/17 17:04:55 [28171]    CurrentStatusUnknown = false
09/08/17 17:04:55 [28171]    GridJobId = undefined
09/08/17 17:04:55 [28171]    LastRemoteStatusUpdate = 0
09/08/17 17:04:55 [28171]    Managed = "ScheddDone"
09/08/17 17:04:55 [28171] Updating classad values for 368.0:
09/08/17 17:04:55 [28171]    CurrentStatusUnknown = false
09/08/17 17:04:55 [28171]    GridJobId = undefined
09/08/17 17:04:55 [28171]    LastRemoteStatusUpdate = 0
09/08/17 17:04:55 [28171]    Managed = "ScheddDone"
09/08/17 17:04:55 [28171] Deleting job 363.0 from schedd
09/08/17 17:04:55 [28171] Deleting job 364.0 from schedd
09/08/17 17:04:55 [28171] Deleting job 365.0 from schedd
09/08/17 17:04:55 [28171] Deleting job 366.0 from schedd
09/08/17 17:04:55 [28171] Deleting job 368.0 from schedd
09/08/17 17:04:55 [28171] No jobs left, shutting down
09/08/17 17:04:55 [28171] leaving doContactSchedd()
09/08/17 17:04:55 [28171] Got SIGTERM. Performing graceful shutdown.
09/08/17 17:04:55 [28171] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 17:04:55 [28171] **** condor_gridmanager (condor_GRIDMANAGER) pid 28171 EXITING WITH STATUS 0
09/08/17 17:06:17 Result of reading /etc/issue:  \S
 
09/08/17 17:06:17 Result of reading /etc/redhat-release:  Scientific Linux release 7.3 (Nitrogen)
 
09/08/17 17:06:17 Using IDs: 32 processors, 16 CPUs, 16 HTs
09/08/17 17:06:17 Enumerating interfaces: lo 127.0.0.1 up
09/08/17 17:06:17 Enumerating interfaces: eth0 10.36.162.46 up
09/08/17 17:06:17 Enumerating interfaces: ib0 128.55.162.46 up
09/08/17 17:06:17 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
09/08/17 17:06:17 Initializing Directory: curr_dir = /etc/condor-ce/config.d
09/08/17 17:06:17 ******************************************************
09/08/17 17:06:17 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/08/17 17:06:17 ** /usr/sbin/condor_gridmanager
09/08/17 17:06:17 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/08/17 17:06:17 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/08/17 17:06:17 ** $CondorVersion: 8.4.11 Feb 24 2017 $
09/08/17 17:06:17 ** $CondorPlatform: X86_64-CentOS_7.3 $
09/08/17 17:06:17 ** PID = 28708
09/08/17 17:06:17 ** Log last touched 9/8 17:04:55
09/08/17 17:06:17 ******************************************************
09/08/17 17:06:17 Using config source: /etc/condor-ce/condor_config
09/08/17 17:06:17 Using local config sources: 
09/08/17 17:06:17    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/02-ce-slurm-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/03-gratia-cleanup.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/05-ce-health-defaults.conf
09/08/17 17:06:17    /usr/share/condor-ce/config.d/05-ce-view-defaults.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/01-ce-auth.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/01-ce-router.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/01-common-auth.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/02-ce-slurm.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/03-managed-fork.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/05-ce-health.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/05-ce-view.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/50-osg-configure.conf
09/08/17 17:06:17    /etc/condor-ce/config.d/99-local.conf
09/08/17 17:06:17    /usr/share/condor-ce/condor_ce_router_defaults|
09/08/17 17:06:17 config Macros = 177, Sorted = 177, StringBytes = 14927, TablesBytes = 6604
09/08/17 17:06:17 CLASSAD_CACHING is ENABLED
09/08/17 17:06:17 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
09/08/17 17:06:17 SharedPortEndpoint: waiting for connections to named socket 28160_0472_4
09/08/17 17:06:17 DaemonCore: command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=28160_0472_4>
09/08/17 17:06:17 DaemonCore: private command socket at <128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=28160_0472_4>
09/08/17 17:06:17 Setting maximum accepts per cycle 8.
09/08/17 17:06:17 Will use TCP to update collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 17:06:17 [28708] Welcome to the all-singing, all dancing, "amazing" GridManager!
09/08/17 17:06:17 [28708] DaemonCore: No more children processes to reap.
09/08/17 17:06:17 [28708] DaemonCore: in SendAliveToParent()
09/08/17 17:06:17 [28708] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:17 [28708] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:17 [28708] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:17 [28708] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:17 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:17 [28708] Completed DC_CHILDALIVE to daemon at <128.55.162.46:54131>
09/08/17 17:06:17 [28708] DaemonCore: Leaving SendAliveToParent() - success
09/08/17 17:06:17 [28708] Checking proxies
09/08/17 17:06:20 [28708] Received ADD_JOBS signal
09/08/17 17:06:20 [28708] in doContactSchedd()
09/08/17 17:06:20 [28708] querying for new jobs
09/08/17 17:06:20 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
09/08/17 17:06:20 [28708] Using job type INFNBatch for job 371.0
09/08/17 17:06:20 [28708] (371.0) SetJobLeaseTimers()
09/08/17 17:06:20 [28708] Found job 371.0 --- inserting
09/08/17 17:06:20 [28708] Using job type INFNBatch for job 372.0
09/08/17 17:06:20 [28708] (372.0) SetJobLeaseTimers()
09/08/17 17:06:20 [28708] Found job 372.0 --- inserting
09/08/17 17:06:20 [28708] Fetched 2 new job ads from schedd
09/08/17 17:06:20 [28708] querying for removed/held jobs
09/08/17 17:06:20 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:06:20 [28708] Fetched 0 job ads from schedd
09/08/17 17:06:20 [28708] leaving doContactSchedd()
09/08/17 17:06:20 [28708] gahp server not up yet, delaying ping
09/08/17 17:06:20 [28708] *** UpdateLeases called
09/08/17 17:06:20 [28708]     Leases not supported, cancelling timer
09/08/17 17:06:20 [28708] BaseResource::UpdateResource: 
AddressV1 = "{[ p=\"primary\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"28160_0472_4\"; noUDP=true; ], [ p=\"IPv4\"; a=\"128.55.162.46\"; port=9619; n=\"Internet\"; spid=\"28160_0472_4\"; noUDP=true; ]}"
MyAddress = "<128.55.162.46:9619?addrs=128.55.162.46-9619&noUDP&sock=28160_0472_4>"
Machine = "mpdsfgrid02.nersc.gov"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.3 $"
MyCurrentTime = 1504915580
IdleJobs = 2
Name = "batch "
HashName = "batch SLURM"
ScheddName = "mpdsfgrid02.nersc.gov"
ScheddIpAddr = "<128.55.162.46:54131>"
RunningJobs = 0
MyType = "Grid"
Owner = "alicesgm"
NumJobs = 2
JobLimit = 10000
SubmitsAllowed = 0
CondorVersion = "$CondorVersion: 8.4.11 Feb 24 2017 $"
SubmitsWanted = 0

09/08/17 17:06:20 [28708] Trying to update collector <128.55.162.46:9619>
09/08/17 17:06:20 [28708] Attempting to send update via TCP to collector mpdsfgrid02.nersc.gov <128.55.162.46:9619>
09/08/17 17:06:20 [28708] File descriptor limits: max 4096, safe 3277
09/08/17 17:06:20 [28708] (371.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:06:20 [28708] GAHP server pid = 28710
09/08/17 17:06:20 [28708] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
09/08/17 17:06:20 [28708] GAHP[28710] <- 'COMMANDS'
09/08/17 17:06:20 [28708] GAHP[28710] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
09/08/17 17:06:20 [28708] GAHP[28710] <- 'ASYNC_MODE_ON'
09/08/17 17:06:20 [28708] GAHP[28710] -> 'S' 'Async mode on'
09/08/17 17:06:20 [28708] (371.0) gm state change: GM_INIT -> GM_START
09/08/17 17:06:20 [28708] (371.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:06:20 [28708] (371.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:06:20 [28708] (371.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:06:20 [28708] (372.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:06:20 [28708] (372.0) gm state change: GM_INIT -> GM_START
09/08/17 17:06:20 [28708] (372.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:06:20 [28708] (372.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:06:20 [28708] (372.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:06:20 [28708] This process has a valid certificate & key
09/08/17 17:06:20 [28708] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:20 [28708] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:20 [28708] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:20 [28708] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=com\/DC\=DigiCert-Grid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=DigiCert-Grid\/DC\=com\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=opensciencegrid\/O=Open Science Grid\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='^\/C\=RU\/O\=RDIG\/OU\=hosts\/OU=*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='^\/C\=BR\/O\=ANSP\/OU\=ANSPGrid\ CA\/OU\=Services\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=org\/DC\=terena\/DC\=tcs.*\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@daemon.opensciencegrid.org'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='^\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=(host\/)?([A-Za-z0-9.\-]*)$' canonicalization='\2@cern.ch'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='(.*)' canonicalization='GSS_ASSIST_GRIDMAP'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='gsi' principal='(/CN=[-.A-Za-z0-9/= ]+)' canonicalization='\1@unmapped.opensciencegrid.org'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='claimtobe' principal='.*' canonicalization='anonymous@claimtobe'
09/08/17 17:06:20 [28708] MapFile: Canonicalization File: method='fs' principal='(.*)' canonicalization='\1'
09/08/17 17:06:20 [28708] ZKM: successful mapping to mpdsfgrid02.nersc.gov@daemon.opensciencegrid.org
09/08/17 17:06:20 [28708] IPVERIFY: checking mpdsfgrid02.nersc.gov against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:20 [28708] IPVERIFY: checking mpdsfgrid02 against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:20 [28708] IPVERIFY: checking mc0151-ib.nersc.gov against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:20 [28708] IPVERIFY: checking mc0151-ib against 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: matched 128.55.162.46 to 128.55.162.46
09/08/17 17:06:20 [28708] IPVERIFY: ip found is 1
09/08/17 17:06:22 [28708] Evaluating staleness of remote job statuses.
09/08/17 17:06:25 [28708] resource  is now up
09/08/17 17:06:25 [28708] in doContactSchedd()
09/08/17 17:06:25 [28708] querying for removed/held jobs
09/08/17 17:06:25 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:06:25 [28708] Fetched 0 job ads from schedd
09/08/17 17:06:25 [28708] Updating classad values for 371.0:
09/08/17 17:06:25 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#371.0#1504915572"
09/08/17 17:06:25 [28708]    LastRemoteStatusUpdate = 1504915580
09/08/17 17:06:25 [28708] Updating classad values for 372.0:
09/08/17 17:06:25 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#372.0#1504915572"
09/08/17 17:06:25 [28708]    LastRemoteStatusUpdate = 1504915580
09/08/17 17:06:25 [28708] leaving doContactSchedd()
09/08/17 17:06:25 [28708] (371.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:06:25 [28708] (371.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:06:25 [28708] (371.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:06:25 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 2 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#371.0#1504915572";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/369/0/cluster369.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/369/0/cluster369.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/369/0/cluster369.proc0.subproc0/test.sh"\ ]'
09/08/17 17:06:25 [28708] GAHP[28710] -> 'S'
09/08/17 17:06:25 [28708] (372.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:06:25 [28708] (372.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:06:25 [28708] (372.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:06:25 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 3 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#372.0#1504915572";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/370/0/cluster370.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/370/0/cluster370.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/370/0/cluster370.proc0.subproc0/test.sh"\ ]'
09/08/17 17:06:25 [28708] GAHP[28710] -> 'S'
09/08/17 17:06:26 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:06:26 [28708] GAHP[28710] -> 'R'
09/08/17 17:06:26 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:06:26 [28708] GAHP[28710] -> '2' '0' 'No error' 'slurm/20170908/144173'
09/08/17 17:06:26 [28708] (371.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:06:26 [28708] (371.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:06:26 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:06:26 [28708] GAHP[28710] -> 'R'
09/08/17 17:06:26 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:06:26 [28708] GAHP[28710] -> '3' '0' 'No error' 'slurm/20170908/144172'
09/08/17 17:06:26 [28708] (372.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:06:26 [28708] (372.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:06:30 [28708] in doContactSchedd()
09/08/17 17:06:30 [28708] querying for removed/held jobs
09/08/17 17:06:30 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:06:30 [28708] Fetched 0 job ads from schedd
09/08/17 17:06:30 [28708] Updating classad values for 371.0:
09/08/17 17:06:30 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:06:30 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#371.0#1504915572 slurm/20170908/144173"
09/08/17 17:06:30 [28708] Updating classad values for 372.0:
09/08/17 17:06:30 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:06:30 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#372.0#1504915572 slurm/20170908/144172"
09/08/17 17:06:30 [28708] leaving doContactSchedd()
09/08/17 17:06:30 [28708] (371.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:06:30 [28708] (371.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:06:30 [28708] (372.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:06:30 [28708] (372.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:17 [28708] Received CHECK_LEASES signal
09/08/17 17:07:17 [28708] in doContactSchedd()
09/08/17 17:07:17 [28708] querying for renewed leases
09/08/17 17:07:17 [28708] querying for removed/held jobs
09/08/17 17:07:17 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:17 [28708] Fetched 0 job ads from schedd
09/08/17 17:07:17 [28708] leaving doContactSchedd()
09/08/17 17:07:19 [28708] Received ADD_JOBS signal
09/08/17 17:07:20 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:20 [28708] GAHP[28710] -> 'S' '0'
09/08/17 17:07:22 [28708] Evaluating staleness of remote job statuses.
09/08/17 17:07:22 [28708] in doContactSchedd()
09/08/17 17:07:22 [28708] querying for new jobs
09/08/17 17:07:22 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
09/08/17 17:07:22 [28708] Using job type INFNBatch for job 386.0
09/08/17 17:07:22 [28708] (386.0) SetJobLeaseTimers()
09/08/17 17:07:22 [28708] Found job 386.0 --- inserting
09/08/17 17:07:22 [28708] Using job type INFNBatch for job 385.0
09/08/17 17:07:22 [28708] (385.0) SetJobLeaseTimers()
09/08/17 17:07:22 [28708] Found job 385.0 --- inserting
09/08/17 17:07:22 [28708] Using job type INFNBatch for job 384.0
09/08/17 17:07:22 [28708] (384.0) SetJobLeaseTimers()
09/08/17 17:07:22 [28708] Found job 384.0 --- inserting
09/08/17 17:07:22 [28708] Using job type INFNBatch for job 383.0
09/08/17 17:07:22 [28708] (383.0) SetJobLeaseTimers()
09/08/17 17:07:22 [28708] Found job 383.0 --- inserting
09/08/17 17:07:22 [28708] Fetched 4 new job ads from schedd
09/08/17 17:07:22 [28708] querying for removed/held jobs
09/08/17 17:07:22 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:22 [28708] Fetched 0 job ads from schedd
09/08/17 17:07:22 [28708] leaving doContactSchedd()
09/08/17 17:07:22 [28708] (386.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:22 [28708] (386.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:22 [28708] (386.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:22 [28708] (386.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:22 [28708] (386.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:22 [28708] (385.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:22 [28708] (385.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:22 [28708] (385.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:22 [28708] (385.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:22 [28708] (385.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:22 [28708] (384.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:22 [28708] (384.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:22 [28708] (384.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:22 [28708] (384.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:22 [28708] (384.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:22 [28708] (383.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:22 [28708] (383.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:22 [28708] (383.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:22 [28708] (383.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:22 [28708] (383.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:24 [28708] Received ADD_JOBS signal
09/08/17 17:07:27 [28708] in doContactSchedd()
09/08/17 17:07:27 [28708] querying for new jobs
09/08/17 17:07:27 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
09/08/17 17:07:27 [28708] Using job type INFNBatch for job 393.0
09/08/17 17:07:27 [28708] (393.0) SetJobLeaseTimers()
09/08/17 17:07:27 [28708] Found job 393.0 --- inserting
09/08/17 17:07:27 [28708] Using job type INFNBatch for job 392.0
09/08/17 17:07:27 [28708] (392.0) SetJobLeaseTimers()
09/08/17 17:07:27 [28708] Found job 392.0 --- inserting
09/08/17 17:07:27 [28708] Using job type INFNBatch for job 391.0
09/08/17 17:07:27 [28708] (391.0) SetJobLeaseTimers()
09/08/17 17:07:27 [28708] Found job 391.0 --- inserting
09/08/17 17:07:27 [28708] Using job type INFNBatch for job 390.0
09/08/17 17:07:27 [28708] (390.0) SetJobLeaseTimers()
09/08/17 17:07:27 [28708] Found job 390.0 --- inserting
09/08/17 17:07:27 [28708] Using job type INFNBatch for job 389.0
09/08/17 17:07:27 [28708] (389.0) SetJobLeaseTimers()
09/08/17 17:07:27 [28708] Found job 389.0 --- inserting
09/08/17 17:07:27 [28708] Using job type INFNBatch for job 388.0
09/08/17 17:07:27 [28708] (388.0) SetJobLeaseTimers()
09/08/17 17:07:27 [28708] Found job 388.0 --- inserting
09/08/17 17:07:27 [28708] Fetched 6 new job ads from schedd
09/08/17 17:07:27 [28708] querying for removed/held jobs
09/08/17 17:07:27 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:27 [28708] Fetched 0 job ads from schedd
09/08/17 17:07:27 [28708] Updating classad values for 385.0:
09/08/17 17:07:27 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#385.0#1504915632"
09/08/17 17:07:27 [28708]    LastRemoteStatusUpdate = 1504915642
09/08/17 17:07:27 [28708] Updating classad values for 386.0:
09/08/17 17:07:27 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#386.0#1504915632"
09/08/17 17:07:27 [28708]    LastRemoteStatusUpdate = 1504915642
09/08/17 17:07:27 [28708] Updating classad values for 383.0:
09/08/17 17:07:27 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#383.0#1504915632"
09/08/17 17:07:27 [28708]    LastRemoteStatusUpdate = 1504915642
09/08/17 17:07:27 [28708] Updating classad values for 384.0:
09/08/17 17:07:27 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#384.0#1504915632"
09/08/17 17:07:27 [28708]    LastRemoteStatusUpdate = 1504915642
09/08/17 17:07:27 [28708] leaving doContactSchedd()
09/08/17 17:07:27 [28708] (393.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:27 [28708] (393.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:27 [28708] (393.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:27 [28708] (393.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:27 [28708] (393.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:27 [28708] (392.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:27 [28708] (392.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:27 [28708] (392.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:27 [28708] (392.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:27 [28708] (392.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:27 [28708] (391.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:27 [28708] (391.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:27 [28708] (391.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:27 [28708] (391.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:27 [28708] (391.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:27 [28708] (390.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:27 [28708] (390.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:27 [28708] (390.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:27 [28708] (390.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:27 [28708] (390.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:27 [28708] (389.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:27 [28708] (389.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:27 [28708] (389.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:27 [28708] (389.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:27 [28708] (389.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:27 [28708] (388.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:27 [28708] (388.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:27 [28708] (388.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:27 [28708] (388.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:27 [28708] (388.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:27 [28708] (385.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:27 [28708] (385.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:27 [28708] (385.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:27 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 4 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#385.0#1504915632";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/375/0/cluster375.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/375/0/cluster375.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/375/0/cluster375.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:27 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:27 [28708] (386.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:27 [28708] (386.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:27 [28708] (386.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:27 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 5 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#386.0#1504915632";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/376/0/cluster376.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/376/0/cluster376.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/376/0/cluster376.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:27 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:27 [28708] (383.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:27 [28708] (383.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:27 [28708] (383.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:27 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 6 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#383.0#1504915632";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/373/0/cluster373.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/373/0/cluster373.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/373/0/cluster373.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:27 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:27 [28708] (384.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:27 [28708] (384.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:27 [28708] (384.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:27 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 7 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#384.0#1504915632";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/374/0/cluster374.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/374/0/cluster374.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/374/0/cluster374.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:27 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:28 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:28 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:28 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:28 [28708] GAHP[28710] -> '7' '0' 'No error' 'slurm/20170908/144174'
09/08/17 17:07:28 [28708] (384.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:28 [28708] (384.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:28 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:28 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:28 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:28 [28708] GAHP[28710] -> '6' '0' 'No error' 'slurm/20170908/144175'
09/08/17 17:07:28 [28708] (383.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:28 [28708] (383.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:29 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:29 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:29 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:29 [28708] GAHP[28710] -> '4' '0' 'No error' 'slurm/20170908/144177'
09/08/17 17:07:29 [28708] (385.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:29 [28708] (385.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:29 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:29 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:29 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:29 [28708] GAHP[28710] -> '5' '0' 'No error' 'slurm/20170908/144176'
09/08/17 17:07:29 [28708] (386.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:29 [28708] (386.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:29 [28708] Received ADD_JOBS signal
09/08/17 17:07:30 [28708] (371.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:07:30 [28708] (371.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:07:30 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 8 slurm/20170908/144173'
09/08/17 17:07:30 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:30 [28708] (372.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:07:30 [28708] (372.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:07:30 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 9 slurm/20170908/144172'
09/08/17 17:07:30 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:31 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:31 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:31 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:31 [28708] GAHP[28710] -> '9' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144172"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:07:31 [28708] (372.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:07:31 [28708] (372.0) ***ProcessRemoteAd
09/08/17 17:07:31 [28708] (372.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:07:31 [28708] (372.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:07:31 [28708] (372.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:07:31 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:31 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:31 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:31 [28708] GAHP[28710] -> '8' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144173"; ImageSize = 0; WorkerNode = "mc1529"; RemoteUserCpu = 0 ]'
09/08/17 17:07:31 [28708] (371.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:07:31 [28708] (371.0) ***ProcessRemoteAd
09/08/17 17:07:31 [28708] (371.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:07:31 [28708] (371.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:07:31 [28708] (371.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:07:32 [28708] in doContactSchedd()
09/08/17 17:07:32 [28708] querying for new jobs
09/08/17 17:07:32 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
09/08/17 17:07:32 [28708] Fetched 0 new job ads from schedd
09/08/17 17:07:32 [28708] querying for removed/held jobs
09/08/17 17:07:32 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:32 [28708] Fetched 0 job ads from schedd
09/08/17 17:07:32 [28708] Updating classad values for 372.0:
09/08/17 17:07:32 [28708]    EnteredCurrentStatus = 1504915651
09/08/17 17:07:32 [28708]    ExitCode = 0
09/08/17 17:07:32 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:07:32 [28708]    ImageSize = 0
09/08/17 17:07:32 [28708]    JobStatus = 4
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915651
09/08/17 17:07:32 [28708]    RemoteUserCpu = 0
09/08/17 17:07:32 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:07:32 [28708] Updating classad values for 383.0:
09/08/17 17:07:32 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#383.0#1504915632 slurm/20170908/144175"
09/08/17 17:07:32 [28708] Updating classad values for 384.0:
09/08/17 17:07:32 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#384.0#1504915632 slurm/20170908/144174"
09/08/17 17:07:32 [28708] Updating classad values for 385.0:
09/08/17 17:07:32 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#385.0#1504915632 slurm/20170908/144177"
09/08/17 17:07:32 [28708] Updating classad values for 386.0:
09/08/17 17:07:32 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#386.0#1504915632 slurm/20170908/144176"
09/08/17 17:07:32 [28708] Updating classad values for 388.0:
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#388.0#1504915642"
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915647
09/08/17 17:07:32 [28708] Updating classad values for 389.0:
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#389.0#1504915642"
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915647
09/08/17 17:07:32 [28708] Updating classad values for 390.0:
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#390.0#1504915642"
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915647
09/08/17 17:07:32 [28708] Updating classad values for 391.0:
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#391.0#1504915642"
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915647
09/08/17 17:07:32 [28708] Updating classad values for 392.0:
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#392.0#1504915642"
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915647
09/08/17 17:07:32 [28708] Updating classad values for 393.0:
09/08/17 17:07:32 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#393.0#1504915642"
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915647
09/08/17 17:07:32 [28708] Updating classad values for 371.0:
09/08/17 17:07:32 [28708]    EnteredCurrentStatus = 1504915651
09/08/17 17:07:32 [28708]    ExitCode = 0
09/08/17 17:07:32 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:07:32 [28708]    ImageSize = 0
09/08/17 17:07:32 [28708]    JobStatus = 4
09/08/17 17:07:32 [28708]    LastRemoteStatusUpdate = 1504915651
09/08/17 17:07:32 [28708]    RemoteUserCpu = 0
09/08/17 17:07:32 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:07:32 [28708] leaving doContactSchedd()
09/08/17 17:07:32 [28708] (372.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:07:32 [28708] (372.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:07:32 [28708] (372.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:07:32 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:07:32 [28708] (372.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:07:32 [28708] (383.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:32 [28708] (383.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:32 [28708] (384.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:32 [28708] (384.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:32 [28708] (385.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:32 [28708] (385.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:32 [28708] (386.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:32 [28708] (386.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:32 [28708] (388.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:32 [28708] (388.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:32 [28708] (388.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:32 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 10 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#388.0#1504915642";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/380/0/cluster380.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/380/0/cluster380.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/380/0/cluster380.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:32 [28708] (389.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:32 [28708] (389.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:32 [28708] (389.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:32 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 11 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#389.0#1504915642";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/381/0/cluster381.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/381/0/cluster381.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/381/0/cluster381.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:32 [28708] (390.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:32 [28708] (390.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:32 [28708] (390.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:32 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 12 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#390.0#1504915642";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/382/0/cluster382.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/382/0/cluster382.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/382/0/cluster382.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:32 [28708] (391.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:32 [28708] (391.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:32 [28708] (391.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:32 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 13 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#391.0#1504915642";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/377/0/cluster377.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/377/0/cluster377.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/377/0/cluster377.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:32 [28708] (392.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:32 [28708] (392.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:32 [28708] (392.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:32 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 14 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#392.0#1504915642";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/378/0/cluster378.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/378/0/cluster378.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/378/0/cluster378.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:32 [28708] (393.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:32 [28708] (393.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:32 [28708] (393.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:32 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 15 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#393.0#1504915642";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/379/0/cluster379.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/379/0/cluster379.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/379/0/cluster379.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:32 [28708] (371.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:07:32 [28708] (371.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:07:32 [28708] (371.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:07:32 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:07:32 [28708] (371.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:07:33 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:33 [28708] GAHP[28710] -> '12' '0' 'No error' 'slurm/20170908/144178'
09/08/17 17:07:33 [28708] (390.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:33 [28708] (390.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:33 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:33 [28708] GAHP[28710] -> '10' '0' 'No error' 'slurm/20170908/144180'
09/08/17 17:07:33 [28708] (388.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:33 [28708] (388.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:33 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:33 [28708] GAHP[28710] -> '11' '0' 'No error' 'slurm/20170908/144181'
09/08/17 17:07:33 [28708] (389.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:33 [28708] (389.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:33 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:33 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:33 [28708] GAHP[28710] -> '15' '0' 'No error' 'slurm/20170908/144183'
09/08/17 17:07:33 [28708] (393.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:33 [28708] (393.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:34 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:34 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:34 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:34 [28708] GAHP[28710] -> '13' '0' 'No error' 'slurm/20170908/144179'
09/08/17 17:07:34 [28708] (391.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:34 [28708] (391.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:34 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:34 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:34 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:34 [28708] GAHP[28710] -> '14' '0' 'No error' 'slurm/20170908/144182'
09/08/17 17:07:34 [28708] (392.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:34 [28708] (392.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:37 [28708] in doContactSchedd()
09/08/17 17:07:37 [28708] querying for removed/held jobs
09/08/17 17:07:37 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:37 [28708] Fetched 2 job ads from schedd
09/08/17 17:07:37 [28708] Updating classad values for 372.0:
09/08/17 17:07:37 [28708]    CurrentStatusUnknown = false
09/08/17 17:07:37 [28708]    GridJobId = undefined
09/08/17 17:07:37 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:07:37 [28708]    Managed = "ScheddDone"
09/08/17 17:07:37 [28708] Updating classad values for 388.0:
09/08/17 17:07:37 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:37 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#388.0#1504915642 slurm/20170908/144180"
09/08/17 17:07:37 [28708] Updating classad values for 389.0:
09/08/17 17:07:37 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:37 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#389.0#1504915642 slurm/20170908/144181"
09/08/17 17:07:37 [28708] Updating classad values for 390.0:
09/08/17 17:07:37 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:37 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#390.0#1504915642 slurm/20170908/144178"
09/08/17 17:07:37 [28708] Updating classad values for 391.0:
09/08/17 17:07:37 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:37 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#391.0#1504915642 slurm/20170908/144179"
09/08/17 17:07:37 [28708] Updating classad values for 392.0:
09/08/17 17:07:37 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:37 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#392.0#1504915642 slurm/20170908/144182"
09/08/17 17:07:37 [28708] Updating classad values for 393.0:
09/08/17 17:07:37 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:37 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#393.0#1504915642 slurm/20170908/144183"
09/08/17 17:07:37 [28708] Updating classad values for 371.0:
09/08/17 17:07:37 [28708]    CurrentStatusUnknown = false
09/08/17 17:07:37 [28708]    GridJobId = undefined
09/08/17 17:07:37 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:07:37 [28708]    Managed = "ScheddDone"
09/08/17 17:07:37 [28708] Deleting job 372.0 from schedd
09/08/17 17:07:37 [28708] Deleting job 371.0 from schedd
09/08/17 17:07:37 [28708] leaving doContactSchedd()
09/08/17 17:07:37 [28708] (388.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:37 [28708] (388.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:37 [28708] (389.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:37 [28708] (389.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:37 [28708] (390.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:37 [28708] (390.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:37 [28708] (391.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:37 [28708] (391.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:37 [28708] (392.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:37 [28708] (392.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:37 [28708] (393.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:37 [28708] (393.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:07:40 [28708] Received ADD_JOBS signal
09/08/17 17:07:42 [28708] in doContactSchedd()
09/08/17 17:07:42 [28708] querying for new jobs
09/08/17 17:07:42 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
09/08/17 17:07:42 [28708] Using job type INFNBatch for job 394.0
09/08/17 17:07:42 [28708] (394.0) SetJobLeaseTimers()
09/08/17 17:07:42 [28708] Found job 394.0 --- inserting
09/08/17 17:07:42 [28708] Fetched 1 new job ads from schedd
09/08/17 17:07:42 [28708] querying for removed/held jobs
09/08/17 17:07:42 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:42 [28708] Fetched 0 job ads from schedd
09/08/17 17:07:42 [28708] leaving doContactSchedd()
09/08/17 17:07:42 [28708] (394.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/08/17 17:07:42 [28708] (394.0) gm state change: GM_INIT -> GM_START
09/08/17 17:07:42 [28708] (394.0) gm state change: GM_START -> GM_CLEAR_REQUEST
09/08/17 17:07:42 [28708] (394.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
09/08/17 17:07:42 [28708] (394.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
09/08/17 17:07:47 [28708] in doContactSchedd()
09/08/17 17:07:47 [28708] querying for removed/held jobs
09/08/17 17:07:47 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:47 [28708] Fetched 0 job ads from schedd
09/08/17 17:07:47 [28708] Updating classad values for 394.0:
09/08/17 17:07:47 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#394.0#1504915652"
09/08/17 17:07:47 [28708]    LastRemoteStatusUpdate = 1504915662
09/08/17 17:07:47 [28708] leaving doContactSchedd()
09/08/17 17:07:47 [28708] (394.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/08/17 17:07:47 [28708] (394.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
09/08/17 17:07:47 [28708] (394.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
09/08/17 17:07:47 [28708] GAHP[28710] <- 'BLAH_JOB_SUBMIT 16 [\ OriginalMemory\ =\ 8000;\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ SMPGranularity\ =\ 1;\ NodeNumber\ =\ 1;\ JobDirectory\ =\ "home_bl_mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#394.0#1504915652";\ Arguments\ =\ "";\ Environment\ =\ "HOME=/global/homes/a/alicesgm\ CONDORCE_COLLECTOR_HOST=mpdsfgrid02.nersc.gov:9619\ OSG_GRID='/usr/common/usg/software/osg/3.1.35-tarball-1/osg-client/'\ OSG_SQUID_LOCATION='msquid01-ib.nersc.gov:3128'\ OSG_SITE_READ='/project/projectdirs/pdsf/osg_temp/pdsf/stage'\ OSG_APP='/project/projectdirs/pdsf/osg_temp/pdsf/app'\ OSG_HOSTNAME='mpdsfgrid02.nersc.gov'\ OSG_DATA='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_WN_TMP='/project/projectdirs/pdsf/osg_temp/pdsf/data'\ OSG_STORAGE_ELEMENT='False'\ OSG_SITE_NAME='NERSC-PDSF'\ GLOBUS_LOCATION='/usr'\ OSG_SITE_WRITE='/project/projectdirs/pdsf/osg_temp/pdsf/write'\ ";\ TransferOutputRemaps\ =\ undefined;\ In\ =\ "/dev/null";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ x509userproxy\ =\ "/common/osg/condor2/387/0/cluster387.proc0.subproc0/x509up_u49514";\ Iwd\ =\ "/common/osg/condor2/387/0/cluster387.proc0.subproc0";\ GridResource\ =\ "batch\ slurm";\ RequestMemory\ =\ ifThenElse(WantWholeNode\ is\ true,\ !isUndefined(TotalMemory)\ ?\ TotalMemory\ *\ 95\ /\ 100\ :\ JobMemory,OriginalMemory);\ Cmd\ =\ "/common/osg/condor2/387/0/cluster387.proc0.subproc0/test.sh"\ ]'
09/08/17 17:07:47 [28708] GAHP[28710] -> 'S'
09/08/17 17:07:48 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:07:48 [28708] GAHP[28710] -> 'R'
09/08/17 17:07:48 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:07:48 [28708] GAHP[28710] -> '16' '0' 'No error' 'slurm/20170908/144184'
09/08/17 17:07:48 [28708] (394.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/08/17 17:07:48 [28708] (394.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
09/08/17 17:07:52 [28708] in doContactSchedd()
09/08/17 17:07:52 [28708] querying for removed/held jobs
09/08/17 17:07:52 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:07:52 [28708] Fetched 0 job ads from schedd
09/08/17 17:07:52 [28708] Updating classad values for 394.0:
09/08/17 17:07:52 [28708]    DelegatedProxyExpiration = 1505345153
09/08/17 17:07:52 [28708]    GridJobId = "batch slurm mpdsfgrid02.nersc.gov_9619_mpdsfgrid02.nersc.gov#394.0#1504915652 slurm/20170908/144184"
09/08/17 17:07:52 [28708] leaving doContactSchedd()
09/08/17 17:07:52 [28708] (394.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
09/08/17 17:07:52 [28708] (394.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
09/08/17 17:08:17 [28708] Received CHECK_LEASES signal
09/08/17 17:08:17 [28708] in doContactSchedd()
09/08/17 17:08:17 [28708] querying for renewed leases
09/08/17 17:08:17 [28708] querying for removed/held jobs
09/08/17 17:08:17 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:08:17 [28708] Fetched 0 job ads from schedd
09/08/17 17:08:17 [28708] leaving doContactSchedd()
09/08/17 17:08:20 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:20 [28708] GAHP[28710] -> 'S' '0'
09/08/17 17:08:22 [28708] Evaluating staleness of remote job statuses.
09/08/17 17:08:32 [28708] (383.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:32 [28708] (383.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:32 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 17 slurm/20170908/144175'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:32 [28708] (384.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:32 [28708] (384.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:32 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 18 slurm/20170908/144174'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:32 [28708] (385.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:32 [28708] (385.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:32 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 19 slurm/20170908/144177'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:32 [28708] (386.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:32 [28708] (386.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:32 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 20 slurm/20170908/144176'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:32 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:32 [28708] GAHP[28710] -> '18' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144174"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:08:32 [28708] (384.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:32 [28708] (384.0) ***ProcessRemoteAd
09/08/17 17:08:32 [28708] (384.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:32 [28708] (384.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:32 [28708] (384.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:32 [28708] in doContactSchedd()
09/08/17 17:08:32 [28708] querying for removed/held jobs
09/08/17 17:08:32 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:08:32 [28708] Fetched 0 job ads from schedd
09/08/17 17:08:32 [28708] Updating classad values for 384.0:
09/08/17 17:08:32 [28708]    EnteredCurrentStatus = 1504915712
09/08/17 17:08:32 [28708]    ExitCode = 0
09/08/17 17:08:32 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:32 [28708]    ImageSize = 0
09/08/17 17:08:32 [28708]    JobStatus = 4
09/08/17 17:08:32 [28708]    LastRemoteStatusUpdate = 1504915712
09/08/17 17:08:32 [28708]    RemoteUserCpu = 0
09/08/17 17:08:32 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:32 [28708] leaving doContactSchedd()
09/08/17 17:08:32 [28708] (384.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:32 [28708] (384.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:32 [28708] (384.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:32 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:32 [28708] (384.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:32 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:32 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:32 [28708] GAHP[28710] -> '17' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144175"; ImageSize = 0; WorkerNode = "mc1529"; RemoteUserCpu = 0 ]'
09/08/17 17:08:32 [28708] (383.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:32 [28708] (383.0) ***ProcessRemoteAd
09/08/17 17:08:32 [28708] (383.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:32 [28708] (383.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:32 [28708] (383.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:33 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:33 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:33 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:33 [28708] GAHP[28710] -> '19' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144177"; ImageSize = 0; WorkerNode = "mc1530"; RemoteUserCpu = 0 ]'
09/08/17 17:08:33 [28708] (385.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:33 [28708] (385.0) ***ProcessRemoteAd
09/08/17 17:08:33 [28708] (385.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:33 [28708] (385.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:33 [28708] (385.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:34 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:34 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:34 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:34 [28708] GAHP[28710] -> '20' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144176"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:08:34 [28708] (386.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:34 [28708] (386.0) ***ProcessRemoteAd
09/08/17 17:08:34 [28708] (386.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:34 [28708] (386.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:34 [28708] (386.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:37 [28708] (388.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:37 [28708] (388.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:37 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 21 slurm/20170908/144180'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:37 [28708] (389.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:37 [28708] (389.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:37 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 22 slurm/20170908/144181'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:37 [28708] (390.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:37 [28708] (390.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:37 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 23 slurm/20170908/144178'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:37 [28708] (391.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:37 [28708] (391.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:37 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 24 slurm/20170908/144179'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:37 [28708] (392.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:37 [28708] (392.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:37 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 25 slurm/20170908/144182'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:37 [28708] (393.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:37 [28708] (393.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:37 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 26 slurm/20170908/144183'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:37 [28708] in doContactSchedd()
09/08/17 17:08:37 [28708] querying for removed/held jobs
09/08/17 17:08:37 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:08:37 [28708] Fetched 1 job ads from schedd
09/08/17 17:08:37 [28708] Updating classad values for 383.0:
09/08/17 17:08:37 [28708]    EnteredCurrentStatus = 1504915712
09/08/17 17:08:37 [28708]    ExitCode = 0
09/08/17 17:08:37 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:37 [28708]    ImageSize = 0
09/08/17 17:08:37 [28708]    JobStatus = 4
09/08/17 17:08:37 [28708]    LastRemoteStatusUpdate = 1504915712
09/08/17 17:08:37 [28708]    RemoteUserCpu = 0
09/08/17 17:08:37 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:37 [28708] Updating classad values for 384.0:
09/08/17 17:08:37 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:37 [28708]    GridJobId = undefined
09/08/17 17:08:37 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:37 [28708]    Managed = "ScheddDone"
09/08/17 17:08:37 [28708] Updating classad values for 385.0:
09/08/17 17:08:37 [28708]    EnteredCurrentStatus = 1504915713
09/08/17 17:08:37 [28708]    ExitCode = 0
09/08/17 17:08:37 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:37 [28708]    ImageSize = 0
09/08/17 17:08:37 [28708]    JobStatus = 4
09/08/17 17:08:37 [28708]    LastRemoteStatusUpdate = 1504915713
09/08/17 17:08:37 [28708]    RemoteUserCpu = 0
09/08/17 17:08:37 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:37 [28708] Updating classad values for 386.0:
09/08/17 17:08:37 [28708]    EnteredCurrentStatus = 1504915714
09/08/17 17:08:37 [28708]    ExitCode = 0
09/08/17 17:08:37 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:37 [28708]    ImageSize = 0
09/08/17 17:08:37 [28708]    JobStatus = 4
09/08/17 17:08:37 [28708]    LastRemoteStatusUpdate = 1504915714
09/08/17 17:08:37 [28708]    RemoteUserCpu = 0
09/08/17 17:08:37 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:37 [28708] Deleting job 384.0 from schedd
09/08/17 17:08:37 [28708] leaving doContactSchedd()
09/08/17 17:08:37 [28708] (383.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:37 [28708] (383.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:37 [28708] (383.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:37 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:37 [28708] (383.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:37 [28708] (385.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:37 [28708] (385.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:37 [28708] (385.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:37 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:37 [28708] (385.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:37 [28708] (386.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:37 [28708] (386.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:37 [28708] (386.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:37 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:37 [28708] (386.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:37 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:37 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:37 [28708] GAHP[28710] -> '24' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144179"; ImageSize = 0; WorkerNode = "mc1529"; RemoteUserCpu = 0 ]'
09/08/17 17:08:37 [28708] (391.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:37 [28708] (391.0) ***ProcessRemoteAd
09/08/17 17:08:37 [28708] (391.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:37 [28708] (391.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:37 [28708] (391.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:38 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:38 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:38 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:38 [28708] GAHP[28710] -> '23' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144178"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:08:38 [28708] (390.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:38 [28708] (390.0) ***ProcessRemoteAd
09/08/17 17:08:38 [28708] (390.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:38 [28708] (390.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:38 [28708] (390.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:38 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:38 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:38 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:38 [28708] GAHP[28710] -> '21' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144180"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:08:38 [28708] (388.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:38 [28708] (388.0) ***ProcessRemoteAd
09/08/17 17:08:38 [28708] (388.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:38 [28708] (388.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:38 [28708] (388.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:38 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:38 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:38 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:38 [28708] GAHP[28710] -> '26' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144183"; ImageSize = 0; WorkerNode = "mc1532"; RemoteUserCpu = 0 ]'
09/08/17 17:08:38 [28708] (393.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:38 [28708] (393.0) ***ProcessRemoteAd
09/08/17 17:08:38 [28708] (393.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:38 [28708] (393.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:38 [28708] (393.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:39 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:39 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:39 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:39 [28708] GAHP[28710] -> '25' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144182"; ImageSize = 0; WorkerNode = "mc1530"; RemoteUserCpu = 0 ]'
09/08/17 17:08:39 [28708] (392.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:39 [28708] (392.0) ***ProcessRemoteAd
09/08/17 17:08:39 [28708] (392.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:39 [28708] (392.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:39 [28708] (392.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:39 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:39 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:39 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:39 [28708] GAHP[28710] -> '22' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144181"; ImageSize = 0; WorkerNode = "mc1529"; RemoteUserCpu = 0 ]'
09/08/17 17:08:39 [28708] (389.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:39 [28708] (389.0) ***ProcessRemoteAd
09/08/17 17:08:39 [28708] (389.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:39 [28708] (389.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:39 [28708] (389.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:42 [28708] in doContactSchedd()
09/08/17 17:08:42 [28708] querying for removed/held jobs
09/08/17 17:08:42 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:08:42 [28708] Fetched 3 job ads from schedd
09/08/17 17:08:42 [28708] Updating classad values for 383.0:
09/08/17 17:08:42 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:42 [28708]    GridJobId = undefined
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:42 [28708]    Managed = "ScheddDone"
09/08/17 17:08:42 [28708] Updating classad values for 385.0:
09/08/17 17:08:42 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:42 [28708]    GridJobId = undefined
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:42 [28708]    Managed = "ScheddDone"
09/08/17 17:08:42 [28708] Updating classad values for 386.0:
09/08/17 17:08:42 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:42 [28708]    GridJobId = undefined
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:42 [28708]    Managed = "ScheddDone"
09/08/17 17:08:42 [28708] Updating classad values for 388.0:
09/08/17 17:08:42 [28708]    EnteredCurrentStatus = 1504915718
09/08/17 17:08:42 [28708]    ExitCode = 0
09/08/17 17:08:42 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:42 [28708]    ImageSize = 0
09/08/17 17:08:42 [28708]    JobStatus = 4
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 1504915718
09/08/17 17:08:42 [28708]    RemoteUserCpu = 0
09/08/17 17:08:42 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:42 [28708] Updating classad values for 389.0:
09/08/17 17:08:42 [28708]    EnteredCurrentStatus = 1504915719
09/08/17 17:08:42 [28708]    ExitCode = 0
09/08/17 17:08:42 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:42 [28708]    ImageSize = 0
09/08/17 17:08:42 [28708]    JobStatus = 4
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 1504915719
09/08/17 17:08:42 [28708]    RemoteUserCpu = 0
09/08/17 17:08:42 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:42 [28708] Updating classad values for 390.0:
09/08/17 17:08:42 [28708]    EnteredCurrentStatus = 1504915718
09/08/17 17:08:42 [28708]    ExitCode = 0
09/08/17 17:08:42 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:42 [28708]    ImageSize = 0
09/08/17 17:08:42 [28708]    JobStatus = 4
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 1504915718
09/08/17 17:08:42 [28708]    RemoteUserCpu = 0
09/08/17 17:08:42 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:42 [28708] Updating classad values for 391.0:
09/08/17 17:08:42 [28708]    EnteredCurrentStatus = 1504915717
09/08/17 17:08:42 [28708]    ExitCode = 0
09/08/17 17:08:42 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:42 [28708]    ImageSize = 0
09/08/17 17:08:42 [28708]    JobStatus = 4
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 1504915717
09/08/17 17:08:42 [28708]    RemoteUserCpu = 0
09/08/17 17:08:42 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:42 [28708] Updating classad values for 392.0:
09/08/17 17:08:42 [28708]    EnteredCurrentStatus = 1504915719
09/08/17 17:08:42 [28708]    ExitCode = 0
09/08/17 17:08:42 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:42 [28708]    ImageSize = 0
09/08/17 17:08:42 [28708]    JobStatus = 4
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 1504915719
09/08/17 17:08:42 [28708]    RemoteUserCpu = 0
09/08/17 17:08:42 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:42 [28708] Updating classad values for 393.0:
09/08/17 17:08:42 [28708]    EnteredCurrentStatus = 1504915718
09/08/17 17:08:42 [28708]    ExitCode = 0
09/08/17 17:08:42 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:42 [28708]    ImageSize = 0
09/08/17 17:08:42 [28708]    JobStatus = 4
09/08/17 17:08:42 [28708]    LastRemoteStatusUpdate = 1504915718
09/08/17 17:08:42 [28708]    RemoteUserCpu = 0
09/08/17 17:08:42 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:42 [28708] Deleting job 383.0 from schedd
09/08/17 17:08:42 [28708] Deleting job 385.0 from schedd
09/08/17 17:08:42 [28708] Deleting job 386.0 from schedd
09/08/17 17:08:42 [28708] leaving doContactSchedd()
09/08/17 17:08:42 [28708] (388.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:42 [28708] (388.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:42 [28708] (388.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:42 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:42 [28708] (388.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:42 [28708] (389.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:42 [28708] (389.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:42 [28708] (389.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:42 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:42 [28708] (389.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:42 [28708] (390.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:42 [28708] (390.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:42 [28708] (390.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:42 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:42 [28708] (390.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:42 [28708] (391.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:42 [28708] (391.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:42 [28708] (391.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:42 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:42 [28708] (391.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:42 [28708] (392.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:42 [28708] (392.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:42 [28708] (392.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:42 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:42 [28708] (392.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:42 [28708] (393.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:42 [28708] (393.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:42 [28708] (393.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:42 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:42 [28708] (393.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:47 [28708] in doContactSchedd()
09/08/17 17:08:47 [28708] querying for removed/held jobs
09/08/17 17:08:47 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:08:47 [28708] Fetched 6 job ads from schedd
09/08/17 17:08:47 [28708] Updating classad values for 388.0:
09/08/17 17:08:47 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:47 [28708]    GridJobId = undefined
09/08/17 17:08:47 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:47 [28708]    Managed = "ScheddDone"
09/08/17 17:08:47 [28708] Updating classad values for 389.0:
09/08/17 17:08:47 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:47 [28708]    GridJobId = undefined
09/08/17 17:08:47 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:47 [28708]    Managed = "ScheddDone"
09/08/17 17:08:47 [28708] Updating classad values for 390.0:
09/08/17 17:08:47 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:47 [28708]    GridJobId = undefined
09/08/17 17:08:47 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:47 [28708]    Managed = "ScheddDone"
09/08/17 17:08:47 [28708] Updating classad values for 391.0:
09/08/17 17:08:47 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:47 [28708]    GridJobId = undefined
09/08/17 17:08:47 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:47 [28708]    Managed = "ScheddDone"
09/08/17 17:08:47 [28708] Updating classad values for 392.0:
09/08/17 17:08:47 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:47 [28708]    GridJobId = undefined
09/08/17 17:08:47 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:47 [28708]    Managed = "ScheddDone"
09/08/17 17:08:47 [28708] Updating classad values for 393.0:
09/08/17 17:08:47 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:47 [28708]    GridJobId = undefined
09/08/17 17:08:47 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:47 [28708]    Managed = "ScheddDone"
09/08/17 17:08:47 [28708] Deleting job 388.0 from schedd
09/08/17 17:08:47 [28708] Deleting job 389.0 from schedd
09/08/17 17:08:47 [28708] Deleting job 390.0 from schedd
09/08/17 17:08:47 [28708] Deleting job 391.0 from schedd
09/08/17 17:08:47 [28708] Deleting job 392.0 from schedd
09/08/17 17:08:47 [28708] Deleting job 393.0 from schedd
09/08/17 17:08:47 [28708] leaving doContactSchedd()
09/08/17 17:08:52 [28708] (394.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
09/08/17 17:08:52 [28708] (394.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
09/08/17 17:08:52 [28708] GAHP[28710] <- 'BLAH_JOB_STATUS 27 slurm/20170908/144184'
09/08/17 17:08:52 [28708] GAHP[28710] -> 'S'
09/08/17 17:08:52 [28708] GAHP[28710] <- 'RESULTS'
09/08/17 17:08:52 [28708] GAHP[28710] -> 'R'
09/08/17 17:08:52 [28708] GAHP[28710] -> 'S' '1'
09/08/17 17:08:52 [28708] GAHP[28710] -> '27' '0' 'No Error' '4' '[ ExitCode = 0; JobStatus = 4; BatchJobId = "144184"; ImageSize = 0; WorkerNode = "mc1528"; RemoteUserCpu = 0 ]'
09/08/17 17:08:52 [28708] (394.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
09/08/17 17:08:52 [28708] (394.0) ***ProcessRemoteAd
09/08/17 17:08:52 [28708] (394.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
09/08/17 17:08:52 [28708] (394.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
09/08/17 17:08:52 [28708] (394.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
09/08/17 17:08:52 [28708] in doContactSchedd()
09/08/17 17:08:52 [28708] querying for removed/held jobs
09/08/17 17:08:52 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:08:52 [28708] Fetched 0 job ads from schedd
09/08/17 17:08:52 [28708] Updating classad values for 394.0:
09/08/17 17:08:52 [28708]    EnteredCurrentStatus = 1504915732
09/08/17 17:08:52 [28708]    ExitCode = 0
09/08/17 17:08:52 [28708]    GridJobStatus = "COMPLETED"
09/08/17 17:08:52 [28708]    ImageSize = 0
09/08/17 17:08:52 [28708]    JobStatus = 4
09/08/17 17:08:52 [28708]    LastRemoteStatusUpdate = 1504915732
09/08/17 17:08:52 [28708]    RemoteUserCpu = 0
09/08/17 17:08:52 [28708]    RemoteWallClockTime = 0.0
09/08/17 17:08:52 [28708] leaving doContactSchedd()
09/08/17 17:08:52 [28708] (394.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
09/08/17 17:08:52 [28708] (394.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
09/08/17 17:08:52 [28708] (394.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
09/08/17 17:08:52 [28708] Initializing Directory: curr_dir = /global/homes/a/alicesgm
09/08/17 17:08:52 [28708] (394.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
09/08/17 17:08:57 [28708] in doContactSchedd()
09/08/17 17:08:57 [28708] querying for removed/held jobs
09/08/17 17:08:57 [28708] Using constraint ((Owner=?="alicesgm"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
09/08/17 17:08:57 [28708] Fetched 1 job ads from schedd
09/08/17 17:08:57 [28708] Updating classad values for 394.0:
09/08/17 17:08:57 [28708]    CurrentStatusUnknown = false
09/08/17 17:08:57 [28708]    GridJobId = undefined
09/08/17 17:08:57 [28708]    LastRemoteStatusUpdate = 0
09/08/17 17:08:57 [28708]    Managed = "ScheddDone"
09/08/17 17:08:57 [28708] Deleting job 394.0 from schedd
09/08/17 17:08:57 [28708] No jobs left, shutting down
09/08/17 17:08:57 [28708] leaving doContactSchedd()
09/08/17 17:08:57 [28708] Got SIGTERM. Performing graceful shutdown.
09/08/17 17:08:57 [28708] Started timer to call main_shutdown_fast in 1800 seconds
09/08/17 17:08:57 [28708] **** condor_gridmanager (condor_GRIDMANAGER) pid 28708 EXITING WITH STATUS 0
