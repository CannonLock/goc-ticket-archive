
This is an automated notification sent by LCG Savannah.
It relates to:
		support #133235, project CMS Computing Infrastructure  Support

==============================================================================
 LATEST MODIFICATIONS of support #133235:
==============================================================================

Follow-up Comment #28, sr #133235 (project cmscompinfrasup):

Hi,
from these plots 
https://cmsweb.cern.ch/phedex/debug/Activity::QualityPlots?src_filter=CNAF;period=l30d;no_mss=true;dest_filter=T1_US_FNAL;upto=;entity=link;graph=quality_all
https://cmsweb.cern.ch/phedex/prod/Activity::QualityPlots?src_filter=CNAF;period=l30d;no_mss=true;dest_filter=T1_US_FNAL;upto=;entity=link;graph=quality_all
since 29th November seems  that the  transfers are improved 
Have you made any changes? ... I confirm that cnaf side nothing has been
done.

Saverio 

 

==============================================================================
 OVERVIEW of support #133235:
==============================================================================

URL:
  <http://savannah.cern.ch/support/?133235>

                 Summary: Degraded transfers CNAF->FNAL
                 Project: CMS Computing Infrastructure  Support
            Submitted by: cassel
            Submitted on: 2012-10-23 12:58
                Category: Data Transfers
                Priority: 5 - Normal
                Severity: 3 - Normal
                  Status: None
                 Privacy: Public
             Assigned to: cmscompinfrasup-t1usfnal
        Originator Email: 
             Open/Closed: Open
         Discussion Lock: Any
         GGUS ticket url: https://ggus.eu/ws/ticket_info.php?ticket=88752
                Use GGUS: Yes
                    Site: T1_US_FNAL

    _______________________________________________________


Dear site admins,

looking at the quality plot:

https://cmsweb.cern.ch/phedex/prod/Activity::QualityPlots?entity=link&src_filter=CNAF&dest_filter=T1_US_FNAL

20-30% of the transfers are failing with:

TRANSFER error during TRANSFER phase: [SRM_COPY] FAILED:  at Tue Oct 23
06:28:43 CDT 2012 state Failed : Request state changed, changing file state

DESTINATION error during TRANSFER phase: [FILE_EXISTS] Failed

TRANSFER error during TRANSFER phase: [SRM_COPY] FAILED:  at Tue Oct 23
06:11:24 CDT 2012 state Failed : lifetime expired

errors. Since there are quite a lot to transfer, this degradation reduces
performance a lot. Could you check if anything can be done to improve the
situation?

Thanks,
Rapolas K.

    _______________________________________________________

Follow-up Comments:


-------------------------------------------------------
Date: 2012-12-10 15:49              By: Saverio Virgilio <savirgil>
Hi,
from these plots 
https://cmsweb.cern.ch/phedex/debug/Activity::QualityPlots?src_filter=CNAF;period=l30d;no_mss=true;dest_filter=T1_US_FNAL;upto=;entity=link;graph=quality_all
https://cmsweb.cern.ch/phedex/prod/Activity::QualityPlots?src_filter=CNAF;period=l30d;no_mss=true;dest_filter=T1_US_FNAL;upto=;entity=link;graph=quality_all
since 29th November seems  that the  transfers are improved 
Have you made any changes? ... I confirm that cnaf side nothing has been
done.

Saverio 

 

-------------------------------------------------------
Date: 2012-11-21 16:23              By: Krista Majewski <klarson1>
Hi Saverio,

Below is some example output from the networking group's tests where you can
see that the outbound rates are much higher than the inbound.  I would
suggest you open a Service Now ticket at FNAL and talk to them directly to
debug network issues.  Also, the original Service Now ticket for checking the
link between CNAF and FNAL was INC000000332454 if you need it.

INBOUND (CNAF-FNAL), Mbps=36.419:
Source Host 	Destination Host 	SessionID 	Number of Streams 	Bytes 	Packets
	Flows 	Mbps 	Pps 	Start 	End 	Duration (msecs)
Stream Index 	SrcPort 	DstPort 	Protocol 	Bytes 	Packets 	Flows 	Mbps 	Pps
	Start 	End 	Duration (msecs)
131.154.130.67 (ds-202-08-39.cr.cnaf.infn.it) 	131.225.204.234 (cmsstor163)
	1:11 	10 	1151850810 	758795 	40 	36.419 	3070 	00:07:10.930 	00:11:18.23
	247093
1 	55873 	20000 	tcp 	119557680 	78760 	4 	3.788 	319 	00:07:11.40
	00:11:17.621 	246581
2 	55866 	20000 	tcp 	111141888 	73216 	4 	3.521 	296 	00:07:11.192
	00:11:17.771 	246579
3 	55871 	20000 	tcp 	119424096 	78672 	4 	3.779 	318 	00:07:11.42
	00:11:17.944 	246902
4 	55870 	20000 	tcp 	112243956 	73942 	4 	3.550 	299 	00:07:10.930
	00:11:17.973 	247043
5 	55867 	20000 	tcp 	118188444 	77858 	4 	3.741 	315 	00:07:10.930
	00:11:17.780 	246850
6 	55872 	20000 	tcp 	111642828 	73546 	4 	3.533 	297 	00:07:11.182
	00:11:18.23 	246841
7 	55869 	20000 	tcp 	118678758 	78181 	4 	3.756 	316 	00:07:11.182
	00:11:18.21 	246839
8 	55868 	20000 	tcp 	117587316 	77462 	4 	3.725 	314 	00:07:11.188
	00:11:17.775 	246587
9 	55864 	20000 	tcp 	111442452 	73414 	4 	3.529 	297 	00:07:11.188
	00:11:17.906 	246718
10 	55865 	20000 	tcp 	111943392 	73744 	4 	3.548 	299 	00:07:11.190
	00:11:17.696 	246506 

OUTBOUND (FNAL-CNAF), Mbps=717.075:
Source Host 	Destination Host 	SessionID 	Number of Streams 	Bytes 	Packets
	Flows 	Mbps 	Pps 	Start 	End 	Duration (msecs)
Stream Index 	SrcPort 	DstPort 	Protocol 	Bytes 	Packets 	Flows 	Mbps 	Pps
	Start 	End 	Duration (msecs)
131.225.191.119 (cmsstor247) 	131.154.130.67 (ds-202-08-39.cr.cnaf.infn.it)
	1 	8 	2501156142 	1647791 	8 	717.075 	60469 	01:32:13.125 	01:32:40.375
	27250
1 	36108 	24383 	tcp 	80960565 	53339 	1 	23.211 	1957 	01:32:13.125
	01:32:40.375 	27250
2 	36109 	24383 	tcp 	324795906 	213979 	1 	93.980 	7925 	01:32:13.125
	01:32:40.125 	27000
3 	36111 	24383 	tcp 	308303851 	203112 	1 	89.208 	7522 	01:32:13.125
	01:32:40.125 	27000
4 	36112 	24383 	tcp 	275183838 	181295 	1 	79.625 	6714 	01:32:13.125
	01:32:40.125 	27000
5 	36114 	24383 	tcp 	487125657 	320923 	1 	140.951 	11886 	01:32:13.125
	01:32:40.125 	27000
6 	36115 	24383 	tcp 	314301099 	207066 	1 	90.944 	7669 	01:32:13.125
	01:32:40.125 	27000
7 	36116 	24383 	tcp 	373286245 	245926 	1 	108.011 	9108 	01:32:13.125
	01:32:40.125 	27000
8 	36117 	24383 	tcp 	337198981 	222151 	1 	97.569 	8227 	01:32:13.125
	01:32:40.125 	27000 

Let me know if you need anything else.  Thanks,
Krista

-------------------------------------------------------
Date: 2012-11-21 10:34              By: Saverio Virgilio <savirgil>
Thanks Krista.
In the precedent  post you wrote "Our network group found that inbound rates
per stream weren't very good from CNAF to FNAL"
What type of information your network staff sees? (Can you post here?) 
You have done some test? 

Thanks,  Saverio.
 

-------------------------------------------------------
Date: 2012-11-20 21:06              By: Krista Majewski <klarson1>
Sorry, I meant cmsstor275, 131.225.205.88.



-------------------------------------------------------
Date: 2012-11-20 21:03              By: Krista Majewski <klarson1>
Hi Saverio,

You can use cmsstor373(131.225.204.153) and cmswn1275(131.225.190.124) for
the gridftp clients.

I am also changing to URLCOPY to take the srm out of the picture.  Maybe that
will help.

Thanks,
Krista

-------------------------------------------------------
Date: 2012-11-13 15:07              By: Saverio Virgilio <savirgil>
Hi ,
can you give us the IP address of your gridftp client?

Thanks, Saverio.

-------------------------------------------------------
Date: 2012-11-12 17:23              By: Si Liu <siliu>
Hi Rapolas,

I also deleted the dataset  with dual subscription to T1 and T3. We will
retransfer it to T1 later.

Hope this will help us to debug this issue.

Thanks
Lucy

-------------------------------------------------------
Date: 2012-11-12 16:11              By: Saverio Virgilio <savirgil>
Hi,
our network staff is informed.
As soon as possible we will perform a network test.
Saverio 

-------------------------------------------------------
Date: 2012-11-12 15:24              By: Rapolas Kaselis <cassel>
Well, I also hoped that the problem would be related to dual subscription,
but suspending it, didn't help, transfers are still failing with the same
errors.

And also I doubt it is related to network, reducing the number of parallel
transfers should have improved situation, yet it stayed the same. To me it
looks like some transfers are scheduled but for some reason are not starting
at all.

Regards,
Rapolas K.

-------------------------------------------------------
Date: 2012-11-12 15:15              By: Burt Holzman <holzman>
Actually since Rapolas noticed the dual subscription I'd like to close this
ticket out.  Rapolas -- do you agree that the problem is solved or are you
still seeing issues?

-------------------------------------------------------
Date: 2012-11-12 08:15              By: Rapolas Kaselis <cassel>
Dear CNAF admins,

could you help friends at Fermilab to debug transfer issues?

Thanks,
Rapolas K.

-------------------------------------------------------
Date: 2012-11-09 22:00              By: Krista Majewski <klarson1>
Hi Rapolas,

Our network group found that inbound rates per stream weren't very good from
CNAF to FNAL.  Outbound looks ok and we aren't have issues with other sites. 
He is asking for an admin at CNAF that could help troubleshoot and do
measurements between nodes.  Would you be able to give me a name and contact
info that I can pass on to him? 

Thanks,
Krista

-------------------------------------------------------
Date: 2012-11-06 14:05              By: Rapolas Kaselis <cassel>
I understand that usually you don't allow, but I suspended one dataset
yesterday going to T3, because it was subscribed to both, and is not
complete.

Rapolas K.

-------------------------------------------------------
Date: 2012-11-06 14:03              By: Si Liu <siliu>
Hi Rapolas,

We usually don't allow T1 and T3 to transfer the same dataset at the same
time.

If T1 has the dataset transferring, we will reject the request  to T3.

So I think they shouldn't go the same storage.

Thanks
Lucy



-------------------------------------------------------
Date: 2012-11-06 11:51              By: Rapolas Kaselis <cassel>
Hi,

somehow I would think, that network is fine. One thing that might cause
issues, doesn't look like it is this time, if there are two subscriptions of
the same dataset, to both T1 and T3. If I understand correctly, they act
independently, while the storage is one and the same.

Actually, the symptoms are similar to those, when one transfer starts, and
then another starts while the first is still in progress, but I couldn't find
any evidence for this (well, I have no access neither to your FTS, nor
storage). So you might want to check if there are two transfers for the same
file at a similar time. Also, if there would be a network issue, I would
expect more files to fail, and more likely big ones, plus reducing the number
of transfers should make things better. Didn't happened. 

Maybe you have misbehaving dCache node if there are multiples to transfer
to?

Regards,
Rapolas K.

-------------------------------------------------------
Date: 2012-11-01 19:08              By: Krista Majewski <klarson1>
Ok.  We'll do some network tests since it's taking hours to transfer small
files.  If anything interesting shows up, we'll let you know.

Thanks,
Krista

-------------------------------------------------------
Date: 2012-10-31 10:14              By: Rapolas Kaselis <cassel>
Hi,

I've run out of ideas what could be wrong.. but just to let you know, this
problem pops up from everywhere, including T0 and other T1s. Just it is not
that visible..

Regards,
Rapolas K.

-------------------------------------------------------
Date: 2012-10-30 18:41              By: Krista Majewski <klarson1>
I'm not sure I understand why we would want to modify the phedex config?  We
are already limiting in FTS to 100 to limit the actual transfer rate. 

We've been watching and the failures occur no matter how many files are being
transferred.  Even with <10 files ongoing and small files, there were errors. 
We're continuing to investigate.

-------------------------------------------------------
Date: 2012-10-30 06:42              By: Rapolas Kaselis <cassel>
Hmm, are you sure you reduced max active files? I still see the same 200...
and I'm talking about phedex config, not FTS... 

Regards,
Rapolas K.

-------------------------------------------------------
Date: 2012-10-29 21:48              By: Krista Majewski <klarson1>
We reduced the number of files from 400 to 100 and will monitor to see if
things have improved by tomorrow.  

Thanks,
Krista

-------------------------------------------------------
Date: 2012-10-29 16:52              By: Rapolas Kaselis <cassel>
Could you check what is the rate for failing transfers from CNAF? and if they
start in the first place, because I see failures with "lifetime expired" after
4 hours, and 4 hours, I believe is your FTS setting for timeout.

And maybe reducing active files from 200 to lower number (like 100 or 150)
could help here?

Thanks,
Rapolas K.

-------------------------------------------------------
Date: 2012-10-26 16:26              By: Rapolas Kaselis <cassel>
Hmm, I don't know about deletion setting in FTS, but I don't think it is the
cure. Well, from the experience I know, that if FTS finds a file on storage,
it fails with "FILE EXISTS and overwrite option is set to never" (seems that
there is an option for overwrite file). And if it transfers the file and at
the end deletes, than something wrong went with the transfer and file should
be deleted (checksum mismatch for example).

Now for the steps you found. PhEDEx doesn't do the physical transfers. It
submits transfer job to FTS server, and FTS is responsible for physical file
transfer. PhEDEx does the preparation for file to arrive if any needed, and
then can check the file if it is fine (and other jobs, like registering file
at site), but it doesn't do any physical transfer. All the errors are coming
from FTS so far. BTW, now it is again different error, usually meaning some
sort of problem with FTS itself:

AGENT error during TRANSFER_SERVICE phase: [INTERNAL_ERROR] no transfer found
for the given ID. Details: the Stat type differs from the requested one

Also, as you might have noticed, there are very little amount of errors with
"file exists" errors, and I didn't see those in the last day I think. I would
be able to give the list, but, as I said, I don't see these errors anymore.

Regards,
Rapolas K.

-------------------------------------------------------
Date: 2012-10-26 16:02              By: Krista Majewski <klarson1>
Hi Rapolas,

Yesterday we had tried to modify the transfer configurations to  fix the
issues which caused the new errors.  We've reverted back to the original
values since it didn't work.  

We've done some further investigation and what seems to be happening is
this:

- Phedex starts the file transfer 
- FTS tries to transfer the file but it already exists.  Since there was an
error (FILE EXISTS), FTS deletes the file.
- Phedex does the post validation and sees the file was deleted and fails.

We found on the CERN twiki that when you get the FILE EXISTS error, you could
try to delete the files.  Would you be able to give us a list?  If we delete
them here hopefully they won't fall into this failing/retrying.

Also, do you have any ideas of how to disable the deletion in FTS? 

Thanks,
Krista

-------------------------------------------------------
Date: 2012-10-26 14:25              By: Rapolas Kaselis <cassel>
Hi,

transfers are still more or less at the same quality, but now there are
different errors:

TRANSFER error during TRANSFER phase: [GRIDFTP_ERROR] globus_ftp_client: the
server responded with an error 500 500-Command failed. : globus_xio: System
error in send: Connection timed out

TRANSFER error during TRANSFER phase: [GRIDFTP_ERROR] globus_ftp_client: the
server responded with an error 500 500-Command failed. : connection
prematurly closed  500-an I/O operation was cancelled

while from T0 the same old errors pop up from time to time, but the rate is
too low to cause problems, although it would be nice to know the reason.

Regards,
Rapolas K.

-------------------------------------------------------
Date: 2012-10-25 07:48              By: Rapolas Kaselis <cassel>
Still there is something wrong. PhEDEx shouldn't remove the file, while it is
being transferred. It removes it either before the transfer and allows for
transfer to happen, or removes after because it is bad. In both cases
transfer should happen, and not fail with File Exists error.

And btw, for the log you gave, I see that there were three deletions, before
the transfer:

FileDownloadDelete starting at 2012-10-21 01:16:12 UTC with 3 arguments
FileDownloadDelete starting at 2012-10-23 11:11:12 UTC with 3 arguments
FileDownloadDelete starting at 2012-10-23 15:14:58 UTC with 3 arguments

and only the last one gave that file does not exist. Are your deletions
failing for some reason? 

By the way, this "file exists" error is not that common as other errors which
are happening also from T0:

TRANSFER error during TRANSFER phase: [SRM_COPY] FAILED:  at Thu Oct 25
02:14:46 CDT 2012 state Failed : Request state changed, changing file state

TRANSFER error during TRANSFER phase: [SRM_COPY] FAILED:  at Thu Oct 25
02:14:46 CDT 2012 state Failed : lifetime expired

and as you can see, there are transfers which are not completed in 9 days
now:

https://cmsweb.cern.ch/phedex/prod/Activity::Routing?tofilter=T1_US_FNAL&showinvalid=on

including delayed custodial T0 transfers:

https://cmsweb.cern.ch/phedex/prod/Activity::Routing?tofilter=T1_US_FNAL&fromfilter=T0&showinvalid=on

Regards,
Rapolas K.

-------------------------------------------------------
Date: 2012-10-23 21:22              By: Krista Majewski <klarson1>
Hi Rapolas,

First, at FNAL the way we have things implemented is that we let all
transfers proceed, if the file is already here is it assumed to be bad and we
retransfer.  We don't really do the pre-validate so all validation is done at
the end.

We looked into this file and it looks like this is what happened:
The transfer of the 10 files was initiated and one of the files was already
here.  That transfer didn't proceed (since it was already here) for that file
but the other 9 went through.  In the meantime, phedex had removed that one
file during this time period so the post-validation step failed because only
9 of the 10 files were here.  The transfer rates for CNAF-FNAL are pretty low
right now so this whole process took a few hours.  Here are the logs:

===== Transfer started for 10 files, one of which was at FNAL
2012-10-23 06:13:06,549 [INFO ] - > Source File   :
srm://storm-fe-cms.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/cms/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00008/7E368EE6-58E0-E111-84D6-E0CB4E19F9A9.root
2012-10-23 06:13:06,549 [INFO ] - > Target File   :
srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_
CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00008/7E368EE6-58E0-E111-84D6-E0CB4E19F9A9.root2012-10-23
06:13:06,549 [INFO ] - > Status        : SRM_FAILURE
2012-10-23 06:13:06,549 [INFO ] - > Message       : FAILED:  at Tue Oct 23
06:12:38 CDT 2012 state Failed :  at Tue Oct 23 06:12:38 CDT 2012 state
Failed :  file exists 

===== Transfer ended 
2012-10-23 10:12:08,417 [INFO ] - ChecksumChecker User checksum algorithm: 
2012-10-23 10:12:08,417 [INFO ] - ChecksumChecker User provided checksum:  
2012-10-23 10:12:08,417 [INFO ] - ChecksumChecker Checksum operations on file
'srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToEE_M-20_CT10_TuneZ2star_v2_8TeV-powheg-pythia6/GEN-SIM/START52_V9-v2/00000/FE7C0BD1-B3E3-E111-8218-00221981BA9F.root'
2012-10-23 10:12:08,417 [INFO ] - ChecksumChecker Skipping checksum
verification
2012-10-23 10:12:08,417 [INFO ] - FINAL:SUCCESS

===== FileRemove logs show the file removed before the transfer
completescata
FileDownloadDelete starting at 2012-10-21 01:16:12 UTC with 3 arguments {
/home/cmsprod/phedex/COMP/SITECONF/T1_US_FNAL/PhEDEx/T1.Config.Prod pre
srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
}
(main::pfnTo)
pfn=srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
(main::pfnTo)
lfn=/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
(main::pfnTo)
localfile=/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
FileDownloadDelete starting at 2012-10-23 11:11:12 UTC with 3 arguments {
/home/cmsprod/phedex/COMP/SITECONF/T1_US_FNAL/PhEDEx/T1.Config.Prod pre
srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
}
(main::pfnTo)
pfn=srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
(main::pfnTo)
lfn=/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
(main::pfnTo)
localfile=/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
FileDownloadDelete starting at 2012-10-23 15:14:58 UTC with 3 arguments {
/home/cmsprod/phedex/COMP/SITECONF/T1_US_FNAL/PhEDEx/T1.Config.Prod post
srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
}
(main::pfnTo)
pfn=srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
(main::pfnTo)
lfn=/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
(main::pfnTo)
localfile=/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
(main)
/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
missing, do nothing

We also verified that the file is in the queue to be retransferred: 
https://cmsweb.cern.ch/phedex/prod/Activity::FileInfo?priority=3;to_node=T1_US_FNAL_Buffer;from_node=T1_IT_CNAF_Buffer;state=exported

Thanks,
Krista

-------------------------------------------------------
Date: 2012-10-23 15:32              By: Rapolas Kaselis <cassel>
Hi Krista,

in general this is the phedex workflow:

1. pre-validate file. Take any necessary actions prior transfer, like prepare
the directory, or check if file exists, and if it is healthy. If so, no new
transfer is needed, otherwise just delete the file and let a fresh copy to
transfer. Exiting from this step with anything but 0, will continue the
workflow.
2. pre-delete. If needed delete the file from storage. Usually, if you get to
this step, then either file doesn't exist on the storage, or it exists but is
bad. So it should be safe here just to delete the file, and don't care if the
deletion failed due to absence of the file, or actually deleted something.
3. transfer. The actual transfer.
4. post-validate. Check the integrity of the file, if it was transferred
successfully.

In your case, you would need to implement the 1. step, although I would be
surprised if it is not done yet. In any case, even if files were transferred,
I would suspect to see successful validations after, but it is not the case
now. For example, error message:

DESTINATION error during TRANSFER phase: [FILE_EXISTS] Failed

but file validation:


2012-10-23 15:13:54 FileDownloadVerify(32185): Executing:
/home/cmsprod/phedex/COMP/SITECONF/T1_US_FNAL/PhEDEx/FileDownloadVerify
/home/cmsprod/phedex/COMP/SITECONF/T1_US_FNAL/PhEDEx/T1.Config.Prod 1
srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
2340104520 adler32:2ed768ba,cksum:3685843400 0
2012-10-23 15:13:55 FileDownloadVerify(32185): FileDownloadVerify starting at
2012-10-23 15:13:55 UTC with 6 arguments {
/home/cmsprod/phedex/COMP/SITECONF/T1_US_FNAL/PhEDEx/T1.Config.Prod 1
srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
2340104520 adler32:2ed768ba,cksum:3685843400 0 }
2012-10-23 15:13:55 FileDownloadVerify(32185): (main) status=1
2012-10-23 15:13:55 FileDownloadVerify(32185): (main) more than one checksum
provided, using adler32:2ed768ba
2012-10-23 15:13:55 FileDownloadVerify(32185): (main::pfnTo) start
2012-10-23 15:13:55 FileDownloadVerify(32185): (main::pfnTo)
pfn=srm://cmssrm.fnal.gov:8443/srm/managerv2?SFN=/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
2012-10-23 15:13:55 FileDownloadVerify(32185): (main::pfnTo)
lfn=/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
2012-10-23 15:13:55 FileDownloadVerify(32185): (main::pfnTo)
localfile=/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
2012-10-23 15:13:55 FileDownloadVerify(32185): (main::poolDbUpdate) start
2012-10-23 15:13:55 FileDownloadVerify(32185): (main::poolDbUpdate)
/opt/poolQuota/extras/filesFromPhEDEx.pl -l
/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
-t T1_US_FNAL -s 0
2012-10-23 15:14:55 FileDownloadVerify(32185): (main::poolDbUpdate) end
2012-10-23 15:14:55 FileDownloadVerify(32185): (main::fileVerify) start
2012-10-23 15:14:55 FileDownloadVerify(32185): (main::fileVerify) file
missing
2012-10-23 15:14:55 FileDownloadVerify(32185): (main::fileVerify) return=1
2012-10-23 15:14:55 FileDownloadVerify(32185): (main)
/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
missing or is not valid
2012-10-23 15:14:55 FileDownloadVerify(32185): (main::poolDbUpdate) start
2012-10-23 15:14:55 FileDownloadVerify(32185): (main::poolDbUpdate)
/opt/poolQuota/extras/filesFromPhEDEx.pl -l
/pnfs/fs/usr/cms/WAX/11/store/mc/Summer12/DYToTauTau_M-20_CT10_TuneZ2star_v2_8TeV-powheg-tauola-pythia6/GEN-SIM/START52_V9-v2/00000/DE2CFC34-27DC-E111-BE9F-002590743042.root
-e 1
2012-10-23 15:14:56 FileDownloadVerify(32185): (main::poolDbUpdate) end
2012-10-23 15:14:56 FileDownloadVerify(32185): FileDownloadVerify ending at
2012-10-23 15:14:56 UTC with validity=1
2012-10-23 15:14:56 FileDownloadVerify(32185): Job exited with status code 1
(256) after 61.931 seconds


says that file doesn't exist...

I hope this answers your question, if not or you want more details, let us
know.

Cheers,
Rapolas K.

-------------------------------------------------------
Date: 2012-10-23 15:13              By: Krista Majewski <klarson1>
Hi Rapolas,

It looks like some files have been transferred here, maybe the local agents
didn't report some of the transfers because of our downtime yesterday?

Do you know if there is a way to tell what files were successfully
transferred here?  Any that weren't could then be retried without
re-transferring all the files.

Thanks,
Krista





    _______________________________________________________

Carbon-Copy List:

CC Address                          | Comment
------------------------------------+-----------------------------
9256                                | -COM-
2614                                | -COM-
9138                                | -COM-
9149                                | -COM-
5616                                | -SUB-




==============================================================================

This item URL is:
  <http://savannah.cern.ch/support/?133235>

_______________________________________________
  Message sent via/by LCG Savannah
  http://savannah.cern.ch/


