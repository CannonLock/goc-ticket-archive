04/15/16 13:51:04 This process has a valid certificate & key
04/15/16 13:51:04 Finishing authenticate_server_gss_post with status=1
04/15/16 13:51:04 ZKM: successful mapping to GSS_ASSIST_GRIDMAP
04/15/16 13:51:04 Command=QMGMT_WRITE_CMD, peer=<200.145.46.37:5287>
04/15/16 13:51:04 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 13:51:04 OwnerCheck retval 1 (success),no ad
04/15/16 13:51:04 Submitting new job 27.0
04/15/16 13:51:04 schedd: NewCluster rval 27 errno 0
04/15/16 13:51:04 OwnerCheck retval 1 (success),no ad
04/15/16 13:51:04 schedd: NewProc rval 0 errno 0
04/15/16 13:51:04 New job: 27.0
04/15/16 13:51:04 Writing record to user logfile=/osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3 owner=winckler
04/15/16 13:51:04 WriteUserLog::initialize: safe_open_wrapper("/osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3") failed - errno 13 (Permission denied)
04/15/16 13:51:04 WriteUserLog::initialize: failed to open file /osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3
04/15/16 13:51:04 WriteUserLog::initialize: opened /osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3 successfully
04/15/16 13:51:04 New job: 27.0, Duplicate Keys: 2, Total Keys: 13 
04/15/16 13:51:04 QMGR Connection closed
04/15/16 13:51:04 Command=SPOOL_JOB_FILES_WITH_PERMS, peer=<200.145.46.37:4318>
04/15/16 13:51:04 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 13:51:04 spoolJobFiles(): read JobAdsArrayLen - 1
04/15/16 13:51:04 Looking at spooling: mode is 488
04/15/16 13:51:04 job_status is 5
04/15/16 13:51:04 Transferring files for jobs 27.0
04/15/16 13:51:04 spoolJobFiles(): started worker process
04/15/16 13:51:04 generalJobFilesWorkerThread(): transfer files for job 27.0
04/15/16 13:51:04 The submitting job ad as the FileTransferObject sees it
NumCkpts_RAW = 0
BufferSize = 524288
NiceUser = false
CoreSize = 0
CumulativeSlotTime = 0
OnExitHold = false
RequestCpus = 1
Err = "_condor_stderr"
BufferBlockSize = 32768
ExecutableSize_RAW = 26
ImageSize = 27
WantCheckpoint = false
CommittedTime = 0
TargetType = "Machine"
WhenToTransferOutput = "ON_EXIT"
JobUniverse = 5
ExitBySignal = false
HoldReasonCode = 16
TransferIn = false
NumRestarts = 0
EncryptExecuteDirectory = false
CommittedSuspensionTime = 0
Owner = "winckler"
NumSystemHolds = 0
CumulativeSuspensionTime = 0
Environment = "GLOBUS_TCP_SOURCE_RANGE=20000,29999 _LMFILES_=/opt/modules/gridunesp/1 BYOBU_SED=sed' '--follow-symlinks LC_MEASUREMENT=pt_BR.UTF-8 BYOBU_READLINK=readlink BYOBU_CONFIG_DIR=/home/winckler/.byobu LCMAPS_DEBUG_LEVEL=3 GV_DIR=/opt/gaussian/gv SHLVL=2 LS_COLORS= PWD=/home/winckler BYOBU_TIME=%H:%M:%S SSH_AUTH_SOCK=/home/winckler/.byobu/.ssh-agent BYOBU_WINDOWS=/home/winckler/.byobu/windows SSH_CLIENT=200.145.46.228' '49266' '22 CVS_RSH=ssh PGI_TERM=trace,abort LC_TELEPHONE=pt_BR.UTF-8 STY=2420116.byobu PATH=/home/winckler/bin:/usr/local/src/goo-client/scripts:/home/winckler/bin:/opt/gridunesp/internals//bin:/usr/lib64/qt-3.3/bin:/usr/local/src/goo-client/scripts:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/home/winckler/bin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 BYOBU_RUN_DIR=/dev/shm/byobu-winckler-yXoDFpVj TOOL_DEBUG=D_FULLDEBUG MODULESHOME=/usr/share/Modules G09BASIS=/opt/gaussian/g09/basis GAUSS_EXEDIR=/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 X509_CERT_DIR=/etc/grid-security/certificates GLOBUS_TCP_PORT_RANGE=20000,29999 QTLIB=/usr/lib64/qt-3.3/lib LC_NAME=pt_BR.UTF-8 GATEKEEPER_JM_ID=2016-04-15.16:49:42.0002420989.0000000000 HISTCONTROL=ignoredups LCMAPS_DIR=/etc BYOBU_DARK=black LC_NUMERIC=pt_BR.UTF-8 GOO_API_URI=https://submit.grid.unesp.br/api/v1/ g09root=/opt/gaussian _DSM_BARRIER=SHM BYOBU_ULIMIT=ulimit SSH_TTY=/dev/pts/20 _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI MYPROXY_SERVER=myproxy.grid.unesp.br SHELL=/bin/bash GAUSS_SCRDIR=/tmp MAIL=/var/spool/mail/winckler GAUSS_ARCHDIR=/opt/gaussian/g09/arch BYOBU_HIGHLIGHT=#DD4814 LCMAPS_DB_FILE=/etc/lcmaps/lcmaps.db LC_CTYPE=pt_BR.UTF-8 LC_ADDRESS=pt_BR.UTF-8 LOADEDMODULES=gridunesp/1 BYOBU_LIGHT=white USER=winckler SSH_CONNECTION=200.145.46.228' '49266' '200.145.46.37' '22 HOSTNAME=access.grid.unesp.br GRIDUNESP_SLURM_PARTITIONS=rack1,rack2,rack3,rack4,rack5,rack6,rack7,rack8 LD_LIBRARY_PATH=/opt/gridunesp/internals//lib:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/opt/gaussian/gv/lib PYTHONPATH=/usr/local/src/python-gooclientlib:/usr/local/src/goo-client:/usr/local/src/python-gooclientlib:/usr/local/src/goo-client: BYOBU_TTY=/dev/pts/20 BYOBU_WINDOW_NAME=- BYOBU_BACKEND=screen GAUSS_LEXEDIR=/opt/gaussian/g09/linda-exe HOME=/home/winckler LOGNAME=winckler BYOBU_PYTHON=python2 LC_PAPER=pt_BR.UTF-8 MODULEPATH=/opt/modules JOB_REPOSITORY_ID=2016-04-15.16:49:42.0002420989.0000000000 _=/usr/bin/condor_ce_run LC_MONETARY=pt_BR.UTF-8 LC_TIME=pt_BR.UTF-8 G_BROKEN_FILENAMES=1 BYOBU_DISTRO=CentOS BYOBU_ACCENT=#75507B LANG=en_US.UTF-8 CONDOR_CONFIG=/etc/condor-ce/condor_config HISTSIZE=1000 BYOBU_PREFIX=/usr QTDIR=/usr/lib64/qt-3.3 BYOBU_DATE=%Y-%m-%d' ' BYOBU_CHARMAP=UTF-8 LCMAPS_POLICY_NAME= BYOBU_PAGER=less QTINC=/usr/lib64/qt-3.3/include TERM=screen-256color-bce WINDOW=0 LESSOPEN=||/usr/bin/lesspipe.sh' '%s LC_IDENTIFICATION=pt_BR.UTF-8"
RequestDisk = DiskUsage
Requirements = ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
MinHosts = 1
JobNotification = 0
NumCkpts = 0
LastSuspensionTime = 0
NumJobStarts = 0
WantRemoteSyscalls = false
JobLeaseDuration = 2400
JobPrio = 0
RootDir = "/"
CurrentHosts = 0
x509UserProxyExpiration = 1460781781
WantRemoteIO = true
StreamOut = false
OnExitRemove = true
In = "/dev/null"
DiskUsage = 27
PeriodicRemove = false
LocalUserCpu = 0.0
RemoteUserCpu = 0.0
ExecutableSize = 27
LocalSysCpu = 0.0
RemoteSysCpu = 0.0
ClusterId = 27
CompletionDate = 0
RemoteWallClockTime = 0.0
Rank = 0.0
LeaveJobInQueue = ( StageOutFinish > 0 ) =!= true
ImageSize_RAW = 26
x509UserProxyEmail = "winckler@ncc.unesp.br"
CondorVersion = "$CondorVersion: 8.4.4 Feb 04 2016 $"
MyType = "Job"
HoldReason = "Spooling input data files"
StreamErr = false
DiskUsage_RAW = 26
PeriodicHold = false
User = "winckler@users.opensciencegrid.org"
Out = "_condor_stdout"
PeriodicRelease = false
MaxHosts = 1
RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,( ImageSize + 1023 ) / 1024)
Args = ""
CommittedSlotTime = 0
TotalSuspensions = 0
x509userproxysubject = "/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.7 $"
TransferInputSizeMB = 0
ExitStatus = 0
ShouldTransferFiles = "YES"
EnteredCurrentStatus = 1460738982
QDate = 1460738982
SUBMIT_Cmd = "/bin/env"
SUBMIT_x509userproxy = "/tmp/x509up_u10001"
ProcId = 0
x509userproxy = "x509up_u10001"
Iwd = "/osg/condor/27/0/cluster27.proc0.subproc0"
SUBMIT_TransferOutputRemaps = "_condor_stdout=/home/winckler/.stdout_2420988_Df4HHD;_condor_stderr=/home/winckler/.stderr_2420988_xg41Wy"
SUBMIT_UserLog = "/home/winckler/.log_2420988_k1WbE3"
Cmd = "env"
GlobalJobId = "ce.grid.unesp.br#27.0#1460739064"
JobStatus = 5
SUBMIT_Iwd = "/home/winckler"
TransferOutputRemaps = undefined
UserLog = ".log_2420988_k1WbE3"
StageInStart = 1460739064
04/15/16 13:51:04 entering FileTransfer::SimpleInit
04/15/16 13:51:04 FILETRANSFER: protocol "http" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 13:51:04 FILETRANSFER: protocol "ftp" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 13:51:04 FILETRANSFER: protocol "file" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 13:51:04 FILETRANSFER: protocol "data" handled by "/usr/libexec/condor/data_plugin"
04/15/16 13:51:04 entering FileTransfer::DownloadFiles
04/15/16 13:51:04 entering FileTransfer::Download
04/15/16 13:51:04 entering FileTransfer::DoDownload sync=1
04/15/16 13:51:04 Sending GoAhead for 200.145.46.37 to send /osg/condor/27/0/cluster27.proc0.subproc0.tmp/x509up_u10001 and all further files.
04/15/16 13:51:04 Received GoAhead from peer to receive /osg/condor/27/0/cluster27.proc0.subproc0.tmp/x509up_u10001 and all further files.
04/15/16 13:51:04 DoDownload: get_x509_delegation() returned 0
04/15/16 13:51:04 get_file(): going to write to filename /osg/condor/27/0/cluster27.proc0.subproc0.tmp/env
04/15/16 13:51:04 get_file: Receiving 26368 bytes
04/15/16 13:51:04 get_file: wrote 26368 bytes to file
04/15/16 13:51:04 ReliSock::get_file_with_permissions(): going to set permissions 755
04/15/16 13:51:04 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0.tmp
04/15/16 13:51:04 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0.swap
04/15/16 13:51:04 Received proxy for job 27.0
04/15/16 13:51:04 proxy path: /osg/condor/27/0/cluster27.proc0.subproc0/x509up_u10001
04/15/16 13:51:04 proxy expiration: 1460781781
04/15/16 13:51:04 proxy identity: /C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler
04/15/16 13:51:04 proxy subject: /C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler/CN=2025260236/CN=1798859497
04/15/16 13:51:04 proxy email: winckler@ncc.unesp.br
04/15/16 13:51:04 Transfer completed
04/15/16 13:51:04 Scheduler::spoolJobFilesWorkerThread(void *arg, Stream* s) NAP TIME
04/15/16 13:51:04 -------- Begin starting jobs --------
04/15/16 13:51:04 -------- Done starting jobs --------
04/15/16 13:51:04 JobsRunning = 0
04/15/16 13:51:04 JobsIdle = 0
04/15/16 13:51:04 JobsHeld = 2
04/15/16 13:51:04 JobsRemoved = 0
04/15/16 13:51:04 LocalUniverseJobsRunning = 0
04/15/16 13:51:04 LocalUniverseJobsIdle = 0
04/15/16 13:51:04 SchedUniverseJobsRunning = 0
04/15/16 13:51:04 SchedUniverseJobsIdle = 0
04/15/16 13:51:04 N_Owners = 2
04/15/16 13:51:04 MaxJobsRunning = 10000
04/15/16 13:51:04 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 13:51:04 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 13:51:04 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 13:51:04 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:04 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:04 Sent HEART BEAT ad to 1 collectors. Number of submittors=2
04/15/16 13:51:04 Changed attribute: IdleJobs = 0
04/15/16 13:51:04 Changed attribute: RunningJobs = 0
04/15/16 13:51:04 Changed attribute: IdleJobs = 0
04/15/16 13:51:04 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:51:04 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:51:04 Changed attribute: HeldJobs = 0
04/15/16 13:51:04 Changed attribute: FlockedJobs = 0
04/15/16 13:51:04 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 13:51:04 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 13:51:04 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:04 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:04 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 13:51:04 Changed attribute: IdleJobs = 0
04/15/16 13:51:04 Changed attribute: RunningJobs = 0
04/15/16 13:51:04 Changed attribute: IdleJobs = 0
04/15/16 13:51:04 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:51:04 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:51:04 Changed attribute: HeldJobs = 0
04/15/16 13:51:04 Changed attribute: FlockedJobs = 0
04/15/16 13:51:04 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 13:51:04 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 13:51:04 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:04 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:04 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 13:51:04 Changed attribute: IdleJobs = 0
04/15/16 13:51:04 Changed attribute: RunningJobs = 0
04/15/16 13:51:04 Changed attribute: IdleJobs = 0
04/15/16 13:51:04 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:51:04 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:51:04 Changed attribute: HeldJobs = 2
04/15/16 13:51:04 Changed attribute: FlockedJobs = 0
04/15/16 13:51:04 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 13:51:04 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 13:51:04 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:04 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:04 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 13:51:04 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 13:51:04 Sent owner (0 jobs) ad to schedd plugins
04/15/16 13:51:04 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:04 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:04 Sent owner (0 jobs) ad to 1 collectors
04/15/16 13:51:04 ============ Begin clean_shadow_recs =============
04/15/16 13:51:04 ============ End clean_shadow_recs =============
04/15/16 13:51:04 Job 27.0 held for spooling. Checking how long...
04/15/16 13:51:04 Job 27.0 on hold for 0 seconds.
04/15/16 13:51:04 Sending RESCHEDULE command to negotiator(s)
04/15/16 13:51:04 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:04 Trying to query collector <200.145.46.35:9619>
04/15/16 13:51:04 Can't find address for negotiator 
04/15/16 13:51:04 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 13:51:04 ForkWorker::Fork: New child of 9936 = 1543063
04/15/16 13:51:04 Number of Active Workers 0
04/15/16 13:51:04 DaemonCore: No more children processes to reap.
04/15/16 13:51:05 DaemonCore: No more children processes to reap.
04/15/16 13:51:05 spoolJobFilesReaper tid=1543045 status=256
04/15/16 13:51:05 No HoldReasonSubCode found for job 27.0
04/15/16 13:51:05 Job 27.0 released from hold: Data files spooled
04/15/16 13:51:05 Setting delay until next queue scan to 5 seconds
04/15/16 13:51:05 -------- Begin starting jobs --------
04/15/16 13:51:05 -------- Done starting jobs --------
04/15/16 13:51:05 ForkWorker::Fork: New child of 9936 = 1543196
04/15/16 13:51:05 Number of Active Workers 0
04/15/16 13:51:05 DaemonCore: No more children processes to reap.
04/15/16 13:51:06 ForkWorker::Fork: New child of 9936 = 1543328
04/15/16 13:51:06 Number of Active Workers 0
04/15/16 13:51:06 DaemonCore: No more children processes to reap.
04/15/16 13:51:08 ForkWorker::Fork: New child of 9936 = 1543424
04/15/16 13:51:08 Number of Active Workers 0
04/15/16 13:51:08 DaemonCore: No more children processes to reap.
04/15/16 13:51:09 ForkWorker::Fork: New child of 9936 = 1543552
04/15/16 13:51:09 Number of Active Workers 0
04/15/16 13:51:09 DaemonCore: No more children processes to reap.
04/15/16 13:51:10 JobsRunning = 0
04/15/16 13:51:10 JobsIdle = 1
04/15/16 13:51:10 JobsHeld = 1
04/15/16 13:51:10 JobsRemoved = 0
04/15/16 13:51:10 LocalUniverseJobsRunning = 0
04/15/16 13:51:10 LocalUniverseJobsIdle = 0
04/15/16 13:51:10 SchedUniverseJobsRunning = 0
04/15/16 13:51:10 SchedUniverseJobsIdle = 0
04/15/16 13:51:10 N_Owners = 2
04/15/16 13:51:10 MaxJobsRunning = 10000
04/15/16 13:51:10 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 13:51:10 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 13:51:10 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 13:51:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:10 Sent HEART BEAT ad to 1 collectors. Number of submittors=2
04/15/16 13:51:10 Changed attribute: IdleJobs = 0
04/15/16 13:51:10 Changed attribute: RunningJobs = 0
04/15/16 13:51:10 Changed attribute: IdleJobs = 0
04/15/16 13:51:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:51:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:51:10 Changed attribute: HeldJobs = 0
04/15/16 13:51:10 Changed attribute: FlockedJobs = 0
04/15/16 13:51:10 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 13:51:10 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 13:51:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:10 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 13:51:10 Changed attribute: IdleJobs = 0
04/15/16 13:51:10 Changed attribute: RunningJobs = 0
04/15/16 13:51:10 Changed attribute: IdleJobs = 0
04/15/16 13:51:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:51:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:51:10 Changed attribute: HeldJobs = 0
04/15/16 13:51:10 Changed attribute: FlockedJobs = 0
04/15/16 13:51:10 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 13:51:10 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 13:51:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:10 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 13:51:10 Changed attribute: IdleJobs = 1
04/15/16 13:51:10 Changed attribute: RunningJobs = 0
04/15/16 13:51:10 Changed attribute: IdleJobs = 1
04/15/16 13:51:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:51:10 Changed attribute: WeightedIdleJobs = 1
04/15/16 13:51:10 Changed attribute: HeldJobs = 1
04/15/16 13:51:10 Changed attribute: FlockedJobs = 0
04/15/16 13:51:10 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 13:51:10 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 13:51:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:10 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 13:51:10 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 13:51:10 Sent owner (0 jobs) ad to schedd plugins
04/15/16 13:51:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:51:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:10 Sent owner (0 jobs) ad to 1 collectors
04/15/16 13:51:10 ============ Begin clean_shadow_recs =============
04/15/16 13:51:10 ============ End clean_shadow_recs =============
04/15/16 13:51:10 Sending RESCHEDULE command to negotiator(s)
04/15/16 13:51:10 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:51:10 Trying to query collector <200.145.46.35:9619>
04/15/16 13:51:10 Can't find address for negotiator 
04/15/16 13:51:10 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 13:51:10 ForkWorker::Fork: New child of 9936 = 1543677
04/15/16 13:51:10 Number of Active Workers 0
04/15/16 13:51:10 DaemonCore: No more children processes to reap.
04/15/16 13:51:11 ForkWorker::Fork: New child of 9936 = 1543728
04/15/16 13:51:11 Number of Active Workers 0
04/15/16 13:51:11 DaemonCore: No more children processes to reap.
04/15/16 13:51:12 ForkWorker::Fork: New child of 9936 = 1543773
04/15/16 13:51:12 Number of Active Workers 0
04/15/16 13:51:12 DaemonCore: No more children processes to reap.
04/15/16 13:51:12 QMGR Connection closed
04/15/16 13:51:12 OwnerCheck retval 1 (success),no ad
04/15/16 13:51:12 Submitting new job 28.0
04/15/16 13:51:12 schedd: NewCluster rval 28 errno 0
04/15/16 13:51:12 OwnerCheck retval 1 (success),no ad
04/15/16 13:51:12 schedd: NewProc rval 0 errno 0
04/15/16 13:51:12 New job: 28.0
04/15/16 13:51:12 New job: 28.0, Duplicate Keys: 2, Total Keys: 102 
04/15/16 13:51:12 QMGR Connection closed
04/15/16 13:51:13 ForkWorker::Fork: New child of 9936 = 1543825
04/15/16 13:51:13 Number of Active Workers 0
04/15/16 13:51:13 DaemonCore: No more children processes to reap.
04/15/16 13:51:14 ForkWorker::Fork: New child of 9936 = 1543835
04/15/16 13:51:14 Number of Active Workers 0
04/15/16 13:51:14 DaemonCore: No more children processes to reap.
04/15/16 13:51:15 ForkWorker::Fork: New child of 9936 = 1543874
04/15/16 13:51:15 Number of Active Workers 0
04/15/16 13:51:15 DaemonCore: No more children processes to reap.
04/15/16 13:51:16 ForkWorker::Fork: New child of 9936 = 1543884
04/15/16 13:51:16 Number of Active Workers 0
04/15/16 13:51:16 DaemonCore: No more children processes to reap.
04/15/16 13:51:17 ForkWorker::Fork: New child of 9936 = 1543885
04/15/16 13:51:17 Number of Active Workers 0
04/15/16 13:51:17 DaemonCore: No more children processes to reap.
04/15/16 13:51:18 ForkWorker::Fork: New child of 9936 = 1543886
04/15/16 13:51:18 Number of Active Workers 0
04/15/16 13:51:18 DaemonCore: No more children processes to reap.
04/15/16 13:51:19 ForkWorker::Fork: New child of 9936 = 1543890
04/15/16 13:51:19 Number of Active Workers 0
04/15/16 13:51:19 DaemonCore: No more children processes to reap.
04/15/16 13:51:20 ForkWorker::Fork: New child of 9936 = 1543895
04/15/16 13:51:20 Number of Active Workers 0
04/15/16 13:51:20 DaemonCore: No more children processes to reap.
04/15/16 13:51:21 ForkWorker::Fork: New child of 9936 = 1543926
04/15/16 13:51:21 Number of Active Workers 0
04/15/16 13:51:21 DaemonCore: No more children processes to reap.
04/15/16 13:51:22 QMGR Connection closed
04/15/16 13:51:22 ForkWorker::Fork: New child of 9936 = 1543936
04/15/16 13:51:22 Number of Active Workers 0
04/15/16 13:51:22 DaemonCore: No more children processes to reap.
04/15/16 13:51:23 ForkWorker::Fork: New child of 9936 = 1544025
04/15/16 13:51:23 Number of Active Workers 0
04/15/16 13:51:23 DaemonCore: No more children processes to reap.
04/15/16 13:51:24 ForkWorker::Fork: New child of 9936 = 1544120
04/15/16 13:51:24 Number of Active Workers 0
04/15/16 13:51:25 DaemonCore: No more children processes to reap.
04/15/16 13:51:26 ForkWorker::Fork: New child of 9936 = 1544240
04/15/16 13:51:26 Number of Active Workers 0
04/15/16 13:51:26 DaemonCore: No more children processes to reap.
04/15/16 13:51:27 ForkWorker::Fork: New child of 9936 = 1544281
04/15/16 13:51:27 Number of Active Workers 0
04/15/16 13:51:27 DaemonCore: No more children processes to reap.
04/15/16 13:51:28 ForkWorker::Fork: New child of 9936 = 1544284
04/15/16 13:51:28 Number of Active Workers 0
04/15/16 13:51:28 DaemonCore: No more children processes to reap.
04/15/16 13:51:29 ForkWorker::Fork: New child of 9936 = 1544298
04/15/16 13:51:29 Number of Active Workers 0
04/15/16 13:51:29 DaemonCore: No more children processes to reap.
04/15/16 13:51:30 ForkWorker::Fork: New child of 9936 = 1544300
04/15/16 13:51:30 Number of Active Workers 0
04/15/16 13:51:30 DaemonCore: No more children processes to reap.
04/15/16 13:51:31 ForkWorker::Fork: New child of 9936 = 1544384
04/15/16 13:51:31 Number of Active Workers 0
04/15/16 13:51:31 DaemonCore: No more children processes to reap.
04/15/16 13:51:32 ForkWorker::Fork: New child of 9936 = 1544437
04/15/16 13:51:32 Number of Active Workers 0
04/15/16 13:51:32 DaemonCore: No more children processes to reap.
04/15/16 13:51:33 ForkWorker::Fork: New child of 9936 = 1544444
04/15/16 13:51:33 Number of Active Workers 0
04/15/16 13:51:33 DaemonCore: No more children processes to reap.
04/15/16 13:51:34 ForkWorker::Fork: New child of 9936 = 1544494
04/15/16 13:51:34 Number of Active Workers 0
04/15/16 13:51:34 DaemonCore: No more children processes to reap.
04/15/16 13:51:35 ForkWorker::Fork: New child of 9936 = 1544589
04/15/16 13:51:35 Number of Active Workers 0
04/15/16 13:51:35 DaemonCore: No more children processes to reap.
04/15/16 13:51:36 ForkWorker::Fork: New child of 9936 = 1544688
04/15/16 13:51:36 Number of Active Workers 0
04/15/16 13:51:36 DaemonCore: No more children processes to reap.
04/15/16 13:51:37 ForkWorker::Fork: New child of 9936 = 1544782
04/15/16 13:51:37 Number of Active Workers 0
04/15/16 13:51:37 DaemonCore: No more children processes to reap.
04/15/16 13:51:38 ForkWorker::Fork: New child of 9936 = 1544825
04/15/16 13:51:38 Number of Active Workers 0
04/15/16 13:51:38 DaemonCore: No more children processes to reap.
04/15/16 13:51:39 ForkWorker::Fork: New child of 9936 = 1544875
04/15/16 13:51:39 Number of Active Workers 0
04/15/16 13:51:39 DaemonCore: No more children processes to reap.
04/15/16 13:51:40 ForkWorker::Fork: New child of 9936 = 1544909
04/15/16 13:51:40 Number of Active Workers 0
04/15/16 13:51:40 DaemonCore: No more children processes to reap.
04/15/16 13:51:42 ForkWorker::Fork: New child of 9936 = 1545002
04/15/16 13:51:42 Number of Active Workers 0
04/15/16 13:51:42 DaemonCore: No more children processes to reap.
04/15/16 13:51:43 ForkWorker::Fork: New child of 9936 = 1545051
04/15/16 13:51:43 Number of Active Workers 0
04/15/16 13:51:43 DaemonCore: No more children processes to reap.
04/15/16 13:51:44 ForkWorker::Fork: New child of 9936 = 1545101
04/15/16 13:51:44 Number of Active Workers 0
04/15/16 13:51:44 DaemonCore: No more children processes to reap.
04/15/16 13:51:45 ForkWorker::Fork: New child of 9936 = 1545182
04/15/16 13:51:45 Number of Active Workers 0
04/15/16 13:51:45 DaemonCore: No more children processes to reap.
04/15/16 13:51:46 ForkWorker::Fork: New child of 9936 = 1545273
04/15/16 13:51:46 Number of Active Workers 0
04/15/16 13:51:46 DaemonCore: No more children processes to reap.
04/15/16 13:51:47 ForkWorker::Fork: New child of 9936 = 1545371
04/15/16 13:51:47 Number of Active Workers 0
04/15/16 13:51:47 DaemonCore: No more children processes to reap.
04/15/16 13:51:48 ForkWorker::Fork: New child of 9936 = 1545469
04/15/16 13:51:48 Number of Active Workers 0
04/15/16 13:51:48 DaemonCore: No more children processes to reap.
04/15/16 13:51:49 ForkWorker::Fork: New child of 9936 = 1545584
04/15/16 13:51:49 Number of Active Workers 0
04/15/16 13:51:49 DaemonCore: No more children processes to reap.
04/15/16 13:51:50 ForkWorker::Fork: New child of 9936 = 1545641
04/15/16 13:51:50 Number of Active Workers 0
04/15/16 13:51:50 DaemonCore: No more children processes to reap.
04/15/16 13:51:51 ForkWorker::Fork: New child of 9936 = 1545736
04/15/16 13:51:51 Number of Active Workers 0
04/15/16 13:51:51 DaemonCore: No more children processes to reap.
04/15/16 13:51:52 -------- Begin starting jobs --------
04/15/16 13:51:52 -------- Done starting jobs --------
04/15/16 13:51:52 ForkWorker::Fork: New child of 9936 = 1545828
04/15/16 13:51:52 Number of Active Workers 0
04/15/16 13:51:52 DaemonCore: No more children processes to reap.
04/15/16 13:51:53 ForkWorker::Fork: New child of 9936 = 1545873
04/15/16 13:51:53 Number of Active Workers 0
04/15/16 13:51:53 DaemonCore: No more children processes to reap.
04/15/16 13:51:54 ForkWorker::Fork: New child of 9936 = 1545887
04/15/16 13:51:54 Number of Active Workers 0
04/15/16 13:51:54 DaemonCore: No more children processes to reap.
04/15/16 13:51:55 ForkWorker::Fork: New child of 9936 = 1545948
04/15/16 13:51:55 Number of Active Workers 0
04/15/16 13:51:55 DaemonCore: No more children processes to reap.
04/15/16 13:51:56 ForkWorker::Fork: New child of 9936 = 1545957
04/15/16 13:51:56 Number of Active Workers 0
04/15/16 13:51:56 DaemonCore: No more children processes to reap.
04/15/16 13:51:57 ForkWorker::Fork: New child of 9936 = 1545984
04/15/16 13:51:57 Number of Active Workers 0
04/15/16 13:51:57 DaemonCore: No more children processes to reap.
04/15/16 13:51:58 ForkWorker::Fork: New child of 9936 = 1546039
04/15/16 13:51:58 Number of Active Workers 0
04/15/16 13:51:58 DaemonCore: No more children processes to reap.
04/15/16 13:52:00 ForkWorker::Fork: New child of 9936 = 1546040
04/15/16 13:52:00 Number of Active Workers 0
04/15/16 13:52:00 DaemonCore: No more children processes to reap.
04/15/16 13:52:01 ForkWorker::Fork: New child of 9936 = 1546087
04/15/16 13:52:01 Number of Active Workers 0
04/15/16 13:52:01 DaemonCore: No more children processes to reap.
04/15/16 13:52:02 ForkWorker::Fork: New child of 9936 = 1546138
04/15/16 13:52:02 Number of Active Workers 0
04/15/16 13:52:02 DaemonCore: No more children processes to reap.
04/15/16 13:52:03 ForkWorker::Fork: New child of 9936 = 1546239
04/15/16 13:52:03 Number of Active Workers 0
04/15/16 13:52:03 DaemonCore: No more children processes to reap.
04/15/16 13:52:04 ForkWorker::Fork: New child of 9936 = 1546470
04/15/16 13:52:04 Number of Active Workers 0
04/15/16 13:52:04 DaemonCore: No more children processes to reap.
04/15/16 13:52:05 ForkWorker::Fork: New child of 9936 = 1546544
04/15/16 13:52:05 Number of Active Workers 0
04/15/16 13:52:05 DaemonCore: No more children processes to reap.
04/15/16 13:52:06 ForkWorker::Fork: New child of 9936 = 1546679
04/15/16 13:52:06 Number of Active Workers 0
04/15/16 13:52:06 DaemonCore: No more children processes to reap.
04/15/16 13:52:07 ForkWorker::Fork: New child of 9936 = 1546799
04/15/16 13:52:07 Number of Active Workers 0
04/15/16 13:52:07 DaemonCore: No more children processes to reap.
04/15/16 13:52:08 ForkWorker::Fork: New child of 9936 = 1546951
04/15/16 13:52:08 Number of Active Workers 0
04/15/16 13:52:08 DaemonCore: No more children processes to reap.
04/15/16 13:52:09 ForkWorker::Fork: New child of 9936 = 1547043
04/15/16 13:52:09 Number of Active Workers 0
04/15/16 13:52:09 DaemonCore: No more children processes to reap.
04/15/16 13:52:10 ForkWorker::Fork: New child of 9936 = 1547148
04/15/16 13:52:10 Number of Active Workers 0
04/15/16 13:52:10 DaemonCore: No more children processes to reap.
04/15/16 13:52:11 ForkWorker::Fork: New child of 9936 = 1547246
04/15/16 13:52:11 Number of Active Workers 0
04/15/16 13:52:11 DaemonCore: No more children processes to reap.
04/15/16 13:52:12 ForkWorker::Fork: New child of 9936 = 1547339
04/15/16 13:52:12 Number of Active Workers 0
04/15/16 13:52:12 DaemonCore: No more children processes to reap.
04/15/16 13:52:13 ForkWorker::Fork: New child of 9936 = 1547459
04/15/16 13:52:13 Number of Active Workers 0
04/15/16 13:52:13 DaemonCore: No more children processes to reap.
04/15/16 13:52:14 ForkWorker::Fork: New child of 9936 = 1547593
04/15/16 13:52:14 Number of Active Workers 0
04/15/16 13:52:14 DaemonCore: No more children processes to reap.
04/15/16 13:52:15 ForkWorker::Fork: New child of 9936 = 1547696
04/15/16 13:52:15 Number of Active Workers 0
04/15/16 13:52:15 DaemonCore: No more children processes to reap.
04/15/16 13:52:16 ForkWorker::Fork: New child of 9936 = 1547831
04/15/16 13:52:16 Number of Active Workers 0
04/15/16 13:52:17 DaemonCore: No more children processes to reap.
04/15/16 13:52:18 ForkWorker::Fork: New child of 9936 = 1547954
04/15/16 13:52:18 Number of Active Workers 0
04/15/16 13:52:18 DaemonCore: No more children processes to reap.
04/15/16 13:52:19 ForkWorker::Fork: New child of 9936 = 1548097
04/15/16 13:52:19 Number of Active Workers 0
04/15/16 13:52:19 DaemonCore: No more children processes to reap.
04/15/16 13:52:20 ForkWorker::Fork: New child of 9936 = 1548213
04/15/16 13:52:20 Number of Active Workers 0
04/15/16 13:52:20 DaemonCore: No more children processes to reap.
04/15/16 13:52:21 ForkWorker::Fork: New child of 9936 = 1548306
04/15/16 13:52:21 Number of Active Workers 0
04/15/16 13:52:21 DaemonCore: No more children processes to reap.
04/15/16 13:52:22 ForkWorker::Fork: New child of 9936 = 1548408
04/15/16 13:52:22 Number of Active Workers 0
04/15/16 13:52:22 DaemonCore: No more children processes to reap.
04/15/16 13:52:23 ForkWorker::Fork: New child of 9936 = 1548494
04/15/16 13:52:23 Number of Active Workers 0
04/15/16 13:52:23 DaemonCore: No more children processes to reap.
04/15/16 13:52:24 ForkWorker::Fork: New child of 9936 = 1548623
04/15/16 13:52:24 Number of Active Workers 0
04/15/16 13:52:24 DaemonCore: No more children processes to reap.
04/15/16 13:52:25 ForkWorker::Fork: New child of 9936 = 1548754
04/15/16 13:52:25 Number of Active Workers 0
04/15/16 13:52:25 DaemonCore: No more children processes to reap.
04/15/16 13:52:26 ForkWorker::Fork: New child of 9936 = 1548857
04/15/16 13:52:26 Number of Active Workers 0
04/15/16 13:52:26 DaemonCore: No more children processes to reap.
04/15/16 13:52:27 ForkWorker::Fork: New child of 9936 = 1548941
04/15/16 13:52:27 Number of Active Workers 0
04/15/16 13:52:27 DaemonCore: No more children processes to reap.
04/15/16 13:52:28 ForkWorker::Fork: New child of 9936 = 1548987
04/15/16 13:52:28 Number of Active Workers 0
04/15/16 13:52:28 DaemonCore: No more children processes to reap.
04/15/16 13:52:29 ForkWorker::Fork: New child of 9936 = 1549037
04/15/16 13:52:29 Number of Active Workers 0
04/15/16 13:52:29 DaemonCore: No more children processes to reap.
04/15/16 13:52:30 ForkWorker::Fork: New child of 9936 = 1549084
04/15/16 13:52:30 Number of Active Workers 0
04/15/16 13:52:30 DaemonCore: No more children processes to reap.
04/15/16 13:52:31 ForkWorker::Fork: New child of 9936 = 1549128
04/15/16 13:52:31 Number of Active Workers 0
04/15/16 13:52:31 DaemonCore: No more children processes to reap.
04/15/16 13:52:32 ForkWorker::Fork: New child of 9936 = 1549175
04/15/16 13:52:32 Number of Active Workers 0
04/15/16 13:52:32 DaemonCore: No more children processes to reap.
04/15/16 13:52:33 ForkWorker::Fork: New child of 9936 = 1549222
04/15/16 13:52:33 Number of Active Workers 0
04/15/16 13:52:33 DaemonCore: No more children processes to reap.
04/15/16 13:52:35 ForkWorker::Fork: New child of 9936 = 1549274
04/15/16 13:52:35 Number of Active Workers 0
04/15/16 13:52:35 DaemonCore: No more children processes to reap.
04/15/16 13:52:36 ForkWorker::Fork: New child of 9936 = 1549361
04/15/16 13:52:36 Number of Active Workers 0
04/15/16 13:52:36 DaemonCore: No more children processes to reap.
04/15/16 13:52:37 ForkWorker::Fork: New child of 9936 = 1549412
04/15/16 13:52:37 Number of Active Workers 0
04/15/16 13:52:37 DaemonCore: No more children processes to reap.
04/15/16 13:52:38 ForkWorker::Fork: New child of 9936 = 1549474
04/15/16 13:52:38 Number of Active Workers 0
04/15/16 13:52:38 DaemonCore: No more children processes to reap.
04/15/16 13:52:39 ForkWorker::Fork: New child of 9936 = 1549544
04/15/16 13:52:39 Number of Active Workers 0
04/15/16 13:52:39 DaemonCore: No more children processes to reap.
04/15/16 13:52:40 ForkWorker::Fork: New child of 9936 = 1549587
04/15/16 13:52:40 Number of Active Workers 0
04/15/16 13:52:40 DaemonCore: No more children processes to reap.
04/15/16 13:52:41 Getting monitoring info for pid 9936
04/15/16 13:52:41 ForkWorker::Fork: New child of 9936 = 1549684
04/15/16 13:52:41 Number of Active Workers 0
04/15/16 13:52:41 DaemonCore: No more children processes to reap.
04/15/16 13:52:42 ForkWorker::Fork: New child of 9936 = 1549728
04/15/16 13:52:42 Number of Active Workers 0
04/15/16 13:52:42 DaemonCore: No more children processes to reap.
04/15/16 13:52:43 ForkWorker::Fork: New child of 9936 = 1549781
04/15/16 13:52:43 Number of Active Workers 0
04/15/16 13:52:43 DaemonCore: No more children processes to reap.
04/15/16 13:52:44 ForkWorker::Fork: New child of 9936 = 1549858
04/15/16 13:52:44 Number of Active Workers 0
04/15/16 13:52:44 DaemonCore: No more children processes to reap.
04/15/16 13:52:45 ForkWorker::Fork: New child of 9936 = 1549913
04/15/16 13:52:45 Number of Active Workers 0
04/15/16 13:52:45 DaemonCore: No more children processes to reap.
04/15/16 13:52:46 ForkWorker::Fork: New child of 9936 = 1549991
04/15/16 13:52:46 Number of Active Workers 0
04/15/16 13:52:46 DaemonCore: No more children processes to reap.
04/15/16 13:52:47 ForkWorker::Fork: New child of 9936 = 1550034
04/15/16 13:52:47 Number of Active Workers 0
04/15/16 13:52:47 DaemonCore: No more children processes to reap.
04/15/16 13:52:48 ForkWorker::Fork: New child of 9936 = 1550146
04/15/16 13:52:48 Number of Active Workers 0
04/15/16 13:52:48 DaemonCore: No more children processes to reap.
04/15/16 13:52:49 ForkWorker::Fork: New child of 9936 = 1550189
04/15/16 13:52:49 Number of Active Workers 0
04/15/16 13:52:49 DaemonCore: No more children processes to reap.
04/15/16 13:52:50 ForkWorker::Fork: New child of 9936 = 1550244
04/15/16 13:52:50 Number of Active Workers 0
04/15/16 13:52:50 DaemonCore: No more children processes to reap.
04/15/16 13:52:52 Evaluated periodic expressions in 0.000s, scheduling next run in 300s
04/15/16 13:52:52 ForkWorker::Fork: New child of 9936 = 1550317
04/15/16 13:52:52 Number of Active Workers 0
04/15/16 13:52:52 DaemonCore: No more children processes to reap.
04/15/16 13:52:53 ForkWorker::Fork: New child of 9936 = 1550398
04/15/16 13:52:53 Number of Active Workers 0
04/15/16 13:52:53 DaemonCore: No more children processes to reap.
04/15/16 13:52:54 ForkWorker::Fork: New child of 9936 = 1550491
04/15/16 13:52:54 Number of Active Workers 0
04/15/16 13:52:54 DaemonCore: No more children processes to reap.
04/15/16 13:52:55 ForkWorker::Fork: New child of 9936 = 1550538
04/15/16 13:52:55 Number of Active Workers 0
04/15/16 13:52:55 DaemonCore: No more children processes to reap.
04/15/16 13:52:56 ForkWorker::Fork: New child of 9936 = 1550769
04/15/16 13:52:56 Number of Active Workers 0
04/15/16 13:52:56 DaemonCore: No more children processes to reap.
04/15/16 13:52:57 ForkWorker::Fork: New child of 9936 = 1550903
04/15/16 13:52:57 Number of Active Workers 0
04/15/16 13:52:57 DaemonCore: No more children processes to reap.
04/15/16 13:52:58 ForkWorker::Fork: New child of 9936 = 1550956
04/15/16 13:52:58 Number of Active Workers 0
04/15/16 13:52:58 DaemonCore: No more children processes to reap.
04/15/16 13:52:59 ForkWorker::Fork: New child of 9936 = 1550973
04/15/16 13:52:59 Number of Active Workers 0
04/15/16 13:52:59 DaemonCore: No more children processes to reap.
04/15/16 13:53:00 ForkWorker::Fork: New child of 9936 = 1551016
04/15/16 13:53:00 Number of Active Workers 0
04/15/16 13:53:00 DaemonCore: No more children processes to reap.
04/15/16 13:53:01 ForkWorker::Fork: New child of 9936 = 1551236
04/15/16 13:53:01 Number of Active Workers 0
04/15/16 13:53:01 DaemonCore: No more children processes to reap.
04/15/16 13:53:02 ForkWorker::Fork: New child of 9936 = 1551239
04/15/16 13:53:02 Number of Active Workers 0
04/15/16 13:53:02 DaemonCore: No more children processes to reap.
04/15/16 13:53:03 ForkWorker::Fork: New child of 9936 = 1551250
04/15/16 13:53:03 Number of Active Workers 0
04/15/16 13:53:03 DaemonCore: No more children processes to reap.
04/15/16 13:53:04 ForkWorker::Fork: New child of 9936 = 1551281
04/15/16 13:53:04 Number of Active Workers 0
04/15/16 13:53:04 DaemonCore: No more children processes to reap.
04/15/16 13:53:05 ForkWorker::Fork: New child of 9936 = 1551310
04/15/16 13:53:05 Number of Active Workers 0
04/15/16 13:53:05 DaemonCore: No more children processes to reap.
04/15/16 13:53:06 ForkWorker::Fork: New child of 9936 = 1551395
04/15/16 13:53:06 Number of Active Workers 0
04/15/16 13:53:06 DaemonCore: No more children processes to reap.
04/15/16 13:53:07 ForkWorker::Fork: New child of 9936 = 1551406
04/15/16 13:53:07 Number of Active Workers 0
04/15/16 13:53:08 DaemonCore: No more children processes to reap.
04/15/16 13:53:09 ForkWorker::Fork: New child of 9936 = 1551444
04/15/16 13:53:09 Number of Active Workers 0
04/15/16 13:53:09 DaemonCore: No more children processes to reap.
04/15/16 13:53:10 ForkWorker::Fork: New child of 9936 = 1551537
04/15/16 13:53:10 Number of Active Workers 0
04/15/16 13:53:10 DaemonCore: No more children processes to reap.
04/15/16 13:53:11 ForkWorker::Fork: New child of 9936 = 1551627
04/15/16 13:53:11 Number of Active Workers 0
04/15/16 13:53:11 DaemonCore: No more children processes to reap.
04/15/16 13:53:12 ForkWorker::Fork: New child of 9936 = 1551653
04/15/16 13:53:12 Number of Active Workers 0
04/15/16 13:53:12 DaemonCore: No more children processes to reap.
04/15/16 13:53:13 ForkWorker::Fork: New child of 9936 = 1551748
04/15/16 13:53:13 Number of Active Workers 0
04/15/16 13:53:13 DaemonCore: No more children processes to reap.
04/15/16 13:53:14 ForkWorker::Fork: New child of 9936 = 1551820
04/15/16 13:53:14 Number of Active Workers 0
04/15/16 13:53:14 DaemonCore: No more children processes to reap.
04/15/16 13:53:15 ForkWorker::Fork: New child of 9936 = 1551906
04/15/16 13:53:15 Number of Active Workers 0
04/15/16 13:53:15 DaemonCore: No more children processes to reap.
04/15/16 13:53:16 ForkWorker::Fork: New child of 9936 = 1552009
04/15/16 13:53:16 Number of Active Workers 0
04/15/16 13:53:16 DaemonCore: No more children processes to reap.
04/15/16 13:53:17 ForkWorker::Fork: New child of 9936 = 1552015
04/15/16 13:53:17 Number of Active Workers 0
04/15/16 13:53:17 DaemonCore: No more children processes to reap.
04/15/16 13:53:18 ForkWorker::Fork: New child of 9936 = 1552055
04/15/16 13:53:18 Number of Active Workers 0
04/15/16 13:53:18 DaemonCore: No more children processes to reap.
04/15/16 13:53:19 ForkWorker::Fork: New child of 9936 = 1552144
04/15/16 13:53:19 Number of Active Workers 0
04/15/16 13:53:19 DaemonCore: No more children processes to reap.
04/15/16 13:53:20 ForkWorker::Fork: New child of 9936 = 1552242
04/15/16 13:53:20 Number of Active Workers 0
04/15/16 13:53:20 DaemonCore: No more children processes to reap.
04/15/16 13:53:21 ForkWorker::Fork: New child of 9936 = 1552334
04/15/16 13:53:21 Number of Active Workers 0
04/15/16 13:53:21 DaemonCore: No more children processes to reap.
04/15/16 13:53:22 ForkWorker::Fork: New child of 9936 = 1552402
04/15/16 13:53:22 Number of Active Workers 0
04/15/16 13:53:22 DaemonCore: No more children processes to reap.
04/15/16 13:53:23 ForkWorker::Fork: New child of 9936 = 1552472
04/15/16 13:53:23 Number of Active Workers 0
04/15/16 13:53:23 DaemonCore: No more children processes to reap.
04/15/16 13:53:24 ForkWorker::Fork: New child of 9936 = 1552534
04/15/16 13:53:24 Number of Active Workers 0
04/15/16 13:53:24 DaemonCore: No more children processes to reap.
04/15/16 13:53:26 ForkWorker::Fork: New child of 9936 = 1552584
04/15/16 13:53:26 Number of Active Workers 0
04/15/16 13:53:26 DaemonCore: No more children processes to reap.
04/15/16 13:53:27 ForkWorker::Fork: New child of 9936 = 1552660
04/15/16 13:53:27 Number of Active Workers 0
04/15/16 13:53:27 DaemonCore: No more children processes to reap.
04/15/16 13:53:28 ForkWorker::Fork: New child of 9936 = 1552714
04/15/16 13:53:28 Number of Active Workers 0
04/15/16 13:53:28 DaemonCore: No more children processes to reap.
04/15/16 13:53:29 ForkWorker::Fork: New child of 9936 = 1552785
04/15/16 13:53:29 Number of Active Workers 0
04/15/16 13:53:29 DaemonCore: No more children processes to reap.
04/15/16 13:53:30 ForkWorker::Fork: New child of 9936 = 1552885
04/15/16 13:53:30 Number of Active Workers 0
04/15/16 13:53:30 DaemonCore: No more children processes to reap.
04/15/16 13:53:31 ForkWorker::Fork: New child of 9936 = 1552977
04/15/16 13:53:31 Number of Active Workers 0
04/15/16 13:53:31 DaemonCore: No more children processes to reap.
04/15/16 13:53:32 ForkWorker::Fork: New child of 9936 = 1553071
04/15/16 13:53:32 Number of Active Workers 0
04/15/16 13:53:32 DaemonCore: No more children processes to reap.
04/15/16 13:53:33 ForkWorker::Fork: New child of 9936 = 1553131
04/15/16 13:53:33 Number of Active Workers 0
04/15/16 13:53:33 DaemonCore: No more children processes to reap.
04/15/16 13:53:34 ForkWorker::Fork: New child of 9936 = 1553216
04/15/16 13:53:34 Number of Active Workers 0
04/15/16 13:53:34 DaemonCore: No more children processes to reap.
04/15/16 13:53:35 ForkWorker::Fork: New child of 9936 = 1553270
04/15/16 13:53:35 Number of Active Workers 0
04/15/16 13:53:35 DaemonCore: No more children processes to reap.
04/15/16 13:53:36 ForkWorker::Fork: New child of 9936 = 1553339
04/15/16 13:53:36 Number of Active Workers 0
04/15/16 13:53:36 DaemonCore: No more children processes to reap.
04/15/16 13:53:37 ForkWorker::Fork: New child of 9936 = 1553356
04/15/16 13:53:37 Number of Active Workers 0
04/15/16 13:53:37 DaemonCore: No more children processes to reap.
04/15/16 13:53:38 ForkWorker::Fork: New child of 9936 = 1553392
04/15/16 13:53:38 Number of Active Workers 0
04/15/16 13:53:38 DaemonCore: No more children processes to reap.
04/15/16 13:53:39 ForkWorker::Fork: New child of 9936 = 1553412
04/15/16 13:53:39 Number of Active Workers 0
04/15/16 13:53:39 DaemonCore: No more children processes to reap.
04/15/16 13:53:40 Received a superuser command
04/15/16 13:53:40 ForkWorker::Fork: New child of 9936 = 1553445
04/15/16 13:53:40 Number of Active Workers 0
04/15/16 13:53:40 DaemonCore: No more children processes to reap.
04/15/16 13:53:40 ForkWorker::Fork: New child of 9936 = 1553449
04/15/16 13:53:40 Number of Active Workers 0
04/15/16 13:53:40 DaemonCore: No more children processes to reap.
04/15/16 13:53:41 ForkWorker::Fork: New child of 9936 = 1553465
04/15/16 13:53:41 Number of Active Workers 0
04/15/16 13:53:41 DaemonCore: No more children processes to reap.
04/15/16 13:53:43 ForkWorker::Fork: New child of 9936 = 1553466
04/15/16 13:53:43 Number of Active Workers 0
04/15/16 13:53:43 DaemonCore: No more children processes to reap.
04/15/16 13:53:44 ForkWorker::Fork: New child of 9936 = 1553515
04/15/16 13:53:44 Number of Active Workers 0
04/15/16 13:53:44 DaemonCore: No more children processes to reap.
04/15/16 13:53:45 ForkWorker::Fork: New child of 9936 = 1553532
04/15/16 13:53:45 Number of Active Workers 0
04/15/16 13:53:45 DaemonCore: No more children processes to reap.
04/15/16 13:53:46 ForkWorker::Fork: New child of 9936 = 1553581
04/15/16 13:53:46 Number of Active Workers 0
04/15/16 13:53:46 DaemonCore: No more children processes to reap.
04/15/16 13:53:47 ForkWorker::Fork: New child of 9936 = 1553588
04/15/16 13:53:47 Number of Active Workers 0
04/15/16 13:53:47 DaemonCore: No more children processes to reap.
04/15/16 13:53:48 ForkWorker::Fork: New child of 9936 = 1553609
04/15/16 13:53:48 Number of Active Workers 0
04/15/16 13:53:48 DaemonCore: No more children processes to reap.
04/15/16 13:53:49 ForkWorker::Fork: New child of 9936 = 1553642
04/15/16 13:53:49 Number of Active Workers 0
04/15/16 13:53:49 DaemonCore: No more children processes to reap.
04/15/16 13:53:50 Received a superuser command
04/15/16 13:53:50 ForkWorker::Fork: New child of 9936 = 1553645
04/15/16 13:53:50 Number of Active Workers 0
04/15/16 13:53:50 DaemonCore: No more children processes to reap.
04/15/16 13:53:50 ForkWorker::Fork: New child of 9936 = 1553646
04/15/16 13:53:50 Number of Active Workers 0
04/15/16 13:53:50 DaemonCore: No more children processes to reap.
04/15/16 13:53:51 Received a superuser command
04/15/16 13:53:51 ForkWorker::Fork: New child of 9936 = 1553695
04/15/16 13:53:51 Number of Active Workers 0
04/15/16 13:53:51 DaemonCore: No more children processes to reap.
04/15/16 13:53:51 ForkWorker::Fork: New child of 9936 = 1553702
04/15/16 13:53:51 Number of Active Workers 0
04/15/16 13:53:51 DaemonCore: No more children processes to reap.
04/15/16 13:53:52 ForkWorker::Fork: New child of 9936 = 1553705
04/15/16 13:53:52 Number of Active Workers 0
04/15/16 13:53:52 DaemonCore: No more children processes to reap.
04/15/16 13:53:52 Received a superuser command
04/15/16 13:53:52 ForkWorker::Fork: New child of 9936 = 1553706
04/15/16 13:53:52 Number of Active Workers 0
04/15/16 13:53:52 DaemonCore: No more children processes to reap.
04/15/16 13:53:53 ForkWorker::Fork: New child of 9936 = 1553748
04/15/16 13:53:53 Number of Active Workers 0
04/15/16 13:53:53 DaemonCore: No more children processes to reap.
04/15/16 13:53:54 ForkWorker::Fork: New child of 9936 = 1553756
04/15/16 13:53:54 Number of Active Workers 0
04/15/16 13:53:54 DaemonCore: No more children processes to reap.
04/15/16 13:53:55 ForkWorker::Fork: New child of 9936 = 1553767
04/15/16 13:53:55 Number of Active Workers 0
04/15/16 13:53:55 DaemonCore: No more children processes to reap.
04/15/16 13:53:56 ForkWorker::Fork: New child of 9936 = 1553825
04/15/16 13:53:56 Number of Active Workers 0
04/15/16 13:53:56 DaemonCore: No more children processes to reap.
04/15/16 13:53:57 ForkWorker::Fork: New child of 9936 = 1553826
04/15/16 13:53:57 Number of Active Workers 0
04/15/16 13:53:57 DaemonCore: No more children processes to reap.
04/15/16 13:53:58 ForkWorker::Fork: New child of 9936 = 1553875
04/15/16 13:53:58 Number of Active Workers 0
04/15/16 13:53:58 DaemonCore: No more children processes to reap.
04/15/16 13:53:59 ForkWorker::Fork: New child of 9936 = 1553878
04/15/16 13:53:59 Number of Active Workers 0
04/15/16 13:53:59 DaemonCore: No more children processes to reap.
04/15/16 13:54:01 ForkWorker::Fork: New child of 9936 = 1553917
04/15/16 13:54:01 Number of Active Workers 0
04/15/16 13:54:01 DaemonCore: No more children processes to reap.
04/15/16 13:54:02 ForkWorker::Fork: New child of 9936 = 1553928
04/15/16 13:54:02 Number of Active Workers 0
04/15/16 13:54:02 DaemonCore: No more children processes to reap.
04/15/16 13:54:03 ForkWorker::Fork: New child of 9936 = 1553930
04/15/16 13:54:03 Number of Active Workers 0
04/15/16 13:54:03 DaemonCore: No more children processes to reap.
04/15/16 13:54:04 ForkWorker::Fork: New child of 9936 = 1553979
04/15/16 13:54:04 Number of Active Workers 0
04/15/16 13:54:04 DaemonCore: No more children processes to reap.
04/15/16 13:54:04 Received a superuser command
04/15/16 13:54:04 ZKM: successful mapping to condor
04/15/16 13:54:04 Remove jobs 13.0,15.0
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:04 Added data to SelfDrainingQueue act_on_job_myself_queue, now has 1 element(s)
04/15/16 13:54:04 Registered timer for SelfDrainingQueue act_on_job_myself_queue, period: 0 (id: 1709)
04/15/16 13:54:04 Added data to SelfDrainingQueue act_on_job_myself_queue, now has 2 element(s)
04/15/16 13:54:04 Timer for SelfDrainingQueue act_on_job_myself_queue is already registered (id: 1709)
04/15/16 13:54:04 Expedited call to StartJobs()
04/15/16 13:54:04 Finished Remove jobs 13.0,15.0
04/15/16 13:54:04 -------- Begin starting jobs --------
04/15/16 13:54:04 -------- Done starting jobs --------
04/15/16 13:54:04 Inside SelfDrainingQueue::timerHandler() for act_on_job_myself_queue
04/15/16 13:54:04 abort_job_myself: 13.0 action:Remove log_hold:true
04/15/16 13:54:04 Cleared dirty attributes for job 13.0
04/15/16 13:54:04 Writing record to user logfile=/osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b owner=winckler
04/15/16 13:54:04 WriteUserLog::initialize: safe_open_wrapper("/osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b") failed - errno 13 (Permission denied)
04/15/16 13:54:04 WriteUserLog::initialize: failed to open file /osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b
04/15/16 13:54:04 WriteUserLog::initialize: opened /osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b successfully
04/15/16 13:54:04 SelfDrainingQueue act_on_job_myself_queue still has 1 element(s), resetting timer
04/15/16 13:54:04 Reset timer for SelfDrainingQueue act_on_job_myself_queue, period: 0 (id: 1709)
04/15/16 13:54:04 Inside SelfDrainingQueue::timerHandler() for act_on_job_myself_queue
04/15/16 13:54:04 abort_job_myself: 15.0 action:Remove log_hold:true
04/15/16 13:54:04 Cleared dirty attributes for job 15.0
04/15/16 13:54:04 Initializing Directory: curr_dir = /osg/condor/15/0/cluster15.proc0.subproc0
04/15/16 13:54:04 Initializing Directory: curr_dir = /osg/condor/15/0/cluster15.proc0.subproc0
04/15/16 13:54:04 Initializing Directory: curr_dir = /osg/condor/15/0/cluster15.proc0.subproc0.tmp
04/15/16 13:54:04 Saving classad to history file
04/15/16 13:54:04 SelfDrainingQueue act_on_job_myself_queue is empty, not resetting timer
04/15/16 13:54:04 Canceling timer for SelfDrainingQueue act_on_job_myself_queue (timer id: 1709)
04/15/16 13:54:05 ForkWorker::Fork: New child of 9936 = 1553982
04/15/16 13:54:05 Number of Active Workers 0
04/15/16 13:54:05 DaemonCore: No more children processes to reap.
04/15/16 13:54:06 ForkWorker::Fork: New child of 9936 = 1554027
04/15/16 13:54:06 Number of Active Workers 0
04/15/16 13:54:06 DaemonCore: No more children processes to reap.
04/15/16 13:54:07 ForkWorker::Fork: New child of 9936 = 1554032
04/15/16 13:54:07 Number of Active Workers 0
04/15/16 13:54:07 DaemonCore: No more children processes to reap.
04/15/16 13:54:08 Received a superuser command
04/15/16 13:54:08 ForkWorker::Fork: New child of 9936 = 1554068
04/15/16 13:54:08 Number of Active Workers 0
04/15/16 13:54:08 DaemonCore: No more children processes to reap.
04/15/16 13:54:08 ForkWorker::Fork: New child of 9936 = 1554073
04/15/16 13:54:08 Number of Active Workers 0
04/15/16 13:54:08 DaemonCore: No more children processes to reap.
04/15/16 13:54:09 ForkWorker::Fork: New child of 9936 = 1554168
04/15/16 13:54:09 Number of Active Workers 0
04/15/16 13:54:09 DaemonCore: No more children processes to reap.
04/15/16 13:54:10 Received a superuser command
04/15/16 13:54:10 ForkWorker::Fork: New child of 9936 = 1554248
04/15/16 13:54:10 Number of Active Workers 0
04/15/16 13:54:10 DaemonCore: No more children processes to reap.
04/15/16 13:54:10 ForkWorker::Fork: New child of 9936 = 1554258
04/15/16 13:54:10 Number of Active Workers 0
04/15/16 13:54:10 DaemonCore: No more children processes to reap.
04/15/16 13:54:11 Received a superuser command
04/15/16 13:54:11 ForkWorker::Fork: New child of 9936 = 1554331
04/15/16 13:54:11 Number of Active Workers 0
04/15/16 13:54:11 DaemonCore: No more children processes to reap.
04/15/16 13:54:11 ForkWorker::Fork: New child of 9936 = 1554348
04/15/16 13:54:11 Number of Active Workers 0
04/15/16 13:54:11 DaemonCore: No more children processes to reap.
04/15/16 13:54:12 ForkWorker::Fork: New child of 9936 = 1554378
04/15/16 13:54:12 Number of Active Workers 0
04/15/16 13:54:12 DaemonCore: No more children processes to reap.
04/15/16 13:54:12 Received a superuser command
04/15/16 13:54:12 ForkWorker::Fork: New child of 9936 = 1554413
04/15/16 13:54:12 Number of Active Workers 0
04/15/16 13:54:12 DaemonCore: No more children processes to reap.
04/15/16 13:54:13 ForkWorker::Fork: New child of 9936 = 1554482
04/15/16 13:54:13 Number of Active Workers 0
04/15/16 13:54:13 DaemonCore: No more children processes to reap.
04/15/16 13:54:14 ForkWorker::Fork: New child of 9936 = 1554539
04/15/16 13:54:14 Number of Active Workers 0
04/15/16 13:54:14 DaemonCore: No more children processes to reap.
04/15/16 13:54:15 Received a superuser command
04/15/16 13:54:15 ForkWorker::Fork: New child of 9936 = 1554555
04/15/16 13:54:15 Number of Active Workers 0
04/15/16 13:54:15 DaemonCore: No more children processes to reap.
04/15/16 13:54:15 ForkWorker::Fork: New child of 9936 = 1554602
04/15/16 13:54:15 Number of Active Workers 0
04/15/16 13:54:15 DaemonCore: No more children processes to reap.
04/15/16 13:54:16 ForkWorker::Fork: New child of 9936 = 1554661
04/15/16 13:54:16 Number of Active Workers 0
04/15/16 13:54:16 DaemonCore: No more children processes to reap.
04/15/16 13:54:17 ForkWorker::Fork: New child of 9936 = 1554711
04/15/16 13:54:17 Number of Active Workers 0
04/15/16 13:54:17 DaemonCore: No more children processes to reap.
04/15/16 13:54:19 ForkWorker::Fork: New child of 9936 = 1554803
04/15/16 13:54:19 Number of Active Workers 0
04/15/16 13:54:19 DaemonCore: No more children processes to reap.
04/15/16 13:54:20 ForkWorker::Fork: New child of 9936 = 1554900
04/15/16 13:54:20 Number of Active Workers 0
04/15/16 13:54:20 DaemonCore: No more children processes to reap.
04/15/16 13:54:21 ForkWorker::Fork: New child of 9936 = 1554949
04/15/16 13:54:21 Number of Active Workers 0
04/15/16 13:54:21 DaemonCore: No more children processes to reap.
04/15/16 13:54:22 ForkWorker::Fork: New child of 9936 = 1554950
04/15/16 13:54:22 Number of Active Workers 0
04/15/16 13:54:22 DaemonCore: No more children processes to reap.
04/15/16 13:54:23 ForkWorker::Fork: New child of 9936 = 1554981
04/15/16 13:54:23 Number of Active Workers 0
04/15/16 13:54:23 DaemonCore: No more children processes to reap.
04/15/16 13:54:24 ForkWorker::Fork: New child of 9936 = 1554992
04/15/16 13:54:24 Number of Active Workers 0
04/15/16 13:54:24 DaemonCore: No more children processes to reap.
04/15/16 13:54:25 Received a superuser command
04/15/16 13:54:25 ZKM: successful mapping to condor
04/15/16 13:54:25 Remove-Force jobs 13.0,15.0
04/15/16 13:54:25 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:25 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:25 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:25 OwnerCheck retval 1 (success), super_user
04/15/16 13:54:25 Added data to SelfDrainingQueue act_on_job_myself_queue, now has 1 element(s)
04/15/16 13:54:25 Registered timer for SelfDrainingQueue act_on_job_myself_queue, period: 0 (id: 1711)
04/15/16 13:54:25 Expedited call to StartJobs()
04/15/16 13:54:25 Finished Remove-Force jobs 13.0,15.0
04/15/16 13:54:25 -------- Begin starting jobs --------
04/15/16 13:54:25 -------- Done starting jobs --------
04/15/16 13:54:25 Inside SelfDrainingQueue::timerHandler() for act_on_job_myself_queue
04/15/16 13:54:25 Writing record to user logfile=/osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b owner=winckler
04/15/16 13:54:25 WriteUserLog::initialize: safe_open_wrapper("/osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b") failed - errno 13 (Permission denied)
04/15/16 13:54:25 WriteUserLog::initialize: failed to open file /osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b
04/15/16 13:54:25 WriteUserLog::initialize: opened /osg/condor/13/0/cluster13.proc0.subproc0/.log_2375334_RIBG8b successfully
04/15/16 13:54:25 Initializing Directory: curr_dir = /osg/condor/13/0/cluster13.proc0.subproc0
04/15/16 13:54:25 Initializing Directory: curr_dir = /osg/condor/13/0/cluster13.proc0.subproc0
04/15/16 13:54:26 Initializing Directory: curr_dir = /osg/condor/13/0/cluster13.proc0.subproc0.tmp
04/15/16 13:54:26 Saving classad to history file
04/15/16 13:54:26 (13.0) Shadow already gone
04/15/16 13:54:26 SelfDrainingQueue act_on_job_myself_queue is empty, not resetting timer
04/15/16 13:54:26 Canceling timer for SelfDrainingQueue act_on_job_myself_queue (timer id: 1711)
04/15/16 13:54:26 ForkWorker::Fork: New child of 9936 = 1554995
04/15/16 13:54:26 Number of Active Workers 0
04/15/16 13:54:26 DaemonCore: No more children processes to reap.
04/15/16 13:54:27 ForkWorker::Fork: New child of 9936 = 1555003
04/15/16 13:54:27 Number of Active Workers 0
04/15/16 13:54:27 DaemonCore: No more children processes to reap.
04/15/16 13:54:27 Received a superuser command
04/15/16 13:54:27 ForkWorker::Fork: New child of 9936 = 1555049
04/15/16 13:54:27 Number of Active Workers 0
04/15/16 13:54:27 DaemonCore: No more children processes to reap.
04/15/16 13:54:28 ForkWorker::Fork: New child of 9936 = 1555097
04/15/16 13:54:28 Number of Active Workers 0
04/15/16 13:54:28 DaemonCore: No more children processes to reap.
04/15/16 13:54:29 Received a superuser command
04/15/16 13:54:29 ForkWorker::Fork: New child of 9936 = 1555170
04/15/16 13:54:29 Number of Active Workers 0
04/15/16 13:54:29 DaemonCore: No more children processes to reap.
04/15/16 13:54:29 ForkWorker::Fork: New child of 9936 = 1555181
04/15/16 13:54:29 Number of Active Workers 0
04/15/16 13:54:29 DaemonCore: No more children processes to reap.
04/15/16 13:54:30 ForkWorker::Fork: New child of 9936 = 1555182
04/15/16 13:54:30 Number of Active Workers 0
04/15/16 13:54:30 DaemonCore: No more children processes to reap.
04/15/16 13:54:31 ForkWorker::Fork: New child of 9936 = 1555183
04/15/16 13:54:31 Number of Active Workers 0
04/15/16 13:54:31 DaemonCore: No more children processes to reap.
04/15/16 13:54:32 ForkWorker::Fork: New child of 9936 = 1555188
04/15/16 13:54:32 Number of Active Workers 0
04/15/16 13:54:32 DaemonCore: No more children processes to reap.
04/15/16 13:54:33 ForkWorker::Fork: New child of 9936 = 1555265
04/15/16 13:54:33 Number of Active Workers 0
04/15/16 13:54:33 DaemonCore: No more children processes to reap.
04/15/16 13:54:34 ForkWorker::Fork: New child of 9936 = 1555360
04/15/16 13:54:34 Number of Active Workers 0
04/15/16 13:54:34 DaemonCore: No more children processes to reap.
04/15/16 13:54:35 ForkWorker::Fork: New child of 9936 = 1555485
04/15/16 13:54:35 Number of Active Workers 0
04/15/16 13:54:35 DaemonCore: No more children processes to reap.
04/15/16 13:54:36 ForkWorker::Fork: New child of 9936 = 1555559
04/15/16 13:54:36 Number of Active Workers 0
04/15/16 13:54:36 DaemonCore: No more children processes to reap.
04/15/16 13:54:38 ForkWorker::Fork: New child of 9936 = 1555649
04/15/16 13:54:38 Number of Active Workers 0
04/15/16 13:54:38 DaemonCore: No more children processes to reap.
04/15/16 13:54:39 ForkWorker::Fork: New child of 9936 = 1555650
04/15/16 13:54:39 Number of Active Workers 0
04/15/16 13:54:39 DaemonCore: No more children processes to reap.
04/15/16 13:54:40 ForkWorker::Fork: New child of 9936 = 1555687
04/15/16 13:54:40 Number of Active Workers 0
04/15/16 13:54:40 DaemonCore: No more children processes to reap.
04/15/16 13:54:41 ForkWorker::Fork: New child of 9936 = 1555735
04/15/16 13:54:41 Number of Active Workers 0
04/15/16 13:54:41 DaemonCore: No more children processes to reap.
04/15/16 13:54:42 ForkWorker::Fork: New child of 9936 = 1555747
04/15/16 13:54:42 Number of Active Workers 0
04/15/16 13:54:42 DaemonCore: No more children processes to reap.
04/15/16 13:54:43 ForkWorker::Fork: New child of 9936 = 1555766
04/15/16 13:54:43 Number of Active Workers 0
04/15/16 13:54:43 DaemonCore: No more children processes to reap.
04/15/16 13:54:44 ForkWorker::Fork: New child of 9936 = 1555820
04/15/16 13:54:44 Number of Active Workers 0
04/15/16 13:54:44 DaemonCore: No more children processes to reap.
04/15/16 13:54:45 ForkWorker::Fork: New child of 9936 = 1555821
04/15/16 13:54:45 Number of Active Workers 0
04/15/16 13:54:45 DaemonCore: No more children processes to reap.
04/15/16 13:54:46 ForkWorker::Fork: New child of 9936 = 1555870
04/15/16 13:54:46 Number of Active Workers 0
04/15/16 13:54:46 DaemonCore: No more children processes to reap.
04/15/16 13:54:47 ForkWorker::Fork: New child of 9936 = 1555871
04/15/16 13:54:47 Number of Active Workers 0
04/15/16 13:54:47 DaemonCore: No more children processes to reap.
04/15/16 13:54:48 ForkWorker::Fork: New child of 9936 = 1555912
04/15/16 13:54:48 Number of Active Workers 0
04/15/16 13:54:48 DaemonCore: No more children processes to reap.
04/15/16 13:54:49 ForkWorker::Fork: New child of 9936 = 1555918
04/15/16 13:54:49 Number of Active Workers 0
04/15/16 13:54:49 DaemonCore: No more children processes to reap.
04/15/16 13:54:50 ForkWorker::Fork: New child of 9936 = 1555922
04/15/16 13:54:50 Number of Active Workers 0
04/15/16 13:54:50 DaemonCore: No more children processes to reap.
04/15/16 13:54:51 ForkWorker::Fork: New child of 9936 = 1555930
04/15/16 13:54:51 Number of Active Workers 0
04/15/16 13:54:51 DaemonCore: No more children processes to reap.
04/15/16 13:54:52 ForkWorker::Fork: New child of 9936 = 1555965
04/15/16 13:54:52 Number of Active Workers 0
04/15/16 13:54:52 DaemonCore: No more children processes to reap.
04/15/16 13:54:53 ForkWorker::Fork: New child of 9936 = 1556036
04/15/16 13:54:53 Number of Active Workers 0
04/15/16 13:54:53 DaemonCore: No more children processes to reap.
04/15/16 13:54:55 ForkWorker::Fork: New child of 9936 = 1556048
04/15/16 13:54:55 Number of Active Workers 0
04/15/16 13:54:55 DaemonCore: No more children processes to reap.
04/15/16 13:54:56 ForkWorker::Fork: New child of 9936 = 1556089
04/15/16 13:54:56 Number of Active Workers 0
04/15/16 13:54:56 DaemonCore: No more children processes to reap.
04/15/16 13:54:57 ForkWorker::Fork: New child of 9936 = 1556119
04/15/16 13:54:57 Number of Active Workers 0
04/15/16 13:54:57 DaemonCore: No more children processes to reap.
04/15/16 13:54:58 ForkWorker::Fork: New child of 9936 = 1556120
04/15/16 13:54:58 Number of Active Workers 0
04/15/16 13:54:58 DaemonCore: No more children processes to reap.
04/15/16 13:54:59 ForkWorker::Fork: New child of 9936 = 1556131
04/15/16 13:54:59 Number of Active Workers 0
04/15/16 13:54:59 DaemonCore: No more children processes to reap.
04/15/16 13:55:00 ForkWorker::Fork: New child of 9936 = 1556162
04/15/16 13:55:00 Number of Active Workers 0
04/15/16 13:55:00 DaemonCore: No more children processes to reap.
04/15/16 13:55:01 ForkWorker::Fork: New child of 9936 = 1556209
04/15/16 13:55:01 Number of Active Workers 0
04/15/16 13:55:01 DaemonCore: No more children processes to reap.
04/15/16 13:55:02 ForkWorker::Fork: New child of 9936 = 1556241
04/15/16 13:55:02 Number of Active Workers 0
04/15/16 13:55:02 DaemonCore: No more children processes to reap.
04/15/16 13:55:03 ForkWorker::Fork: New child of 9936 = 1556292
04/15/16 13:55:03 Number of Active Workers 0
04/15/16 13:55:03 DaemonCore: No more children processes to reap.
04/15/16 13:55:04 ForkWorker::Fork: New child of 9936 = 1556294
04/15/16 13:55:04 Number of Active Workers 0
04/15/16 13:55:04 DaemonCore: No more children processes to reap.
04/15/16 13:55:05 ForkWorker::Fork: New child of 9936 = 1556351
04/15/16 13:55:05 Number of Active Workers 0
04/15/16 13:55:05 DaemonCore: No more children processes to reap.
04/15/16 13:55:06 ForkWorker::Fork: New child of 9936 = 1556444
04/15/16 13:55:06 Number of Active Workers 0
04/15/16 13:55:06 DaemonCore: No more children processes to reap.
04/15/16 13:55:07 ForkWorker::Fork: New child of 9936 = 1556538
04/15/16 13:55:07 Number of Active Workers 0
04/15/16 13:55:07 DaemonCore: No more children processes to reap.
04/15/16 13:55:08 ForkWorker::Fork: New child of 9936 = 1556561
04/15/16 13:55:08 Number of Active Workers 0
04/15/16 13:55:08 DaemonCore: No more children processes to reap.
04/15/16 13:55:09 ForkWorker::Fork: New child of 9936 = 1556569
04/15/16 13:55:09 Number of Active Workers 0
04/15/16 13:55:09 DaemonCore: No more children processes to reap.
04/15/16 13:55:10 ForkWorker::Fork: New child of 9936 = 1556657
04/15/16 13:55:10 Number of Active Workers 0
04/15/16 13:55:10 DaemonCore: No more children processes to reap.
04/15/16 13:55:12 ForkWorker::Fork: New child of 9936 = 1556736
04/15/16 13:55:12 Number of Active Workers 0
04/15/16 13:55:12 DaemonCore: No more children processes to reap.
04/15/16 13:55:13 ForkWorker::Fork: New child of 9936 = 1556776
04/15/16 13:55:13 Number of Active Workers 0
04/15/16 13:55:13 DaemonCore: No more children processes to reap.
04/15/16 13:55:14 ForkWorker::Fork: New child of 9936 = 1556792
04/15/16 13:55:14 Number of Active Workers 0
04/15/16 13:55:14 DaemonCore: No more children processes to reap.
04/15/16 13:55:15 ForkWorker::Fork: New child of 9936 = 1556831
04/15/16 13:55:15 Number of Active Workers 0
04/15/16 13:55:15 DaemonCore: No more children processes to reap.
04/15/16 13:55:16 ForkWorker::Fork: New child of 9936 = 1556918
04/15/16 13:55:16 Number of Active Workers 0
04/15/16 13:55:16 DaemonCore: No more children processes to reap.
04/15/16 13:55:17 ForkWorker::Fork: New child of 9936 = 1557006
04/15/16 13:55:17 Number of Active Workers 0
04/15/16 13:55:17 DaemonCore: No more children processes to reap.
04/15/16 13:55:18 ForkWorker::Fork: New child of 9936 = 1557097
04/15/16 13:55:18 Number of Active Workers 0
04/15/16 13:55:18 DaemonCore: No more children processes to reap.
04/15/16 13:55:19 ForkWorker::Fork: New child of 9936 = 1557145
04/15/16 13:55:19 Number of Active Workers 0
04/15/16 13:55:19 DaemonCore: No more children processes to reap.
04/15/16 13:55:20 ForkWorker::Fork: New child of 9936 = 1557199
04/15/16 13:55:20 Number of Active Workers 0
04/15/16 13:55:20 DaemonCore: No more children processes to reap.
04/15/16 13:55:21 ForkWorker::Fork: New child of 9936 = 1557319
04/15/16 13:55:21 Number of Active Workers 0
04/15/16 13:55:21 DaemonCore: No more children processes to reap.
04/15/16 13:55:22 ForkWorker::Fork: New child of 9936 = 1557386
04/15/16 13:55:22 Number of Active Workers 0
04/15/16 13:55:22 DaemonCore: No more children processes to reap.
04/15/16 13:55:23 ForkWorker::Fork: New child of 9936 = 1557428
04/15/16 13:55:23 Number of Active Workers 0
04/15/16 13:55:23 DaemonCore: No more children processes to reap.
04/15/16 13:55:24 ForkWorker::Fork: New child of 9936 = 1557431
04/15/16 13:55:24 Number of Active Workers 0
04/15/16 13:55:24 DaemonCore: No more children processes to reap.
04/15/16 13:55:25 ForkWorker::Fork: New child of 9936 = 1557437
04/15/16 13:55:25 Number of Active Workers 0
04/15/16 13:55:25 DaemonCore: No more children processes to reap.
04/15/16 13:55:26 ForkWorker::Fork: New child of 9936 = 1557438
04/15/16 13:55:26 Number of Active Workers 0
04/15/16 13:55:26 DaemonCore: No more children processes to reap.
04/15/16 13:55:27 ForkWorker::Fork: New child of 9936 = 1557469
04/15/16 13:55:27 Number of Active Workers 0
04/15/16 13:55:27 DaemonCore: No more children processes to reap.
04/15/16 13:55:29 ForkWorker::Fork: New child of 9936 = 1557552
04/15/16 13:55:29 Number of Active Workers 0
04/15/16 13:55:29 DaemonCore: No more children processes to reap.
04/15/16 13:55:30 ForkWorker::Fork: New child of 9936 = 1557639
04/15/16 13:55:30 Number of Active Workers 0
04/15/16 13:55:30 DaemonCore: No more children processes to reap.
04/15/16 13:55:31 ForkWorker::Fork: New child of 9936 = 1557733
04/15/16 13:55:31 Number of Active Workers 0
04/15/16 13:55:31 DaemonCore: No more children processes to reap.
04/15/16 13:55:32 ForkWorker::Fork: New child of 9936 = 1557850
04/15/16 13:55:32 Number of Active Workers 0
04/15/16 13:55:32 DaemonCore: No more children processes to reap.
04/15/16 13:55:33 ForkWorker::Fork: New child of 9936 = 1557920
04/15/16 13:55:33 Number of Active Workers 0
04/15/16 13:55:33 DaemonCore: No more children processes to reap.
04/15/16 13:55:34 ForkWorker::Fork: New child of 9936 = 1557923
04/15/16 13:55:34 Number of Active Workers 0
04/15/16 13:55:34 DaemonCore: No more children processes to reap.
04/15/16 13:55:35 ForkWorker::Fork: New child of 9936 = 1557924
04/15/16 13:55:35 Number of Active Workers 0
04/15/16 13:55:35 DaemonCore: No more children processes to reap.
04/15/16 13:55:36 ForkWorker::Fork: New child of 9936 = 1557953
04/15/16 13:55:36 Number of Active Workers 0
04/15/16 13:55:36 DaemonCore: No more children processes to reap.
04/15/16 13:55:37 ForkWorker::Fork: New child of 9936 = 1557961
04/15/16 13:55:37 Number of Active Workers 0
04/15/16 13:55:37 DaemonCore: No more children processes to reap.
04/15/16 13:55:38 ForkWorker::Fork: New child of 9936 = 1557973
04/15/16 13:55:38 Number of Active Workers 0
04/15/16 13:55:38 DaemonCore: No more children processes to reap.
04/15/16 13:55:39 ForkWorker::Fork: New child of 9936 = 1558009
04/15/16 13:55:39 Number of Active Workers 0
04/15/16 13:55:39 DaemonCore: No more children processes to reap.
04/15/16 13:55:40 ForkWorker::Fork: New child of 9936 = 1558043
04/15/16 13:55:40 Number of Active Workers 0
04/15/16 13:55:40 DaemonCore: No more children processes to reap.
04/15/16 13:55:41 ForkWorker::Fork: New child of 9936 = 1558094
04/15/16 13:55:41 Number of Active Workers 0
04/15/16 13:55:41 DaemonCore: No more children processes to reap.
04/15/16 13:55:42 ForkWorker::Fork: New child of 9936 = 1558113
04/15/16 13:55:42 Number of Active Workers 0
04/15/16 13:55:42 DaemonCore: No more children processes to reap.
04/15/16 13:55:43 ForkWorker::Fork: New child of 9936 = 1558154
04/15/16 13:55:43 Number of Active Workers 0
04/15/16 13:55:43 DaemonCore: No more children processes to reap.
04/15/16 13:55:44 ForkWorker::Fork: New child of 9936 = 1558159
04/15/16 13:55:44 Number of Active Workers 0
04/15/16 13:55:44 DaemonCore: No more children processes to reap.
04/15/16 13:55:45 ForkWorker::Fork: New child of 9936 = 1558165
04/15/16 13:55:45 Number of Active Workers 0
04/15/16 13:55:45 DaemonCore: No more children processes to reap.
04/15/16 13:55:47 ForkWorker::Fork: New child of 9936 = 1558258
04/15/16 13:55:47 Number of Active Workers 0
04/15/16 13:55:47 DaemonCore: No more children processes to reap.
04/15/16 13:55:48 ForkWorker::Fork: New child of 9936 = 1558352
04/15/16 13:55:48 Number of Active Workers 0
04/15/16 13:55:48 DaemonCore: No more children processes to reap.
04/15/16 13:55:49 ForkWorker::Fork: New child of 9936 = 1558454
04/15/16 13:55:49 Number of Active Workers 0
04/15/16 13:55:49 DaemonCore: No more children processes to reap.
04/15/16 13:55:50 ForkWorker::Fork: New child of 9936 = 1558519
04/15/16 13:55:50 Number of Active Workers 0
04/15/16 13:55:50 DaemonCore: No more children processes to reap.
04/15/16 13:55:51 ForkWorker::Fork: New child of 9936 = 1558588
04/15/16 13:55:51 Number of Active Workers 0
04/15/16 13:55:51 DaemonCore: No more children processes to reap.
04/15/16 13:55:52 ForkWorker::Fork: New child of 9936 = 1558641
04/15/16 13:55:52 Number of Active Workers 0
04/15/16 13:55:52 DaemonCore: No more children processes to reap.
04/15/16 13:55:53 ForkWorker::Fork: New child of 9936 = 1558684
04/15/16 13:55:53 Number of Active Workers 0
04/15/16 13:55:53 DaemonCore: No more children processes to reap.
04/15/16 13:55:54 ForkWorker::Fork: New child of 9936 = 1558776
04/15/16 13:55:54 Number of Active Workers 0
04/15/16 13:55:54 DaemonCore: No more children processes to reap.
04/15/16 13:55:55 ForkWorker::Fork: New child of 9936 = 1558818
04/15/16 13:55:55 Number of Active Workers 0
04/15/16 13:55:55 DaemonCore: No more children processes to reap.
04/15/16 13:55:56 ForkWorker::Fork: New child of 9936 = 1558898
04/15/16 13:55:56 Number of Active Workers 0
04/15/16 13:55:56 DaemonCore: No more children processes to reap.
04/15/16 13:55:57 ForkWorker::Fork: New child of 9936 = 1558982
04/15/16 13:55:57 Number of Active Workers 0
04/15/16 13:55:57 DaemonCore: No more children processes to reap.
04/15/16 13:55:58 ForkWorker::Fork: New child of 9936 = 1559081
04/15/16 13:55:58 Number of Active Workers 0
04/15/16 13:55:58 DaemonCore: No more children processes to reap.
04/15/16 13:55:59 ForkWorker::Fork: New child of 9936 = 1559165
04/15/16 13:55:59 Number of Active Workers 0
04/15/16 13:55:59 DaemonCore: No more children processes to reap.
04/15/16 13:56:00 ForkWorker::Fork: New child of 9936 = 1559235
04/15/16 13:56:00 Number of Active Workers 0
04/15/16 13:56:00 DaemonCore: No more children processes to reap.
04/15/16 13:56:01 ForkWorker::Fork: New child of 9936 = 1559367
04/15/16 13:56:01 Number of Active Workers 0
04/15/16 13:56:01 DaemonCore: No more children processes to reap.
04/15/16 13:56:02 ForkWorker::Fork: New child of 9936 = 1559495
04/15/16 13:56:02 Number of Active Workers 0
04/15/16 13:56:02 DaemonCore: No more children processes to reap.
04/15/16 13:56:03 ForkWorker::Fork: New child of 9936 = 1559555
04/15/16 13:56:03 Number of Active Workers 0
04/15/16 13:56:03 DaemonCore: No more children processes to reap.
04/15/16 13:56:05 ForkWorker::Fork: New child of 9936 = 1559645
04/15/16 13:56:05 Number of Active Workers 0
04/15/16 13:56:05 DaemonCore: No more children processes to reap.
04/15/16 13:56:06 ForkWorker::Fork: New child of 9936 = 1559696
04/15/16 13:56:06 Number of Active Workers 0
04/15/16 13:56:06 DaemonCore: No more children processes to reap.
04/15/16 13:56:07 ForkWorker::Fork: New child of 9936 = 1559789
04/15/16 13:56:07 Number of Active Workers 0
04/15/16 13:56:07 DaemonCore: No more children processes to reap.
04/15/16 13:56:08 ForkWorker::Fork: New child of 9936 = 1559880
04/15/16 13:56:08 Number of Active Workers 0
04/15/16 13:56:08 DaemonCore: No more children processes to reap.
04/15/16 13:56:09 ForkWorker::Fork: New child of 9936 = 1559967
04/15/16 13:56:09 Number of Active Workers 0
04/15/16 13:56:09 DaemonCore: No more children processes to reap.
04/15/16 13:56:10 Clearing userlog file cache
04/15/16 13:56:10 JobsRunning = 0
04/15/16 13:56:10 JobsIdle = 2
04/15/16 13:56:10 JobsHeld = 0
04/15/16 13:56:10 JobsRemoved = 0
04/15/16 13:56:10 LocalUniverseJobsRunning = 0
04/15/16 13:56:10 LocalUniverseJobsIdle = 0
04/15/16 13:56:10 SchedUniverseJobsRunning = 0
04/15/16 13:56:10 SchedUniverseJobsIdle = 0
04/15/16 13:56:10 N_Owners = 1
04/15/16 13:56:10 MaxJobsRunning = 10000
04/15/16 13:56:10 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 13:56:10 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 13:56:10 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 13:56:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:56:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:56:10 Sent HEART BEAT ad to 1 collectors. Number of submittors=1
04/15/16 13:56:10 Changed attribute: IdleJobs = 0
04/15/16 13:56:10 Changed attribute: RunningJobs = 0
04/15/16 13:56:10 Changed attribute: IdleJobs = 0
04/15/16 13:56:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:56:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:56:10 Changed attribute: HeldJobs = 0
04/15/16 13:56:10 Changed attribute: FlockedJobs = 0
04/15/16 13:56:10 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 13:56:10 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 13:56:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:56:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:56:10 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 13:56:10 Changed attribute: IdleJobs = 0
04/15/16 13:56:10 Changed attribute: RunningJobs = 0
04/15/16 13:56:10 Changed attribute: IdleJobs = 0
04/15/16 13:56:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:56:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:56:10 Changed attribute: HeldJobs = 0
04/15/16 13:56:10 Changed attribute: FlockedJobs = 0
04/15/16 13:56:10 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 13:56:10 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 13:56:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:56:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:56:10 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 13:56:10 Changed attribute: IdleJobs = 0
04/15/16 13:56:10 Changed attribute: RunningJobs = 0
04/15/16 13:56:10 Changed attribute: IdleJobs = 0
04/15/16 13:56:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 13:56:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 13:56:10 Changed attribute: HeldJobs = 0
04/15/16 13:56:10 Changed attribute: FlockedJobs = 0
04/15/16 13:56:10 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 13:56:10 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 13:56:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:56:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:56:10 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 13:56:10 Starting condor_gmanager for owner winckler (0.0)
04/15/16 13:56:10 Really Execing condor_gridmanager -f -C (Owner=?="winckler"&&JobUniverse==9) -o winckler -S /tmp/condor_g_scratch.0x7fa72203e500.9936
04/15/16 13:56:10 Create_Process: using fast clone() to create child process.
04/15/16 13:56:10 Started condor_gmanager for owner winckler pid=1560049
04/15/16 13:56:10 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 13:56:10 Sent owner (0 jobs) ad to schedd plugins
04/15/16 13:56:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:56:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:56:10 Sent owner (0 jobs) ad to 1 collectors
04/15/16 13:56:10 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 13:56:10 Sent owner (0 jobs) ad to schedd plugins
04/15/16 13:56:10 Trying to update collector <200.145.46.35:9619>
04/15/16 13:56:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:56:10 Sent owner (0 jobs) ad to 1 collectors
04/15/16 13:56:10 ============ Begin clean_shadow_recs =============
04/15/16 13:56:10 ============ End clean_shadow_recs =============
04/15/16 13:56:10 Sending RESCHEDULE command to negotiator(s)
04/15/16 13:56:10 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 13:56:10 Trying to query collector <200.145.46.35:9619>
04/15/16 13:56:10 Can't find address for negotiator 
04/15/16 13:56:10 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 13:56:10 ForkWorker::Fork: New child of 9936 = 1560055
04/15/16 13:56:10 Number of Active Workers 0
04/15/16 13:56:10 DaemonCore: No more children processes to reap.
04/15/16 13:56:11 ForkWorker::Fork: New child of 9936 = 1560113
04/15/16 13:56:11 Number of Active Workers 0
04/15/16 13:56:11 DaemonCore: No more children processes to reap.
04/15/16 13:56:12 ForkWorker::Fork: New child of 9936 = 1560188
04/15/16 13:56:12 Number of Active Workers 0
04/15/16 13:56:12 DaemonCore: No more children processes to reap.
04/15/16 13:56:13 Send_Signal(): Doing kill(1560049,12) [SIGUSR2]
04/15/16 13:56:13 QMGR Connection closed
04/15/16 13:56:13 ForkWorker::Fork: New child of 9936 = 1560252
04/15/16 13:56:13 Number of Active Workers 0
04/15/16 13:56:13 DaemonCore: No more children processes to reap.
04/15/16 13:56:14 ForkWorker::Fork: New child of 9936 = 1560312
04/15/16 13:56:14 Number of Active Workers 0
04/15/16 13:56:14 DaemonCore: No more children processes to reap.
04/15/16 13:56:15 ForkWorker::Fork: New child of 9936 = 1560388
04/15/16 13:56:15 Number of Active Workers 0
04/15/16 13:56:15 DaemonCore: No more children processes to reap.
04/15/16 13:56:16 ForkWorker::Fork: New child of 9936 = 1560429
04/15/16 13:56:16 Number of Active Workers 0
04/15/16 13:56:16 DaemonCore: No more children processes to reap.
04/15/16 13:56:17 ForkWorker::Fork: New child of 9936 = 1560450
04/15/16 13:56:17 Number of Active Workers 0
04/15/16 13:56:17 DaemonCore: No more children processes to reap.
04/15/16 13:56:18 QMGR Connection closed
04/15/16 13:56:18 ForkWorker::Fork: New child of 9936 = 1560524
04/15/16 13:56:18 Number of Active Workers 0
04/15/16 13:56:18 DaemonCore: No more children processes to reap.
04/15/16 13:56:19 ForkWorker::Fork: New child of 9936 = 1560544
04/15/16 13:56:19 Number of Active Workers 0
04/15/16 13:56:19 DaemonCore: No more children processes to reap.
04/15/16 13:56:20 ForkWorker::Fork: New child of 9936 = 1560556
04/15/16 13:56:20 Number of Active Workers 0
04/15/16 13:56:20 DaemonCore: No more children processes to reap.
04/15/16 13:56:22 ForkWorker::Fork: New child of 9936 = 1560647
04/15/16 13:56:22 Number of Active Workers 0
04/15/16 13:56:22 DaemonCore: No more children processes to reap.
04/15/16 13:56:23 ForkWorker::Fork: New child of 9936 = 1560735
04/15/16 13:56:23 Number of Active Workers 0
04/15/16 13:56:23 DaemonCore: No more children processes to reap.
04/15/16 13:56:23 QMGR Connection closed
04/15/16 13:56:24 ForkWorker::Fork: New child of 9936 = 1560831
04/15/16 13:56:24 Number of Active Workers 0
04/15/16 13:56:24 DaemonCore: No more children processes to reap.
04/15/16 13:56:25 ForkWorker::Fork: New child of 9936 = 1560921
04/15/16 13:56:25 Number of Active Workers 0
04/15/16 13:56:25 DaemonCore: No more children processes to reap.
04/15/16 13:56:26 ForkWorker::Fork: New child of 9936 = 1560983
04/15/16 13:56:26 Number of Active Workers 0
04/15/16 13:56:26 DaemonCore: No more children processes to reap.
04/15/16 13:56:27 ForkWorker::Fork: New child of 9936 = 1561028
04/15/16 13:56:27 Number of Active Workers 0
04/15/16 13:56:27 DaemonCore: No more children processes to reap.
04/15/16 13:56:28 ForkWorker::Fork: New child of 9936 = 1561054
04/15/16 13:56:28 Number of Active Workers 0
04/15/16 13:56:28 DaemonCore: No more children processes to reap.
04/15/16 13:56:29 ForkWorker::Fork: New child of 9936 = 1561080
04/15/16 13:56:29 Number of Active Workers 0
04/15/16 13:56:29 DaemonCore: No more children processes to reap.
04/15/16 13:56:30 ForkWorker::Fork: New child of 9936 = 1561081
04/15/16 13:56:30 Number of Active Workers 0
04/15/16 13:56:30 DaemonCore: No more children processes to reap.
04/15/16 13:56:31 ForkWorker::Fork: New child of 9936 = 1561111
04/15/16 13:56:31 Number of Active Workers 0
04/15/16 13:56:31 DaemonCore: No more children processes to reap.
04/15/16 13:56:32 ForkWorker::Fork: New child of 9936 = 1561163
04/15/16 13:56:32 Number of Active Workers 0
04/15/16 13:56:32 DaemonCore: No more children processes to reap.
04/15/16 13:56:33 ForkWorker::Fork: New child of 9936 = 1561217
04/15/16 13:56:33 Number of Active Workers 0
04/15/16 13:56:33 DaemonCore: No more children processes to reap.
04/15/16 13:56:34 ForkWorker::Fork: New child of 9936 = 1561296
04/15/16 13:56:34 Number of Active Workers 0
04/15/16 13:56:34 DaemonCore: No more children processes to reap.
04/15/16 13:56:35 ForkWorker::Fork: New child of 9936 = 1561387
04/15/16 13:56:35 Number of Active Workers 0
04/15/16 13:56:35 DaemonCore: No more children processes to reap.
04/15/16 13:56:36 ForkWorker::Fork: New child of 9936 = 1561448
04/15/16 13:56:36 Number of Active Workers 0
04/15/16 13:56:36 DaemonCore: No more children processes to reap.
04/15/16 13:56:37 ForkWorker::Fork: New child of 9936 = 1561495
04/15/16 13:56:37 Number of Active Workers 0
04/15/16 13:56:37 DaemonCore: No more children processes to reap.
04/15/16 13:56:39 ForkWorker::Fork: New child of 9936 = 1561548
04/15/16 13:56:39 Number of Active Workers 0
04/15/16 13:56:39 DaemonCore: No more children processes to reap.
04/15/16 13:56:40 ForkWorker::Fork: New child of 9936 = 1561619
04/15/16 13:56:40 Number of Active Workers 0
04/15/16 13:56:40 DaemonCore: No more children processes to reap.
04/15/16 13:56:41 Getting monitoring info for pid 9936
04/15/16 13:56:41 ForkWorker::Fork: New child of 9936 = 1561664
04/15/16 13:56:41 Number of Active Workers 0
04/15/16 13:56:41 DaemonCore: No more children processes to reap.
04/15/16 13:56:42 ForkWorker::Fork: New child of 9936 = 1561751
04/15/16 13:56:42 Number of Active Workers 0
04/15/16 13:56:42 DaemonCore: No more children processes to reap.
04/15/16 13:56:43 ForkWorker::Fork: New child of 9936 = 1561840
04/15/16 13:56:43 Number of Active Workers 0
04/15/16 13:56:43 DaemonCore: No more children processes to reap.
04/15/16 13:56:44 ForkWorker::Fork: New child of 9936 = 1561938
04/15/16 13:56:44 Number of Active Workers 0
04/15/16 13:56:44 DaemonCore: No more children processes to reap.
04/15/16 13:56:45 ForkWorker::Fork: New child of 9936 = 1562026
04/15/16 13:56:45 Number of Active Workers 0
04/15/16 13:56:45 DaemonCore: No more children processes to reap.
04/15/16 13:56:46 ForkWorker::Fork: New child of 9936 = 1562121
04/15/16 13:56:46 Number of Active Workers 0
04/15/16 13:56:46 DaemonCore: No more children processes to reap.
04/15/16 13:56:47 ForkWorker::Fork: New child of 9936 = 1562169
04/15/16 13:56:47 Number of Active Workers 0
04/15/16 13:56:47 DaemonCore: No more children processes to reap.
04/15/16 13:56:48 ForkWorker::Fork: New child of 9936 = 1562218
04/15/16 13:56:48 Number of Active Workers 0
04/15/16 13:56:48 DaemonCore: No more children processes to reap.
04/15/16 13:56:49 ForkWorker::Fork: New child of 9936 = 1562279
04/15/16 13:56:49 Number of Active Workers 0
04/15/16 13:56:49 DaemonCore: No more children processes to reap.
04/15/16 13:56:50 ForkWorker::Fork: New child of 9936 = 1562336
04/15/16 13:56:50 Number of Active Workers 0
04/15/16 13:56:50 DaemonCore: No more children processes to reap.
04/15/16 13:56:51 ForkWorker::Fork: New child of 9936 = 1562383
04/15/16 13:56:51 Number of Active Workers 0
04/15/16 13:56:51 DaemonCore: No more children processes to reap.
04/15/16 13:56:52 ForkWorker::Fork: New child of 9936 = 1562470
04/15/16 13:56:52 Number of Active Workers 0
04/15/16 13:56:52 DaemonCore: No more children processes to reap.
04/15/16 13:56:53 ForkWorker::Fork: New child of 9936 = 1562569
04/15/16 13:56:53 Number of Active Workers 0
04/15/16 13:56:53 DaemonCore: No more children processes to reap.
04/15/16 13:56:54 ForkWorker::Fork: New child of 9936 = 1562668
04/15/16 13:56:54 Number of Active Workers 0
04/15/16 13:56:54 DaemonCore: No more children processes to reap.
04/15/16 13:56:55 ForkWorker::Fork: New child of 9936 = 1562766
04/15/16 13:56:55 Number of Active Workers 0
04/15/16 13:56:55 DaemonCore: No more children processes to reap.
04/15/16 13:56:57 ForkWorker::Fork: New child of 9936 = 1562853
04/15/16 13:56:57 Number of Active Workers 0
04/15/16 13:56:57 DaemonCore: No more children processes to reap.
04/15/16 13:56:58 ForkWorker::Fork: New child of 9936 = 1562902
04/15/16 13:56:58 Number of Active Workers 0
04/15/16 13:56:58 DaemonCore: No more children processes to reap.
04/15/16 13:56:59 ForkWorker::Fork: New child of 9936 = 1562945
04/15/16 13:56:59 Number of Active Workers 0
04/15/16 13:56:59 DaemonCore: No more children processes to reap.
04/15/16 13:57:00 ForkWorker::Fork: New child of 9936 = 1563014
04/15/16 13:57:00 Number of Active Workers 0
04/15/16 13:57:00 DaemonCore: No more children processes to reap.
04/15/16 13:57:01 ForkWorker::Fork: New child of 9936 = 1563064
04/15/16 13:57:01 Number of Active Workers 0
04/15/16 13:57:01 DaemonCore: No more children processes to reap.
04/15/16 13:57:02 ForkWorker::Fork: New child of 9936 = 1563146
04/15/16 13:57:02 Number of Active Workers 0
04/15/16 13:57:02 DaemonCore: No more children processes to reap.
04/15/16 13:57:03 ForkWorker::Fork: New child of 9936 = 1563240
04/15/16 13:57:03 Number of Active Workers 0
04/15/16 13:57:03 DaemonCore: No more children processes to reap.
04/15/16 13:57:04 ForkWorker::Fork: New child of 9936 = 1563339
04/15/16 13:57:04 Number of Active Workers 0
04/15/16 13:57:04 DaemonCore: No more children processes to reap.
04/15/16 13:57:05 ForkWorker::Fork: New child of 9936 = 1563428
04/15/16 13:57:05 Number of Active Workers 0
04/15/16 13:57:05 DaemonCore: No more children processes to reap.
04/15/16 13:57:06 ForkWorker::Fork: New child of 9936 = 1563515
04/15/16 13:57:06 Number of Active Workers 0
04/15/16 13:57:06 DaemonCore: No more children processes to reap.
04/15/16 13:57:07 ForkWorker::Fork: New child of 9936 = 1563602
04/15/16 13:57:07 Number of Active Workers 0
04/15/16 13:57:07 DaemonCore: No more children processes to reap.
04/15/16 13:57:08 ForkWorker::Fork: New child of 9936 = 1563652
04/15/16 13:57:08 Number of Active Workers 0
04/15/16 13:57:08 DaemonCore: No more children processes to reap.
04/15/16 13:57:09 ForkWorker::Fork: New child of 9936 = 1563700
04/15/16 13:57:09 Number of Active Workers 0
04/15/16 13:57:09 DaemonCore: No more children processes to reap.
04/15/16 13:57:10 ForkWorker::Fork: New child of 9936 = 1563767
04/15/16 13:57:10 Number of Active Workers 0
04/15/16 13:57:10 DaemonCore: No more children processes to reap.
04/15/16 13:57:10 QMGR Connection closed
04/15/16 13:57:11 ForkWorker::Fork: New child of 9936 = 1563818
04/15/16 13:57:11 Number of Active Workers 0
04/15/16 13:57:11 DaemonCore: No more children processes to reap.
04/15/16 13:57:12 ForkWorker::Fork: New child of 9936 = 1563873
04/15/16 13:57:12 Number of Active Workers 0
04/15/16 13:57:12 DaemonCore: No more children processes to reap.
04/15/16 13:57:14 ForkWorker::Fork: New child of 9936 = 1563964
04/15/16 13:57:14 Number of Active Workers 0
04/15/16 13:57:14 DaemonCore: No more children processes to reap.
04/15/16 13:57:15 ForkWorker::Fork: New child of 9936 = 1564057
04/15/16 13:57:15 Number of Active Workers 0
04/15/16 13:57:15 DaemonCore: No more children processes to reap.
04/15/16 13:57:16 ForkWorker::Fork: New child of 9936 = 1564146
04/15/16 13:57:16 Number of Active Workers 0
04/15/16 13:57:16 DaemonCore: No more children processes to reap.
04/15/16 13:57:17 ForkWorker::Fork: New child of 9936 = 1564228
04/15/16 13:57:17 Number of Active Workers 0
04/15/16 13:57:17 DaemonCore: No more children processes to reap.
04/15/16 13:57:18 ForkWorker::Fork: New child of 9936 = 1564331
04/15/16 13:57:18 Number of Active Workers 0
04/15/16 13:57:18 DaemonCore: No more children processes to reap.
04/15/16 13:57:19 ForkWorker::Fork: New child of 9936 = 1564383
04/15/16 13:57:19 Number of Active Workers 0
04/15/16 13:57:19 DaemonCore: No more children processes to reap.
04/15/16 13:57:20 ForkWorker::Fork: New child of 9936 = 1564437
04/15/16 13:57:20 Number of Active Workers 0
04/15/16 13:57:20 DaemonCore: No more children processes to reap.
04/15/16 13:57:21 ForkWorker::Fork: New child of 9936 = 1564505
04/15/16 13:57:21 Number of Active Workers 0
04/15/16 13:57:21 DaemonCore: No more children processes to reap.
04/15/16 13:57:22 ForkWorker::Fork: New child of 9936 = 1564510
04/15/16 13:57:22 Number of Active Workers 0
04/15/16 13:57:22 DaemonCore: No more children processes to reap.
04/15/16 13:57:23 ForkWorker::Fork: New child of 9936 = 1564550
04/15/16 13:57:23 Number of Active Workers 0
04/15/16 13:57:23 DaemonCore: No more children processes to reap.
04/15/16 13:57:24 QMGR Connection closed
04/15/16 13:57:24 ForkWorker::Fork: New child of 9936 = 1564599
04/15/16 13:57:24 Number of Active Workers 0
04/15/16 13:57:24 DaemonCore: No more children processes to reap.
04/15/16 13:57:25 ForkWorker::Fork: New child of 9936 = 1564666
04/15/16 13:57:25 Number of Active Workers 0
04/15/16 13:57:25 DaemonCore: No more children processes to reap.
04/15/16 13:57:26 ForkWorker::Fork: New child of 9936 = 1564790
04/15/16 13:57:26 Number of Active Workers 0
04/15/16 13:57:26 DaemonCore: No more children processes to reap.
04/15/16 13:57:27 ForkWorker::Fork: New child of 9936 = 1564800
04/15/16 13:57:27 Number of Active Workers 0
04/15/16 13:57:27 DaemonCore: No more children processes to reap.
04/15/16 13:57:28 ForkWorker::Fork: New child of 9936 = 1564865
04/15/16 13:57:28 Number of Active Workers 0
04/15/16 13:57:28 DaemonCore: No more children processes to reap.
04/15/16 13:57:29 Added data to SelfDrainingQueue job_is_finished_queue, now has 1 element(s)
04/15/16 13:57:29 Registered timer for SelfDrainingQueue job_is_finished_queue, period: 0 (id: 1720)
04/15/16 13:57:29 Job 28.0 is finished
04/15/16 13:57:29 schedd: DestroyProc cluster 28 proc 0 rval 1 errno 0
04/15/16 13:57:29 QMGR Connection closed
04/15/16 13:57:29 Inside SelfDrainingQueue::timerHandler() for job_is_finished_queue
04/15/16 13:57:29 Job cleanup for 28.0 will not block, calling jobIsFinished() directly
04/15/16 13:57:29 jobIsFinished() completed, calling DestroyProc(28.0)
04/15/16 13:57:29 SelfDrainingQueue job_is_finished_queue is empty, not resetting timer
04/15/16 13:57:29 Canceling timer for SelfDrainingQueue job_is_finished_queue (timer id: 1720)
04/15/16 13:57:29 DaemonCore: No more children processes to reap.
04/15/16 13:57:29 condor_gridmanager (PID 1560049, owner winckler) exited with return code 0.
04/15/16 13:57:29 Initializing Directory: curr_dir = /tmp/condor_g_scratch.0x7fa72203e500.9936
04/15/16 13:57:29 Removed scratch dir /tmp/condor_g_scratch.0x7fa72203e500.9936
04/15/16 13:57:29 ForkWorker::Fork: New child of 9936 = 1564888
04/15/16 13:57:29 Number of Active Workers 0
04/15/16 13:57:29 DaemonCore: No more children processes to reap.
04/15/16 13:57:30 ForkWorker::Fork: New child of 9936 = 1564893
04/15/16 13:57:30 Number of Active Workers 0
04/15/16 13:57:30 DaemonCore: No more children processes to reap.
04/15/16 13:57:32 ForkWorker::Fork: New child of 9936 = 1564949
04/15/16 13:57:32 Number of Active Workers 0
04/15/16 13:57:32 DaemonCore: No more children processes to reap.
04/15/16 13:57:33 ForkWorker::Fork: New child of 9936 = 1564950
04/15/16 13:57:33 Number of Active Workers 0
04/15/16 13:57:33 DaemonCore: No more children processes to reap.
04/15/16 13:57:33 QMGR Connection closed
04/15/16 13:57:34 ForkWorker::Fork: New child of 9936 = 1564997
04/15/16 13:57:34 Number of Active Workers 0
04/15/16 13:57:34 DaemonCore: No more children processes to reap.
04/15/16 13:57:34 This process has a valid certificate & key
04/15/16 13:57:34 Finishing authenticate_server_gss_post with status=1
04/15/16 13:57:34 Finishing authenticate_server_gss_post with status=1
04/15/16 13:57:34 ZKM: successful mapping to GSS_ASSIST_GRIDMAP
04/15/16 13:57:34 Command=TRANSFER_DATA_WITH_PERMS, peer=<200.145.46.37:53823>
04/15/16 13:57:34 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 13:57:34 Looking at spooling: mode is 489
04/15/16 13:57:34 Scheduler::spoolJobFiles: TRANSFER_DATA/WITH_PERMS: 1 jobs matched constraint (ClusterId==27)
04/15/16 13:57:34 Transferring files for jobs 27.0
04/15/16 13:57:34 Scheduler::generalJobFilesWorkerThread: TRANSFER_DATA/WITH_PERMS: 1 jobs to be sent
04/15/16 13:57:34 spoolJobFiles(): started worker process
04/15/16 13:57:34 generalJobFilesWorkerThread(): transfer files for job 27.0
04/15/16 13:57:34 The submitting job ad as the FileTransferObject sees it
NumCkpts_RAW = 0
BufferSize = 524288
NiceUser = false
CoreSize = 0
CumulativeSlotTime = 0
OnExitHold = false
RequestCpus = 1
Err = "_condor_stderr"
BufferBlockSize = 32768
ExecutableSize_RAW = 26
WantCheckpoint = false
CommittedTime = 0
TargetType = "Machine"
WhenToTransferOutput = "ON_EXIT"
JobUniverse = 5
ExitBySignal = false
TransferIn = false
NumRestarts = 0
EncryptExecuteDirectory = false
CommittedSuspensionTime = 0
Owner = "winckler"
NumSystemHolds = 0
CumulativeSuspensionTime = 0
Environment = "GLOBUS_TCP_SOURCE_RANGE=20000,29999 _LMFILES_=/opt/modules/gridunesp/1 BYOBU_SED=sed' '--follow-symlinks LC_MEASUREMENT=pt_BR.UTF-8 BYOBU_READLINK=readlink BYOBU_CONFIG_DIR=/home/winckler/.byobu LCMAPS_DEBUG_LEVEL=3 GV_DIR=/opt/gaussian/gv SHLVL=2 LS_COLORS= PWD=/home/winckler BYOBU_TIME=%H:%M:%S SSH_AUTH_SOCK=/home/winckler/.byobu/.ssh-agent BYOBU_WINDOWS=/home/winckler/.byobu/windows SSH_CLIENT=200.145.46.228' '49266' '22 CVS_RSH=ssh PGI_TERM=trace,abort LC_TELEPHONE=pt_BR.UTF-8 STY=2420116.byobu PATH=/home/winckler/bin:/usr/local/src/goo-client/scripts:/home/winckler/bin:/opt/gridunesp/internals//bin:/usr/lib64/qt-3.3/bin:/usr/local/src/goo-client/scripts:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/home/winckler/bin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 BYOBU_RUN_DIR=/dev/shm/byobu-winckler-yXoDFpVj TOOL_DEBUG=D_FULLDEBUG MODULESHOME=/usr/share/Modules G09BASIS=/opt/gaussian/g09/basis GAUSS_EXEDIR=/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 X509_CERT_DIR=/etc/grid-security/certificates GLOBUS_TCP_PORT_RANGE=20000,29999 QTLIB=/usr/lib64/qt-3.3/lib LC_NAME=pt_BR.UTF-8 GATEKEEPER_JM_ID=2016-04-15.16:49:42.0002420989.0000000000 HISTCONTROL=ignoredups LCMAPS_DIR=/etc BYOBU_DARK=black LC_NUMERIC=pt_BR.UTF-8 GOO_API_URI=https://submit.grid.unesp.br/api/v1/ g09root=/opt/gaussian _DSM_BARRIER=SHM BYOBU_ULIMIT=ulimit SSH_TTY=/dev/pts/20 _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI MYPROXY_SERVER=myproxy.grid.unesp.br SHELL=/bin/bash GAUSS_SCRDIR=/tmp MAIL=/var/spool/mail/winckler GAUSS_ARCHDIR=/opt/gaussian/g09/arch BYOBU_HIGHLIGHT=#DD4814 LCMAPS_DB_FILE=/etc/lcmaps/lcmaps.db LC_CTYPE=pt_BR.UTF-8 LC_ADDRESS=pt_BR.UTF-8 LOADEDMODULES=gridunesp/1 BYOBU_LIGHT=white USER=winckler SSH_CONNECTION=200.145.46.228' '49266' '200.145.46.37' '22 HOSTNAME=access.grid.unesp.br GRIDUNESP_SLURM_PARTITIONS=rack1,rack2,rack3,rack4,rack5,rack6,rack7,rack8 LD_LIBRARY_PATH=/opt/gridunesp/internals//lib:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/opt/gaussian/gv/lib PYTHONPATH=/usr/local/src/python-gooclientlib:/usr/local/src/goo-client:/usr/local/src/python-gooclientlib:/usr/local/src/goo-client: BYOBU_TTY=/dev/pts/20 BYOBU_WINDOW_NAME=- BYOBU_BACKEND=screen GAUSS_LEXEDIR=/opt/gaussian/g09/linda-exe HOME=/home/winckler LOGNAME=winckler BYOBU_PYTHON=python2 LC_PAPER=pt_BR.UTF-8 MODULEPATH=/opt/modules JOB_REPOSITORY_ID=2016-04-15.16:49:42.0002420989.0000000000 _=/usr/bin/condor_ce_run LC_MONETARY=pt_BR.UTF-8 LC_TIME=pt_BR.UTF-8 G_BROKEN_FILENAMES=1 BYOBU_DISTRO=CentOS BYOBU_ACCENT=#75507B LANG=en_US.UTF-8 CONDOR_CONFIG=/etc/condor-ce/condor_config HISTSIZE=1000 BYOBU_PREFIX=/usr QTDIR=/usr/lib64/qt-3.3 BYOBU_DATE=%Y-%m-%d' ' BYOBU_CHARMAP=UTF-8 LCMAPS_POLICY_NAME= BYOBU_PAGER=less QTINC=/usr/lib64/qt-3.3/include TERM=screen-256color-bce WINDOW=0 LESSOPEN=||/usr/bin/lesspipe.sh' '%s LC_IDENTIFICATION=pt_BR.UTF-8"
RequestDisk = DiskUsage
Requirements = ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
MinHosts = 1
JobNotification = 0
NumCkpts = 0
NumJobStarts = 0
WantRemoteSyscalls = false
JobLeaseDuration = 2400
JobPrio = 0
RootDir = "/"
CurrentHosts = 0
x509UserProxyExpiration = 1460781781
WantRemoteIO = true
StreamOut = false
OnExitRemove = true
In = "/dev/null"
DiskUsage = 27
PeriodicRemove = false
LocalUserCpu = 0.0
ExecutableSize = 27
LocalSysCpu = 0.0
RemoteSysCpu = 0.0
ClusterId = 27
RemoteWallClockTime = 0.0
Rank = 0.0
LeaveJobInQueue = ( StageOutFinish > 0 ) =!= true
x509UserProxyEmail = "winckler@ncc.unesp.br"
CondorVersion = "$CondorVersion: 8.4.4 Feb 04 2016 $"
MyType = "Job"
StreamErr = false
DiskUsage_RAW = 26
PeriodicHold = false
User = "winckler@users.opensciencegrid.org"
Out = "_condor_stdout"
PeriodicRelease = false
MaxHosts = 1
RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,( ImageSize + 1023 ) / 1024)
Args = ""
CommittedSlotTime = 0
TotalSuspensions = 0
x509userproxysubject = "/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.7 $"
TransferInputSizeMB = 0
ExitStatus = 0
ShouldTransferFiles = "YES"
QDate = 1460738982
TransferOutputRemaps = undefined
SUBMIT_TransferOutputRemaps = "_condor_stdout=/home/winckler/.stdout_2420988_Df4HHD;_condor_stderr=/home/winckler/.stderr_2420988_xg41Wy"
EnteredCurrentStatus = 1460739065
ImageSize = 0
SUBMIT_UserLog = "/home/winckler/.log_2420988_k1WbE3"
x509userproxy = "x509up_u10001"
CompletionDate = 1460739449
RemoteUserCpu = 0
ManagedManager = "htcondor-ce"
JobStatus = 4
UserLog = ".log_2420988_k1WbE3"
ExitCode = 0
Cmd = "env"
GlobalJobId = "ce.grid.unesp.br#27.0#1460739064"
LastSuspensionTime = 0
LastJobStatus = 1
StageInFinish = 1460739064
SUBMIT_Cmd = "/bin/env"
HoldReason = undefined
Managed = "External"
HoldReasonCode = undefined
SUBMIT_x509userproxy = "/tmp/x509up_u10001"
Iwd = "/osg/condor/27/0/cluster27.proc0.subproc0"
StageOutStart = 1460739454
SUBMIT_Iwd = "/home/winckler"
ImageSize_RAW = 0
StageInStart = 1460739064
ReleaseReason = "Data files spooled"
RoutedToJobId = "28.0"
LastHoldReason = "Spooling input data files"
LastHoldReasonCode = 16
ProcId = 0
04/15/16 13:57:34 entering FileTransfer::SimpleInit
04/15/16 13:57:34 FILETRANSFER: protocol "http" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 13:57:34 FILETRANSFER: protocol "ftp" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 13:57:34 FILETRANSFER: protocol "file" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 13:57:34 FILETRANSFER: protocol "data" handled by "/usr/libexec/condor/data_plugin"
04/15/16 13:57:34 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0
04/15/16 13:57:34 set_user_egid() called when UserIds not inited!
04/15/16 13:57:34 set_user_euid() called when UserIds not inited!
04/15/16 13:57:34 set_user_egid() called when UserIds not inited!
04/15/16 13:57:34 set_user_euid() called when UserIds not inited!
04/15/16 13:57:34 set_user_egid() called when UserIds not inited!
04/15/16 13:57:34 set_user_euid() called when UserIds not inited!
04/15/16 13:57:34 set_user_egid() called when UserIds not inited!
04/15/16 13:57:34 set_user_euid() called when UserIds not inited!
04/15/16 13:57:34 set_user_egid() called when UserIds not inited!
04/15/16 13:57:34 set_user_euid() called when UserIds not inited!
04/15/16 13:57:34 entering FileTransfer::UploadFiles (final_transfer=1)
04/15/16 13:57:34 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0
04/15/16 13:57:34 Sending changed file .log_2420988_k1WbE3, t: 1460739362, 1460739064, s: 768, N/A
04/15/16 13:57:34 Skipping file env, t: 1460739064<=1460739064, s: N/A
04/15/16 13:57:34 Sending changed file _condor_stdout, t: 1460739356, 1460739064, s: 5472, N/A
04/15/16 13:57:34 Sending dynamically added output file _condor_stderr
04/15/16 13:57:34 entering FileTransfer::Upload
04/15/16 13:57:34 entering FileTransfer::DoUpload
04/15/16 13:57:34 DoUpload: sending file .log_2420988_k1WbE3
04/15/16 13:57:34 FILETRANSFER: outgoing file_command is 1 for .log_2420988_k1WbE3
04/15/16 13:57:34 Received GoAhead from peer to send /osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3 and all further files.
04/15/16 13:57:34 Sending GoAhead for 200.145.46.37 to receive /osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3 and all further files.
04/15/16 13:57:34 ReliSock::put_file_with_permissions(): going to send permissions 100644
04/15/16 13:57:34 put_file: going to send from filename /osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3
04/15/16 13:57:34 put_file: Found file size 768
04/15/16 13:57:34 put_file: sending 768 bytes
04/15/16 13:57:34 ReliSock: put_file: sent 768 bytes
04/15/16 13:57:34 DoUpload: sending file _condor_stdout
04/15/16 13:57:34 FILETRANSFER: outgoing file_command is 1 for _condor_stdout
04/15/16 13:57:34 ReliSock::put_file_with_permissions(): going to send permissions 100644
04/15/16 13:57:34 put_file: going to send from filename /osg/condor/27/0/cluster27.proc0.subproc0/_condor_stdout
04/15/16 13:57:34 put_file: Found file size 5472
04/15/16 13:57:34 put_file: sending 5472 bytes
04/15/16 13:57:34 ReliSock: put_file: sent 5472 bytes
04/15/16 13:57:34 DoUpload: sending file _condor_stderr
04/15/16 13:57:34 FILETRANSFER: outgoing file_command is 1 for _condor_stderr
04/15/16 13:57:34 ReliSock::put_file_with_permissions(): going to send permissions 100644
04/15/16 13:57:34 put_file: going to send from filename /osg/condor/27/0/cluster27.proc0.subproc0/_condor_stderr
04/15/16 13:57:34 put_file: Found file size 0
04/15/16 13:57:34 put_file: sending 0 bytes
04/15/16 13:57:34 ReliSock: put_file: sent 0 bytes
04/15/16 13:57:34 DoUpload: exiting at 3366
04/15/16 13:57:34 Transfer completed
04/15/16 13:57:34 DaemonCore: No more children processes to reap.
04/15/16 13:57:34 transferJobFilesReaper tid=1565004 status=256
04/15/16 13:57:43 QMGR Connection closed
04/15/16 13:57:43 Remove jobs 28.0
04/15/16 13:57:43 OwnerCheck retval 1 (success), super_user
04/15/16 13:57:43 OwnerCheck retval 1 (success), super_user
04/15/16 13:57:43 OwnerCheck retval 1 (success), super_user
04/15/16 13:57:43 OwnerCheck retval 1 (success), super_user
04/15/16 13:57:43 OwnerCheck retval 1 (success), super_user
04/15/16 13:57:43 Added data to SelfDrainingQueue act_on_job_myself_queue, now has 1 element(s)
04/15/16 13:57:43 Registered timer for SelfDrainingQueue act_on_job_myself_queue, period: 0 (id: 1722)
04/15/16 13:57:43 Expedited call to StartJobs()
04/15/16 13:57:43 Finished Remove jobs 28.0
04/15/16 13:57:43 -------- Begin starting jobs --------
04/15/16 13:57:43 -------- Done starting jobs --------
04/15/16 13:57:43 Inside SelfDrainingQueue::timerHandler() for act_on_job_myself_queue
04/15/16 13:57:43 abort_job_myself: 28.0 action:Remove log_hold:true
04/15/16 13:57:43 Cleared dirty attributes for job 28.0
04/15/16 13:57:43 Writing record to user logfile=/osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3 owner=winckler
04/15/16 13:57:43 WriteUserLog::initialize: opened /osg/condor/27/0/cluster27.proc0.subproc0/.log_2420988_k1WbE3 successfully
04/15/16 13:57:43 Saving classad to history file
04/15/16 13:57:43 SelfDrainingQueue act_on_job_myself_queue is empty, not resetting timer
04/15/16 13:57:43 Canceling timer for SelfDrainingQueue act_on_job_myself_queue (timer id: 1722)
04/15/16 13:57:43 QMGR Connection closed
04/15/16 13:57:52 Added data to SelfDrainingQueue job_is_finished_queue, now has 1 element(s)
04/15/16 13:57:52 Registered timer for SelfDrainingQueue job_is_finished_queue, period: 0 (id: 1724)
04/15/16 13:57:52 Job 27.0 is finished
04/15/16 13:57:52 Evaluated periodic expressions in 0.000s, scheduling next run in 300s
04/15/16 13:57:52 Inside SelfDrainingQueue::timerHandler() for job_is_finished_queue
04/15/16 13:57:52 Job cleanup for 27.0 will not block, calling jobIsFinished() directly
04/15/16 13:57:52 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0
04/15/16 13:57:52 jobIsFinished() completed, calling DestroyProc(27.0)
04/15/16 13:57:52 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0
04/15/16 13:57:52 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0
04/15/16 13:57:52 Initializing Directory: curr_dir = /osg/condor/27/0/cluster27.proc0.subproc0.tmp
04/15/16 13:57:52 Saving classad to history file
04/15/16 13:57:52 SelfDrainingQueue job_is_finished_queue is empty, not resetting timer
04/15/16 13:57:52 Canceling timer for SelfDrainingQueue job_is_finished_queue (timer id: 1724)
04/15/16 14:00:41 Getting monitoring info for pid 9936
04/15/16 14:01:10 Clearing userlog file cache
04/15/16 14:01:10 JobsRunning = 0
04/15/16 14:01:10 JobsIdle = 0
04/15/16 14:01:10 JobsHeld = 0
04/15/16 14:01:10 JobsRemoved = 0
04/15/16 14:01:10 LocalUniverseJobsRunning = 0
04/15/16 14:01:10 LocalUniverseJobsIdle = 0
04/15/16 14:01:10 SchedUniverseJobsRunning = 0
04/15/16 14:01:10 SchedUniverseJobsIdle = 0
04/15/16 14:01:10 N_Owners = 0
04/15/16 14:01:10 MaxJobsRunning = 10000
04/15/16 14:01:10 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:01:10 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:01:10 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:01:10 Trying to update collector <200.145.46.35:9619>
04/15/16 14:01:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Sent HEART BEAT ad to 1 collectors. Number of submittors=0
04/15/16 14:01:10 Changed attribute: IdleJobs = 0
04/15/16 14:01:10 Changed attribute: RunningJobs = 0
04/15/16 14:01:10 Changed attribute: IdleJobs = 0
04/15/16 14:01:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:01:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:01:10 Changed attribute: HeldJobs = 0
04/15/16 14:01:10 Changed attribute: FlockedJobs = 0
04/15/16 14:01:10 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:01:10 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:01:10 Trying to update collector <200.145.46.35:9619>
04/15/16 14:01:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:01:10 Changed attribute: IdleJobs = 0
04/15/16 14:01:10 Changed attribute: RunningJobs = 0
04/15/16 14:01:10 Changed attribute: IdleJobs = 0
04/15/16 14:01:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:01:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:01:10 Changed attribute: HeldJobs = 0
04/15/16 14:01:10 Changed attribute: FlockedJobs = 0
04/15/16 14:01:10 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:01:10 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:01:10 Trying to update collector <200.145.46.35:9619>
04/15/16 14:01:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:01:10 Changed attribute: IdleJobs = 0
04/15/16 14:01:10 Changed attribute: RunningJobs = 0
04/15/16 14:01:10 Changed attribute: IdleJobs = 0
04/15/16 14:01:10 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:01:10 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:01:10 Changed attribute: HeldJobs = 0
04/15/16 14:01:10 Changed attribute: FlockedJobs = 0
04/15/16 14:01:10 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:01:10 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:01:10 Trying to update collector <200.145.46.35:9619>
04/15/16 14:01:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:01:10 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:01:10 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:01:10 Trying to update collector <200.145.46.35:9619>
04/15/16 14:01:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:01:10 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:01:10 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:01:10 Trying to update collector <200.145.46.35:9619>
04/15/16 14:01:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:01:10 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:01:10 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:01:10 Trying to update collector <200.145.46.35:9619>
04/15/16 14:01:10 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:01:10 ============ Begin clean_shadow_recs =============
04/15/16 14:01:10 ============ End clean_shadow_recs =============
04/15/16 14:01:10 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:01:10 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:01:10 Trying to query collector <200.145.46.35:9619>
04/15/16 14:01:10 Can't find address for negotiator 
04/15/16 14:01:10 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:02:43 -------- Begin starting jobs --------
04/15/16 14:02:43 -------- Done starting jobs --------
04/15/16 14:02:52 Evaluated periodic expressions in 0.000s, scheduling next run in 301s
04/15/16 14:04:41 Getting monitoring info for pid 9936
04/15/16 14:05:43 DaemonCore: in SendAliveToParent()
04/15/16 14:05:43 DaemonCore: Leaving SendAliveToParent() - pending
04/15/16 14:05:43 SharedPortClient: sent connection request to daemon at <200.145.46.35:9619> for shared port id 9790_5514
04/15/16 14:05:43 Completed DC_CHILDALIVE to daemon at <200.145.46.35:9619>
04/15/16 14:06:10 Clearing userlog file cache
04/15/16 14:06:10 JobsRunning = 0
04/15/16 14:06:10 JobsIdle = 0
04/15/16 14:06:10 JobsHeld = 0
04/15/16 14:06:10 JobsRemoved = 0
04/15/16 14:06:10 LocalUniverseJobsRunning = 0
04/15/16 14:06:10 LocalUniverseJobsIdle = 0
04/15/16 14:06:10 SchedUniverseJobsRunning = 0
04/15/16 14:06:10 SchedUniverseJobsIdle = 0
04/15/16 14:06:10 N_Owners = 0
04/15/16 14:06:10 MaxJobsRunning = 10000
04/15/16 14:06:10 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:06:10 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:06:10 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:06:11 Trying to update collector <200.145.46.35:9619>
04/15/16 14:06:11 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Sent HEART BEAT ad to 1 collectors. Number of submittors=0
04/15/16 14:06:11 Changed attribute: IdleJobs = 0
04/15/16 14:06:11 Changed attribute: RunningJobs = 0
04/15/16 14:06:11 Changed attribute: IdleJobs = 0
04/15/16 14:06:11 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:06:11 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:06:11 Changed attribute: HeldJobs = 0
04/15/16 14:06:11 Changed attribute: FlockedJobs = 0
04/15/16 14:06:11 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:06:11 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:06:11 Trying to update collector <200.145.46.35:9619>
04/15/16 14:06:11 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:06:11 Changed attribute: IdleJobs = 0
04/15/16 14:06:11 Changed attribute: RunningJobs = 0
04/15/16 14:06:11 Changed attribute: IdleJobs = 0
04/15/16 14:06:11 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:06:11 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:06:11 Changed attribute: HeldJobs = 0
04/15/16 14:06:11 Changed attribute: FlockedJobs = 0
04/15/16 14:06:11 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:06:11 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:06:11 Trying to update collector <200.145.46.35:9619>
04/15/16 14:06:11 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:06:11 Changed attribute: IdleJobs = 0
04/15/16 14:06:11 Changed attribute: RunningJobs = 0
04/15/16 14:06:11 Changed attribute: IdleJobs = 0
04/15/16 14:06:11 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:06:11 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:06:11 Changed attribute: HeldJobs = 0
04/15/16 14:06:11 Changed attribute: FlockedJobs = 0
04/15/16 14:06:11 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:06:11 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:06:11 Trying to update collector <200.145.46.35:9619>
04/15/16 14:06:11 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:06:11 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:06:11 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:06:11 Trying to update collector <200.145.46.35:9619>
04/15/16 14:06:11 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:06:11 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:06:11 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:06:11 Trying to update collector <200.145.46.35:9619>
04/15/16 14:06:11 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:06:11 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:06:11 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:06:11 Trying to update collector <200.145.46.35:9619>
04/15/16 14:06:11 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:06:11 ============ Begin clean_shadow_recs =============
04/15/16 14:06:11 ============ End clean_shadow_recs =============
04/15/16 14:06:11 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:06:11 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:06:11 Trying to query collector <200.145.46.35:9619>
04/15/16 14:06:11 Can't find address for negotiator 
04/15/16 14:06:11 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:07:23 This process has a valid certificate & key
04/15/16 14:07:23 Finishing authenticate_server_gss_post with status=1
04/15/16 14:07:23 Finishing authenticate_server_gss_post with status=1
04/15/16 14:07:23 ZKM: successful mapping to GSS_ASSIST_GRIDMAP
04/15/16 14:07:23 Command=QMGMT_WRITE_CMD, peer=<200.145.46.37:59652>
04/15/16 14:07:23 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 14:07:23 OwnerCheck retval 1 (success),no ad
04/15/16 14:07:23 Submitting new job 29.0
04/15/16 14:07:23 schedd: NewCluster rval 29 errno 0
04/15/16 14:07:23 OwnerCheck retval 1 (success),no ad
04/15/16 14:07:23 schedd: NewProc rval 0 errno 0
04/15/16 14:07:23 New job: 29.0
04/15/16 14:07:23 Writing record to user logfile=/osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI owner=winckler
04/15/16 14:07:23 WriteUserLog::initialize: safe_open_wrapper("/osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI") failed - errno 13 (Permission denied)
04/15/16 14:07:23 WriteUserLog::initialize: failed to open file /osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI
04/15/16 14:07:23 WriteUserLog::initialize: opened /osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI successfully
04/15/16 14:07:23 New job: 29.0, Duplicate Keys: 2, Total Keys: 13 
04/15/16 14:07:23 QMGR Connection closed
04/15/16 14:07:23 Command=SPOOL_JOB_FILES_WITH_PERMS, peer=<200.145.46.37:65307>
04/15/16 14:07:23 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 14:07:23 spoolJobFiles(): read JobAdsArrayLen - 1
04/15/16 14:07:23 Looking at spooling: mode is 488
04/15/16 14:07:23 job_status is 5
04/15/16 14:07:23 Transferring files for jobs 29.0
04/15/16 14:07:23 generalJobFilesWorkerThread(): transfer files for job 29.0
04/15/16 14:07:23 spoolJobFiles(): started worker process
04/15/16 14:07:23 The submitting job ad as the FileTransferObject sees it
NumCkpts_RAW = 0
BufferSize = 524288
NiceUser = false
CoreSize = 0
CumulativeSlotTime = 0
OnExitHold = false
RequestCpus = 1
Err = "_condor_stderr"
BufferBlockSize = 32768
ExecutableSize_RAW = 18
ImageSize = 20
WantCheckpoint = false
CommittedTime = 0
TargetType = "Machine"
WhenToTransferOutput = "ON_EXIT"
JobUniverse = 5
ExitBySignal = false
HoldReasonCode = 16
TransferIn = false
NumRestarts = 0
EncryptExecuteDirectory = false
CommittedSuspensionTime = 0
Owner = "winckler"
NumSystemHolds = 0
CumulativeSuspensionTime = 0
Environment = "GLOBUS_TCP_SOURCE_RANGE=20000,29999 _LMFILES_=/opt/modules/gridunesp/1 BYOBU_SED=sed' '--follow-symlinks LC_MEASUREMENT=pt_BR.UTF-8 BYOBU_READLINK=readlink BYOBU_CONFIG_DIR=/home/winckler/.byobu LCMAPS_DEBUG_LEVEL=3 GV_DIR=/opt/gaussian/gv SHLVL=2 LS_COLORS= PWD=/home/winckler BYOBU_TIME=%H:%M:%S SSH_AUTH_SOCK=/home/winckler/.byobu/.ssh-agent BYOBU_WINDOWS=/home/winckler/.byobu/windows SSH_CLIENT=200.145.46.228' '49266' '22 CVS_RSH=ssh PGI_TERM=trace,abort LC_TELEPHONE=pt_BR.UTF-8 STY=2420116.byobu PATH=/home/winckler/bin:/usr/local/src/goo-client/scripts:/home/winckler/bin:/opt/gridunesp/internals//bin:/usr/lib64/qt-3.3/bin:/usr/local/src/goo-client/scripts:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/home/winckler/bin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 BYOBU_RUN_DIR=/dev/shm/byobu-winckler-yXoDFpVj TOOL_DEBUG=D_FULLDEBUG MODULESHOME=/usr/share/Modules G09BASIS=/opt/gaussian/g09/basis GAUSS_EXEDIR=/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 X509_CERT_DIR=/etc/grid-security/certificates GLOBUS_TCP_PORT_RANGE=20000,29999 QTLIB=/usr/lib64/qt-3.3/lib LC_NAME=pt_BR.UTF-8 GATEKEEPER_JM_ID=2016-04-15.17:06:01.0002423004.0000000000 HISTCONTROL=ignoredups LCMAPS_DIR=/etc BYOBU_DARK=black LC_NUMERIC=pt_BR.UTF-8 GOO_API_URI=https://submit.grid.unesp.br/api/v1/ g09root=/opt/gaussian _DSM_BARRIER=SHM BYOBU_ULIMIT=ulimit SSH_TTY=/dev/pts/20 _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI MYPROXY_SERVER=myproxy.grid.unesp.br SHELL=/bin/bash GAUSS_SCRDIR=/tmp MAIL=/var/spool/mail/winckler GAUSS_ARCHDIR=/opt/gaussian/g09/arch BYOBU_HIGHLIGHT=#DD4814 LCMAPS_DB_FILE=/etc/lcmaps/lcmaps.db LC_CTYPE=pt_BR.UTF-8 LC_ADDRESS=pt_BR.UTF-8 LOADEDMODULES=gridunesp/1 BYOBU_LIGHT=white USER=winckler SSH_CONNECTION=200.145.46.228' '49266' '200.145.46.37' '22 HOSTNAME=access.grid.unesp.br GRIDUNESP_SLURM_PARTITIONS=rack1,rack2,rack3,rack4,rack5,rack6,rack7,rack8 LD_LIBRARY_PATH=/opt/gridunesp/internals//lib:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/opt/gaussian/gv/lib PYTHONPATH=/usr/local/src/python-gooclientlib:/usr/local/src/goo-client:/usr/local/src/python-gooclientlib:/usr/local/src/goo-client: BYOBU_TTY=/dev/pts/20 BYOBU_WINDOW_NAME=- BYOBU_BACKEND=screen GAUSS_LEXEDIR=/opt/gaussian/g09/linda-exe HOME=/home/winckler LOGNAME=winckler BYOBU_PYTHON=python2 LC_PAPER=pt_BR.UTF-8 MODULEPATH=/opt/modules JOB_REPOSITORY_ID=2016-04-15.17:06:01.0002423004.0000000000 _=/usr/bin/condor_ce_run LC_MONETARY=pt_BR.UTF-8 LC_TIME=pt_BR.UTF-8 G_BROKEN_FILENAMES=1 BYOBU_DISTRO=CentOS BYOBU_ACCENT=#75507B LANG=en_US.UTF-8 CONDOR_CONFIG=/etc/condor-ce/condor_config HISTSIZE=1000 BYOBU_PREFIX=/usr QTDIR=/usr/lib64/qt-3.3 BYOBU_DATE=%Y-%m-%d' ' BYOBU_CHARMAP=UTF-8 LCMAPS_POLICY_NAME= BYOBU_PAGER=less QTINC=/usr/lib64/qt-3.3/include TERM=screen-256color-bce WINDOW=0 LESSOPEN=||/usr/bin/lesspipe.sh' '%s LC_IDENTIFICATION=pt_BR.UTF-8"
RequestDisk = DiskUsage
Requirements = ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
MinHosts = 1
JobNotification = 0
NumCkpts = 0
LastSuspensionTime = 0
NumJobStarts = 0
WantRemoteSyscalls = false
JobLeaseDuration = 2400
JobPrio = 0
RootDir = "/"
CurrentHosts = 0
x509UserProxyExpiration = 1460781781
WantRemoteIO = true
StreamOut = false
OnExitRemove = true
In = "/dev/null"
DiskUsage = 20
PeriodicRemove = false
LocalUserCpu = 0.0
RemoteUserCpu = 0.0
ExecutableSize = 20
LocalSysCpu = 0.0
RemoteSysCpu = 0.0
ClusterId = 29
CompletionDate = 0
RemoteWallClockTime = 0.0
Rank = 0.0
LeaveJobInQueue = ( StageOutFinish > 0 ) =!= true
ImageSize_RAW = 18
x509UserProxyEmail = "winckler@ncc.unesp.br"
CondorVersion = "$CondorVersion: 8.4.4 Feb 04 2016 $"
MyType = "Job"
HoldReason = "Spooling input data files"
StreamErr = false
DiskUsage_RAW = 18
PeriodicHold = false
User = "winckler@users.opensciencegrid.org"
Out = "_condor_stdout"
PeriodicRelease = false
MaxHosts = 1
RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,( ImageSize + 1023 ) / 1024)
Args = ""
CommittedSlotTime = 0
TotalSuspensions = 0
x509userproxysubject = "/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.7 $"
TransferInputSizeMB = 0
ExitStatus = 0
ShouldTransferFiles = "YES"
EnteredCurrentStatus = 1460739961
QDate = 1460739961
SUBMIT_Cmd = "/bin/hostname"
SUBMIT_x509userproxy = "/tmp/x509up_u10001"
ProcId = 0
x509userproxy = "x509up_u10001"
Iwd = "/osg/condor/29/0/cluster29.proc0.subproc0"
SUBMIT_TransferOutputRemaps = "_condor_stdout=/home/winckler/.stdout_2423003_eCP6tB;_condor_stderr=/home/winckler/.stderr_2423003__KuXP1"
SUBMIT_UserLog = "/home/winckler/.log_2423003_NYp6fI"
Cmd = "hostname"
GlobalJobId = "ce.grid.unesp.br#29.0#1460740043"
JobStatus = 5
SUBMIT_Iwd = "/home/winckler"
TransferOutputRemaps = undefined
UserLog = ".log_2423003_NYp6fI"
StageInStart = 1460740043
04/15/16 14:07:23 entering FileTransfer::SimpleInit
04/15/16 14:07:23 FILETRANSFER: protocol "http" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:07:23 FILETRANSFER: protocol "ftp" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:07:23 FILETRANSFER: protocol "file" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:07:23 FILETRANSFER: protocol "data" handled by "/usr/libexec/condor/data_plugin"
04/15/16 14:07:23 entering FileTransfer::DownloadFiles
04/15/16 14:07:23 entering FileTransfer::Download
04/15/16 14:07:23 entering FileTransfer::DoDownload sync=1
04/15/16 14:07:23 Sending GoAhead for 200.145.46.37 to send /osg/condor/29/0/cluster29.proc0.subproc0.tmp/x509up_u10001 and all further files.
04/15/16 14:07:23 Received GoAhead from peer to receive /osg/condor/29/0/cluster29.proc0.subproc0.tmp/x509up_u10001 and all further files.
04/15/16 14:07:23 DoDownload: get_x509_delegation() returned 0
04/15/16 14:07:23 get_file(): going to write to filename /osg/condor/29/0/cluster29.proc0.subproc0.tmp/hostname
04/15/16 14:07:23 get_file: Receiving 17848 bytes
04/15/16 14:07:23 get_file: wrote 17848 bytes to file
04/15/16 14:07:23 ReliSock::get_file_with_permissions(): going to set permissions 755
04/15/16 14:07:23 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0.tmp
04/15/16 14:07:23 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0.swap
04/15/16 14:07:23 Received proxy for job 29.0
04/15/16 14:07:23 proxy path: /osg/condor/29/0/cluster29.proc0.subproc0/x509up_u10001
04/15/16 14:07:23 proxy expiration: 1460781781
04/15/16 14:07:23 proxy identity: /C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler
04/15/16 14:07:23 proxy subject: /C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler/CN=2025260236/CN=1114492430
04/15/16 14:07:23 proxy email: winckler@ncc.unesp.br
04/15/16 14:07:23 Transfer completed
04/15/16 14:07:23 Scheduler::spoolJobFilesWorkerThread(void *arg, Stream* s) NAP TIME
04/15/16 14:07:23 -------- Begin starting jobs --------
04/15/16 14:07:23 -------- Done starting jobs --------
04/15/16 14:07:23 Clearing userlog file cache
04/15/16 14:07:23 JobsRunning = 0
04/15/16 14:07:23 JobsIdle = 0
04/15/16 14:07:23 JobsHeld = 1
04/15/16 14:07:23 JobsRemoved = 0
04/15/16 14:07:23 LocalUniverseJobsRunning = 0
04/15/16 14:07:23 LocalUniverseJobsIdle = 0
04/15/16 14:07:23 SchedUniverseJobsRunning = 0
04/15/16 14:07:23 SchedUniverseJobsIdle = 0
04/15/16 14:07:23 N_Owners = 1
04/15/16 14:07:23 MaxJobsRunning = 10000
04/15/16 14:07:23 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:07:23 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:07:23 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:07:23 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:23 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:23 Sent HEART BEAT ad to 1 collectors. Number of submittors=1
04/15/16 14:07:23 Changed attribute: IdleJobs = 0
04/15/16 14:07:23 Changed attribute: RunningJobs = 0
04/15/16 14:07:23 Changed attribute: IdleJobs = 0
04/15/16 14:07:23 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:07:23 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:07:23 Changed attribute: HeldJobs = 0
04/15/16 14:07:23 Changed attribute: FlockedJobs = 0
04/15/16 14:07:23 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:07:23 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:07:23 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:23 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:23 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:07:23 Changed attribute: IdleJobs = 0
04/15/16 14:07:23 Changed attribute: RunningJobs = 0
04/15/16 14:07:23 Changed attribute: IdleJobs = 0
04/15/16 14:07:23 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:07:23 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:07:23 Changed attribute: HeldJobs = 0
04/15/16 14:07:23 Changed attribute: FlockedJobs = 0
04/15/16 14:07:23 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:07:23 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:07:23 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:23 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:23 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:07:23 Changed attribute: IdleJobs = 0
04/15/16 14:07:23 Changed attribute: RunningJobs = 0
04/15/16 14:07:23 Changed attribute: IdleJobs = 0
04/15/16 14:07:23 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:07:23 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:07:23 Changed attribute: HeldJobs = 1
04/15/16 14:07:23 Changed attribute: FlockedJobs = 0
04/15/16 14:07:23 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:07:23 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:07:23 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:23 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:23 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:07:23 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:07:23 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:07:23 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:23 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:23 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:07:23 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:07:23 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:07:23 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:23 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:23 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:07:23 ============ Begin clean_shadow_recs =============
04/15/16 14:07:23 ============ End clean_shadow_recs =============
04/15/16 14:07:23 Job 29.0 held for spooling. Checking how long...
04/15/16 14:07:23 Job 29.0 on hold for 0 seconds.
04/15/16 14:07:23 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:07:23 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:23 Trying to query collector <200.145.46.35:9619>
04/15/16 14:07:23 Can't find address for negotiator 
04/15/16 14:07:23 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:07:23 ForkWorker::Fork: New child of 9936 = 1604082
04/15/16 14:07:23 Number of Active Workers 0
04/15/16 14:07:23 DaemonCore: No more children processes to reap.
04/15/16 14:07:24 DaemonCore: No more children processes to reap.
04/15/16 14:07:24 spoolJobFilesReaper tid=1604071 status=256
04/15/16 14:07:24 No HoldReasonSubCode found for job 29.0
04/15/16 14:07:24 Job 29.0 released from hold: Data files spooled
04/15/16 14:07:24 Setting delay until next queue scan to 5 seconds
04/15/16 14:07:24 -------- Begin starting jobs --------
04/15/16 14:07:24 -------- Done starting jobs --------
04/15/16 14:07:24 ForkWorker::Fork: New child of 9936 = 1604150
04/15/16 14:07:24 Number of Active Workers 0
04/15/16 14:07:24 DaemonCore: No more children processes to reap.
04/15/16 14:07:25 ForkWorker::Fork: New child of 9936 = 1604198
04/15/16 14:07:25 Number of Active Workers 0
04/15/16 14:07:25 DaemonCore: No more children processes to reap.
04/15/16 14:07:26 ForkWorker::Fork: New child of 9936 = 1604221
04/15/16 14:07:26 Number of Active Workers 0
04/15/16 14:07:26 DaemonCore: No more children processes to reap.
04/15/16 14:07:27 ForkWorker::Fork: New child of 9936 = 1604223
04/15/16 14:07:27 Number of Active Workers 0
04/15/16 14:07:27 DaemonCore: No more children processes to reap.
04/15/16 14:07:29 JobsRunning = 0
04/15/16 14:07:29 JobsIdle = 1
04/15/16 14:07:29 JobsHeld = 0
04/15/16 14:07:29 JobsRemoved = 0
04/15/16 14:07:29 LocalUniverseJobsRunning = 0
04/15/16 14:07:29 LocalUniverseJobsIdle = 0
04/15/16 14:07:29 SchedUniverseJobsRunning = 0
04/15/16 14:07:29 SchedUniverseJobsIdle = 0
04/15/16 14:07:29 N_Owners = 1
04/15/16 14:07:29 MaxJobsRunning = 10000
04/15/16 14:07:29 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:07:29 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:07:29 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:07:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:29 Sent HEART BEAT ad to 1 collectors. Number of submittors=1
04/15/16 14:07:29 Changed attribute: IdleJobs = 0
04/15/16 14:07:29 Changed attribute: RunningJobs = 0
04/15/16 14:07:29 Changed attribute: IdleJobs = 0
04/15/16 14:07:29 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:07:29 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:07:29 Changed attribute: HeldJobs = 0
04/15/16 14:07:29 Changed attribute: FlockedJobs = 0
04/15/16 14:07:29 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:07:29 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:07:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:29 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:07:29 Changed attribute: IdleJobs = 0
04/15/16 14:07:29 Changed attribute: RunningJobs = 0
04/15/16 14:07:29 Changed attribute: IdleJobs = 0
04/15/16 14:07:29 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:07:29 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:07:29 Changed attribute: HeldJobs = 0
04/15/16 14:07:29 Changed attribute: FlockedJobs = 0
04/15/16 14:07:29 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:07:29 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:07:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:29 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:07:29 Changed attribute: IdleJobs = 1
04/15/16 14:07:29 Changed attribute: RunningJobs = 0
04/15/16 14:07:29 Changed attribute: IdleJobs = 1
04/15/16 14:07:29 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:07:29 Changed attribute: WeightedIdleJobs = 1
04/15/16 14:07:29 Changed attribute: HeldJobs = 0
04/15/16 14:07:29 Changed attribute: FlockedJobs = 0
04/15/16 14:07:29 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:07:29 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:07:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:29 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:07:29 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:07:29 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:07:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:29 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:07:29 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:07:29 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:07:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:07:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:29 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:07:29 ============ Begin clean_shadow_recs =============
04/15/16 14:07:29 ============ End clean_shadow_recs =============
04/15/16 14:07:29 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:07:29 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:07:29 Trying to query collector <200.145.46.35:9619>
04/15/16 14:07:29 Can't find address for negotiator 
04/15/16 14:07:29 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:07:29 ForkWorker::Fork: New child of 9936 = 1604227
04/15/16 14:07:29 Number of Active Workers 0
04/15/16 14:07:29 DaemonCore: No more children processes to reap.
04/15/16 14:07:30 ForkWorker::Fork: New child of 9936 = 1604239
04/15/16 14:07:30 Number of Active Workers 0
04/15/16 14:07:30 DaemonCore: No more children processes to reap.
04/15/16 14:07:31 ForkWorker::Fork: New child of 9936 = 1604324
04/15/16 14:07:31 Number of Active Workers 0
04/15/16 14:07:31 DaemonCore: No more children processes to reap.
04/15/16 14:07:32 ForkWorker::Fork: New child of 9936 = 1604423
04/15/16 14:07:32 Number of Active Workers 0
04/15/16 14:07:32 DaemonCore: No more children processes to reap.
04/15/16 14:07:33 QMGR Connection closed
04/15/16 14:07:33 OwnerCheck retval 1 (success),no ad
04/15/16 14:07:33 Submitting new job 30.0
04/15/16 14:07:33 schedd: NewCluster rval 30 errno 0
04/15/16 14:07:33 OwnerCheck retval 1 (success),no ad
04/15/16 14:07:33 schedd: NewProc rval 0 errno 0
04/15/16 14:07:33 New job: 30.0
04/15/16 14:07:33 New job: 30.0, Duplicate Keys: 2, Total Keys: 102 
04/15/16 14:07:33 QMGR Connection closed
04/15/16 14:07:33 ForkWorker::Fork: New child of 9936 = 1604558
04/15/16 14:07:33 Number of Active Workers 0
04/15/16 14:07:33 DaemonCore: No more children processes to reap.
04/15/16 14:07:34 ForkWorker::Fork: New child of 9936 = 1604655
04/15/16 14:07:34 Number of Active Workers 0
04/15/16 14:07:34 DaemonCore: No more children processes to reap.
04/15/16 14:07:35 ForkWorker::Fork: New child of 9936 = 1604797
04/15/16 14:07:35 Number of Active Workers 0
04/15/16 14:07:35 DaemonCore: No more children processes to reap.
04/15/16 14:07:36 ForkWorker::Fork: New child of 9936 = 1604804
04/15/16 14:07:36 Number of Active Workers 0
04/15/16 14:07:36 DaemonCore: No more children processes to reap.
04/15/16 14:07:37 ForkWorker::Fork: New child of 9936 = 1604806
04/15/16 14:07:37 Number of Active Workers 0
04/15/16 14:07:37 DaemonCore: No more children processes to reap.
04/15/16 14:07:38 ForkWorker::Fork: New child of 9936 = 1604842
04/15/16 14:07:38 Number of Active Workers 0
04/15/16 14:07:38 DaemonCore: No more children processes to reap.
04/15/16 14:07:39 ForkWorker::Fork: New child of 9936 = 1604893
04/15/16 14:07:39 Number of Active Workers 0
04/15/16 14:07:39 DaemonCore: No more children processes to reap.
04/15/16 14:07:40 ForkWorker::Fork: New child of 9936 = 1604934
04/15/16 14:07:40 Number of Active Workers 0
04/15/16 14:07:40 DaemonCore: No more children processes to reap.
04/15/16 14:07:41 ForkWorker::Fork: New child of 9936 = 1604936
04/15/16 14:07:41 Number of Active Workers 0
04/15/16 14:07:41 DaemonCore: No more children processes to reap.
04/15/16 14:07:42 ForkWorker::Fork: New child of 9936 = 1604979
04/15/16 14:07:42 Number of Active Workers 0
04/15/16 14:07:42 DaemonCore: No more children processes to reap.
04/15/16 14:07:43 QMGR Connection closed
04/15/16 14:07:43 ForkWorker::Fork: New child of 9936 = 1605080
04/15/16 14:07:43 Number of Active Workers 0
04/15/16 14:07:43 DaemonCore: No more children processes to reap.
04/15/16 14:07:44 -------- Begin starting jobs --------
04/15/16 14:07:44 -------- Done starting jobs --------
04/15/16 14:07:45 ForkWorker::Fork: New child of 9936 = 1605175
04/15/16 14:07:45 Number of Active Workers 0
04/15/16 14:07:45 DaemonCore: No more children processes to reap.
04/15/16 14:07:46 ForkWorker::Fork: New child of 9936 = 1605268
04/15/16 14:07:46 Number of Active Workers 0
04/15/16 14:07:46 DaemonCore: No more children processes to reap.
04/15/16 14:07:47 ForkWorker::Fork: New child of 9936 = 1605357
04/15/16 14:07:47 Number of Active Workers 0
04/15/16 14:07:47 DaemonCore: No more children processes to reap.
04/15/16 14:07:48 ForkWorker::Fork: New child of 9936 = 1605425
04/15/16 14:07:48 Number of Active Workers 0
04/15/16 14:07:48 DaemonCore: No more children processes to reap.
04/15/16 14:07:49 ForkWorker::Fork: New child of 9936 = 1605471
04/15/16 14:07:49 Number of Active Workers 0
04/15/16 14:07:49 DaemonCore: No more children processes to reap.
04/15/16 14:07:50 ForkWorker::Fork: New child of 9936 = 1605523
04/15/16 14:07:50 Number of Active Workers 0
04/15/16 14:07:50 DaemonCore: No more children processes to reap.
04/15/16 14:07:51 ForkWorker::Fork: New child of 9936 = 1605557
04/15/16 14:07:51 Number of Active Workers 0
04/15/16 14:07:51 DaemonCore: No more children processes to reap.
04/15/16 14:07:52 ForkWorker::Fork: New child of 9936 = 1605558
04/15/16 14:07:52 Number of Active Workers 0
04/15/16 14:07:52 DaemonCore: No more children processes to reap.
04/15/16 14:07:53 Evaluated periodic expressions in 0.000s, scheduling next run in 300s
04/15/16 14:07:53 ForkWorker::Fork: New child of 9936 = 1605567
04/15/16 14:07:53 Number of Active Workers 0
04/15/16 14:07:53 DaemonCore: No more children processes to reap.
04/15/16 14:07:54 ForkWorker::Fork: New child of 9936 = 1605637
04/15/16 14:07:54 Number of Active Workers 0
04/15/16 14:07:54 DaemonCore: No more children processes to reap.
04/15/16 14:07:55 ForkWorker::Fork: New child of 9936 = 1605734
04/15/16 14:07:55 Number of Active Workers 0
04/15/16 14:07:55 DaemonCore: No more children processes to reap.
04/15/16 14:07:56 ForkWorker::Fork: New child of 9936 = 1605793
04/15/16 14:07:56 Number of Active Workers 0
04/15/16 14:07:56 DaemonCore: No more children processes to reap.
04/15/16 14:07:57 ForkWorker::Fork: New child of 9936 = 1605796
04/15/16 14:07:57 Number of Active Workers 0
04/15/16 14:07:57 DaemonCore: No more children processes to reap.
04/15/16 14:07:57 Received a superuser command
04/15/16 14:07:57 ForkWorker::Fork: New child of 9936 = 1605797
04/15/16 14:07:57 Number of Active Workers 0
04/15/16 14:07:57 DaemonCore: No more children processes to reap.
04/15/16 14:07:58 ForkWorker::Fork: New child of 9936 = 1605858
04/15/16 14:07:58 Number of Active Workers 0
04/15/16 14:07:58 DaemonCore: No more children processes to reap.
04/15/16 14:07:59 ForkWorker::Fork: New child of 9936 = 1605949
04/15/16 14:07:59 Number of Active Workers 0
04/15/16 14:07:59 DaemonCore: No more children processes to reap.
04/15/16 14:08:00 Received a superuser command
04/15/16 14:08:00 ForkWorker::Fork: New child of 9936 = 1605974
04/15/16 14:08:00 Number of Active Workers 0
04/15/16 14:08:00 DaemonCore: No more children processes to reap.
04/15/16 14:08:00 ForkWorker::Fork: New child of 9936 = 1606036
04/15/16 14:08:00 Number of Active Workers 0
04/15/16 14:08:00 DaemonCore: No more children processes to reap.
04/15/16 14:08:01 Received a superuser command
04/15/16 14:08:01 ForkWorker::Fork: New child of 9936 = 1606342
04/15/16 14:08:01 Number of Active Workers 0
04/15/16 14:08:01 DaemonCore: No more children processes to reap.
04/15/16 14:08:02 ForkWorker::Fork: New child of 9936 = 1606347
04/15/16 14:08:02 Number of Active Workers 0
04/15/16 14:08:02 DaemonCore: No more children processes to reap.
04/15/16 14:08:02 Received a superuser command
04/15/16 14:08:02 ForkWorker::Fork: New child of 9936 = 1606351
04/15/16 14:08:02 Number of Active Workers 0
04/15/16 14:08:02 DaemonCore: No more children processes to reap.
04/15/16 14:08:03 ForkWorker::Fork: New child of 9936 = 1606352
04/15/16 14:08:03 Number of Active Workers 0
04/15/16 14:08:03 DaemonCore: No more children processes to reap.
04/15/16 14:08:04 ForkWorker::Fork: New child of 9936 = 1606388
04/15/16 14:08:04 Number of Active Workers 0
04/15/16 14:08:04 DaemonCore: No more children processes to reap.
04/15/16 14:08:05 ForkWorker::Fork: New child of 9936 = 1606492
04/15/16 14:08:05 Number of Active Workers 0
04/15/16 14:08:05 DaemonCore: No more children processes to reap.
04/15/16 14:08:05 Received a superuser command
04/15/16 14:08:05 ForkWorker::Fork: New child of 9936 = 1606555
04/15/16 14:08:05 Number of Active Workers 0
04/15/16 14:08:05 DaemonCore: No more children processes to reap.
04/15/16 14:08:06 ForkWorker::Fork: New child of 9936 = 1606584
04/15/16 14:08:06 Number of Active Workers 0
04/15/16 14:08:06 DaemonCore: No more children processes to reap.
04/15/16 14:08:07 ForkWorker::Fork: New child of 9936 = 1606607
04/15/16 14:08:07 Number of Active Workers 0
04/15/16 14:08:07 DaemonCore: No more children processes to reap.
04/15/16 14:08:08 Received a superuser command
04/15/16 14:08:08 ForkWorker::Fork: New child of 9936 = 1606610
04/15/16 14:08:08 Number of Active Workers 0
04/15/16 14:08:08 DaemonCore: No more children processes to reap.
04/15/16 14:08:08 ForkWorker::Fork: New child of 9936 = 1606620
04/15/16 14:08:08 Number of Active Workers 0
04/15/16 14:08:08 DaemonCore: No more children processes to reap.
04/15/16 14:08:09 ForkWorker::Fork: New child of 9936 = 1606701
04/15/16 14:08:09 Number of Active Workers 0
04/15/16 14:08:09 DaemonCore: No more children processes to reap.
04/15/16 14:08:10 ForkWorker::Fork: New child of 9936 = 1606794
04/15/16 14:08:10 Number of Active Workers 0
04/15/16 14:08:10 DaemonCore: No more children processes to reap.
04/15/16 14:08:10 Received a superuser command
04/15/16 14:08:10 ForkWorker::Fork: New child of 9936 = 1606838
04/15/16 14:08:10 Number of Active Workers 0
04/15/16 14:08:10 DaemonCore: No more children processes to reap.
04/15/16 14:08:11 ForkWorker::Fork: New child of 9936 = 1606867
04/15/16 14:08:11 Number of Active Workers 0
04/15/16 14:08:11 DaemonCore: No more children processes to reap.
04/15/16 14:08:12 ForkWorker::Fork: New child of 9936 = 1606898
04/15/16 14:08:12 Number of Active Workers 0
04/15/16 14:08:12 DaemonCore: No more children processes to reap.
04/15/16 14:08:13 Received a superuser command
04/15/16 14:08:13 ForkWorker::Fork: New child of 9936 = 1606901
04/15/16 14:08:13 Number of Active Workers 0
04/15/16 14:08:13 DaemonCore: No more children processes to reap.
04/15/16 14:08:13 ForkWorker::Fork: New child of 9936 = 1606949
04/15/16 14:08:13 Number of Active Workers 0
04/15/16 14:08:13 DaemonCore: No more children processes to reap.
04/15/16 14:08:14 ForkWorker::Fork: New child of 9936 = 1606992
04/15/16 14:08:14 Number of Active Workers 0
04/15/16 14:08:14 DaemonCore: No more children processes to reap.
04/15/16 14:08:15 Received a superuser command
04/15/16 14:08:15 ForkWorker::Fork: New child of 9936 = 1607062
04/15/16 14:08:15 Number of Active Workers 0
04/15/16 14:08:15 DaemonCore: No more children processes to reap.
04/15/16 14:08:15 ForkWorker::Fork: New child of 9936 = 1607106
04/15/16 14:08:15 Number of Active Workers 0
04/15/16 14:08:15 DaemonCore: No more children processes to reap.
04/15/16 14:08:16 ForkWorker::Fork: New child of 9936 = 1607181
04/15/16 14:08:16 Number of Active Workers 0
04/15/16 14:08:16 DaemonCore: No more children processes to reap.
04/15/16 14:08:17 Received a superuser command
04/15/16 14:08:17 ForkWorker::Fork: New child of 9936 = 1607210
04/15/16 14:08:17 Number of Active Workers 0
04/15/16 14:08:17 DaemonCore: No more children processes to reap.
04/15/16 14:08:17 ForkWorker::Fork: New child of 9936 = 1607235
04/15/16 14:08:17 Number of Active Workers 0
04/15/16 14:08:17 DaemonCore: No more children processes to reap.
04/15/16 14:08:19 ForkWorker::Fork: New child of 9936 = 1607293
04/15/16 14:08:19 Number of Active Workers 0
04/15/16 14:08:19 DaemonCore: No more children processes to reap.
04/15/16 14:08:20 ForkWorker::Fork: New child of 9936 = 1607355
04/15/16 14:08:20 Number of Active Workers 0
04/15/16 14:08:20 DaemonCore: No more children processes to reap.
04/15/16 14:08:20 Received a superuser command
04/15/16 14:08:20 ForkWorker::Fork: New child of 9936 = 1607358
04/15/16 14:08:20 Number of Active Workers 0
04/15/16 14:08:20 DaemonCore: No more children processes to reap.
04/15/16 14:08:21 ForkWorker::Fork: New child of 9936 = 1607359
04/15/16 14:08:21 Number of Active Workers 0
04/15/16 14:08:21 DaemonCore: No more children processes to reap.
04/15/16 14:08:22 ForkWorker::Fork: New child of 9936 = 1607404
04/15/16 14:08:22 Number of Active Workers 0
04/15/16 14:08:22 DaemonCore: No more children processes to reap.
04/15/16 14:08:22 Received a superuser command
04/15/16 14:08:22 ForkWorker::Fork: New child of 9936 = 1607416
04/15/16 14:08:22 Number of Active Workers 0
04/15/16 14:08:22 DaemonCore: No more children processes to reap.
04/15/16 14:08:23 ForkWorker::Fork: New child of 9936 = 1607417
04/15/16 14:08:23 Number of Active Workers 0
04/15/16 14:08:23 DaemonCore: No more children processes to reap.
04/15/16 14:08:24 ForkWorker::Fork: New child of 9936 = 1607436
04/15/16 14:08:24 Number of Active Workers 0
04/15/16 14:08:24 DaemonCore: No more children processes to reap.
04/15/16 14:08:25 ForkWorker::Fork: New child of 9936 = 1607472
04/15/16 14:08:25 Number of Active Workers 0
04/15/16 14:08:25 DaemonCore: No more children processes to reap.
04/15/16 14:08:26 Received a superuser command
04/15/16 14:08:26 ForkWorker::Fork: New child of 9936 = 1607494
04/15/16 14:08:26 Number of Active Workers 0
04/15/16 14:08:26 DaemonCore: No more children processes to reap.
04/15/16 14:08:26 ForkWorker::Fork: New child of 9936 = 1607495
04/15/16 14:08:26 Number of Active Workers 0
04/15/16 14:08:26 DaemonCore: No more children processes to reap.
04/15/16 14:08:27 ForkWorker::Fork: New child of 9936 = 1607574
04/15/16 14:08:27 Number of Active Workers 0
04/15/16 14:08:27 DaemonCore: No more children processes to reap.
04/15/16 14:08:28 ForkWorker::Fork: New child of 9936 = 1607580
04/15/16 14:08:28 Number of Active Workers 0
04/15/16 14:08:28 DaemonCore: No more children processes to reap.
04/15/16 14:08:28 Received a superuser command
04/15/16 14:08:28 ForkWorker::Fork: New child of 9936 = 1607583
04/15/16 14:08:28 Number of Active Workers 0
04/15/16 14:08:28 DaemonCore: No more children processes to reap.
04/15/16 14:08:29 ForkWorker::Fork: New child of 9936 = 1607629
04/15/16 14:08:29 Number of Active Workers 0
04/15/16 14:08:29 DaemonCore: No more children processes to reap.
04/15/16 14:08:30 ForkWorker::Fork: New child of 9936 = 1607638
04/15/16 14:08:30 Number of Active Workers 0
04/15/16 14:08:30 DaemonCore: No more children processes to reap.
04/15/16 14:08:31 ForkWorker::Fork: New child of 9936 = 1607645
04/15/16 14:08:31 Number of Active Workers 0
04/15/16 14:08:31 DaemonCore: No more children processes to reap.
04/15/16 14:08:32 Received a superuser command
04/15/16 14:08:32 ForkWorker::Fork: New child of 9936 = 1607702
04/15/16 14:08:32 Number of Active Workers 0
04/15/16 14:08:32 DaemonCore: No more children processes to reap.
04/15/16 14:08:32 ForkWorker::Fork: New child of 9936 = 1607705
04/15/16 14:08:32 Number of Active Workers 0
04/15/16 14:08:32 DaemonCore: No more children processes to reap.
04/15/16 14:08:33 ForkWorker::Fork: New child of 9936 = 1607709
04/15/16 14:08:33 Number of Active Workers 0
04/15/16 14:08:33 DaemonCore: No more children processes to reap.
04/15/16 14:08:34 Received a superuser command
04/15/16 14:08:34 ForkWorker::Fork: New child of 9936 = 1607720
04/15/16 14:08:34 Number of Active Workers 0
04/15/16 14:08:34 DaemonCore: No more children processes to reap.
04/15/16 14:08:34 ForkWorker::Fork: New child of 9936 = 1607768
04/15/16 14:08:34 Number of Active Workers 0
04/15/16 14:08:34 DaemonCore: No more children processes to reap.
04/15/16 14:08:35 ForkWorker::Fork: New child of 9936 = 1607777
04/15/16 14:08:35 Number of Active Workers 0
04/15/16 14:08:35 DaemonCore: No more children processes to reap.
04/15/16 14:08:36 Received a superuser command
04/15/16 14:08:36 ForkWorker::Fork: New child of 9936 = 1607779
04/15/16 14:08:36 Number of Active Workers 0
04/15/16 14:08:36 DaemonCore: No more children processes to reap.
04/15/16 14:08:37 ForkWorker::Fork: New child of 9936 = 1607826
04/15/16 14:08:37 Number of Active Workers 0
04/15/16 14:08:37 DaemonCore: No more children processes to reap.
04/15/16 14:08:38 ForkWorker::Fork: New child of 9936 = 1607852
04/15/16 14:08:38 Number of Active Workers 0
04/15/16 14:08:38 DaemonCore: No more children processes to reap.
04/15/16 14:08:39 ForkWorker::Fork: New child of 9936 = 1607853
04/15/16 14:08:39 Number of Active Workers 0
04/15/16 14:08:39 DaemonCore: No more children processes to reap.
04/15/16 14:08:39 Received a superuser command
04/15/16 14:08:39 ForkWorker::Fork: New child of 9936 = 1607866
04/15/16 14:08:39 Number of Active Workers 0
04/15/16 14:08:39 DaemonCore: No more children processes to reap.
04/15/16 14:08:40 ForkWorker::Fork: New child of 9936 = 1607909
04/15/16 14:08:40 Number of Active Workers 0
04/15/16 14:08:40 DaemonCore: No more children processes to reap.
04/15/16 14:08:41 Getting monitoring info for pid 9936
04/15/16 14:08:41 ForkWorker::Fork: New child of 9936 = 1607910
04/15/16 14:08:41 Number of Active Workers 0
04/15/16 14:08:41 DaemonCore: No more children processes to reap.
04/15/16 14:08:42 ForkWorker::Fork: New child of 9936 = 1607955
04/15/16 14:08:42 Number of Active Workers 0
04/15/16 14:08:42 DaemonCore: No more children processes to reap.
04/15/16 14:08:43 Received a superuser command
04/15/16 14:08:43 ForkWorker::Fork: New child of 9936 = 1607962
04/15/16 14:08:43 Number of Active Workers 0
04/15/16 14:08:43 DaemonCore: No more children processes to reap.
04/15/16 14:08:43 ForkWorker::Fork: New child of 9936 = 1607963
04/15/16 14:08:43 Number of Active Workers 0
04/15/16 14:08:43 DaemonCore: No more children processes to reap.
04/15/16 14:08:44 ForkWorker::Fork: New child of 9936 = 1607986
04/15/16 14:08:44 Number of Active Workers 0
04/15/16 14:08:44 DaemonCore: No more children processes to reap.
04/15/16 14:08:45 ForkWorker::Fork: New child of 9936 = 1608013
04/15/16 14:08:45 Number of Active Workers 0
04/15/16 14:08:45 DaemonCore: No more children processes to reap.
04/15/16 14:08:46 ForkWorker::Fork: New child of 9936 = 1608014
04/15/16 14:08:46 Number of Active Workers 0
04/15/16 14:08:46 DaemonCore: No more children processes to reap.
04/15/16 14:08:47 Received a superuser command
04/15/16 14:08:47 ForkWorker::Fork: New child of 9936 = 1608058
04/15/16 14:08:47 Number of Active Workers 0
04/15/16 14:08:47 DaemonCore: No more children processes to reap.
04/15/16 14:08:47 ForkWorker::Fork: New child of 9936 = 1608070
04/15/16 14:08:47 Number of Active Workers 0
04/15/16 14:08:47 DaemonCore: No more children processes to reap.
04/15/16 14:08:48 ForkWorker::Fork: New child of 9936 = 1608072
04/15/16 14:08:48 Number of Active Workers 0
04/15/16 14:08:48 DaemonCore: No more children processes to reap.
04/15/16 14:08:49 Received a superuser command
04/15/16 14:08:49 ForkWorker::Fork: New child of 9936 = 1608123
04/15/16 14:08:49 Number of Active Workers 0
04/15/16 14:08:49 DaemonCore: No more children processes to reap.
04/15/16 14:08:49 ForkWorker::Fork: New child of 9936 = 1608146
04/15/16 14:08:49 Number of Active Workers 0
04/15/16 14:08:49 DaemonCore: No more children processes to reap.
04/15/16 14:08:50 ForkWorker::Fork: New child of 9936 = 1608242
04/15/16 14:08:50 Number of Active Workers 0
04/15/16 14:08:50 DaemonCore: No more children processes to reap.
04/15/16 14:08:51 Received a superuser command
04/15/16 14:08:51 ForkWorker::Fork: New child of 9936 = 1608292
04/15/16 14:08:51 Number of Active Workers 0
04/15/16 14:08:51 DaemonCore: No more children processes to reap.
04/15/16 14:08:51 ForkWorker::Fork: New child of 9936 = 1608340
04/15/16 14:08:51 Number of Active Workers 0
04/15/16 14:08:51 DaemonCore: No more children processes to reap.
04/15/16 14:08:52 ForkWorker::Fork: New child of 9936 = 1608428
04/15/16 14:08:52 Number of Active Workers 0
04/15/16 14:08:52 DaemonCore: No more children processes to reap.
04/15/16 14:08:54 ForkWorker::Fork: New child of 9936 = 1608474
04/15/16 14:08:54 Number of Active Workers 0
04/15/16 14:08:54 DaemonCore: No more children processes to reap.
04/15/16 14:08:54 Received a superuser command
04/15/16 14:08:54 ForkWorker::Fork: New child of 9936 = 1608507
04/15/16 14:08:54 Number of Active Workers 0
04/15/16 14:08:54 DaemonCore: No more children processes to reap.
04/15/16 14:08:55 ForkWorker::Fork: New child of 9936 = 1608556
04/15/16 14:08:55 Number of Active Workers 0
04/15/16 14:08:55 DaemonCore: No more children processes to reap.
04/15/16 14:08:56 ForkWorker::Fork: New child of 9936 = 1608602
04/15/16 14:08:56 Number of Active Workers 0
04/15/16 14:08:56 DaemonCore: No more children processes to reap.
04/15/16 14:08:57 ForkWorker::Fork: New child of 9936 = 1608676
04/15/16 14:08:57 Number of Active Workers 0
04/15/16 14:08:57 DaemonCore: No more children processes to reap.
04/15/16 14:08:57 Received a superuser command
04/15/16 14:08:57 ForkWorker::Fork: New child of 9936 = 1608762
04/15/16 14:08:57 Number of Active Workers 0
04/15/16 14:08:57 DaemonCore: No more children processes to reap.
04/15/16 14:08:58 ForkWorker::Fork: New child of 9936 = 1608787
04/15/16 14:08:58 Number of Active Workers 0
04/15/16 14:08:58 DaemonCore: No more children processes to reap.
04/15/16 14:08:59 ForkWorker::Fork: New child of 9936 = 1608842
04/15/16 14:08:59 Number of Active Workers 0
04/15/16 14:08:59 DaemonCore: No more children processes to reap.
04/15/16 14:09:00 ForkWorker::Fork: New child of 9936 = 1608957
04/15/16 14:09:00 Number of Active Workers 0
04/15/16 14:09:00 DaemonCore: No more children processes to reap.
04/15/16 14:09:01 ForkWorker::Fork: New child of 9936 = 1609054
04/15/16 14:09:01 Number of Active Workers 0
04/15/16 14:09:01 DaemonCore: No more children processes to reap.
04/15/16 14:09:01 Received a superuser command
04/15/16 14:09:01 ForkWorker::Fork: New child of 9936 = 1609057
04/15/16 14:09:01 Number of Active Workers 0
04/15/16 14:09:01 DaemonCore: No more children processes to reap.
04/15/16 14:09:02 ForkWorker::Fork: New child of 9936 = 1609058
04/15/16 14:09:02 Number of Active Workers 0
04/15/16 14:09:02 DaemonCore: No more children processes to reap.
04/15/16 14:09:03 Received a superuser command
04/15/16 14:09:03 ForkWorker::Fork: New child of 9936 = 1609065
04/15/16 14:09:03 Number of Active Workers 0
04/15/16 14:09:03 DaemonCore: No more children processes to reap.
04/15/16 14:09:03 ForkWorker::Fork: New child of 9936 = 1609066
04/15/16 14:09:03 Number of Active Workers 0
04/15/16 14:09:03 DaemonCore: No more children processes to reap.
04/15/16 14:09:04 ForkWorker::Fork: New child of 9936 = 1609071
04/15/16 14:09:04 Number of Active Workers 0
04/15/16 14:09:04 DaemonCore: No more children processes to reap.
04/15/16 14:09:05 ForkWorker::Fork: New child of 9936 = 1609072
04/15/16 14:09:05 Number of Active Workers 0
04/15/16 14:09:05 DaemonCore: No more children processes to reap.
04/15/16 14:09:06 Received a superuser command
04/15/16 14:09:06 ForkWorker::Fork: New child of 9936 = 1609075
04/15/16 14:09:06 Number of Active Workers 0
04/15/16 14:09:06 DaemonCore: No more children processes to reap.
04/15/16 14:09:06 ForkWorker::Fork: New child of 9936 = 1609076
04/15/16 14:09:06 Number of Active Workers 0
04/15/16 14:09:06 DaemonCore: No more children processes to reap.
04/15/16 14:09:07 ForkWorker::Fork: New child of 9936 = 1609077
04/15/16 14:09:07 Number of Active Workers 0
04/15/16 14:09:07 DaemonCore: No more children processes to reap.
04/15/16 14:09:08 ForkWorker::Fork: New child of 9936 = 1609082
04/15/16 14:09:08 Number of Active Workers 0
04/15/16 14:09:08 DaemonCore: No more children processes to reap.
04/15/16 14:09:09 ForkWorker::Fork: New child of 9936 = 1609162
04/15/16 14:09:09 Number of Active Workers 0
04/15/16 14:09:09 DaemonCore: No more children processes to reap.
04/15/16 14:09:10 Received a superuser command
04/15/16 14:09:10 ForkWorker::Fork: New child of 9936 = 1609194
04/15/16 14:09:10 Number of Active Workers 0
04/15/16 14:09:10 DaemonCore: No more children processes to reap.
04/15/16 14:09:10 ForkWorker::Fork: New child of 9936 = 1609278
04/15/16 14:09:10 Number of Active Workers 0
04/15/16 14:09:10 DaemonCore: No more children processes to reap.
04/15/16 14:09:12 ForkWorker::Fork: New child of 9936 = 1609373
04/15/16 14:09:12 Number of Active Workers 0
04/15/16 14:09:12 DaemonCore: No more children processes to reap.
04/15/16 14:09:13 ForkWorker::Fork: New child of 9936 = 1609466
04/15/16 14:09:13 Number of Active Workers 0
04/15/16 14:09:13 DaemonCore: No more children processes to reap.
04/15/16 14:09:14 ForkWorker::Fork: New child of 9936 = 1609547
04/15/16 14:09:14 Number of Active Workers 0
04/15/16 14:09:14 DaemonCore: No more children processes to reap.
04/15/16 14:09:14 Received a superuser command
04/15/16 14:09:14 ForkWorker::Fork: New child of 9936 = 1609552
04/15/16 14:09:14 Number of Active Workers 0
04/15/16 14:09:14 DaemonCore: No more children processes to reap.
04/15/16 14:09:15 ForkWorker::Fork: New child of 9936 = 1609553
04/15/16 14:09:15 Number of Active Workers 0
04/15/16 14:09:15 DaemonCore: No more children processes to reap.
04/15/16 14:09:16 ForkWorker::Fork: New child of 9936 = 1609569
04/15/16 14:09:16 Number of Active Workers 0
04/15/16 14:09:16 DaemonCore: No more children processes to reap.
04/15/16 14:09:17 Received a superuser command
04/15/16 14:09:17 ForkWorker::Fork: New child of 9936 = 1609619
04/15/16 14:09:17 Number of Active Workers 0
04/15/16 14:09:17 DaemonCore: No more children processes to reap.
04/15/16 14:09:17 ForkWorker::Fork: New child of 9936 = 1609622
04/15/16 14:09:17 Number of Active Workers 0
04/15/16 14:09:17 DaemonCore: No more children processes to reap.
04/15/16 14:09:18 ForkWorker::Fork: New child of 9936 = 1609669
04/15/16 14:09:18 Number of Active Workers 0
04/15/16 14:09:18 DaemonCore: No more children processes to reap.
04/15/16 14:09:19 ForkWorker::Fork: New child of 9936 = 1609729
04/15/16 14:09:19 Number of Active Workers 0
04/15/16 14:09:19 DaemonCore: No more children processes to reap.
04/15/16 14:09:20 Received a superuser command
04/15/16 14:09:20 ForkWorker::Fork: New child of 9936 = 1609765
04/15/16 14:09:20 Number of Active Workers 0
04/15/16 14:09:20 DaemonCore: No more children processes to reap.
04/15/16 14:09:20 ForkWorker::Fork: New child of 9936 = 1609766
04/15/16 14:09:20 Number of Active Workers 0
04/15/16 14:09:20 DaemonCore: No more children processes to reap.
04/15/16 14:09:21 ForkWorker::Fork: New child of 9936 = 1609785
04/15/16 14:09:21 Number of Active Workers 0
04/15/16 14:09:21 DaemonCore: No more children processes to reap.
04/15/16 14:09:22 ForkWorker::Fork: New child of 9936 = 1609835
04/15/16 14:09:22 Number of Active Workers 0
04/15/16 14:09:22 DaemonCore: No more children processes to reap.
04/15/16 14:09:23 Received a superuser command
04/15/16 14:09:23 ForkWorker::Fork: New child of 9936 = 1609839
04/15/16 14:09:23 Number of Active Workers 0
04/15/16 14:09:23 DaemonCore: No more children processes to reap.
04/15/16 14:09:23 ForkWorker::Fork: New child of 9936 = 1609841
04/15/16 14:09:23 Number of Active Workers 0
04/15/16 14:09:23 DaemonCore: No more children processes to reap.
04/15/16 14:09:24 ForkWorker::Fork: New child of 9936 = 1609883
04/15/16 14:09:24 Number of Active Workers 0
04/15/16 14:09:24 DaemonCore: No more children processes to reap.
04/15/16 14:09:25 ForkWorker::Fork: New child of 9936 = 1609895
04/15/16 14:09:25 Number of Active Workers 0
04/15/16 14:09:25 DaemonCore: No more children processes to reap.
04/15/16 14:09:26 Received a superuser command
04/15/16 14:09:26 ForkWorker::Fork: New child of 9936 = 1609898
04/15/16 14:09:26 Number of Active Workers 0
04/15/16 14:09:26 DaemonCore: No more children processes to reap.
04/15/16 14:09:26 ForkWorker::Fork: New child of 9936 = 1609904
04/15/16 14:09:26 Number of Active Workers 0
04/15/16 14:09:26 DaemonCore: No more children processes to reap.
04/15/16 14:09:27 ForkWorker::Fork: New child of 9936 = 1609948
04/15/16 14:09:27 Number of Active Workers 0
04/15/16 14:09:27 DaemonCore: No more children processes to reap.
04/15/16 14:09:28 ForkWorker::Fork: New child of 9936 = 1609955
04/15/16 14:09:28 Number of Active Workers 0
04/15/16 14:09:28 DaemonCore: No more children processes to reap.
04/15/16 14:09:30 ForkWorker::Fork: New child of 9936 = 1610016
04/15/16 14:09:30 Number of Active Workers 0
04/15/16 14:09:30 DaemonCore: No more children processes to reap.
04/15/16 14:09:30 Received a superuser command
04/15/16 14:09:30 ForkWorker::Fork: New child of 9936 = 1610019
04/15/16 14:09:30 Number of Active Workers 0
04/15/16 14:09:30 DaemonCore: No more children processes to reap.
04/15/16 14:09:31 ForkWorker::Fork: New child of 9936 = 1610020
04/15/16 14:09:31 Number of Active Workers 0
04/15/16 14:09:31 DaemonCore: No more children processes to reap.
04/15/16 14:09:32 ForkWorker::Fork: New child of 9936 = 1610065
04/15/16 14:09:32 Number of Active Workers 0
04/15/16 14:09:32 DaemonCore: No more children processes to reap.
04/15/16 14:09:33 ForkWorker::Fork: New child of 9936 = 1610088
04/15/16 14:09:33 Number of Active Workers 0
04/15/16 14:09:33 DaemonCore: No more children processes to reap.
04/15/16 14:09:33 Received a superuser command
04/15/16 14:09:33 ForkWorker::Fork: New child of 9936 = 1610091
04/15/16 14:09:33 Number of Active Workers 0
04/15/16 14:09:33 DaemonCore: No more children processes to reap.
04/15/16 14:09:34 ForkWorker::Fork: New child of 9936 = 1610092
04/15/16 14:09:34 Number of Active Workers 0
04/15/16 14:09:34 DaemonCore: No more children processes to reap.
04/15/16 14:09:35 ForkWorker::Fork: New child of 9936 = 1610146
04/15/16 14:09:35 Number of Active Workers 0
04/15/16 14:09:35 DaemonCore: No more children processes to reap.
04/15/16 14:09:35 Received a superuser command
04/15/16 14:09:35 ForkWorker::Fork: New child of 9936 = 1610149
04/15/16 14:09:35 Number of Active Workers 0
04/15/16 14:09:35 DaemonCore: No more children processes to reap.
04/15/16 14:09:36 ForkWorker::Fork: New child of 9936 = 1610150
04/15/16 14:09:36 Number of Active Workers 0
04/15/16 14:09:36 DaemonCore: No more children processes to reap.
04/15/16 14:09:37 ForkWorker::Fork: New child of 9936 = 1610195
04/15/16 14:09:37 Number of Active Workers 0
04/15/16 14:09:37 DaemonCore: No more children processes to reap.
04/15/16 14:09:38 Received a superuser command
04/15/16 14:09:38 ForkWorker::Fork: New child of 9936 = 1610225
04/15/16 14:09:38 Number of Active Workers 0
04/15/16 14:09:38 DaemonCore: No more children processes to reap.
04/15/16 14:09:38 ForkWorker::Fork: New child of 9936 = 1610226
04/15/16 14:09:38 Number of Active Workers 0
04/15/16 14:09:38 DaemonCore: No more children processes to reap.
04/15/16 14:09:39 ForkWorker::Fork: New child of 9936 = 1610260
04/15/16 14:09:39 Number of Active Workers 0
04/15/16 14:09:39 DaemonCore: No more children processes to reap.
04/15/16 14:09:40 ForkWorker::Fork: New child of 9936 = 1610356
04/15/16 14:09:40 Number of Active Workers 0
04/15/16 14:09:40 DaemonCore: No more children processes to reap.
04/15/16 14:09:41 ForkWorker::Fork: New child of 9936 = 1610449
04/15/16 14:09:41 Number of Active Workers 0
04/15/16 14:09:41 DaemonCore: No more children processes to reap.
04/15/16 14:09:42 Received a superuser command
04/15/16 14:09:42 ForkWorker::Fork: New child of 9936 = 1610477
04/15/16 14:09:42 Number of Active Workers 0
04/15/16 14:09:42 DaemonCore: No more children processes to reap.
04/15/16 14:09:42 ForkWorker::Fork: New child of 9936 = 1610541
04/15/16 14:09:42 Number of Active Workers 0
04/15/16 14:09:42 DaemonCore: No more children processes to reap.
04/15/16 14:09:43 ForkWorker::Fork: New child of 9936 = 1610597
04/15/16 14:09:43 Number of Active Workers 0
04/15/16 14:09:43 DaemonCore: No more children processes to reap.
04/15/16 14:09:44 ForkWorker::Fork: New child of 9936 = 1610687
04/15/16 14:09:44 Number of Active Workers 0
04/15/16 14:09:44 DaemonCore: No more children processes to reap.
04/15/16 14:09:45 ForkWorker::Fork: New child of 9936 = 1610739
04/15/16 14:09:45 Number of Active Workers 0
04/15/16 14:09:45 DaemonCore: No more children processes to reap.
04/15/16 14:09:46 Received a superuser command
04/15/16 14:09:46 ForkWorker::Fork: New child of 9936 = 1610772
04/15/16 14:09:46 Number of Active Workers 0
04/15/16 14:09:46 DaemonCore: No more children processes to reap.
04/15/16 14:09:46 ForkWorker::Fork: New child of 9936 = 1610806
04/15/16 14:09:46 Number of Active Workers 0
04/15/16 14:09:46 DaemonCore: No more children processes to reap.
04/15/16 14:09:48 ForkWorker::Fork: New child of 9936 = 1610891
04/15/16 14:09:48 Number of Active Workers 0
04/15/16 14:09:48 DaemonCore: No more children processes to reap.
04/15/16 14:09:48 Received a superuser command
04/15/16 14:09:48 ForkWorker::Fork: New child of 9936 = 1610903
04/15/16 14:09:48 Number of Active Workers 0
04/15/16 14:09:48 DaemonCore: No more children processes to reap.
04/15/16 14:09:49 ForkWorker::Fork: New child of 9936 = 1610950
04/15/16 14:09:49 Number of Active Workers 0
04/15/16 14:09:49 DaemonCore: No more children processes to reap.
04/15/16 14:09:50 ForkWorker::Fork: New child of 9936 = 1611031
04/15/16 14:09:50 Number of Active Workers 0
04/15/16 14:09:50 DaemonCore: No more children processes to reap.
04/15/16 14:09:51 ForkWorker::Fork: New child of 9936 = 1611125
04/15/16 14:09:51 Number of Active Workers 0
04/15/16 14:09:51 DaemonCore: No more children processes to reap.
04/15/16 14:09:52 ForkWorker::Fork: New child of 9936 = 1611205
04/15/16 14:09:52 Number of Active Workers 0
04/15/16 14:09:52 DaemonCore: No more children processes to reap.
04/15/16 14:09:53 ForkWorker::Fork: New child of 9936 = 1611215
04/15/16 14:09:53 Number of Active Workers 0
04/15/16 14:09:53 DaemonCore: No more children processes to reap.
04/15/16 14:09:54 ForkWorker::Fork: New child of 9936 = 1611222
04/15/16 14:09:54 Number of Active Workers 0
04/15/16 14:09:54 DaemonCore: No more children processes to reap.
04/15/16 14:09:55 ForkWorker::Fork: New child of 9936 = 1611226
04/15/16 14:09:55 Number of Active Workers 0
04/15/16 14:09:55 DaemonCore: No more children processes to reap.
04/15/16 14:09:56 ForkWorker::Fork: New child of 9936 = 1611234
04/15/16 14:09:56 Number of Active Workers 0
04/15/16 14:09:56 DaemonCore: No more children processes to reap.
04/15/16 14:09:57 ForkWorker::Fork: New child of 9936 = 1611236
04/15/16 14:09:57 Number of Active Workers 0
04/15/16 14:09:57 DaemonCore: No more children processes to reap.
04/15/16 14:09:58 ForkWorker::Fork: New child of 9936 = 1611275
04/15/16 14:09:58 Number of Active Workers 0
04/15/16 14:09:58 DaemonCore: No more children processes to reap.
04/15/16 14:09:59 ForkWorker::Fork: New child of 9936 = 1611329
04/15/16 14:09:59 Number of Active Workers 0
04/15/16 14:09:59 DaemonCore: No more children processes to reap.
04/15/16 14:10:00 ForkWorker::Fork: New child of 9936 = 1611422
04/15/16 14:10:00 Number of Active Workers 0
04/15/16 14:10:00 DaemonCore: No more children processes to reap.
04/15/16 14:10:01 ForkWorker::Fork: New child of 9936 = 1611512
04/15/16 14:10:01 Number of Active Workers 0
04/15/16 14:10:01 DaemonCore: No more children processes to reap.
04/15/16 14:10:02 ForkWorker::Fork: New child of 9936 = 1611668
04/15/16 14:10:02 Number of Active Workers 0
04/15/16 14:10:02 DaemonCore: No more children processes to reap.
04/15/16 14:10:03 ForkWorker::Fork: New child of 9936 = 1611744
04/15/16 14:10:03 Number of Active Workers 0
04/15/16 14:10:03 DaemonCore: No more children processes to reap.
04/15/16 14:10:04 ForkWorker::Fork: New child of 9936 = 1611788
04/15/16 14:10:04 Number of Active Workers 0
04/15/16 14:10:04 DaemonCore: No more children processes to reap.
04/15/16 14:10:06 ForkWorker::Fork: New child of 9936 = 1611834
04/15/16 14:10:06 Number of Active Workers 0
04/15/16 14:10:06 DaemonCore: No more children processes to reap.
04/15/16 14:10:07 ForkWorker::Fork: New child of 9936 = 1611851
04/15/16 14:10:07 Number of Active Workers 0
04/15/16 14:10:07 DaemonCore: No more children processes to reap.
04/15/16 14:10:08 ForkWorker::Fork: New child of 9936 = 1611862
04/15/16 14:10:08 Number of Active Workers 0
04/15/16 14:10:08 DaemonCore: No more children processes to reap.
04/15/16 14:10:09 ForkWorker::Fork: New child of 9936 = 1611917
04/15/16 14:10:09 Number of Active Workers 0
04/15/16 14:10:09 DaemonCore: No more children processes to reap.
04/15/16 14:10:10 ForkWorker::Fork: New child of 9936 = 1612003
04/15/16 14:10:10 Number of Active Workers 0
04/15/16 14:10:10 DaemonCore: No more children processes to reap.
04/15/16 14:10:11 ForkWorker::Fork: New child of 9936 = 1612005
04/15/16 14:10:11 Number of Active Workers 0
04/15/16 14:10:11 DaemonCore: No more children processes to reap.
04/15/16 14:10:11 Received a superuser command
04/15/16 14:10:11 ForkWorker::Fork: New child of 9936 = 1612008
04/15/16 14:10:11 Number of Active Workers 0
04/15/16 14:10:11 DaemonCore: No more children processes to reap.
04/15/16 14:10:12 ForkWorker::Fork: New child of 9936 = 1612049
04/15/16 14:10:12 Number of Active Workers 0
04/15/16 14:10:12 DaemonCore: No more children processes to reap.
04/15/16 14:10:13 ForkWorker::Fork: New child of 9936 = 1612058
04/15/16 14:10:13 Number of Active Workers 0
04/15/16 14:10:13 DaemonCore: No more children processes to reap.
04/15/16 14:10:13 Received a superuser command
04/15/16 14:10:13 ForkWorker::Fork: New child of 9936 = 1612061
04/15/16 14:10:13 Number of Active Workers 0
04/15/16 14:10:13 DaemonCore: No more children processes to reap.
04/15/16 14:10:14 ForkWorker::Fork: New child of 9936 = 1612066
04/15/16 14:10:14 Number of Active Workers 0
04/15/16 14:10:14 DaemonCore: No more children processes to reap.
04/15/16 14:10:15 ForkWorker::Fork: New child of 9936 = 1612073
04/15/16 14:10:15 Number of Active Workers 0
04/15/16 14:10:15 DaemonCore: No more children processes to reap.
04/15/16 14:10:16 ForkWorker::Fork: New child of 9936 = 1612106
04/15/16 14:10:16 Number of Active Workers 0
04/15/16 14:10:16 DaemonCore: No more children processes to reap.
04/15/16 14:10:17 ForkWorker::Fork: New child of 9936 = 1612160
04/15/16 14:10:17 Number of Active Workers 0
04/15/16 14:10:17 DaemonCore: No more children processes to reap.
04/15/16 14:10:18 Received a superuser command
04/15/16 14:10:18 ForkWorker::Fork: New child of 9936 = 1612167
04/15/16 14:10:18 Number of Active Workers 0
04/15/16 14:10:18 DaemonCore: No more children processes to reap.
04/15/16 14:10:18 ForkWorker::Fork: New child of 9936 = 1612168
04/15/16 14:10:18 Number of Active Workers 0
04/15/16 14:10:18 DaemonCore: No more children processes to reap.
04/15/16 14:10:19 ForkWorker::Fork: New child of 9936 = 1612240
04/15/16 14:10:19 Number of Active Workers 0
04/15/16 14:10:19 DaemonCore: No more children processes to reap.
04/15/16 14:10:20 ForkWorker::Fork: New child of 9936 = 1612383
04/15/16 14:10:20 Number of Active Workers 0
04/15/16 14:10:20 DaemonCore: No more children processes to reap.
04/15/16 14:10:21 ForkWorker::Fork: New child of 9936 = 1612507
04/15/16 14:10:21 Number of Active Workers 0
04/15/16 14:10:21 DaemonCore: No more children processes to reap.
04/15/16 14:10:22 ForkWorker::Fork: New child of 9936 = 1612603
04/15/16 14:10:22 Number of Active Workers 0
04/15/16 14:10:22 DaemonCore: No more children processes to reap.
04/15/16 14:10:24 ForkWorker::Fork: New child of 9936 = 1612655
04/15/16 14:10:24 Number of Active Workers 0
04/15/16 14:10:24 DaemonCore: No more children processes to reap.
04/15/16 14:10:25 ForkWorker::Fork: New child of 9936 = 1612674
04/15/16 14:10:25 Number of Active Workers 0
04/15/16 14:10:25 DaemonCore: No more children processes to reap.
04/15/16 14:10:26 ForkWorker::Fork: New child of 9936 = 1612675
04/15/16 14:10:26 Number of Active Workers 0
04/15/16 14:10:26 DaemonCore: No more children processes to reap.
04/15/16 14:10:27 ForkWorker::Fork: New child of 9936 = 1612677
04/15/16 14:10:27 Number of Active Workers 0
04/15/16 14:10:27 DaemonCore: No more children processes to reap.
04/15/16 14:10:28 Received a superuser command
04/15/16 14:10:28 ForkWorker::Fork: New child of 9936 = 1612681
04/15/16 14:10:28 Number of Active Workers 0
04/15/16 14:10:28 DaemonCore: No more children processes to reap.
04/15/16 14:10:28 ForkWorker::Fork: New child of 9936 = 1612695
04/15/16 14:10:28 Number of Active Workers 0
04/15/16 14:10:28 DaemonCore: No more children processes to reap.
04/15/16 14:10:29 ForkWorker::Fork: New child of 9936 = 1612781
04/15/16 14:10:29 Number of Active Workers 0
04/15/16 14:10:29 DaemonCore: No more children processes to reap.
04/15/16 14:10:30 ForkWorker::Fork: New child of 9936 = 1612873
04/15/16 14:10:30 Number of Active Workers 0
04/15/16 14:10:30 DaemonCore: No more children processes to reap.
04/15/16 14:10:31 Received a superuser command
04/15/16 14:10:31 ForkWorker::Fork: New child of 9936 = 1612943
04/15/16 14:10:31 Number of Active Workers 0
04/15/16 14:10:31 DaemonCore: No more children processes to reap.
04/15/16 14:10:31 ForkWorker::Fork: New child of 9936 = 1612975
04/15/16 14:10:31 Number of Active Workers 0
04/15/16 14:10:31 DaemonCore: No more children processes to reap.
04/15/16 14:10:32 ForkWorker::Fork: New child of 9936 = 1613047
04/15/16 14:10:32 Number of Active Workers 0
04/15/16 14:10:32 DaemonCore: No more children processes to reap.
04/15/16 14:10:33 ForkWorker::Fork: New child of 9936 = 1613109
04/15/16 14:10:33 Number of Active Workers 0
04/15/16 14:10:33 DaemonCore: No more children processes to reap.
04/15/16 14:10:34 ForkWorker::Fork: New child of 9936 = 1613110
04/15/16 14:10:34 Number of Active Workers 0
04/15/16 14:10:34 DaemonCore: No more children processes to reap.
04/15/16 14:10:35 ForkWorker::Fork: New child of 9936 = 1613117
04/15/16 14:10:35 Number of Active Workers 0
04/15/16 14:10:35 DaemonCore: No more children processes to reap.
04/15/16 14:10:36 ForkWorker::Fork: New child of 9936 = 1613118
04/15/16 14:10:36 Number of Active Workers 0
04/15/16 14:10:36 DaemonCore: No more children processes to reap.
04/15/16 14:10:37 ForkWorker::Fork: New child of 9936 = 1613181
04/15/16 14:10:37 Number of Active Workers 0
04/15/16 14:10:37 DaemonCore: No more children processes to reap.
04/15/16 14:10:38 ForkWorker::Fork: New child of 9936 = 1613231
04/15/16 14:10:38 Number of Active Workers 0
04/15/16 14:10:38 DaemonCore: No more children processes to reap.
04/15/16 14:10:39 ForkWorker::Fork: New child of 9936 = 1613281
04/15/16 14:10:39 Number of Active Workers 0
04/15/16 14:10:39 DaemonCore: No more children processes to reap.
04/15/16 14:10:40 ForkWorker::Fork: New child of 9936 = 1613374
04/15/16 14:10:40 Number of Active Workers 0
04/15/16 14:10:40 DaemonCore: No more children processes to reap.
04/15/16 14:10:41 ForkWorker::Fork: New child of 9936 = 1613482
04/15/16 14:10:41 Number of Active Workers 0
04/15/16 14:10:41 DaemonCore: No more children processes to reap.
04/15/16 14:10:43 ForkWorker::Fork: New child of 9936 = 1613572
04/15/16 14:10:43 Number of Active Workers 0
04/15/16 14:10:43 DaemonCore: No more children processes to reap.
04/15/16 14:10:44 ForkWorker::Fork: New child of 9936 = 1613646
04/15/16 14:10:44 Number of Active Workers 0
04/15/16 14:10:44 DaemonCore: No more children processes to reap.
04/15/16 14:10:45 ForkWorker::Fork: New child of 9936 = 1613695
04/15/16 14:10:45 Number of Active Workers 0
04/15/16 14:10:45 DaemonCore: No more children processes to reap.
04/15/16 14:10:46 ForkWorker::Fork: New child of 9936 = 1613745
04/15/16 14:10:46 Number of Active Workers 0
04/15/16 14:10:46 DaemonCore: No more children processes to reap.
04/15/16 14:10:47 ForkWorker::Fork: New child of 9936 = 1613793
04/15/16 14:10:47 Number of Active Workers 0
04/15/16 14:10:47 DaemonCore: No more children processes to reap.
04/15/16 14:10:48 ForkWorker::Fork: New child of 9936 = 1613871
04/15/16 14:10:48 Number of Active Workers 0
04/15/16 14:10:48 DaemonCore: No more children processes to reap.
04/15/16 14:10:49 ForkWorker::Fork: New child of 9936 = 1613918
04/15/16 14:10:49 Number of Active Workers 0
04/15/16 14:10:49 DaemonCore: No more children processes to reap.
04/15/16 14:10:50 ForkWorker::Fork: New child of 9936 = 1613969
04/15/16 14:10:50 Number of Active Workers 0
04/15/16 14:10:50 DaemonCore: No more children processes to reap.
04/15/16 14:10:51 ForkWorker::Fork: New child of 9936 = 1613970
04/15/16 14:10:51 Number of Active Workers 0
04/15/16 14:10:51 DaemonCore: No more children processes to reap.
04/15/16 14:10:52 ForkWorker::Fork: New child of 9936 = 1614035
04/15/16 14:10:52 Number of Active Workers 0
04/15/16 14:10:52 DaemonCore: No more children processes to reap.
04/15/16 14:10:53 ForkWorker::Fork: New child of 9936 = 1614042
04/15/16 14:10:53 Number of Active Workers 0
04/15/16 14:10:53 DaemonCore: No more children processes to reap.
04/15/16 14:10:54 ForkWorker::Fork: New child of 9936 = 1614067
04/15/16 14:10:54 Number of Active Workers 0
04/15/16 14:10:54 DaemonCore: No more children processes to reap.
04/15/16 14:10:55 ForkWorker::Fork: New child of 9936 = 1614093
04/15/16 14:10:55 Number of Active Workers 0
04/15/16 14:10:55 DaemonCore: No more children processes to reap.
04/15/16 14:10:56 ForkWorker::Fork: New child of 9936 = 1614094
04/15/16 14:10:56 Number of Active Workers 0
04/15/16 14:10:56 DaemonCore: No more children processes to reap.
04/15/16 14:10:57 ForkWorker::Fork: New child of 9936 = 1614152
04/15/16 14:10:57 Number of Active Workers 0
04/15/16 14:10:57 DaemonCore: No more children processes to reap.
04/15/16 14:10:58 ForkWorker::Fork: New child of 9936 = 1614183
04/15/16 14:10:58 Number of Active Workers 0
04/15/16 14:10:58 DaemonCore: No more children processes to reap.
04/15/16 14:10:59 ForkWorker::Fork: New child of 9936 = 1614232
04/15/16 14:10:59 Number of Active Workers 0
04/15/16 14:10:59 DaemonCore: No more children processes to reap.
04/15/16 14:11:01 ForkWorker::Fork: New child of 9936 = 1614243
04/15/16 14:11:01 Number of Active Workers 0
04/15/16 14:11:01 DaemonCore: No more children processes to reap.
04/15/16 14:11:02 ForkWorker::Fork: New child of 9936 = 1614254
04/15/16 14:11:02 Number of Active Workers 0
04/15/16 14:11:02 DaemonCore: No more children processes to reap.
04/15/16 14:11:03 ForkWorker::Fork: New child of 9936 = 1614326
04/15/16 14:11:03 Number of Active Workers 0
04/15/16 14:11:03 DaemonCore: No more children processes to reap.
04/15/16 14:11:04 ForkWorker::Fork: New child of 9936 = 1614332
04/15/16 14:11:04 Number of Active Workers 0
04/15/16 14:11:04 DaemonCore: No more children processes to reap.
04/15/16 14:11:05 ForkWorker::Fork: New child of 9936 = 1614381
04/15/16 14:11:05 Number of Active Workers 0
04/15/16 14:11:05 DaemonCore: No more children processes to reap.
04/15/16 14:11:06 ForkWorker::Fork: New child of 9936 = 1614382
04/15/16 14:11:06 Number of Active Workers 0
04/15/16 14:11:06 DaemonCore: No more children processes to reap.
04/15/16 14:11:07 Received a superuser command
04/15/16 14:11:07 ForkWorker::Fork: New child of 9936 = 1614389
04/15/16 14:11:07 Number of Active Workers 0
04/15/16 14:11:07 DaemonCore: No more children processes to reap.
04/15/16 14:11:07 ForkWorker::Fork: New child of 9936 = 1614420
04/15/16 14:11:07 Number of Active Workers 0
04/15/16 14:11:07 DaemonCore: No more children processes to reap.
04/15/16 14:11:08 ForkWorker::Fork: New child of 9936 = 1614436
04/15/16 14:11:08 Number of Active Workers 0
04/15/16 14:11:08 DaemonCore: No more children processes to reap.
04/15/16 14:11:09 ForkWorker::Fork: New child of 9936 = 1614437
04/15/16 14:11:09 Number of Active Workers 0
04/15/16 14:11:09 DaemonCore: No more children processes to reap.
04/15/16 14:11:10 ForkWorker::Fork: New child of 9936 = 1614520
04/15/16 14:11:10 Number of Active Workers 0
04/15/16 14:11:10 DaemonCore: No more children processes to reap.
04/15/16 14:11:11 ForkWorker::Fork: New child of 9936 = 1614528
04/15/16 14:11:11 Number of Active Workers 0
04/15/16 14:11:11 DaemonCore: No more children processes to reap.
04/15/16 14:11:12 ForkWorker::Fork: New child of 9936 = 1614580
04/15/16 14:11:12 Number of Active Workers 0
04/15/16 14:11:12 DaemonCore: No more children processes to reap.
04/15/16 14:11:13 ForkWorker::Fork: New child of 9936 = 1614588
04/15/16 14:11:13 Number of Active Workers 0
04/15/16 14:11:13 DaemonCore: No more children processes to reap.
04/15/16 14:11:14 ForkWorker::Fork: New child of 9936 = 1614625
04/15/16 14:11:14 Number of Active Workers 0
04/15/16 14:11:14 DaemonCore: No more children processes to reap.
04/15/16 14:11:15 ForkWorker::Fork: New child of 9936 = 1614660
04/15/16 14:11:15 Number of Active Workers 0
04/15/16 14:11:15 DaemonCore: No more children processes to reap.
04/15/16 14:11:16 ForkWorker::Fork: New child of 9936 = 1614661
04/15/16 14:11:16 Number of Active Workers 0
04/15/16 14:11:16 DaemonCore: No more children processes to reap.
04/15/16 14:11:17 ForkWorker::Fork: New child of 9936 = 1614711
04/15/16 14:11:17 Number of Active Workers 0
04/15/16 14:11:17 DaemonCore: No more children processes to reap.
04/15/16 14:11:19 ForkWorker::Fork: New child of 9936 = 1614713
04/15/16 14:11:19 Number of Active Workers 0
04/15/16 14:11:19 DaemonCore: No more children processes to reap.
04/15/16 14:11:20 ForkWorker::Fork: New child of 9936 = 1614761
04/15/16 14:11:20 Number of Active Workers 0
04/15/16 14:11:20 DaemonCore: No more children processes to reap.
04/15/16 14:11:21 ForkWorker::Fork: New child of 9936 = 1614769
04/15/16 14:11:21 Number of Active Workers 0
04/15/16 14:11:21 DaemonCore: No more children processes to reap.
04/15/16 14:11:22 ForkWorker::Fork: New child of 9936 = 1614785
04/15/16 14:11:22 Number of Active Workers 0
04/15/16 14:11:22 DaemonCore: No more children processes to reap.
04/15/16 14:11:23 ForkWorker::Fork: New child of 9936 = 1614879
04/15/16 14:11:23 Number of Active Workers 0
04/15/16 14:11:23 DaemonCore: No more children processes to reap.
04/15/16 14:11:24 ForkWorker::Fork: New child of 9936 = 1614973
04/15/16 14:11:24 Number of Active Workers 0
04/15/16 14:11:24 DaemonCore: No more children processes to reap.
04/15/16 14:11:25 ForkWorker::Fork: New child of 9936 = 1615063
04/15/16 14:11:25 Number of Active Workers 0
04/15/16 14:11:25 DaemonCore: No more children processes to reap.
04/15/16 14:11:26 ForkWorker::Fork: New child of 9936 = 1615120
04/15/16 14:11:26 Number of Active Workers 0
04/15/16 14:11:26 DaemonCore: No more children processes to reap.
04/15/16 14:11:27 ForkWorker::Fork: New child of 9936 = 1615129
04/15/16 14:11:27 Number of Active Workers 0
04/15/16 14:11:27 DaemonCore: No more children processes to reap.
04/15/16 14:11:28 ForkWorker::Fork: New child of 9936 = 1615133
04/15/16 14:11:28 Number of Active Workers 0
04/15/16 14:11:28 DaemonCore: No more children processes to reap.
04/15/16 14:11:29 ForkWorker::Fork: New child of 9936 = 1615135
04/15/16 14:11:29 Number of Active Workers 0
04/15/16 14:11:29 DaemonCore: No more children processes to reap.
04/15/16 14:11:30 ForkWorker::Fork: New child of 9936 = 1615136
04/15/16 14:11:30 Number of Active Workers 0
04/15/16 14:11:30 DaemonCore: No more children processes to reap.
04/15/16 14:11:31 ForkWorker::Fork: New child of 9936 = 1615137
04/15/16 14:11:31 Number of Active Workers 0
04/15/16 14:11:31 DaemonCore: No more children processes to reap.
04/15/16 14:11:32 ForkWorker::Fork: New child of 9936 = 1615143
04/15/16 14:11:32 Number of Active Workers 0
04/15/16 14:11:32 DaemonCore: No more children processes to reap.
04/15/16 14:11:33 ForkWorker::Fork: New child of 9936 = 1615145
04/15/16 14:11:33 Number of Active Workers 0
04/15/16 14:11:33 DaemonCore: No more children processes to reap.
04/15/16 14:11:34 ForkWorker::Fork: New child of 9936 = 1615151
04/15/16 14:11:34 Number of Active Workers 0
04/15/16 14:11:34 DaemonCore: No more children processes to reap.
04/15/16 14:11:36 ForkWorker::Fork: New child of 9936 = 1615214
04/15/16 14:11:36 Number of Active Workers 0
04/15/16 14:11:36 DaemonCore: No more children processes to reap.
04/15/16 14:11:37 ForkWorker::Fork: New child of 9936 = 1615251
04/15/16 14:11:37 Number of Active Workers 0
04/15/16 14:11:37 DaemonCore: No more children processes to reap.
04/15/16 14:11:38 ForkWorker::Fork: New child of 9936 = 1615335
04/15/16 14:11:38 Number of Active Workers 0
04/15/16 14:11:38 DaemonCore: No more children processes to reap.
04/15/16 14:11:39 ForkWorker::Fork: New child of 9936 = 1615434
04/15/16 14:11:39 Number of Active Workers 0
04/15/16 14:11:39 DaemonCore: No more children processes to reap.
04/15/16 14:11:40 ForkWorker::Fork: New child of 9936 = 1615529
04/15/16 14:11:40 Number of Active Workers 0
04/15/16 14:11:40 DaemonCore: No more children processes to reap.
04/15/16 14:11:41 ForkWorker::Fork: New child of 9936 = 1615623
04/15/16 14:11:41 Number of Active Workers 0
04/15/16 14:11:41 DaemonCore: No more children processes to reap.
04/15/16 14:11:42 ForkWorker::Fork: New child of 9936 = 1615714
04/15/16 14:11:42 Number of Active Workers 0
04/15/16 14:11:42 DaemonCore: No more children processes to reap.
04/15/16 14:11:43 ForkWorker::Fork: New child of 9936 = 1615767
04/15/16 14:11:43 Number of Active Workers 0
04/15/16 14:11:43 DaemonCore: No more children processes to reap.
04/15/16 14:11:44 ForkWorker::Fork: New child of 9936 = 1615846
04/15/16 14:11:44 Number of Active Workers 0
04/15/16 14:11:44 DaemonCore: No more children processes to reap.
04/15/16 14:11:45 ForkWorker::Fork: New child of 9936 = 1615890
04/15/16 14:11:45 Number of Active Workers 0
04/15/16 14:11:45 DaemonCore: No more children processes to reap.
04/15/16 14:11:46 ForkWorker::Fork: New child of 9936 = 1615982
04/15/16 14:11:46 Number of Active Workers 0
04/15/16 14:11:46 DaemonCore: No more children processes to reap.
04/15/16 14:11:47 ForkWorker::Fork: New child of 9936 = 1616037
04/15/16 14:11:47 Number of Active Workers 0
04/15/16 14:11:47 DaemonCore: No more children processes to reap.
04/15/16 14:11:48 ForkWorker::Fork: New child of 9936 = 1616137
04/15/16 14:11:48 Number of Active Workers 0
04/15/16 14:11:48 DaemonCore: No more children processes to reap.
04/15/16 14:11:49 ForkWorker::Fork: New child of 9936 = 1616227
04/15/16 14:11:49 Number of Active Workers 0
04/15/16 14:11:49 DaemonCore: No more children processes to reap.
04/15/16 14:11:50 ForkWorker::Fork: New child of 9936 = 1616323
04/15/16 14:11:50 Number of Active Workers 0
04/15/16 14:11:50 DaemonCore: No more children processes to reap.
04/15/16 14:11:51 ForkWorker::Fork: New child of 9936 = 1616413
04/15/16 14:11:51 Number of Active Workers 0
04/15/16 14:11:51 DaemonCore: No more children processes to reap.
04/15/16 14:11:52 ForkWorker::Fork: New child of 9936 = 1616455
04/15/16 14:11:52 Number of Active Workers 0
04/15/16 14:11:53 DaemonCore: No more children processes to reap.
04/15/16 14:11:54 ForkWorker::Fork: New child of 9936 = 1616468
04/15/16 14:11:54 Number of Active Workers 0
04/15/16 14:11:54 DaemonCore: No more children processes to reap.
04/15/16 14:11:55 ForkWorker::Fork: New child of 9936 = 1616502
04/15/16 14:11:55 Number of Active Workers 0
04/15/16 14:11:55 DaemonCore: No more children processes to reap.
04/15/16 14:11:56 ForkWorker::Fork: New child of 9936 = 1616519
04/15/16 14:11:56 Number of Active Workers 0
04/15/16 14:11:56 DaemonCore: No more children processes to reap.
04/15/16 14:11:57 ForkWorker::Fork: New child of 9936 = 1616560
04/15/16 14:11:57 Number of Active Workers 0
04/15/16 14:11:57 DaemonCore: No more children processes to reap.
04/15/16 14:11:58 ForkWorker::Fork: New child of 9936 = 1616600
04/15/16 14:11:58 Number of Active Workers 0
04/15/16 14:11:58 DaemonCore: No more children processes to reap.
04/15/16 14:11:59 ForkWorker::Fork: New child of 9936 = 1616603
04/15/16 14:11:59 Number of Active Workers 0
04/15/16 14:11:59 DaemonCore: No more children processes to reap.
04/15/16 14:12:00 ForkWorker::Fork: New child of 9936 = 1616652
04/15/16 14:12:00 Number of Active Workers 0
04/15/16 14:12:00 DaemonCore: No more children processes to reap.
04/15/16 14:12:01 ForkWorker::Fork: New child of 9936 = 1616653
04/15/16 14:12:01 Number of Active Workers 0
04/15/16 14:12:01 DaemonCore: No more children processes to reap.
04/15/16 14:12:02 ForkWorker::Fork: New child of 9936 = 1616694
04/15/16 14:12:02 Number of Active Workers 0
04/15/16 14:12:02 DaemonCore: No more children processes to reap.
04/15/16 14:12:03 ForkWorker::Fork: New child of 9936 = 1616707
04/15/16 14:12:03 Number of Active Workers 0
04/15/16 14:12:03 DaemonCore: No more children processes to reap.
04/15/16 14:12:04 ForkWorker::Fork: New child of 9936 = 1616712
04/15/16 14:12:04 Number of Active Workers 0
04/15/16 14:12:04 DaemonCore: No more children processes to reap.
04/15/16 14:12:05 ForkWorker::Fork: New child of 9936 = 1616763
04/15/16 14:12:05 Number of Active Workers 0
04/15/16 14:12:05 DaemonCore: No more children processes to reap.
04/15/16 14:12:06 ForkWorker::Fork: New child of 9936 = 1616764
04/15/16 14:12:06 Number of Active Workers 0
04/15/16 14:12:06 DaemonCore: No more children processes to reap.
04/15/16 14:12:07 ForkWorker::Fork: New child of 9936 = 1616831
04/15/16 14:12:07 Number of Active Workers 0
04/15/16 14:12:07 DaemonCore: No more children processes to reap.
04/15/16 14:12:08 ForkWorker::Fork: New child of 9936 = 1616836
04/15/16 14:12:08 Number of Active Workers 0
04/15/16 14:12:08 DaemonCore: No more children processes to reap.
04/15/16 14:12:09 ForkWorker::Fork: New child of 9936 = 1616863
04/15/16 14:12:09 Number of Active Workers 0
04/15/16 14:12:09 DaemonCore: No more children processes to reap.
04/15/16 14:12:11 ForkWorker::Fork: New child of 9936 = 1616886
04/15/16 14:12:11 Number of Active Workers 0
04/15/16 14:12:11 DaemonCore: No more children processes to reap.
04/15/16 14:12:12 ForkWorker::Fork: New child of 9936 = 1616887
04/15/16 14:12:12 Number of Active Workers 0
04/15/16 14:12:12 DaemonCore: No more children processes to reap.
04/15/16 14:12:13 ForkWorker::Fork: New child of 9936 = 1616940
04/15/16 14:12:13 Number of Active Workers 0
04/15/16 14:12:13 DaemonCore: No more children processes to reap.
04/15/16 14:12:14 ForkWorker::Fork: New child of 9936 = 1616941
04/15/16 14:12:14 Number of Active Workers 0
04/15/16 14:12:14 DaemonCore: No more children processes to reap.
04/15/16 14:12:15 ForkWorker::Fork: New child of 9936 = 1616952
04/15/16 14:12:15 Number of Active Workers 0
04/15/16 14:12:15 DaemonCore: No more children processes to reap.
04/15/16 14:12:16 ForkWorker::Fork: New child of 9936 = 1616953
04/15/16 14:12:16 Number of Active Workers 0
04/15/16 14:12:16 DaemonCore: No more children processes to reap.
04/15/16 14:12:17 ForkWorker::Fork: New child of 9936 = 1617089
04/15/16 14:12:17 Number of Active Workers 0
04/15/16 14:12:17 DaemonCore: No more children processes to reap.
04/15/16 14:12:18 ForkWorker::Fork: New child of 9936 = 1617110
04/15/16 14:12:18 Number of Active Workers 0
04/15/16 14:12:18 DaemonCore: No more children processes to reap.
04/15/16 14:12:19 ForkWorker::Fork: New child of 9936 = 1617141
04/15/16 14:12:19 Number of Active Workers 0
04/15/16 14:12:19 DaemonCore: No more children processes to reap.
04/15/16 14:12:20 ForkWorker::Fork: New child of 9936 = 1617284
04/15/16 14:12:20 Number of Active Workers 0
04/15/16 14:12:20 DaemonCore: No more children processes to reap.
04/15/16 14:12:21 ForkWorker::Fork: New child of 9936 = 1617327
04/15/16 14:12:21 Number of Active Workers 0
04/15/16 14:12:21 DaemonCore: No more children processes to reap.
04/15/16 14:12:22 ForkWorker::Fork: New child of 9936 = 1617380
04/15/16 14:12:22 Number of Active Workers 0
04/15/16 14:12:22 DaemonCore: No more children processes to reap.
04/15/16 14:12:23 ForkWorker::Fork: New child of 9936 = 1617467
04/15/16 14:12:23 Number of Active Workers 0
04/15/16 14:12:23 DaemonCore: No more children processes to reap.
04/15/16 14:12:24 ForkWorker::Fork: New child of 9936 = 1617524
04/15/16 14:12:24 Number of Active Workers 0
04/15/16 14:12:24 DaemonCore: No more children processes to reap.
04/15/16 14:12:25 ForkWorker::Fork: New child of 9936 = 1617659
04/15/16 14:12:25 Number of Active Workers 0
04/15/16 14:12:25 DaemonCore: No more children processes to reap.
04/15/16 14:12:26 ForkWorker::Fork: New child of 9936 = 1617793
04/15/16 14:12:26 Number of Active Workers 0
04/15/16 14:12:26 DaemonCore: No more children processes to reap.
04/15/16 14:12:27 ForkWorker::Fork: New child of 9936 = 1617928
04/15/16 14:12:27 Number of Active Workers 0
04/15/16 14:12:27 DaemonCore: No more children processes to reap.
04/15/16 14:12:29 Clearing userlog file cache
04/15/16 14:12:29 JobsRunning = 0
04/15/16 14:12:29 JobsIdle = 2
04/15/16 14:12:29 JobsHeld = 0
04/15/16 14:12:29 JobsRemoved = 0
04/15/16 14:12:29 LocalUniverseJobsRunning = 0
04/15/16 14:12:29 LocalUniverseJobsIdle = 0
04/15/16 14:12:29 SchedUniverseJobsRunning = 0
04/15/16 14:12:29 SchedUniverseJobsIdle = 0
04/15/16 14:12:29 N_Owners = 1
04/15/16 14:12:29 MaxJobsRunning = 10000
04/15/16 14:12:29 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:12:29 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:12:29 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:12:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:12:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:12:29 Sent HEART BEAT ad to 1 collectors. Number of submittors=1
04/15/16 14:12:29 Changed attribute: IdleJobs = 0
04/15/16 14:12:29 Changed attribute: RunningJobs = 0
04/15/16 14:12:29 Changed attribute: IdleJobs = 0
04/15/16 14:12:29 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:12:29 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:12:29 Changed attribute: HeldJobs = 0
04/15/16 14:12:29 Changed attribute: FlockedJobs = 0
04/15/16 14:12:29 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:12:29 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:12:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:12:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:12:29 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:12:29 Changed attribute: IdleJobs = 0
04/15/16 14:12:29 Changed attribute: RunningJobs = 0
04/15/16 14:12:29 Changed attribute: IdleJobs = 0
04/15/16 14:12:29 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:12:29 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:12:29 Changed attribute: HeldJobs = 0
04/15/16 14:12:29 Changed attribute: FlockedJobs = 0
04/15/16 14:12:29 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:12:29 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:12:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:12:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:12:29 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:12:29 Changed attribute: IdleJobs = 0
04/15/16 14:12:29 Changed attribute: RunningJobs = 0
04/15/16 14:12:29 Changed attribute: IdleJobs = 0
04/15/16 14:12:29 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:12:29 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:12:29 Changed attribute: HeldJobs = 0
04/15/16 14:12:29 Changed attribute: FlockedJobs = 0
04/15/16 14:12:29 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:12:29 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:12:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:12:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:12:29 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:12:29 Starting condor_gmanager for owner winckler (0.0)
04/15/16 14:12:29 Really Execing condor_gridmanager -f -C (Owner=?="winckler"&&JobUniverse==9) -o winckler -S /tmp/condor_g_scratch.0x7fa721ffa9c0.9936
04/15/16 14:12:29 Create_Process: using fast clone() to create child process.
04/15/16 14:12:29 Started condor_gmanager for owner winckler pid=1618058
04/15/16 14:12:29 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:12:29 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:12:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:12:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:12:29 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:12:29 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:12:29 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:12:29 Trying to update collector <200.145.46.35:9619>
04/15/16 14:12:29 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:12:29 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:12:29 ============ Begin clean_shadow_recs =============
04/15/16 14:12:29 ============ End clean_shadow_recs =============
04/15/16 14:12:29 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:12:29 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:12:29 Trying to query collector <200.145.46.35:9619>
04/15/16 14:12:29 Can't find address for negotiator 
04/15/16 14:12:29 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:12:29 ForkWorker::Fork: New child of 9936 = 1618059
04/15/16 14:12:29 Number of Active Workers 0
04/15/16 14:12:29 DaemonCore: No more children processes to reap.
04/15/16 14:12:30 ForkWorker::Fork: New child of 9936 = 1618171
04/15/16 14:12:30 Number of Active Workers 0
04/15/16 14:12:30 DaemonCore: No more children processes to reap.
04/15/16 14:12:31 ForkWorker::Fork: New child of 9936 = 1618270
04/15/16 14:12:31 Number of Active Workers 0
04/15/16 14:12:31 DaemonCore: No more children processes to reap.
04/15/16 14:12:32 Send_Signal(): Doing kill(1618058,12) [SIGUSR2]
04/15/16 14:12:32 QMGR Connection closed
04/15/16 14:12:32 ForkWorker::Fork: New child of 9936 = 1618399
04/15/16 14:12:32 Number of Active Workers 0
04/15/16 14:12:32 DaemonCore: No more children processes to reap.
04/15/16 14:12:33 ForkWorker::Fork: New child of 9936 = 1618498
04/15/16 14:12:33 Number of Active Workers 0
04/15/16 14:12:33 DaemonCore: No more children processes to reap.
04/15/16 14:12:34 ForkWorker::Fork: New child of 9936 = 1618581
04/15/16 14:12:34 Number of Active Workers 0
04/15/16 14:12:34 DaemonCore: No more children processes to reap.
04/15/16 14:12:35 ForkWorker::Fork: New child of 9936 = 1618633
04/15/16 14:12:35 Number of Active Workers 0
04/15/16 14:12:35 DaemonCore: No more children processes to reap.
04/15/16 14:12:36 ForkWorker::Fork: New child of 9936 = 1618697
04/15/16 14:12:36 Number of Active Workers 0
04/15/16 14:12:36 DaemonCore: No more children processes to reap.
04/15/16 14:12:37 QMGR Connection closed
04/15/16 14:12:37 ForkWorker::Fork: New child of 9936 = 1618860
04/15/16 14:12:37 Number of Active Workers 0
04/15/16 14:12:37 DaemonCore: No more children processes to reap.
04/15/16 14:12:38 ForkWorker::Fork: New child of 9936 = 1619002
04/15/16 14:12:38 Number of Active Workers 0
04/15/16 14:12:38 DaemonCore: No more children processes to reap.
04/15/16 14:12:39 ForkWorker::Fork: New child of 9936 = 1619126
04/15/16 14:12:39 Number of Active Workers 0
04/15/16 14:12:39 DaemonCore: No more children processes to reap.
04/15/16 14:12:40 ForkWorker::Fork: New child of 9936 = 1619173
04/15/16 14:12:40 Number of Active Workers 0
04/15/16 14:12:40 DaemonCore: No more children processes to reap.
04/15/16 14:12:41 Getting monitoring info for pid 9936
04/15/16 14:12:41 ForkWorker::Fork: New child of 9936 = 1619247
04/15/16 14:12:41 Number of Active Workers 0
04/15/16 14:12:41 DaemonCore: No more children processes to reap.
04/15/16 14:12:42 QMGR Connection closed
04/15/16 14:12:42 ForkWorker::Fork: New child of 9936 = 1619379
04/15/16 14:12:42 Number of Active Workers 0
04/15/16 14:12:42 DaemonCore: No more children processes to reap.
04/15/16 14:12:43 ForkWorker::Fork: New child of 9936 = 1619499
04/15/16 14:12:43 Number of Active Workers 0
04/15/16 14:12:43 DaemonCore: No more children processes to reap.
04/15/16 14:12:44 ForkWorker::Fork: New child of 9936 = 1619577
04/15/16 14:12:44 Number of Active Workers 0
04/15/16 14:12:44 DaemonCore: No more children processes to reap.
04/15/16 14:12:45 -------- Begin starting jobs --------
04/15/16 14:12:45 -------- Done starting jobs --------
04/15/16 14:12:46 ForkWorker::Fork: New child of 9936 = 1619624
04/15/16 14:12:46 Number of Active Workers 0
04/15/16 14:12:46 DaemonCore: No more children processes to reap.
04/15/16 14:12:47 ForkWorker::Fork: New child of 9936 = 1619719
04/15/16 14:12:47 Number of Active Workers 0
04/15/16 14:12:47 DaemonCore: No more children processes to reap.
04/15/16 14:12:48 ForkWorker::Fork: New child of 9936 = 1619872
04/15/16 14:12:48 Number of Active Workers 0
04/15/16 14:12:48 DaemonCore: No more children processes to reap.
04/15/16 14:12:49 ForkWorker::Fork: New child of 9936 = 1620001
04/15/16 14:12:49 Number of Active Workers 0
04/15/16 14:12:49 DaemonCore: No more children processes to reap.
04/15/16 14:12:50 ForkWorker::Fork: New child of 9936 = 1620089
04/15/16 14:12:50 Number of Active Workers 0
04/15/16 14:12:50 DaemonCore: No more children processes to reap.
04/15/16 14:12:51 ForkWorker::Fork: New child of 9936 = 1620131
04/15/16 14:12:51 Number of Active Workers 0
04/15/16 14:12:51 DaemonCore: No more children processes to reap.
04/15/16 14:12:52 ForkWorker::Fork: New child of 9936 = 1620238
04/15/16 14:12:52 Number of Active Workers 0
04/15/16 14:12:52 DaemonCore: No more children processes to reap.
04/15/16 14:12:53 Evaluated periodic expressions in 0.000s, scheduling next run in 300s
04/15/16 14:12:53 ForkWorker::Fork: New child of 9936 = 1620374
04/15/16 14:12:53 Number of Active Workers 0
04/15/16 14:12:53 DaemonCore: No more children processes to reap.
04/15/16 14:12:54 ForkWorker::Fork: New child of 9936 = 1620483
04/15/16 14:12:54 Number of Active Workers 0
04/15/16 14:12:54 DaemonCore: No more children processes to reap.
04/15/16 14:12:55 ForkWorker::Fork: New child of 9936 = 1620541
04/15/16 14:12:55 Number of Active Workers 0
04/15/16 14:12:55 DaemonCore: No more children processes to reap.
04/15/16 14:12:56 ForkWorker::Fork: New child of 9936 = 1620583
04/15/16 14:12:56 Number of Active Workers 0
04/15/16 14:12:56 DaemonCore: No more children processes to reap.
04/15/16 14:12:57 ForkWorker::Fork: New child of 9936 = 1620679
04/15/16 14:12:57 Number of Active Workers 0
04/15/16 14:12:57 DaemonCore: No more children processes to reap.
04/15/16 14:12:58 ForkWorker::Fork: New child of 9936 = 1620746
04/15/16 14:12:58 Number of Active Workers 0
04/15/16 14:12:58 DaemonCore: No more children processes to reap.
04/15/16 14:12:59 ForkWorker::Fork: New child of 9936 = 1620793
04/15/16 14:12:59 Number of Active Workers 0
04/15/16 14:12:59 DaemonCore: No more children processes to reap.
04/15/16 14:13:00 ForkWorker::Fork: New child of 9936 = 1620875
04/15/16 14:13:00 Number of Active Workers 0
04/15/16 14:13:00 DaemonCore: No more children processes to reap.
04/15/16 14:13:01 ForkWorker::Fork: New child of 9936 = 1621108
04/15/16 14:13:01 Number of Active Workers 0
04/15/16 14:13:01 DaemonCore: No more children processes to reap.
04/15/16 14:13:02 ForkWorker::Fork: New child of 9936 = 1621200
04/15/16 14:13:02 Number of Active Workers 0
04/15/16 14:13:02 DaemonCore: No more children processes to reap.
04/15/16 14:13:04 ForkWorker::Fork: New child of 9936 = 1621243
04/15/16 14:13:04 Number of Active Workers 0
04/15/16 14:13:04 DaemonCore: No more children processes to reap.
04/15/16 14:13:05 ForkWorker::Fork: New child of 9936 = 1621483
04/15/16 14:13:05 Number of Active Workers 0
04/15/16 14:13:05 DaemonCore: No more children processes to reap.
04/15/16 14:13:06 ForkWorker::Fork: New child of 9936 = 1621630
04/15/16 14:13:06 Number of Active Workers 0
04/15/16 14:13:06 DaemonCore: No more children processes to reap.
04/15/16 14:13:07 ForkWorker::Fork: New child of 9936 = 1621675
04/15/16 14:13:07 Number of Active Workers 0
04/15/16 14:13:07 DaemonCore: No more children processes to reap.
04/15/16 14:13:08 ForkWorker::Fork: New child of 9936 = 1621793
04/15/16 14:13:08 Number of Active Workers 0
04/15/16 14:13:08 DaemonCore: No more children processes to reap.
04/15/16 14:13:09 ForkWorker::Fork: New child of 9936 = 1621869
04/15/16 14:13:09 Number of Active Workers 0
04/15/16 14:13:09 DaemonCore: No more children processes to reap.
04/15/16 14:13:10 ForkWorker::Fork: New child of 9936 = 1622008
04/15/16 14:13:10 Number of Active Workers 0
04/15/16 14:13:10 DaemonCore: No more children processes to reap.
04/15/16 14:13:11 ForkWorker::Fork: New child of 9936 = 1622057
04/15/16 14:13:11 Number of Active Workers 0
04/15/16 14:13:11 DaemonCore: No more children processes to reap.
04/15/16 14:13:12 ForkWorker::Fork: New child of 9936 = 1622125
04/15/16 14:13:12 Number of Active Workers 0
04/15/16 14:13:12 DaemonCore: No more children processes to reap.
04/15/16 14:13:13 ForkWorker::Fork: New child of 9936 = 1622190
04/15/16 14:13:13 Number of Active Workers 0
04/15/16 14:13:13 DaemonCore: No more children processes to reap.
04/15/16 14:13:14 ForkWorker::Fork: New child of 9936 = 1622231
04/15/16 14:13:14 Number of Active Workers 0
04/15/16 14:13:14 DaemonCore: No more children processes to reap.
04/15/16 14:13:15 ForkWorker::Fork: New child of 9936 = 1622296
04/15/16 14:13:15 Number of Active Workers 0
04/15/16 14:13:15 DaemonCore: No more children processes to reap.
04/15/16 14:13:16 ForkWorker::Fork: New child of 9936 = 1622297
04/15/16 14:13:16 Number of Active Workers 0
04/15/16 14:13:16 DaemonCore: No more children processes to reap.
04/15/16 14:13:17 ForkWorker::Fork: New child of 9936 = 1622342
04/15/16 14:13:17 Number of Active Workers 0
04/15/16 14:13:17 DaemonCore: No more children processes to reap.
04/15/16 14:13:18 ForkWorker::Fork: New child of 9936 = 1622349
04/15/16 14:13:18 Number of Active Workers 0
04/15/16 14:13:18 DaemonCore: No more children processes to reap.
04/15/16 14:13:20 ForkWorker::Fork: New child of 9936 = 1622377
04/15/16 14:13:20 Number of Active Workers 0
04/15/16 14:13:20 DaemonCore: No more children processes to reap.
04/15/16 14:13:21 ForkWorker::Fork: New child of 9936 = 1622469
04/15/16 14:13:21 Number of Active Workers 0
04/15/16 14:13:21 DaemonCore: No more children processes to reap.
04/15/16 14:13:22 ForkWorker::Fork: New child of 9936 = 1622560
04/15/16 14:13:22 Number of Active Workers 0
04/15/16 14:13:22 DaemonCore: No more children processes to reap.
04/15/16 14:13:23 ForkWorker::Fork: New child of 9936 = 1622667
04/15/16 14:13:23 Number of Active Workers 0
04/15/16 14:13:23 DaemonCore: No more children processes to reap.
04/15/16 14:13:24 ForkWorker::Fork: New child of 9936 = 1622724
04/15/16 14:13:24 Number of Active Workers 0
04/15/16 14:13:24 DaemonCore: No more children processes to reap.
04/15/16 14:13:25 ForkWorker::Fork: New child of 9936 = 1622799
04/15/16 14:13:25 Number of Active Workers 0
04/15/16 14:13:25 DaemonCore: No more children processes to reap.
04/15/16 14:13:26 ForkWorker::Fork: New child of 9936 = 1622850
04/15/16 14:13:26 Number of Active Workers 0
04/15/16 14:13:26 DaemonCore: No more children processes to reap.
04/15/16 14:13:27 ForkWorker::Fork: New child of 9936 = 1622900
04/15/16 14:13:27 Number of Active Workers 0
04/15/16 14:13:27 DaemonCore: No more children processes to reap.
04/15/16 14:13:28 ForkWorker::Fork: New child of 9936 = 1622968
04/15/16 14:13:28 Number of Active Workers 0
04/15/16 14:13:28 DaemonCore: No more children processes to reap.
04/15/16 14:13:29 ForkWorker::Fork: New child of 9936 = 1623017
04/15/16 14:13:29 Number of Active Workers 0
04/15/16 14:13:29 DaemonCore: No more children processes to reap.
04/15/16 14:13:29 QMGR Connection closed
04/15/16 14:13:30 ForkWorker::Fork: New child of 9936 = 1623094
04/15/16 14:13:30 Number of Active Workers 0
04/15/16 14:13:30 DaemonCore: No more children processes to reap.
04/15/16 14:13:31 ForkWorker::Fork: New child of 9936 = 1623191
04/15/16 14:13:31 Number of Active Workers 0
04/15/16 14:13:31 DaemonCore: No more children processes to reap.
04/15/16 14:13:32 ForkWorker::Fork: New child of 9936 = 1623277
04/15/16 14:13:32 Number of Active Workers 0
04/15/16 14:13:32 DaemonCore: No more children processes to reap.
04/15/16 14:13:33 ForkWorker::Fork: New child of 9936 = 1623387
04/15/16 14:13:33 Number of Active Workers 0
04/15/16 14:13:33 DaemonCore: No more children processes to reap.
04/15/16 14:13:34 ForkWorker::Fork: New child of 9936 = 1623440
04/15/16 14:13:34 Number of Active Workers 0
04/15/16 14:13:34 DaemonCore: No more children processes to reap.
04/15/16 14:13:35 ForkWorker::Fork: New child of 9936 = 1623507
04/15/16 14:13:35 Number of Active Workers 0
04/15/16 14:13:35 DaemonCore: No more children processes to reap.
04/15/16 14:13:36 ForkWorker::Fork: New child of 9936 = 1623559
04/15/16 14:13:36 Number of Active Workers 0
04/15/16 14:13:36 DaemonCore: No more children processes to reap.
04/15/16 14:13:37 ForkWorker::Fork: New child of 9936 = 1623617
04/15/16 14:13:37 Number of Active Workers 0
04/15/16 14:13:37 DaemonCore: No more children processes to reap.
04/15/16 14:13:39 ForkWorker::Fork: New child of 9936 = 1623676
04/15/16 14:13:39 Number of Active Workers 0
04/15/16 14:13:39 DaemonCore: No more children processes to reap.
04/15/16 14:13:40 ForkWorker::Fork: New child of 9936 = 1623722
04/15/16 14:13:40 Number of Active Workers 0
04/15/16 14:13:40 DaemonCore: No more children processes to reap.
04/15/16 14:13:41 ForkWorker::Fork: New child of 9936 = 1623808
04/15/16 14:13:41 Number of Active Workers 0
04/15/16 14:13:41 DaemonCore: No more children processes to reap.
04/15/16 14:13:42 ForkWorker::Fork: New child of 9936 = 1623902
04/15/16 14:13:42 Number of Active Workers 0
04/15/16 14:13:42 DaemonCore: No more children processes to reap.
04/15/16 14:13:43 ForkWorker::Fork: New child of 9936 = 1624003
04/15/16 14:13:43 Number of Active Workers 0
04/15/16 14:13:43 DaemonCore: No more children processes to reap.
04/15/16 14:13:44 QMGR Connection closed
04/15/16 14:13:44 ForkWorker::Fork: New child of 9936 = 1624118
04/15/16 14:13:44 Number of Active Workers 0
04/15/16 14:13:44 DaemonCore: No more children processes to reap.
04/15/16 14:13:45 ForkWorker::Fork: New child of 9936 = 1624168
04/15/16 14:13:45 Number of Active Workers 0
04/15/16 14:13:45 DaemonCore: No more children processes to reap.
04/15/16 14:13:46 ForkWorker::Fork: New child of 9936 = 1624242
04/15/16 14:13:46 Number of Active Workers 0
04/15/16 14:13:46 DaemonCore: No more children processes to reap.
04/15/16 14:13:47 ForkWorker::Fork: New child of 9936 = 1624317
04/15/16 14:13:47 Number of Active Workers 0
04/15/16 14:13:47 DaemonCore: No more children processes to reap.
04/15/16 14:13:48 ForkWorker::Fork: New child of 9936 = 1624385
04/15/16 14:13:48 Number of Active Workers 0
04/15/16 14:13:48 DaemonCore: No more children processes to reap.
04/15/16 14:13:49 Added data to SelfDrainingQueue job_is_finished_queue, now has 1 element(s)
04/15/16 14:13:49 Registered timer for SelfDrainingQueue job_is_finished_queue, period: 0 (id: 1750)
04/15/16 14:13:49 Job 30.0 is finished
04/15/16 14:13:49 schedd: DestroyProc cluster 30 proc 0 rval 1 errno 0
04/15/16 14:13:49 QMGR Connection closed
04/15/16 14:13:49 Inside SelfDrainingQueue::timerHandler() for job_is_finished_queue
04/15/16 14:13:49 Job cleanup for 30.0 will not block, calling jobIsFinished() directly
04/15/16 14:13:49 jobIsFinished() completed, calling DestroyProc(30.0)
04/15/16 14:13:49 SelfDrainingQueue job_is_finished_queue is empty, not resetting timer
04/15/16 14:13:49 Canceling timer for SelfDrainingQueue job_is_finished_queue (timer id: 1750)
04/15/16 14:13:49 DaemonCore: No more children processes to reap.
04/15/16 14:13:49 condor_gridmanager (PID 1618058, owner winckler) exited with return code 0.
04/15/16 14:13:49 Initializing Directory: curr_dir = /tmp/condor_g_scratch.0x7fa721ffa9c0.9936
04/15/16 14:13:49 Removed scratch dir /tmp/condor_g_scratch.0x7fa721ffa9c0.9936
04/15/16 14:13:49 ForkWorker::Fork: New child of 9936 = 1624453
04/15/16 14:13:49 Number of Active Workers 0
04/15/16 14:13:49 DaemonCore: No more children processes to reap.
04/15/16 14:13:50 ForkWorker::Fork: New child of 9936 = 1624498
04/15/16 14:13:50 Number of Active Workers 0
04/15/16 14:13:50 DaemonCore: No more children processes to reap.
04/15/16 14:13:51 ForkWorker::Fork: New child of 9936 = 1624519
04/15/16 14:13:51 Number of Active Workers 0
04/15/16 14:13:51 DaemonCore: No more children processes to reap.
04/15/16 14:13:52 ForkWorker::Fork: New child of 9936 = 1624560
04/15/16 14:13:52 Number of Active Workers 0
04/15/16 14:13:52 DaemonCore: No more children processes to reap.
04/15/16 14:13:53 QMGR Connection closed
04/15/16 14:13:53 ForkWorker::Fork: New child of 9936 = 1624573
04/15/16 14:13:53 Number of Active Workers 0
04/15/16 14:13:53 DaemonCore: No more children processes to reap.
04/15/16 14:13:53 This process has a valid certificate & key
04/15/16 14:13:53 Finishing authenticate_server_gss_post with status=1
04/15/16 14:13:53 Finishing authenticate_server_gss_post with status=1
04/15/16 14:13:53 ZKM: successful mapping to GSS_ASSIST_GRIDMAP
04/15/16 14:13:53 Command=TRANSFER_DATA_WITH_PERMS, peer=<200.145.46.37:55049>
04/15/16 14:13:53 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 14:13:54 Looking at spooling: mode is 489
04/15/16 14:13:54 Scheduler::spoolJobFiles: TRANSFER_DATA/WITH_PERMS: 1 jobs matched constraint (ClusterId==29)
04/15/16 14:13:54 Transferring files for jobs 29.0
04/15/16 14:13:54 Scheduler::generalJobFilesWorkerThread: TRANSFER_DATA/WITH_PERMS: 1 jobs to be sent
04/15/16 14:13:54 spoolJobFiles(): started worker process
04/15/16 14:13:54 generalJobFilesWorkerThread(): transfer files for job 29.0
04/15/16 14:13:54 The submitting job ad as the FileTransferObject sees it
NumCkpts_RAW = 0
BufferSize = 524288
NiceUser = false
CoreSize = 0
CumulativeSlotTime = 0
OnExitHold = false
RequestCpus = 1
Err = "_condor_stderr"
BufferBlockSize = 32768
ExecutableSize_RAW = 18
WantCheckpoint = false
CommittedTime = 0
TargetType = "Machine"
WhenToTransferOutput = "ON_EXIT"
JobUniverse = 5
ExitBySignal = false
TransferIn = false
NumRestarts = 0
EncryptExecuteDirectory = false
CommittedSuspensionTime = 0
Owner = "winckler"
NumSystemHolds = 0
CumulativeSuspensionTime = 0
Environment = "GLOBUS_TCP_SOURCE_RANGE=20000,29999 _LMFILES_=/opt/modules/gridunesp/1 BYOBU_SED=sed' '--follow-symlinks LC_MEASUREMENT=pt_BR.UTF-8 BYOBU_READLINK=readlink BYOBU_CONFIG_DIR=/home/winckler/.byobu LCMAPS_DEBUG_LEVEL=3 GV_DIR=/opt/gaussian/gv SHLVL=2 LS_COLORS= PWD=/home/winckler BYOBU_TIME=%H:%M:%S SSH_AUTH_SOCK=/home/winckler/.byobu/.ssh-agent BYOBU_WINDOWS=/home/winckler/.byobu/windows SSH_CLIENT=200.145.46.228' '49266' '22 CVS_RSH=ssh PGI_TERM=trace,abort LC_TELEPHONE=pt_BR.UTF-8 STY=2420116.byobu PATH=/home/winckler/bin:/usr/local/src/goo-client/scripts:/home/winckler/bin:/opt/gridunesp/internals//bin:/usr/lib64/qt-3.3/bin:/usr/local/src/goo-client/scripts:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/home/winckler/bin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 BYOBU_RUN_DIR=/dev/shm/byobu-winckler-yXoDFpVj TOOL_DEBUG=D_FULLDEBUG MODULESHOME=/usr/share/Modules G09BASIS=/opt/gaussian/g09/basis GAUSS_EXEDIR=/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 X509_CERT_DIR=/etc/grid-security/certificates GLOBUS_TCP_PORT_RANGE=20000,29999 QTLIB=/usr/lib64/qt-3.3/lib LC_NAME=pt_BR.UTF-8 GATEKEEPER_JM_ID=2016-04-15.17:06:01.0002423004.0000000000 HISTCONTROL=ignoredups LCMAPS_DIR=/etc BYOBU_DARK=black LC_NUMERIC=pt_BR.UTF-8 GOO_API_URI=https://submit.grid.unesp.br/api/v1/ g09root=/opt/gaussian _DSM_BARRIER=SHM BYOBU_ULIMIT=ulimit SSH_TTY=/dev/pts/20 _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI MYPROXY_SERVER=myproxy.grid.unesp.br SHELL=/bin/bash GAUSS_SCRDIR=/tmp MAIL=/var/spool/mail/winckler GAUSS_ARCHDIR=/opt/gaussian/g09/arch BYOBU_HIGHLIGHT=#DD4814 LCMAPS_DB_FILE=/etc/lcmaps/lcmaps.db LC_CTYPE=pt_BR.UTF-8 LC_ADDRESS=pt_BR.UTF-8 LOADEDMODULES=gridunesp/1 BYOBU_LIGHT=white USER=winckler SSH_CONNECTION=200.145.46.228' '49266' '200.145.46.37' '22 HOSTNAME=access.grid.unesp.br GRIDUNESP_SLURM_PARTITIONS=rack1,rack2,rack3,rack4,rack5,rack6,rack7,rack8 LD_LIBRARY_PATH=/opt/gridunesp/internals//lib:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/opt/gaussian/gv/lib PYTHONPATH=/usr/local/src/python-gooclientlib:/usr/local/src/goo-client:/usr/local/src/python-gooclientlib:/usr/local/src/goo-client: BYOBU_TTY=/dev/pts/20 BYOBU_WINDOW_NAME=- BYOBU_BACKEND=screen GAUSS_LEXEDIR=/opt/gaussian/g09/linda-exe HOME=/home/winckler LOGNAME=winckler BYOBU_PYTHON=python2 LC_PAPER=pt_BR.UTF-8 MODULEPATH=/opt/modules JOB_REPOSITORY_ID=2016-04-15.17:06:01.0002423004.0000000000 _=/usr/bin/condor_ce_run LC_MONETARY=pt_BR.UTF-8 LC_TIME=pt_BR.UTF-8 G_BROKEN_FILENAMES=1 BYOBU_DISTRO=CentOS BYOBU_ACCENT=#75507B LANG=en_US.UTF-8 CONDOR_CONFIG=/etc/condor-ce/condor_config HISTSIZE=1000 BYOBU_PREFIX=/usr QTDIR=/usr/lib64/qt-3.3 BYOBU_DATE=%Y-%m-%d' ' BYOBU_CHARMAP=UTF-8 LCMAPS_POLICY_NAME= BYOBU_PAGER=less QTINC=/usr/lib64/qt-3.3/include TERM=screen-256color-bce WINDOW=0 LESSOPEN=||/usr/bin/lesspipe.sh' '%s LC_IDENTIFICATION=pt_BR.UTF-8"
RequestDisk = DiskUsage
Requirements = ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
MinHosts = 1
JobNotification = 0
NumCkpts = 0
NumJobStarts = 0
WantRemoteSyscalls = false
JobLeaseDuration = 2400
JobPrio = 0
RootDir = "/"
CurrentHosts = 0
x509UserProxyExpiration = 1460781781
WantRemoteIO = true
StreamOut = false
OnExitRemove = true
In = "/dev/null"
DiskUsage = 20
PeriodicRemove = false
LocalUserCpu = 0.0
ExecutableSize = 20
LocalSysCpu = 0.0
RemoteSysCpu = 0.0
ClusterId = 29
RemoteWallClockTime = 0.0
Rank = 0.0
LeaveJobInQueue = ( StageOutFinish > 0 ) =!= true
x509UserProxyEmail = "winckler@ncc.unesp.br"
CondorVersion = "$CondorVersion: 8.4.4 Feb 04 2016 $"
MyType = "Job"
StreamErr = false
DiskUsage_RAW = 18
PeriodicHold = false
User = "winckler@users.opensciencegrid.org"
Out = "_condor_stdout"
PeriodicRelease = false
MaxHosts = 1
RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,( ImageSize + 1023 ) / 1024)
Args = ""
CommittedSlotTime = 0
TotalSuspensions = 0
x509userproxysubject = "/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.7 $"
TransferInputSizeMB = 0
ExitStatus = 0
ShouldTransferFiles = "YES"
QDate = 1460739961
TransferOutputRemaps = undefined
SUBMIT_TransferOutputRemaps = "_condor_stdout=/home/winckler/.stdout_2423003_eCP6tB;_condor_stderr=/home/winckler/.stderr_2423003__KuXP1"
EnteredCurrentStatus = 1460740044
ImageSize = 0
SUBMIT_UserLog = "/home/winckler/.log_2423003_NYp6fI"
x509userproxy = "x509up_u10001"
CompletionDate = 1460740429
RemoteUserCpu = 0
ManagedManager = "htcondor-ce"
JobStatus = 4
UserLog = ".log_2423003_NYp6fI"
ExitCode = 0
Cmd = "hostname"
GlobalJobId = "ce.grid.unesp.br#29.0#1460740043"
LastSuspensionTime = 0
LastJobStatus = 1
StageInFinish = 1460740043
SUBMIT_Cmd = "/bin/hostname"
HoldReason = undefined
Managed = "External"
HoldReasonCode = undefined
SUBMIT_x509userproxy = "/tmp/x509up_u10001"
Iwd = "/osg/condor/29/0/cluster29.proc0.subproc0"
StageOutStart = 1460740434
SUBMIT_Iwd = "/home/winckler"
ImageSize_RAW = 0
StageInStart = 1460740043
ReleaseReason = "Data files spooled"
RoutedToJobId = "30.0"
LastHoldReason = "Spooling input data files"
LastHoldReasonCode = 16
ProcId = 0
04/15/16 14:13:54 entering FileTransfer::SimpleInit
04/15/16 14:13:54 FILETRANSFER: protocol "http" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:13:54 FILETRANSFER: protocol "ftp" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:13:54 FILETRANSFER: protocol "file" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:13:54 FILETRANSFER: protocol "data" handled by "/usr/libexec/condor/data_plugin"
04/15/16 14:13:54 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0
04/15/16 14:13:54 set_user_egid() called when UserIds not inited!
04/15/16 14:13:54 set_user_euid() called when UserIds not inited!
04/15/16 14:13:54 set_user_egid() called when UserIds not inited!
04/15/16 14:13:54 set_user_euid() called when UserIds not inited!
04/15/16 14:13:54 set_user_egid() called when UserIds not inited!
04/15/16 14:13:54 set_user_euid() called when UserIds not inited!
04/15/16 14:13:54 set_user_egid() called when UserIds not inited!
04/15/16 14:13:54 set_user_euid() called when UserIds not inited!
04/15/16 14:13:54 set_user_egid() called when UserIds not inited!
04/15/16 14:13:54 set_user_euid() called when UserIds not inited!
04/15/16 14:13:54 entering FileTransfer::UploadFiles (final_transfer=1)
04/15/16 14:13:54 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0
04/15/16 14:13:54 Sending changed file .log_2423003_NYp6fI, t: 1460740342, 1460740043, s: 768, N/A
04/15/16 14:13:54 Skipping file hostname, t: 1460740043<=1460740043, s: N/A
04/15/16 14:13:54 Sending changed file _condor_stdout, t: 1460740278, 1460740043, s: 14, N/A
04/15/16 14:13:54 Sending dynamically added output file _condor_stderr
04/15/16 14:13:54 entering FileTransfer::Upload
04/15/16 14:13:54 entering FileTransfer::DoUpload
04/15/16 14:13:54 DoUpload: sending file .log_2423003_NYp6fI
04/15/16 14:13:54 FILETRANSFER: outgoing file_command is 1 for .log_2423003_NYp6fI
04/15/16 14:13:54 Received GoAhead from peer to send /osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI and all further files.
04/15/16 14:13:54 Sending GoAhead for 200.145.46.37 to receive /osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI and all further files.
04/15/16 14:13:54 ReliSock::put_file_with_permissions(): going to send permissions 100644
04/15/16 14:13:54 put_file: going to send from filename /osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI
04/15/16 14:13:54 put_file: Found file size 768
04/15/16 14:13:54 put_file: sending 768 bytes
04/15/16 14:13:54 ReliSock: put_file: sent 768 bytes
04/15/16 14:13:54 DoUpload: sending file _condor_stdout
04/15/16 14:13:54 FILETRANSFER: outgoing file_command is 1 for _condor_stdout
04/15/16 14:13:54 ReliSock::put_file_with_permissions(): going to send permissions 100644
04/15/16 14:13:54 put_file: going to send from filename /osg/condor/29/0/cluster29.proc0.subproc0/_condor_stdout
04/15/16 14:13:54 put_file: Found file size 14
04/15/16 14:13:54 put_file: sending 14 bytes
04/15/16 14:13:54 ReliSock: put_file: sent 14 bytes
04/15/16 14:13:54 DoUpload: sending file _condor_stderr
04/15/16 14:13:54 FILETRANSFER: outgoing file_command is 1 for _condor_stderr
04/15/16 14:13:54 ReliSock::put_file_with_permissions(): going to send permissions 100644
04/15/16 14:13:54 put_file: going to send from filename /osg/condor/29/0/cluster29.proc0.subproc0/_condor_stderr
04/15/16 14:13:54 put_file: Found file size 0
04/15/16 14:13:54 put_file: sending 0 bytes
04/15/16 14:13:54 ReliSock: put_file: sent 0 bytes
04/15/16 14:13:54 DoUpload: exiting at 3366
04/15/16 14:13:54 Transfer completed
04/15/16 14:13:54 DaemonCore: No more children processes to reap.
04/15/16 14:13:54 transferJobFilesReaper tid=1624574 status=256
04/15/16 14:14:03 QMGR Connection closed
04/15/16 14:14:03 Remove jobs 30.0
04/15/16 14:14:03 OwnerCheck retval 1 (success), super_user
04/15/16 14:14:03 OwnerCheck retval 1 (success), super_user
04/15/16 14:14:03 OwnerCheck retval 1 (success), super_user
04/15/16 14:14:03 OwnerCheck retval 1 (success), super_user
04/15/16 14:14:03 OwnerCheck retval 1 (success), super_user
04/15/16 14:14:03 Added data to SelfDrainingQueue act_on_job_myself_queue, now has 1 element(s)
04/15/16 14:14:03 Registered timer for SelfDrainingQueue act_on_job_myself_queue, period: 0 (id: 1751)
04/15/16 14:14:03 Expedited call to StartJobs()
04/15/16 14:14:03 Finished Remove jobs 30.0
04/15/16 14:14:03 -------- Begin starting jobs --------
04/15/16 14:14:03 -------- Done starting jobs --------
04/15/16 14:14:03 Inside SelfDrainingQueue::timerHandler() for act_on_job_myself_queue
04/15/16 14:14:03 abort_job_myself: 30.0 action:Remove log_hold:true
04/15/16 14:14:03 Cleared dirty attributes for job 30.0
04/15/16 14:14:03 Writing record to user logfile=/osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI owner=winckler
04/15/16 14:14:03 WriteUserLog::initialize: opened /osg/condor/29/0/cluster29.proc0.subproc0/.log_2423003_NYp6fI successfully
04/15/16 14:14:03 Saving classad to history file
04/15/16 14:14:03 SelfDrainingQueue act_on_job_myself_queue is empty, not resetting timer
04/15/16 14:14:03 Canceling timer for SelfDrainingQueue act_on_job_myself_queue (timer id: 1751)
04/15/16 14:14:03 QMGR Connection closed
04/15/16 14:14:06 Received a superuser command
04/15/16 14:14:06 ForkWorker::Fork: New child of 9936 = 1625308
04/15/16 14:14:06 Number of Active Workers 0
04/15/16 14:14:06 DaemonCore: No more children processes to reap.
04/15/16 14:14:23 Received a superuser command
04/15/16 14:14:23 ForkWorker::Fork: New child of 9936 = 1625889
04/15/16 14:14:23 Number of Active Workers 0
04/15/16 14:14:23 DaemonCore: No more children processes to reap.
04/15/16 14:14:25 Received a superuser command
04/15/16 14:14:25 ForkWorker::Fork: New child of 9936 = 1626026
04/15/16 14:14:25 Number of Active Workers 0
04/15/16 14:14:25 DaemonCore: No more children processes to reap.
04/15/16 14:14:26 Received a superuser command
04/15/16 14:14:26 ForkWorker::Fork: New child of 9936 = 1626165
04/15/16 14:14:26 Number of Active Workers 0
04/15/16 14:14:26 DaemonCore: No more children processes to reap.
04/15/16 14:14:28 Received a superuser command
04/15/16 14:14:28 ForkWorker::Fork: New child of 9936 = 1626244
04/15/16 14:14:28 Number of Active Workers 0
04/15/16 14:14:28 DaemonCore: No more children processes to reap.
04/15/16 14:14:29 Received a superuser command
04/15/16 14:14:29 ForkWorker::Fork: New child of 9936 = 1626263
04/15/16 14:14:29 Number of Active Workers 0
04/15/16 14:14:29 DaemonCore: No more children processes to reap.
04/15/16 14:14:32 Received a superuser command
04/15/16 14:14:32 ForkWorker::Fork: New child of 9936 = 1626298
04/15/16 14:14:32 Number of Active Workers 0
04/15/16 14:14:32 DaemonCore: No more children processes to reap.
04/15/16 14:14:33 Received a superuser command
04/15/16 14:14:33 ForkWorker::Fork: New child of 9936 = 1626369
04/15/16 14:14:33 Number of Active Workers 0
04/15/16 14:14:33 DaemonCore: No more children processes to reap.
04/15/16 14:14:34 Received a superuser command
04/15/16 14:14:34 ForkWorker::Fork: New child of 9936 = 1626410
04/15/16 14:14:34 Number of Active Workers 0
04/15/16 14:14:34 DaemonCore: No more children processes to reap.
04/15/16 14:14:40 Received a superuser command
04/15/16 14:14:40 ForkWorker::Fork: New child of 9936 = 1626649
04/15/16 14:14:40 Number of Active Workers 0
04/15/16 14:14:40 DaemonCore: No more children processes to reap.
04/15/16 14:14:43 Received a superuser command
04/15/16 14:14:43 ForkWorker::Fork: New child of 9936 = 1626732
04/15/16 14:14:43 Number of Active Workers 0
04/15/16 14:14:43 DaemonCore: No more children processes to reap.
04/15/16 14:14:53 Received a superuser command
04/15/16 14:14:53 ForkWorker::Fork: New child of 9936 = 1627229
04/15/16 14:14:53 Number of Active Workers 0
04/15/16 14:14:53 DaemonCore: No more children processes to reap.
04/15/16 14:15:12 Received a superuser command
04/15/16 14:15:12 ForkWorker::Fork: New child of 9936 = 1629344
04/15/16 14:15:12 Number of Active Workers 0
04/15/16 14:15:12 DaemonCore: No more children processes to reap.
04/15/16 14:15:15 Received a superuser command
04/15/16 14:15:15 ForkWorker::Fork: New child of 9936 = 1629650
04/15/16 14:15:15 Number of Active Workers 0
04/15/16 14:15:15 DaemonCore: No more children processes to reap.
04/15/16 14:16:12 Received a superuser command
04/15/16 14:16:12 ForkWorker::Fork: New child of 9936 = 1633605
04/15/16 14:16:12 Number of Active Workers 0
04/15/16 14:16:12 DaemonCore: No more children processes to reap.
04/15/16 14:16:20 This process has a valid certificate & key
04/15/16 14:16:20 Finishing authenticate_server_gss_post with status=1
04/15/16 14:16:20 Finishing authenticate_server_gss_post with status=1
04/15/16 14:16:20 ZKM: successful mapping to GSS_ASSIST_GRIDMAP
04/15/16 14:16:20 Command=QMGMT_WRITE_CMD, peer=<200.145.46.37:3089>
04/15/16 14:16:20 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 14:16:20 OwnerCheck retval 1 (success),no ad
04/15/16 14:16:20 Submitting new job 31.0
04/15/16 14:16:20 schedd: NewCluster rval 31 errno 0
04/15/16 14:16:20 OwnerCheck retval 1 (success),no ad
04/15/16 14:16:20 schedd: NewProc rval 0 errno 0
04/15/16 14:16:20 New job: 31.0
04/15/16 14:16:20 Writing record to user logfile=/osg/condor/31/0/cluster31.proc0.subproc0/.log_2424195_0qm8sM owner=winckler
04/15/16 14:16:20 WriteUserLog::initialize: safe_open_wrapper("/osg/condor/31/0/cluster31.proc0.subproc0/.log_2424195_0qm8sM") failed - errno 13 (Permission denied)
04/15/16 14:16:20 WriteUserLog::initialize: failed to open file /osg/condor/31/0/cluster31.proc0.subproc0/.log_2424195_0qm8sM
04/15/16 14:16:20 WriteUserLog::initialize: opened /osg/condor/31/0/cluster31.proc0.subproc0/.log_2424195_0qm8sM successfully
04/15/16 14:16:20 New job: 31.0, Duplicate Keys: 2, Total Keys: 13 
04/15/16 14:16:20 QMGR Connection closed
04/15/16 14:16:20 Command=SPOOL_JOB_FILES_WITH_PERMS, peer=<200.145.46.37:48801>
04/15/16 14:16:20 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 14:16:20 spoolJobFiles(): read JobAdsArrayLen - 1
04/15/16 14:16:20 Looking at spooling: mode is 488
04/15/16 14:16:20 job_status is 5
04/15/16 14:16:20 Transferring files for jobs 31.0
04/15/16 14:16:20 spoolJobFiles(): started worker process
04/15/16 14:16:20 generalJobFilesWorkerThread(): transfer files for job 31.0
04/15/16 14:16:20 The submitting job ad as the FileTransferObject sees it
NumCkpts_RAW = 0
BufferSize = 524288
NiceUser = false
CoreSize = 0
CumulativeSlotTime = 0
OnExitHold = false
RequestCpus = 1
Err = "_condor_stderr"
BufferBlockSize = 32768
ExecutableSize_RAW = 18
ImageSize = 20
WantCheckpoint = false
CommittedTime = 0
TargetType = "Machine"
WhenToTransferOutput = "ON_EXIT"
JobUniverse = 5
ExitBySignal = false
HoldReasonCode = 16
TransferIn = false
NumRestarts = 0
EncryptExecuteDirectory = false
CommittedSuspensionTime = 0
Owner = "winckler"
NumSystemHolds = 0
CumulativeSuspensionTime = 0
Environment = "GLOBUS_TCP_SOURCE_RANGE=20000,29999 _LMFILES_=/opt/modules/gridunesp/1 BYOBU_SED=sed' '--follow-symlinks LC_MEASUREMENT=pt_BR.UTF-8 BYOBU_READLINK=readlink BYOBU_CONFIG_DIR=/home/winckler/.byobu LCMAPS_DEBUG_LEVEL=3 GV_DIR=/opt/gaussian/gv SHLVL=2 LS_COLORS= PWD=/home/winckler BYOBU_TIME=%H:%M:%S SSH_AUTH_SOCK=/home/winckler/.byobu/.ssh-agent BYOBU_WINDOWS=/home/winckler/.byobu/windows SSH_CLIENT=200.145.46.228' '49266' '22 CVS_RSH=ssh PGI_TERM=trace,abort LC_TELEPHONE=pt_BR.UTF-8 STY=2420116.byobu PATH=/home/winckler/bin:/usr/local/src/goo-client/scripts:/home/winckler/bin:/opt/gridunesp/internals//bin:/usr/lib64/qt-3.3/bin:/usr/local/src/goo-client/scripts:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/home/winckler/bin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 BYOBU_RUN_DIR=/dev/shm/byobu-winckler-yXoDFpVj TOOL_DEBUG=D_FULLDEBUG MODULESHOME=/usr/share/Modules G09BASIS=/opt/gaussian/g09/basis GAUSS_EXEDIR=/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 X509_CERT_DIR=/etc/grid-security/certificates GLOBUS_TCP_PORT_RANGE=20000,29999 QTLIB=/usr/lib64/qt-3.3/lib LC_NAME=pt_BR.UTF-8 GATEKEEPER_JM_ID=2016-04-15.17:14:58.0002424196.0000000000 HISTCONTROL=ignoredups LCMAPS_DIR=/etc BYOBU_DARK=black LC_NUMERIC=pt_BR.UTF-8 GOO_API_URI=https://submit.grid.unesp.br/api/v1/ g09root=/opt/gaussian _DSM_BARRIER=SHM BYOBU_ULIMIT=ulimit SSH_TTY=/dev/pts/20 _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI MYPROXY_SERVER=myproxy.grid.unesp.br SHELL=/bin/bash GAUSS_SCRDIR=/tmp MAIL=/var/spool/mail/winckler GAUSS_ARCHDIR=/opt/gaussian/g09/arch BYOBU_HIGHLIGHT=#DD4814 LCMAPS_DB_FILE=/etc/lcmaps/lcmaps.db LC_CTYPE=pt_BR.UTF-8 LC_ADDRESS=pt_BR.UTF-8 LOADEDMODULES=gridunesp/1 BYOBU_LIGHT=white USER=winckler SSH_CONNECTION=200.145.46.228' '49266' '200.145.46.37' '22 HOSTNAME=access.grid.unesp.br GRIDUNESP_SLURM_PARTITIONS=rack1,rack2,rack3,rack4,rack5,rack6,rack7,rack8 LD_LIBRARY_PATH=/opt/gridunesp/internals//lib:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/opt/gaussian/gv/lib PYTHONPATH=/usr/local/src/python-gooclientlib:/usr/local/src/goo-client:/usr/local/src/python-gooclientlib:/usr/local/src/goo-client: BYOBU_TTY=/dev/pts/20 BYOBU_WINDOW_NAME=- BYOBU_BACKEND=screen GAUSS_LEXEDIR=/opt/gaussian/g09/linda-exe HOME=/home/winckler LOGNAME=winckler BYOBU_PYTHON=python2 LC_PAPER=pt_BR.UTF-8 MODULEPATH=/opt/modules JOB_REPOSITORY_ID=2016-04-15.17:14:58.0002424196.0000000000 _=/usr/bin/condor_ce_run LC_MONETARY=pt_BR.UTF-8 LC_TIME=pt_BR.UTF-8 G_BROKEN_FILENAMES=1 BYOBU_DISTRO=CentOS BYOBU_ACCENT=#75507B LANG=en_US.UTF-8 CONDOR_CONFIG=/etc/condor-ce/condor_config HISTSIZE=1000 BYOBU_PREFIX=/usr QTDIR=/usr/lib64/qt-3.3 BYOBU_DATE=%Y-%m-%d' ' BYOBU_CHARMAP=UTF-8 LCMAPS_POLICY_NAME= BYOBU_PAGER=less QTINC=/usr/lib64/qt-3.3/include TERM=screen-256color-bce WINDOW=0 LESSOPEN=||/usr/bin/lesspipe.sh' '%s LC_IDENTIFICATION=pt_BR.UTF-8"
RequestDisk = DiskUsage
Requirements = ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
MinHosts = 1
JobNotification = 0
NumCkpts = 0
LastSuspensionTime = 0
NumJobStarts = 0
WantRemoteSyscalls = false
JobLeaseDuration = 2400
JobPrio = 0
RootDir = "/"
CurrentHosts = 0
x509UserProxyExpiration = 1460781781
WantRemoteIO = true
StreamOut = false
OnExitRemove = true
In = "/dev/null"
DiskUsage = 20
PeriodicRemove = false
LocalUserCpu = 0.0
RemoteUserCpu = 0.0
ExecutableSize = 20
LocalSysCpu = 0.0
RemoteSysCpu = 0.0
ClusterId = 31
CompletionDate = 0
RemoteWallClockTime = 0.0
Rank = 0.0
LeaveJobInQueue = ( StageOutFinish > 0 ) =!= true
ImageSize_RAW = 18
x509UserProxyEmail = "winckler@ncc.unesp.br"
CondorVersion = "$CondorVersion: 8.4.4 Feb 04 2016 $"
MyType = "Job"
HoldReason = "Spooling input data files"
StreamErr = false
DiskUsage_RAW = 18
PeriodicHold = false
User = "winckler@users.opensciencegrid.org"
Out = "_condor_stdout"
PeriodicRelease = false
MaxHosts = 1
RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,( ImageSize + 1023 ) / 1024)
Args = ""
CommittedSlotTime = 0
TotalSuspensions = 0
x509userproxysubject = "/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.7 $"
TransferInputSizeMB = 0
ExitStatus = 0
ShouldTransferFiles = "YES"
EnteredCurrentStatus = 1460740498
QDate = 1460740498
SUBMIT_Cmd = "/bin/hostname"
SUBMIT_x509userproxy = "/tmp/x509up_u10001"
ProcId = 0
x509userproxy = "x509up_u10001"
Iwd = "/osg/condor/31/0/cluster31.proc0.subproc0"
SUBMIT_TransferOutputRemaps = "_condor_stdout=/home/winckler/.stdout_2424195_wqvCns;_condor_stderr=/home/winckler/.stderr_2424195_ITigNv"
SUBMIT_UserLog = "/home/winckler/.log_2424195_0qm8sM"
Cmd = "hostname"
GlobalJobId = "ce.grid.unesp.br#31.0#1460740580"
JobStatus = 5
SUBMIT_Iwd = "/home/winckler"
TransferOutputRemaps = undefined
UserLog = ".log_2424195_0qm8sM"
StageInStart = 1460740580
04/15/16 14:16:20 entering FileTransfer::SimpleInit
04/15/16 14:16:20 FILETRANSFER: protocol "http" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:16:20 FILETRANSFER: protocol "ftp" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:16:20 FILETRANSFER: protocol "file" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:16:20 FILETRANSFER: protocol "data" handled by "/usr/libexec/condor/data_plugin"
04/15/16 14:16:20 entering FileTransfer::DownloadFiles
04/15/16 14:16:20 entering FileTransfer::Download
04/15/16 14:16:20 entering FileTransfer::DoDownload sync=1
04/15/16 14:16:20 Sending GoAhead for 200.145.46.37 to send /osg/condor/31/0/cluster31.proc0.subproc0.tmp/x509up_u10001 and all further files.
04/15/16 14:16:20 Received GoAhead from peer to receive /osg/condor/31/0/cluster31.proc0.subproc0.tmp/x509up_u10001 and all further files.
04/15/16 14:16:20 DoDownload: get_x509_delegation() returned 0
04/15/16 14:16:20 get_file(): going to write to filename /osg/condor/31/0/cluster31.proc0.subproc0.tmp/hostname
04/15/16 14:16:20 get_file: Receiving 17848 bytes
04/15/16 14:16:20 get_file: wrote 17848 bytes to file
04/15/16 14:16:20 ReliSock::get_file_with_permissions(): going to set permissions 755
04/15/16 14:16:20 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0.tmp
04/15/16 14:16:20 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0.swap
04/15/16 14:16:20 Received proxy for job 31.0
04/15/16 14:16:20 proxy path: /osg/condor/31/0/cluster31.proc0.subproc0/x509up_u10001
04/15/16 14:16:20 proxy expiration: 1460781781
04/15/16 14:16:20 proxy identity: /C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler
04/15/16 14:16:20 proxy subject: /C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler/CN=2025260236/CN=533974396
04/15/16 14:16:20 proxy email: winckler@ncc.unesp.br
04/15/16 14:16:20 Transfer completed
04/15/16 14:16:20 Scheduler::spoolJobFilesWorkerThread(void *arg, Stream* s) NAP TIME
04/15/16 14:16:20 -------- Begin starting jobs --------
04/15/16 14:16:20 -------- Done starting jobs --------
04/15/16 14:16:20 Clearing userlog file cache
04/15/16 14:16:20 JobsRunning = 0
04/15/16 14:16:20 JobsIdle = 0
04/15/16 14:16:20 JobsHeld = 1
04/15/16 14:16:20 JobsRemoved = 0
04/15/16 14:16:20 LocalUniverseJobsRunning = 0
04/15/16 14:16:20 LocalUniverseJobsIdle = 0
04/15/16 14:16:20 SchedUniverseJobsRunning = 0
04/15/16 14:16:20 SchedUniverseJobsIdle = 0
04/15/16 14:16:20 N_Owners = 1
04/15/16 14:16:20 MaxJobsRunning = 10000
04/15/16 14:16:20 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:16:20 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:16:20 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:16:20 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:20 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:20 Sent HEART BEAT ad to 1 collectors. Number of submittors=1
04/15/16 14:16:20 Changed attribute: IdleJobs = 0
04/15/16 14:16:20 Changed attribute: RunningJobs = 0
04/15/16 14:16:20 Changed attribute: IdleJobs = 0
04/15/16 14:16:20 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:16:20 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:16:20 Changed attribute: HeldJobs = 0
04/15/16 14:16:20 Changed attribute: FlockedJobs = 0
04/15/16 14:16:20 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:16:20 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:16:20 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:20 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:20 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:16:20 Changed attribute: IdleJobs = 0
04/15/16 14:16:20 Changed attribute: RunningJobs = 0
04/15/16 14:16:20 Changed attribute: IdleJobs = 0
04/15/16 14:16:20 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:16:20 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:16:20 Changed attribute: HeldJobs = 0
04/15/16 14:16:20 Changed attribute: FlockedJobs = 0
04/15/16 14:16:20 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:16:20 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:16:20 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:20 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:20 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:16:20 Changed attribute: IdleJobs = 0
04/15/16 14:16:20 Changed attribute: RunningJobs = 0
04/15/16 14:16:20 Changed attribute: IdleJobs = 0
04/15/16 14:16:20 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:16:20 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:16:20 Changed attribute: HeldJobs = 1
04/15/16 14:16:20 Changed attribute: FlockedJobs = 0
04/15/16 14:16:20 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:16:20 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:16:20 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:20 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:20 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:16:20 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:16:20 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:16:20 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:20 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:20 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:16:20 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:16:20 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:16:20 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:20 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:20 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:16:20 ============ Begin clean_shadow_recs =============
04/15/16 14:16:20 ============ End clean_shadow_recs =============
04/15/16 14:16:20 Job 31.0 held for spooling. Checking how long...
04/15/16 14:16:20 Job 31.0 on hold for 0 seconds.
04/15/16 14:16:20 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:16:20 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:20 Trying to query collector <200.145.46.35:9619>
04/15/16 14:16:20 Can't find address for negotiator 
04/15/16 14:16:20 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:16:20 ForkWorker::Fork: New child of 9936 = 1634020
04/15/16 14:16:20 Number of Active Workers 0
04/15/16 14:16:20 DaemonCore: No more children processes to reap.
04/15/16 14:16:21 Received a superuser command
04/15/16 14:16:21 ForkWorker::Fork: New child of 9936 = 1634034
04/15/16 14:16:21 Number of Active Workers 0
04/15/16 14:16:21 DaemonCore: No more children processes to reap.
04/15/16 14:16:21 DaemonCore: No more children processes to reap.
04/15/16 14:16:21 spoolJobFilesReaper tid=1633999 status=256
04/15/16 14:16:21 No HoldReasonSubCode found for job 31.0
04/15/16 14:16:21 Job 31.0 released from hold: Data files spooled
04/15/16 14:16:21 Setting delay until next queue scan to 5 seconds
04/15/16 14:16:21 -------- Begin starting jobs --------
04/15/16 14:16:21 -------- Done starting jobs --------
04/15/16 14:16:22 ForkWorker::Fork: New child of 9936 = 1634077
04/15/16 14:16:22 Number of Active Workers 0
04/15/16 14:16:22 DaemonCore: No more children processes to reap.
04/15/16 14:16:23 ForkWorker::Fork: New child of 9936 = 1634146
04/15/16 14:16:23 Number of Active Workers 0
04/15/16 14:16:23 DaemonCore: No more children processes to reap.
04/15/16 14:16:23 QMGR Connection closed
04/15/16 14:16:23 OwnerCheck retval 1 (success),no ad
04/15/16 14:16:23 Submitting new job 32.0
04/15/16 14:16:23 schedd: NewCluster rval 32 errno 0
04/15/16 14:16:23 OwnerCheck retval 1 (success),no ad
04/15/16 14:16:23 schedd: NewProc rval 0 errno 0
04/15/16 14:16:23 New job: 32.0
04/15/16 14:16:23 New job: 32.0, Duplicate Keys: 2, Total Keys: 102 
04/15/16 14:16:23 QMGR Connection closed
04/15/16 14:16:24 ForkWorker::Fork: New child of 9936 = 1634196
04/15/16 14:16:24 Number of Active Workers 0
04/15/16 14:16:24 DaemonCore: No more children processes to reap.
04/15/16 14:16:24 Received a superuser command
04/15/16 14:16:24 ForkWorker::Fork: New child of 9936 = 1634235
04/15/16 14:16:24 Number of Active Workers 0
04/15/16 14:16:24 DaemonCore: No more children processes to reap.
04/15/16 14:16:25 ForkWorker::Fork: New child of 9936 = 1634286
04/15/16 14:16:25 Number of Active Workers 0
04/15/16 14:16:25 DaemonCore: No more children processes to reap.
04/15/16 14:16:26 JobsRunning = 0
04/15/16 14:16:26 JobsIdle = 2
04/15/16 14:16:26 JobsHeld = 0
04/15/16 14:16:26 JobsRemoved = 0
04/15/16 14:16:26 LocalUniverseJobsRunning = 0
04/15/16 14:16:26 LocalUniverseJobsIdle = 0
04/15/16 14:16:26 SchedUniverseJobsRunning = 0
04/15/16 14:16:26 SchedUniverseJobsIdle = 0
04/15/16 14:16:26 N_Owners = 1
04/15/16 14:16:26 MaxJobsRunning = 10000
04/15/16 14:16:26 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:16:26 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:16:26 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:16:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:26 Sent HEART BEAT ad to 1 collectors. Number of submittors=1
04/15/16 14:16:26 Changed attribute: IdleJobs = 0
04/15/16 14:16:26 Changed attribute: RunningJobs = 0
04/15/16 14:16:26 Changed attribute: IdleJobs = 0
04/15/16 14:16:26 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:16:26 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:16:26 Changed attribute: HeldJobs = 0
04/15/16 14:16:26 Changed attribute: FlockedJobs = 0
04/15/16 14:16:26 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:16:26 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:16:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:26 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:16:26 Changed attribute: IdleJobs = 0
04/15/16 14:16:26 Changed attribute: RunningJobs = 0
04/15/16 14:16:26 Changed attribute: IdleJobs = 0
04/15/16 14:16:26 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:16:26 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:16:26 Changed attribute: HeldJobs = 0
04/15/16 14:16:26 Changed attribute: FlockedJobs = 0
04/15/16 14:16:26 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:16:26 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:16:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:26 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:16:26 Changed attribute: IdleJobs = 0
04/15/16 14:16:26 Changed attribute: RunningJobs = 0
04/15/16 14:16:26 Changed attribute: IdleJobs = 0
04/15/16 14:16:26 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:16:26 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:16:26 Changed attribute: HeldJobs = 0
04/15/16 14:16:26 Changed attribute: FlockedJobs = 0
04/15/16 14:16:26 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:16:26 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:16:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:26 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:16:26 Starting condor_gmanager for owner winckler (0.0)
04/15/16 14:16:26 Really Execing condor_gridmanager -f -C (Owner=?="winckler"&&JobUniverse==9) -o winckler -S /tmp/condor_g_scratch.0x7fa721fc6990.9936
04/15/16 14:16:26 Create_Process: using fast clone() to create child process.
04/15/16 14:16:26 Started condor_gmanager for owner winckler pid=1634355
04/15/16 14:16:26 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:16:26 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:16:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:26 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:16:26 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:16:26 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:16:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:16:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:26 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:16:26 ============ Begin clean_shadow_recs =============
04/15/16 14:16:26 ============ End clean_shadow_recs =============
04/15/16 14:16:26 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:16:26 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:16:26 Trying to query collector <200.145.46.35:9619>
04/15/16 14:16:26 Can't find address for negotiator 
04/15/16 14:16:26 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:16:26 ForkWorker::Fork: New child of 9936 = 1634358
04/15/16 14:16:26 Number of Active Workers 0
04/15/16 14:16:26 DaemonCore: No more children processes to reap.
04/15/16 14:16:26 Received a superuser command
04/15/16 14:16:26 ForkWorker::Fork: New child of 9936 = 1634361
04/15/16 14:16:26 Number of Active Workers 0
04/15/16 14:16:26 DaemonCore: No more children processes to reap.
04/15/16 14:16:27 ForkWorker::Fork: New child of 9936 = 1634400
04/15/16 14:16:27 Number of Active Workers 0
04/15/16 14:16:27 DaemonCore: No more children processes to reap.
04/15/16 14:16:28 ForkWorker::Fork: New child of 9936 = 1634422
04/15/16 14:16:28 Number of Active Workers 0
04/15/16 14:16:28 DaemonCore: No more children processes to reap.
04/15/16 14:16:29 Send_Signal(): Doing kill(1634355,12) [SIGUSR2]
04/15/16 14:16:29 QMGR Connection closed
04/15/16 14:16:29 ForkWorker::Fork: New child of 9936 = 1634427
04/15/16 14:16:29 Number of Active Workers 0
04/15/16 14:16:29 DaemonCore: No more children processes to reap.
04/15/16 14:16:30 ForkWorker::Fork: New child of 9936 = 1634438
04/15/16 14:16:30 Number of Active Workers 0
04/15/16 14:16:30 DaemonCore: No more children processes to reap.
04/15/16 14:16:31 ForkWorker::Fork: New child of 9936 = 1634442
04/15/16 14:16:31 Number of Active Workers 0
04/15/16 14:16:31 DaemonCore: No more children processes to reap.
04/15/16 14:16:32 ForkWorker::Fork: New child of 9936 = 1634453
04/15/16 14:16:32 Number of Active Workers 0
04/15/16 14:16:32 DaemonCore: No more children processes to reap.
04/15/16 14:16:33 Received a superuser command
04/15/16 14:16:33 ForkWorker::Fork: New child of 9936 = 1634464
04/15/16 14:16:33 Number of Active Workers 0
04/15/16 14:16:33 DaemonCore: No more children processes to reap.
04/15/16 14:16:33 ForkWorker::Fork: New child of 9936 = 1634465
04/15/16 14:16:33 Number of Active Workers 0
04/15/16 14:16:33 DaemonCore: No more children processes to reap.
04/15/16 14:16:33 QMGR Connection closed
04/15/16 14:16:34 QMGR Connection closed
04/15/16 14:16:34 ForkWorker::Fork: New child of 9936 = 1634522
04/15/16 14:16:34 Number of Active Workers 0
04/15/16 14:16:34 DaemonCore: No more children processes to reap.
04/15/16 14:16:35 ForkWorker::Fork: New child of 9936 = 1634648
04/15/16 14:16:35 Number of Active Workers 0
04/15/16 14:16:35 DaemonCore: No more children processes to reap.
04/15/16 14:16:36 ForkWorker::Fork: New child of 9936 = 1634761
04/15/16 14:16:36 Number of Active Workers 0
04/15/16 14:16:36 DaemonCore: No more children processes to reap.
04/15/16 14:16:38 ForkWorker::Fork: New child of 9936 = 1634853
04/15/16 14:16:38 Number of Active Workers 0
04/15/16 14:16:38 DaemonCore: No more children processes to reap.
04/15/16 14:16:39 ForkWorker::Fork: New child of 9936 = 1634896
04/15/16 14:16:39 Number of Active Workers 0
04/15/16 14:16:39 DaemonCore: No more children processes to reap.
04/15/16 14:16:39 QMGR Connection closed
04/15/16 14:16:40 ForkWorker::Fork: New child of 9936 = 1634960
04/15/16 14:16:40 Number of Active Workers 0
04/15/16 14:16:40 DaemonCore: No more children processes to reap.
04/15/16 14:16:41 Getting monitoring info for pid 9936
04/15/16 14:16:41 ForkWorker::Fork: New child of 9936 = 1635038
04/15/16 14:16:41 Number of Active Workers 0
04/15/16 14:16:41 DaemonCore: No more children processes to reap.
04/15/16 14:16:42 ForkWorker::Fork: New child of 9936 = 1635080
04/15/16 14:16:42 Number of Active Workers 0
04/15/16 14:16:42 DaemonCore: No more children processes to reap.
04/15/16 14:16:43 ForkWorker::Fork: New child of 9936 = 1635132
04/15/16 14:16:43 Number of Active Workers 0
04/15/16 14:16:43 DaemonCore: No more children processes to reap.
04/15/16 14:16:44 ForkWorker::Fork: New child of 9936 = 1635181
04/15/16 14:16:44 Number of Active Workers 0
04/15/16 14:16:44 DaemonCore: No more children processes to reap.
04/15/16 14:16:45 ForkWorker::Fork: New child of 9936 = 1635272
04/15/16 14:16:45 Number of Active Workers 0
04/15/16 14:16:45 DaemonCore: No more children processes to reap.
04/15/16 14:16:46 ForkWorker::Fork: New child of 9936 = 1635364
04/15/16 14:16:46 Number of Active Workers 0
04/15/16 14:16:46 DaemonCore: No more children processes to reap.
04/15/16 14:16:47 ForkWorker::Fork: New child of 9936 = 1635458
04/15/16 14:16:47 Number of Active Workers 0
04/15/16 14:16:47 DaemonCore: No more children processes to reap.
04/15/16 14:16:48 ForkWorker::Fork: New child of 9936 = 1635515
04/15/16 14:16:48 Number of Active Workers 0
04/15/16 14:16:48 DaemonCore: No more children processes to reap.
04/15/16 14:16:49 ForkWorker::Fork: New child of 9936 = 1635516
04/15/16 14:16:49 Number of Active Workers 0
04/15/16 14:16:49 DaemonCore: No more children processes to reap.
04/15/16 14:16:50 ForkWorker::Fork: New child of 9936 = 1635683
04/15/16 14:16:50 Number of Active Workers 0
04/15/16 14:16:50 DaemonCore: No more children processes to reap.
04/15/16 14:16:51 ForkWorker::Fork: New child of 9936 = 1635771
04/15/16 14:16:51 Number of Active Workers 0
04/15/16 14:16:51 DaemonCore: No more children processes to reap.
04/15/16 14:16:52 ForkWorker::Fork: New child of 9936 = 1635822
04/15/16 14:16:52 Number of Active Workers 0
04/15/16 14:16:52 DaemonCore: No more children processes to reap.
04/15/16 14:16:53 ForkWorker::Fork: New child of 9936 = 1635871
04/15/16 14:16:53 Number of Active Workers 0
04/15/16 14:16:53 DaemonCore: No more children processes to reap.
04/15/16 14:16:54 ForkWorker::Fork: New child of 9936 = 1635928
04/15/16 14:16:54 Number of Active Workers 0
04/15/16 14:16:54 DaemonCore: No more children processes to reap.
04/15/16 14:16:56 ForkWorker::Fork: New child of 9936 = 1636025
04/15/16 14:16:56 Number of Active Workers 0
04/15/16 14:16:56 DaemonCore: No more children processes to reap.
04/15/16 14:16:57 ForkWorker::Fork: New child of 9936 = 1636115
04/15/16 14:16:57 Number of Active Workers 0
04/15/16 14:16:57 DaemonCore: No more children processes to reap.
04/15/16 14:16:58 ForkWorker::Fork: New child of 9936 = 1636205
04/15/16 14:16:58 Number of Active Workers 0
04/15/16 14:16:58 DaemonCore: No more children processes to reap.
04/15/16 14:16:59 ForkWorker::Fork: New child of 9936 = 1636263
04/15/16 14:16:59 Number of Active Workers 0
04/15/16 14:16:59 DaemonCore: No more children processes to reap.
04/15/16 14:17:00 ForkWorker::Fork: New child of 9936 = 1636315
04/15/16 14:17:00 Number of Active Workers 0
04/15/16 14:17:00 DaemonCore: No more children processes to reap.
04/15/16 14:17:01 ForkWorker::Fork: New child of 9936 = 1636409
04/15/16 14:17:01 Number of Active Workers 0
04/15/16 14:17:01 DaemonCore: No more children processes to reap.
04/15/16 14:17:02 ForkWorker::Fork: New child of 9936 = 1636495
04/15/16 14:17:02 Number of Active Workers 0
04/15/16 14:17:02 DaemonCore: No more children processes to reap.
04/15/16 14:17:03 ForkWorker::Fork: New child of 9936 = 1636549
04/15/16 14:17:03 Number of Active Workers 0
04/15/16 14:17:03 DaemonCore: No more children processes to reap.
04/15/16 14:17:04 ForkWorker::Fork: New child of 9936 = 1636605
04/15/16 14:17:04 Number of Active Workers 0
04/15/16 14:17:04 DaemonCore: No more children processes to reap.
04/15/16 14:17:05 ForkWorker::Fork: New child of 9936 = 1636655
04/15/16 14:17:05 Number of Active Workers 0
04/15/16 14:17:05 DaemonCore: No more children processes to reap.
04/15/16 14:17:06 ForkWorker::Fork: New child of 9936 = 1636757
04/15/16 14:17:06 Number of Active Workers 0
04/15/16 14:17:06 DaemonCore: No more children processes to reap.
04/15/16 14:17:07 ForkWorker::Fork: New child of 9936 = 1636846
04/15/16 14:17:07 Number of Active Workers 0
04/15/16 14:17:07 DaemonCore: No more children processes to reap.
04/15/16 14:17:08 ForkWorker::Fork: New child of 9936 = 1636934
04/15/16 14:17:08 Number of Active Workers 0
04/15/16 14:17:08 DaemonCore: No more children processes to reap.
04/15/16 14:17:09 ForkWorker::Fork: New child of 9936 = 1636983
04/15/16 14:17:09 Number of Active Workers 0
04/15/16 14:17:09 DaemonCore: No more children processes to reap.
04/15/16 14:17:10 ForkWorker::Fork: New child of 9936 = 1637046
04/15/16 14:17:10 Number of Active Workers 0
04/15/16 14:17:10 DaemonCore: No more children processes to reap.
04/15/16 14:17:11 ForkWorker::Fork: New child of 9936 = 1637138
04/15/16 14:17:11 Number of Active Workers 0
04/15/16 14:17:11 DaemonCore: No more children processes to reap.
04/15/16 14:17:12 ForkWorker::Fork: New child of 9936 = 1637218
04/15/16 14:17:12 Number of Active Workers 0
04/15/16 14:17:12 DaemonCore: No more children processes to reap.
04/15/16 14:17:14 ForkWorker::Fork: New child of 9936 = 1637266
04/15/16 14:17:14 Number of Active Workers 0
04/15/16 14:17:14 DaemonCore: No more children processes to reap.
04/15/16 14:17:15 ForkWorker::Fork: New child of 9936 = 1637316
04/15/16 14:17:15 Number of Active Workers 0
04/15/16 14:17:15 DaemonCore: No more children processes to reap.
04/15/16 14:17:16 ForkWorker::Fork: New child of 9936 = 1637373
04/15/16 14:17:16 Number of Active Workers 0
04/15/16 14:17:16 DaemonCore: No more children processes to reap.
04/15/16 14:17:17 ForkWorker::Fork: New child of 9936 = 1637470
04/15/16 14:17:17 Number of Active Workers 0
04/15/16 14:17:17 DaemonCore: No more children processes to reap.
04/15/16 14:17:18 ForkWorker::Fork: New child of 9936 = 1637566
04/15/16 14:17:18 Number of Active Workers 0
04/15/16 14:17:18 DaemonCore: No more children processes to reap.
04/15/16 14:17:19 ForkWorker::Fork: New child of 9936 = 1637649
04/15/16 14:17:19 Number of Active Workers 0
04/15/16 14:17:19 DaemonCore: No more children processes to reap.
04/15/16 14:17:20 ForkWorker::Fork: New child of 9936 = 1637700
04/15/16 14:17:20 Number of Active Workers 0
04/15/16 14:17:20 DaemonCore: No more children processes to reap.
04/15/16 14:17:21 ForkWorker::Fork: New child of 9936 = 1637738
04/15/16 14:17:21 Number of Active Workers 0
04/15/16 14:17:21 DaemonCore: No more children processes to reap.
04/15/16 14:17:22 ForkWorker::Fork: New child of 9936 = 1637753
04/15/16 14:17:22 Number of Active Workers 0
04/15/16 14:17:22 DaemonCore: No more children processes to reap.
04/15/16 14:17:23 ForkWorker::Fork: New child of 9936 = 1637835
04/15/16 14:17:23 Number of Active Workers 0
04/15/16 14:17:23 DaemonCore: No more children processes to reap.
04/15/16 14:17:24 ForkWorker::Fork: New child of 9936 = 1637843
04/15/16 14:17:24 Number of Active Workers 0
04/15/16 14:17:24 DaemonCore: No more children processes to reap.
04/15/16 14:17:25 ForkWorker::Fork: New child of 9936 = 1637926
04/15/16 14:17:25 Number of Active Workers 0
04/15/16 14:17:25 DaemonCore: No more children processes to reap.
04/15/16 14:17:26 QMGR Connection closed
04/15/16 14:17:26 ForkWorker::Fork: New child of 9936 = 1637964
04/15/16 14:17:26 Number of Active Workers 0
04/15/16 14:17:26 DaemonCore: No more children processes to reap.
04/15/16 14:17:27 ForkWorker::Fork: New child of 9936 = 1637999
04/15/16 14:17:27 Number of Active Workers 0
04/15/16 14:17:27 DaemonCore: No more children processes to reap.
04/15/16 14:17:28 ForkWorker::Fork: New child of 9936 = 1638026
04/15/16 14:17:28 Number of Active Workers 0
04/15/16 14:17:28 DaemonCore: No more children processes to reap.
04/15/16 14:17:29 ForkWorker::Fork: New child of 9936 = 1638033
04/15/16 14:17:29 Number of Active Workers 0
04/15/16 14:17:29 DaemonCore: No more children processes to reap.
04/15/16 14:17:31 ForkWorker::Fork: New child of 9936 = 1638044
04/15/16 14:17:31 Number of Active Workers 0
04/15/16 14:17:31 DaemonCore: No more children processes to reap.
04/15/16 14:17:32 ForkWorker::Fork: New child of 9936 = 1638045
04/15/16 14:17:32 Number of Active Workers 0
04/15/16 14:17:32 DaemonCore: No more children processes to reap.
04/15/16 14:17:33 ForkWorker::Fork: New child of 9936 = 1638046
04/15/16 14:17:33 Number of Active Workers 0
04/15/16 14:17:33 DaemonCore: No more children processes to reap.
04/15/16 14:17:34 ForkWorker::Fork: New child of 9936 = 1638087
04/15/16 14:17:34 Number of Active Workers 0
04/15/16 14:17:34 DaemonCore: No more children processes to reap.
04/15/16 14:17:35 ForkWorker::Fork: New child of 9936 = 1638118
04/15/16 14:17:35 Number of Active Workers 0
04/15/16 14:17:35 DaemonCore: No more children processes to reap.
04/15/16 14:17:36 ForkWorker::Fork: New child of 9936 = 1638195
04/15/16 14:17:36 Number of Active Workers 0
04/15/16 14:17:36 DaemonCore: No more children processes to reap.
04/15/16 14:17:37 ForkWorker::Fork: New child of 9936 = 1638286
04/15/16 14:17:37 Number of Active Workers 0
04/15/16 14:17:37 DaemonCore: No more children processes to reap.
04/15/16 14:17:38 ForkWorker::Fork: New child of 9936 = 1638336
04/15/16 14:17:38 Number of Active Workers 0
04/15/16 14:17:38 DaemonCore: No more children processes to reap.
04/15/16 14:17:39 ForkWorker::Fork: New child of 9936 = 1638363
04/15/16 14:17:39 Number of Active Workers 0
04/15/16 14:17:39 DaemonCore: No more children processes to reap.
04/15/16 14:17:40 QMGR Connection closed
04/15/16 14:17:40 ForkWorker::Fork: New child of 9936 = 1638416
04/15/16 14:17:40 Number of Active Workers 0
04/15/16 14:17:40 DaemonCore: No more children processes to reap.
04/15/16 14:17:41 ForkWorker::Fork: New child of 9936 = 1638431
04/15/16 14:17:41 Number of Active Workers 0
04/15/16 14:17:41 DaemonCore: No more children processes to reap.
04/15/16 14:17:42 ForkWorker::Fork: New child of 9936 = 1638525
04/15/16 14:17:42 Number of Active Workers 0
04/15/16 14:17:42 DaemonCore: No more children processes to reap.
04/15/16 14:17:43 QMGR Connection closed
04/15/16 14:17:44 ForkWorker::Fork: New child of 9936 = 1638579
04/15/16 14:17:44 Number of Active Workers 0
04/15/16 14:17:44 DaemonCore: No more children processes to reap.
04/15/16 14:17:44 This process has a valid certificate & key
04/15/16 14:17:44 Finishing authenticate_server_gss_post with status=1
04/15/16 14:17:44 Finishing authenticate_server_gss_post with status=1
04/15/16 14:17:44 ZKM: successful mapping to GSS_ASSIST_GRIDMAP
04/15/16 14:17:44 Command=TRANSFER_DATA_WITH_PERMS, peer=<200.145.46.37:12583>
04/15/16 14:17:44 AuthMethod=GSI, AuthId=/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler, CondorId=winckler@users.opensciencegrid.org
04/15/16 14:17:44 Looking at spooling: mode is 489
04/15/16 14:17:44 Scheduler::spoolJobFiles: TRANSFER_DATA/WITH_PERMS: 1 jobs matched constraint (ClusterId==31)
04/15/16 14:17:44 Transferring files for jobs 31.0
04/15/16 14:17:44 Scheduler::generalJobFilesWorkerThread: TRANSFER_DATA/WITH_PERMS: 1 jobs to be sent
04/15/16 14:17:44 spoolJobFiles(): started worker process
04/15/16 14:17:44 generalJobFilesWorkerThread(): transfer files for job 31.0
04/15/16 14:17:44 The submitting job ad as the FileTransferObject sees it
NumCkpts_RAW = 0
BufferSize = 524288
NiceUser = false
CoreSize = 0
CumulativeSlotTime = 0
OnExitHold = false
RequestCpus = 1
Err = "_condor_stderr"
BufferBlockSize = 32768
ExecutableSize_RAW = 18
WantCheckpoint = false
CommittedTime = 0
TargetType = "Machine"
WhenToTransferOutput = "ON_EXIT"
JobUniverse = 5
ExitBySignal = false
TransferIn = false
NumRestarts = 0
EncryptExecuteDirectory = false
CommittedSuspensionTime = 0
Owner = "winckler"
NumSystemHolds = 0
CumulativeSuspensionTime = 0
Environment = "GLOBUS_TCP_SOURCE_RANGE=20000,29999 _LMFILES_=/opt/modules/gridunesp/1 BYOBU_SED=sed' '--follow-symlinks LC_MEASUREMENT=pt_BR.UTF-8 BYOBU_READLINK=readlink BYOBU_CONFIG_DIR=/home/winckler/.byobu LCMAPS_DEBUG_LEVEL=3 GV_DIR=/opt/gaussian/gv SHLVL=2 LS_COLORS= PWD=/home/winckler BYOBU_TIME=%H:%M:%S SSH_AUTH_SOCK=/home/winckler/.byobu/.ssh-agent BYOBU_WINDOWS=/home/winckler/.byobu/windows SSH_CLIENT=200.145.46.228' '49266' '22 CVS_RSH=ssh PGI_TERM=trace,abort LC_TELEPHONE=pt_BR.UTF-8 STY=2420116.byobu PATH=/home/winckler/bin:/usr/local/src/goo-client/scripts:/home/winckler/bin:/opt/gridunesp/internals//bin:/usr/lib64/qt-3.3/bin:/usr/local/src/goo-client/scripts:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/home/winckler/bin:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 BYOBU_RUN_DIR=/dev/shm/byobu-winckler-yXoDFpVj TOOL_DEBUG=D_FULLDEBUG MODULESHOME=/usr/share/Modules G09BASIS=/opt/gaussian/g09/basis GAUSS_EXEDIR=/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09 X509_CERT_DIR=/etc/grid-security/certificates GLOBUS_TCP_PORT_RANGE=20000,29999 QTLIB=/usr/lib64/qt-3.3/lib LC_NAME=pt_BR.UTF-8 GATEKEEPER_JM_ID=2016-04-15.17:14:58.0002424196.0000000000 HISTCONTROL=ignoredups LCMAPS_DIR=/etc BYOBU_DARK=black LC_NUMERIC=pt_BR.UTF-8 GOO_API_URI=https://submit.grid.unesp.br/api/v1/ g09root=/opt/gaussian _DSM_BARRIER=SHM BYOBU_ULIMIT=ulimit SSH_TTY=/dev/pts/20 _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI MYPROXY_SERVER=myproxy.grid.unesp.br SHELL=/bin/bash GAUSS_SCRDIR=/tmp MAIL=/var/spool/mail/winckler GAUSS_ARCHDIR=/opt/gaussian/g09/arch BYOBU_HIGHLIGHT=#DD4814 LCMAPS_DB_FILE=/etc/lcmaps/lcmaps.db LC_CTYPE=pt_BR.UTF-8 LC_ADDRESS=pt_BR.UTF-8 LOADEDMODULES=gridunesp/1 BYOBU_LIGHT=white USER=winckler SSH_CONNECTION=200.145.46.228' '49266' '200.145.46.37' '22 HOSTNAME=access.grid.unesp.br GRIDUNESP_SLURM_PARTITIONS=rack1,rack2,rack3,rack4,rack5,rack6,rack7,rack8 LD_LIBRARY_PATH=/opt/gridunesp/internals//lib:/opt/gaussian/g09/bsd:/opt/gaussian/g09/local:/opt/gaussian/g09/extras:/opt/gaussian/g09:/opt/gaussian/gv/lib PYTHONPATH=/usr/local/src/python-gooclientlib:/usr/local/src/goo-client:/usr/local/src/python-gooclientlib:/usr/local/src/goo-client: BYOBU_TTY=/dev/pts/20 BYOBU_WINDOW_NAME=- BYOBU_BACKEND=screen GAUSS_LEXEDIR=/opt/gaussian/g09/linda-exe HOME=/home/winckler LOGNAME=winckler BYOBU_PYTHON=python2 LC_PAPER=pt_BR.UTF-8 MODULEPATH=/opt/modules JOB_REPOSITORY_ID=2016-04-15.17:14:58.0002424196.0000000000 _=/usr/bin/condor_ce_run LC_MONETARY=pt_BR.UTF-8 LC_TIME=pt_BR.UTF-8 G_BROKEN_FILENAMES=1 BYOBU_DISTRO=CentOS BYOBU_ACCENT=#75507B LANG=en_US.UTF-8 CONDOR_CONFIG=/etc/condor-ce/condor_config HISTSIZE=1000 BYOBU_PREFIX=/usr QTDIR=/usr/lib64/qt-3.3 BYOBU_DATE=%Y-%m-%d' ' BYOBU_CHARMAP=UTF-8 LCMAPS_POLICY_NAME= BYOBU_PAGER=less QTINC=/usr/lib64/qt-3.3/include TERM=screen-256color-bce WINDOW=0 LESSOPEN=||/usr/bin/lesspipe.sh' '%s LC_IDENTIFICATION=pt_BR.UTF-8"
RequestDisk = DiskUsage
Requirements = ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
MinHosts = 1
JobNotification = 0
NumCkpts = 0
NumJobStarts = 0
WantRemoteSyscalls = false
JobLeaseDuration = 2400
JobPrio = 0
RootDir = "/"
CurrentHosts = 0
x509UserProxyExpiration = 1460781781
WantRemoteIO = true
StreamOut = false
OnExitRemove = true
In = "/dev/null"
DiskUsage = 20
PeriodicRemove = false
LocalUserCpu = 0.0
ExecutableSize = 20
LocalSysCpu = 0.0
RemoteSysCpu = 0.0
ClusterId = 31
CompletionDate = 0
RemoteWallClockTime = 0.0
Rank = 0.0
LeaveJobInQueue = ( StageOutFinish > 0 ) =!= true
x509UserProxyEmail = "winckler@ncc.unesp.br"
CondorVersion = "$CondorVersion: 8.4.4 Feb 04 2016 $"
MyType = "Job"
StreamErr = false
DiskUsage_RAW = 18
PeriodicHold = false
User = "winckler@users.opensciencegrid.org"
Out = "_condor_stdout"
PeriodicRelease = false
MaxHosts = 1
RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,( ImageSize + 1023 ) / 1024)
Args = ""
CommittedSlotTime = 0
TotalSuspensions = 0
x509userproxysubject = "/C=BR/O=ANSP/OU=ANSPGrid CA/OU=People/CN=Gabriel A. von Winckler"
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.7 $"
TransferInputSizeMB = 0
ExitStatus = 0
ShouldTransferFiles = "YES"
QDate = 1460740498
TransferOutputRemaps = undefined
SUBMIT_TransferOutputRemaps = "_condor_stdout=/home/winckler/.stdout_2424195_wqvCns;_condor_stderr=/home/winckler/.stderr_2424195_ITigNv"
EnteredCurrentStatus = 1460740581
ImageSize = 0
SUBMIT_UserLog = "/home/winckler/.log_2424195_0qm8sM"
x509userproxy = "x509up_u10001"
RemoteUserCpu = 0
ManagedManager = "htcondor-ce"
JobStatus = 4
UserLog = ".log_2424195_0qm8sM"
ExitCode = 0
Cmd = "hostname"
GlobalJobId = "ce.grid.unesp.br#31.0#1460740580"
LastSuspensionTime = 0
LastJobStatus = 1
StageInFinish = 1460740580
SUBMIT_Cmd = "/bin/hostname"
HoldReason = undefined
Managed = "External"
HoldReasonCode = undefined
SUBMIT_x509userproxy = "/tmp/x509up_u10001"
Iwd = "/osg/condor/31/0/cluster31.proc0.subproc0"
StageOutStart = 1460740664
SUBMIT_Iwd = "/home/winckler"
ImageSize_RAW = 0
StageInStart = 1460740580
ReleaseReason = "Data files spooled"
RoutedToJobId = "32.0"
LastHoldReason = "Spooling input data files"
LastHoldReasonCode = 16
ProcId = 0
04/15/16 14:17:44 entering FileTransfer::SimpleInit
04/15/16 14:17:44 FILETRANSFER: protocol "http" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:17:44 FILETRANSFER: protocol "ftp" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:17:44 FILETRANSFER: protocol "file" handled by "/usr/libexec/condor/curl_plugin"
04/15/16 14:17:44 FILETRANSFER: protocol "data" handled by "/usr/libexec/condor/data_plugin"
04/15/16 14:17:44 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0
04/15/16 14:17:44 set_user_egid() called when UserIds not inited!
04/15/16 14:17:44 set_user_euid() called when UserIds not inited!
04/15/16 14:17:44 set_user_egid() called when UserIds not inited!
04/15/16 14:17:44 set_user_euid() called when UserIds not inited!
04/15/16 14:17:44 set_user_egid() called when UserIds not inited!
04/15/16 14:17:44 set_user_euid() called when UserIds not inited!
04/15/16 14:17:44 set_user_egid() called when UserIds not inited!
04/15/16 14:17:44 set_user_euid() called when UserIds not inited!
04/15/16 14:17:44 set_user_egid() called when UserIds not inited!
04/15/16 14:17:44 set_user_euid() called when UserIds not inited!
04/15/16 14:17:44 entering FileTransfer::UploadFiles (final_transfer=1)
04/15/16 14:17:44 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0
04/15/16 14:17:44 Skipping file .log_2424195_0qm8sM, t: 1460740580<=1460740580, s: N/A
04/15/16 14:17:44 Skipping file hostname, t: 1460740580<=1460740580, s: N/A
04/15/16 14:17:44 Skipping file _condor_stdout, t: 1460740515<=1460740580, s: N/A
04/15/16 14:17:44 Sending dynamically added output file _condor_stderr
04/15/16 14:17:44 entering FileTransfer::Upload
04/15/16 14:17:44 entering FileTransfer::DoUpload
04/15/16 14:17:44 DoUpload: sending file _condor_stderr
04/15/16 14:17:44 FILETRANSFER: outgoing file_command is 1 for _condor_stderr
04/15/16 14:17:44 Received GoAhead from peer to send /osg/condor/31/0/cluster31.proc0.subproc0/_condor_stderr and all further files.
04/15/16 14:17:44 Sending GoAhead for 200.145.46.37 to receive /osg/condor/31/0/cluster31.proc0.subproc0/_condor_stderr and all further files.
04/15/16 14:17:44 ReliSock::put_file_with_permissions(): going to send permissions 100644
04/15/16 14:17:44 put_file: going to send from filename /osg/condor/31/0/cluster31.proc0.subproc0/_condor_stderr
04/15/16 14:17:44 put_file: Found file size 0
04/15/16 14:17:44 put_file: sending 0 bytes
04/15/16 14:17:44 ReliSock: put_file: sent 0 bytes
04/15/16 14:17:44 DoUpload: exiting at 3366
04/15/16 14:17:44 Transfer completed
04/15/16 14:17:44 DaemonCore: No more children processes to reap.
04/15/16 14:17:44 transferJobFilesReaper tid=1638597 status=256
04/15/16 14:17:45 Added data to SelfDrainingQueue job_is_finished_queue, now has 1 element(s)
04/15/16 14:17:45 Registered timer for SelfDrainingQueue job_is_finished_queue, period: 0 (id: 1763)
04/15/16 14:17:45 Job 32.0 is finished
04/15/16 14:17:45 schedd: DestroyProc cluster 32 proc 0 rval 1 errno 0
04/15/16 14:17:45 QMGR Connection closed
04/15/16 14:17:45 Inside SelfDrainingQueue::timerHandler() for job_is_finished_queue
04/15/16 14:17:45 Job cleanup for 32.0 will not block, calling jobIsFinished() directly
04/15/16 14:17:45 jobIsFinished() completed, calling DestroyProc(32.0)
04/15/16 14:17:45 SelfDrainingQueue job_is_finished_queue is empty, not resetting timer
04/15/16 14:17:45 Canceling timer for SelfDrainingQueue job_is_finished_queue (timer id: 1763)
04/15/16 14:17:45 DaemonCore: No more children processes to reap.
04/15/16 14:17:45 condor_gridmanager (PID 1634355, owner winckler) exited with return code 0.
04/15/16 14:17:45 Initializing Directory: curr_dir = /tmp/condor_g_scratch.0x7fa721fc6990.9936
04/15/16 14:17:45 Removed scratch dir /tmp/condor_g_scratch.0x7fa721fc6990.9936
04/15/16 14:17:53 QMGR Connection closed
04/15/16 14:17:53 Added data to SelfDrainingQueue job_is_finished_queue, now has 1 element(s)
04/15/16 14:17:53 Registered timer for SelfDrainingQueue job_is_finished_queue, period: 0 (id: 1764)
04/15/16 14:17:53 Job 29.0 is finished
04/15/16 14:17:53 Evaluated periodic expressions in 0.000s, scheduling next run in 300s
04/15/16 14:17:53 Inside SelfDrainingQueue::timerHandler() for job_is_finished_queue
04/15/16 14:17:53 Job cleanup for 29.0 will not block, calling jobIsFinished() directly
04/15/16 14:17:53 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0
04/15/16 14:17:53 jobIsFinished() completed, calling DestroyProc(29.0)
04/15/16 14:17:53 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0
04/15/16 14:17:53 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0
04/15/16 14:17:53 Initializing Directory: curr_dir = /osg/condor/29/0/cluster29.proc0.subproc0.tmp
04/15/16 14:17:53 Saving classad to history file
04/15/16 14:17:53 SelfDrainingQueue job_is_finished_queue is empty, not resetting timer
04/15/16 14:17:53 Canceling timer for SelfDrainingQueue job_is_finished_queue (timer id: 1764)
04/15/16 14:17:53 QMGR Connection closed
04/15/16 14:17:53 Remove jobs 32.0
04/15/16 14:17:53 OwnerCheck retval 1 (success), super_user
04/15/16 14:17:53 OwnerCheck retval 1 (success), super_user
04/15/16 14:17:53 OwnerCheck retval 1 (success), super_user
04/15/16 14:17:53 OwnerCheck retval 1 (success), super_user
04/15/16 14:17:53 OwnerCheck retval 1 (success), super_user
04/15/16 14:17:53 Added data to SelfDrainingQueue act_on_job_myself_queue, now has 1 element(s)
04/15/16 14:17:53 Registered timer for SelfDrainingQueue act_on_job_myself_queue, period: 0 (id: 1766)
04/15/16 14:17:53 Expedited call to StartJobs()
04/15/16 14:17:53 Finished Remove jobs 32.0
04/15/16 14:17:53 -------- Begin starting jobs --------
04/15/16 14:17:53 -------- Done starting jobs --------
04/15/16 14:17:53 Inside SelfDrainingQueue::timerHandler() for act_on_job_myself_queue
04/15/16 14:17:53 abort_job_myself: 32.0 action:Remove log_hold:true
04/15/16 14:17:53 Cleared dirty attributes for job 32.0
04/15/16 14:17:53 Writing record to user logfile=/osg/condor/31/0/cluster31.proc0.subproc0/.log_2424195_0qm8sM owner=winckler
04/15/16 14:17:53 WriteUserLog::initialize: opened /osg/condor/31/0/cluster31.proc0.subproc0/.log_2424195_0qm8sM successfully
04/15/16 14:17:53 Saving classad to history file
04/15/16 14:17:53 SelfDrainingQueue act_on_job_myself_queue is empty, not resetting timer
04/15/16 14:17:53 Canceling timer for SelfDrainingQueue act_on_job_myself_queue (timer id: 1766)
04/15/16 14:17:53 QMGR Connection closed
04/15/16 14:20:41 Getting monitoring info for pid 9936
04/15/16 14:21:26 Clearing userlog file cache
04/15/16 14:21:26 JobsRunning = 0
04/15/16 14:21:26 JobsIdle = 0
04/15/16 14:21:26 JobsHeld = 0
04/15/16 14:21:26 JobsRemoved = 0
04/15/16 14:21:26 LocalUniverseJobsRunning = 0
04/15/16 14:21:26 LocalUniverseJobsIdle = 0
04/15/16 14:21:26 SchedUniverseJobsRunning = 0
04/15/16 14:21:26 SchedUniverseJobsIdle = 0
04/15/16 14:21:26 N_Owners = 1
04/15/16 14:21:26 MaxJobsRunning = 10000
04/15/16 14:21:26 TransferQueueManager stats: active up=0/10 down=0/10; waiting up=0 down=0; wait time up=0s down=0s
04/15/16 14:21:26 TransferQueueManager upload 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:21:26 TransferQueueManager download 1m I/O load: 0 bytes/s  0.000 disk load  0.000 net load
04/15/16 14:21:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:21:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:21:26 Sent HEART BEAT ad to 1 collectors. Number of submittors=1
04/15/16 14:21:26 Changed attribute: IdleJobs = 0
04/15/16 14:21:26 Changed attribute: RunningJobs = 0
04/15/16 14:21:26 Changed attribute: IdleJobs = 0
04/15/16 14:21:26 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:21:26 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:21:26 Changed attribute: HeldJobs = 0
04/15/16 14:21:26 Changed attribute: FlockedJobs = 0
04/15/16 14:21:26 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:21:26 Sent ad to central manager for cmsprod@users.opensciencegrid.org
04/15/16 14:21:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:21:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:21:26 Sent ad to 1 collectors for cmsprod@users.opensciencegrid.org
04/15/16 14:21:26 Changed attribute: IdleJobs = 0
04/15/16 14:21:26 Changed attribute: RunningJobs = 0
04/15/16 14:21:26 Changed attribute: IdleJobs = 0
04/15/16 14:21:26 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:21:26 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:21:26 Changed attribute: HeldJobs = 0
04/15/16 14:21:26 Changed attribute: FlockedJobs = 0
04/15/16 14:21:26 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:21:26 Sent ad to central manager for osg@users.opensciencegrid.org
04/15/16 14:21:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:21:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:21:26 Sent ad to 1 collectors for osg@users.opensciencegrid.org
04/15/16 14:21:26 Changed attribute: IdleJobs = 0
04/15/16 14:21:26 Changed attribute: RunningJobs = 0
04/15/16 14:21:26 Changed attribute: IdleJobs = 0
04/15/16 14:21:26 Changed attribute: WeightedRunningJobs = 0
04/15/16 14:21:26 Changed attribute: WeightedIdleJobs = 0
04/15/16 14:21:26 Changed attribute: HeldJobs = 0
04/15/16 14:21:26 Changed attribute: FlockedJobs = 0
04/15/16 14:21:26 Changed attribute: Name = winckler@users.opensciencegrid.org
04/15/16 14:21:26 Sent ad to central manager for winckler@users.opensciencegrid.org
04/15/16 14:21:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:21:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:21:26 Sent ad to 1 collectors for winckler@users.opensciencegrid.org
04/15/16 14:21:26 Changed attribute: Name = cmsprod@users.opensciencegrid.org
04/15/16 14:21:26 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:21:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:21:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:21:26 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:21:26 Changed attribute: Name = osg@users.opensciencegrid.org
04/15/16 14:21:26 Sent owner (0 jobs) ad to schedd plugins
04/15/16 14:21:26 Trying to update collector <200.145.46.35:9619>
04/15/16 14:21:26 Attempting to send update via TCP to collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:21:26 Sent owner (0 jobs) ad to 1 collectors
04/15/16 14:21:26 ============ Begin clean_shadow_recs =============
04/15/16 14:21:26 ============ End clean_shadow_recs =============
04/15/16 14:21:26 Sending RESCHEDULE command to negotiator(s)
04/15/16 14:21:26 Will use TCP to update collector ce.grid.unesp.br <200.145.46.35:9619>
04/15/16 14:21:26 Trying to query collector <200.145.46.35:9619>
04/15/16 14:21:26 Can't find address for negotiator 
04/15/16 14:21:26 Failed to send RESCHEDULE to unknown daemon: 
04/15/16 14:22:50 Received a superuser command
04/15/16 14:22:50 ForkWorker::Fork: New child of 9936 = 1654157
04/15/16 14:22:50 Number of Active Workers 0
04/15/16 14:22:50 DaemonCore: No more children processes to reap.
04/15/16 14:22:53 Added data to SelfDrainingQueue job_is_finished_queue, now has 1 element(s)
04/15/16 14:22:53 Registered timer for SelfDrainingQueue job_is_finished_queue, period: 0 (id: 1773)
04/15/16 14:22:53 Job 31.0 is finished
04/15/16 14:22:53 Evaluated periodic expressions in 0.000s, scheduling next run in 301s
04/15/16 14:22:53 -------- Begin starting jobs --------
04/15/16 14:22:53 -------- Done starting jobs --------
04/15/16 14:22:53 Inside SelfDrainingQueue::timerHandler() for job_is_finished_queue
04/15/16 14:22:53 Job cleanup for 31.0 will not block, calling jobIsFinished() directly
04/15/16 14:22:53 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0
04/15/16 14:22:53 jobIsFinished() completed, calling DestroyProc(31.0)
04/15/16 14:22:53 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0
04/15/16 14:22:53 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0
04/15/16 14:22:53 Initializing Directory: curr_dir = /osg/condor/31/0/cluster31.proc0.subproc0.tmp
04/15/16 14:22:53 Saving classad to history file
04/15/16 14:22:53 SelfDrainingQueue job_is_finished_queue is empty, not resetting timer
04/15/16 14:22:53 Canceling timer for SelfDrainingQueue job_is_finished_queue (timer id: 1773)
