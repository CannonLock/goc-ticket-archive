09/10/15 10:59:05 ******************************************************
09/10/15 10:59:05 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/10/15 10:59:05 ** /usr/sbin/condor_gridmanager
09/10/15 10:59:05 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/10/15 10:59:05 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/10/15 10:59:05 ** $CondorVersion: 8.2.8 Apr 08 2015 $
09/10/15 10:59:05 ** $CondorPlatform: X86_64-CentOS_6.6 $
09/10/15 10:59:05 ** PID = 20487
09/10/15 10:59:05 ** Log last touched time unavailable (No such file or directory)
09/10/15 10:59:05 ******************************************************
09/10/15 10:59:05 Using config source: /etc/condor-ce/condor_config
09/10/15 10:59:05 Using local config sources: 
09/10/15 10:59:05    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/10/15 10:59:05    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/01-ce-auth.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/01-ce-router.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/01-common-auth.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/02-ce-lsf.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/02-ce-pbs.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/03-managed-fork.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/50-osg-configure.conf
09/10/15 10:59:05    /etc/condor-ce/config.d/99-local.conf
09/10/15 10:59:05    /usr/share/condor-ce/condor_ce_router_defaults|
09/10/15 10:59:05 config Macros = 141, Sorted = 141, StringBytes = 12496, TablesBytes = 5284
09/10/15 10:59:05 CLASSAD_CACHING is ENABLED
09/10/15 10:59:05 Daemon Log is logging: D_ALWAYS D_ERROR
09/10/15 10:59:05 SharedPortEndpoint: waiting for connections to named socket 16486_a0d7_2
09/10/15 10:59:05 DaemonCore: command socket at <140.247.179.131:9620?sock=16486_a0d7_2>
09/10/15 10:59:05 DaemonCore: private command socket at <140.247.179.131:9620?sock=16486_a0d7_2>
09/10/15 10:59:05 [20487] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
09/10/15 10:59:05 [20487] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
09/10/15 10:59:05 [20487] Buf::write(): condor_write() failed
09/10/15 10:59:05 [20487] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
09/10/15 10:59:08 [20487] Found job 6.0 --- inserting
09/10/15 10:59:08 [20487] gahp server not up yet, delaying ping
09/10/15 10:59:08 [20487] (6.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/10/15 10:59:08 [20487] GAHP server pid = 22406
09/10/15 10:59:13 [20487] resource  is now up
09/10/15 10:59:13 [20487] (6.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/10/15 10:59:14 [20487] (6.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/10/15 10:59:14 [20487] (6.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
09/10/15 10:59:18 [20487] (6.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
09/10/15 10:59:24 [20487] (6.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/10/15 10:59:29 [20487] No jobs left, shutting down
09/10/15 10:59:29 [20487] Got SIGTERM. Performing graceful shutdown.
09/10/15 10:59:29 [20487] **** condor_gridmanager (condor_GRIDMANAGER) pid 20487 EXITING WITH STATUS 0
09/10/15 11:10:47 ******************************************************
09/10/15 11:10:47 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/10/15 11:10:47 ** /usr/sbin/condor_gridmanager
09/10/15 11:10:47 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/10/15 11:10:47 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/10/15 11:10:47 ** $CondorVersion: 8.2.8 Apr 08 2015 $
09/10/15 11:10:47 ** $CondorPlatform: X86_64-CentOS_6.6 $
09/10/15 11:10:47 ** PID = 6497
09/10/15 11:10:47 ** Log last touched 9/10 10:59:29
09/10/15 11:10:47 ******************************************************
09/10/15 11:10:47 Using config source: /etc/condor-ce/condor_config
09/10/15 11:10:47 Using local config sources: 
09/10/15 11:10:47    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/10/15 11:10:47    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/01-ce-auth.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/01-ce-router.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/01-common-auth.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/02-ce-lsf.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/02-ce-pbs.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/03-managed-fork.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/50-osg-configure.conf
09/10/15 11:10:47    /etc/condor-ce/config.d/99-local.conf
09/10/15 11:10:47    /usr/share/condor-ce/condor_ce_router_defaults|
09/10/15 11:10:47 config Macros = 141, Sorted = 141, StringBytes = 12494, TablesBytes = 5284
09/10/15 11:10:47 CLASSAD_CACHING is ENABLED
09/10/15 11:10:47 Daemon Log is logging: D_ALWAYS D_ERROR
09/10/15 11:10:47 SharedPortEndpoint: waiting for connections to named socket 16486_a0d7_5
09/10/15 11:10:47 DaemonCore: command socket at <140.247.179.131:9620?sock=16486_a0d7_5>
09/10/15 11:10:47 DaemonCore: private command socket at <140.247.179.131:9620?sock=16486_a0d7_5>
09/10/15 11:10:47 [6497] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
09/10/15 11:10:47 [6497] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
09/10/15 11:10:47 [6497] Buf::write(): condor_write() failed
09/10/15 11:10:47 [6497] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
09/10/15 11:10:50 [6497] Found job 11.0 --- inserting
09/10/15 11:10:50 [6497] gahp server not up yet, delaying ping
09/10/15 11:10:50 [6497] (11.0) doEvaluateState called: gmState GM_INIT, remoteState 0
09/10/15 11:10:50 [6497] GAHP server pid = 6657
09/10/15 11:10:55 [6497] resource  is now up
09/10/15 11:10:55 [6497] (11.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/10/15 11:10:56 [6497] (11.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
09/10/15 11:10:56 [6497] (11.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
09/10/15 11:11:00 [6497] (11.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
09/10/15 11:11:05 [6497] (11.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
09/10/15 11:11:10 [6497] No jobs left, shutting down
09/10/15 11:11:10 [6497] Got SIGTERM. Performing graceful shutdown.
09/10/15 11:11:10 [6497] **** condor_gridmanager (condor_GRIDMANAGER) pid 6497 EXITING WITH STATUS 0
09/11/15 11:03:39 ******************************************************
09/11/15 11:03:39 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/11/15 11:03:39 ** /usr/sbin/condor_gridmanager
09/11/15 11:03:39 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/11/15 11:03:39 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/11/15 11:03:39 ** $CondorVersion: 8.2.8 Apr 08 2015 $
09/11/15 11:03:39 ** $CondorPlatform: X86_64-CentOS_6.6 $
09/11/15 11:03:39 ** PID = 3846
09/11/15 11:03:39 ** Log last touched 9/10 11:11:10
09/11/15 11:03:39 ******************************************************
09/11/15 11:03:39 Using config source: /etc/condor-ce/condor_config
09/11/15 11:03:39 Using local config sources: 
09/11/15 11:03:39    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/11/15 11:03:39    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/01-ce-auth.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/01-ce-router.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/01-common-auth.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/02-ce-lsf.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/02-ce-pbs.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/03-managed-fork.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/50-osg-configure.conf
09/11/15 11:03:39    /etc/condor-ce/config.d/99-local.conf
09/11/15 11:03:39    /usr/share/condor-ce/condor_ce_router_defaults|
09/11/15 11:03:39 config Macros = 141, Sorted = 141, StringBytes = 12494, TablesBytes = 5284
09/11/15 11:03:39 CLASSAD_CACHING is ENABLED
09/11/15 11:03:39 Daemon Log is logging: D_ALWAYS D_ERROR
09/11/15 11:03:39 SharedPortEndpoint: waiting for connections to named socket 16486_a0d7_9
09/11/15 11:03:39 DaemonCore: command socket at <140.247.179.131:9620?sock=16486_a0d7_9>
09/11/15 11:03:39 DaemonCore: private command socket at <140.247.179.131:9620?sock=16486_a0d7_9>
09/11/15 11:03:40 [3846] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
09/11/15 11:03:40 [3846] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
09/11/15 11:03:40 [3846] Buf::write(): condor_write() failed
09/11/15 11:03:40 [3846] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
09/11/15 11:03:41 [3846] Found job 6.0 --- inserting
09/11/15 11:03:41 [3846] gahp server not up yet, delaying ping
09/11/15 11:03:41 [3846] (6.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/11/15 11:03:41 [3846] GAHP server pid = 4028
09/11/15 11:03:44 [3846] (6.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/11/15 11:03:44 [3846] (6.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
09/11/15 11:03:46 [3846] resource  is now up
09/11/15 11:03:46 [3846] No jobs left, shutting down
09/11/15 11:03:46 [3846] Got SIGTERM. Performing graceful shutdown.
09/11/15 11:03:46 [3846] **** condor_gridmanager (condor_GRIDMANAGER) pid 3846 EXITING WITH STATUS 0
09/11/15 11:13:41 ******************************************************
09/11/15 11:13:41 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
09/11/15 11:13:41 ** /usr/sbin/condor_gridmanager
09/11/15 11:13:41 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
09/11/15 11:13:41 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
09/11/15 11:13:41 ** $CondorVersion: 8.2.8 Apr 08 2015 $
09/11/15 11:13:41 ** $CondorPlatform: X86_64-CentOS_6.6 $
09/11/15 11:13:41 ** PID = 13214
09/11/15 11:13:41 ** Log last touched 9/11 11:03:46
09/11/15 11:13:41 ******************************************************
09/11/15 11:13:41 Using config source: /etc/condor-ce/condor_config
09/11/15 11:13:41 Using local config sources: 
09/11/15 11:13:41    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
09/11/15 11:13:41    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/01-ce-auth.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/01-ce-router.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/01-common-auth.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/02-ce-lsf.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/02-ce-pbs.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/03-ce-shared-port.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/03-managed-fork.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/10-ce-collector-generated.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/50-osg-configure.conf
09/11/15 11:13:41    /etc/condor-ce/config.d/99-local.conf
09/11/15 11:13:41    /usr/share/condor-ce/condor_ce_router_defaults|
09/11/15 11:13:41 config Macros = 141, Sorted = 141, StringBytes = 12496, TablesBytes = 5284
09/11/15 11:13:41 CLASSAD_CACHING is ENABLED
09/11/15 11:13:41 Daemon Log is logging: D_ALWAYS D_ERROR
09/11/15 11:13:41 SharedPortEndpoint: waiting for connections to named socket 16486_a0d7_11
09/11/15 11:13:41 DaemonCore: command socket at <140.247.179.131:9620?sock=16486_a0d7_11>
09/11/15 11:13:41 DaemonCore: private command socket at <140.247.179.131:9620?sock=16486_a0d7_11>
09/11/15 11:13:41 [13214] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
09/11/15 11:13:41 [13214] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
09/11/15 11:13:41 [13214] Buf::write(): condor_write() failed
09/11/15 11:13:41 [13214] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
09/11/15 11:13:43 [13214] Found job 11.0 --- inserting
09/11/15 11:13:43 [13214] gahp server not up yet, delaying ping
09/11/15 11:13:43 [13214] (11.0) doEvaluateState called: gmState GM_INIT, remoteState -1
09/11/15 11:13:43 [13214] GAHP server pid = 13557
09/11/15 11:13:45 [13214] (11.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
09/11/15 11:13:45 [13214] (11.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
09/11/15 11:13:48 [13214] resource  is now up
09/11/15 11:13:48 [13214] No jobs left, shutting down
09/11/15 11:13:48 [13214] Got SIGTERM. Performing graceful shutdown.
09/11/15 11:13:48 [13214] **** condor_gridmanager (condor_GRIDMANAGER) pid 13214 EXITING WITH STATUS 0
11/06/15 13:51:23 ******************************************************
11/06/15 13:51:23 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
11/06/15 13:51:23 ** /usr/sbin/condor_gridmanager
11/06/15 13:51:23 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
11/06/15 13:51:23 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
11/06/15 13:51:23 ** $CondorVersion: 8.2.8 Apr 08 2015 $
11/06/15 13:51:23 ** $CondorPlatform: X86_64-CentOS_6.6 $
11/06/15 13:51:23 ** PID = 17875
11/06/15 13:51:23 ** Log last touched 9/11 11:13:48
11/06/15 13:51:23 ******************************************************
11/06/15 13:51:23 Using config source: /etc/condor-ce/condor_config
11/06/15 13:51:23 Using local config sources: 
11/06/15 13:51:23    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
11/06/15 13:51:23    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/01-ce-auth.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/01-ce-router.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/01-common-auth.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/02-ce-lsf.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/02-ce-pbs.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/03-ce-shared-port.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/03-managed-fork.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/10-ce-collector-generated.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/50-osg-configure.conf
11/06/15 13:51:23    /etc/condor-ce/config.d/99-local.conf
11/06/15 13:51:23    /usr/share/condor-ce/condor_ce_router_defaults|
11/06/15 13:51:23 config Macros = 141, Sorted = 141, StringBytes = 12496, TablesBytes = 5284
11/06/15 13:51:23 CLASSAD_CACHING is ENABLED
11/06/15 13:51:23 Daemon Log is logging: D_ALWAYS D_ERROR
11/06/15 13:51:23 SharedPortEndpoint: waiting for connections to named socket 18810_18b6_1
11/06/15 13:51:23 DaemonCore: command socket at <140.247.179.131:9620?sock=18810_18b6_1>
11/06/15 13:51:23 DaemonCore: private command socket at <140.247.179.131:9620?sock=18810_18b6_1>
11/06/15 13:51:23 [17875] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
11/06/15 13:51:23 [17875] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
11/06/15 13:51:23 [17875] Buf::write(): condor_write() failed
11/06/15 13:51:23 [17875] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
11/06/15 13:51:26 [17875] Found job 16.0 --- inserting
11/06/15 13:51:26 [17875] gahp server not up yet, delaying ping
11/06/15 13:51:26 [17875] (16.0) doEvaluateState called: gmState GM_INIT, remoteState 0
11/06/15 13:51:26 [17875] GAHP server pid = 18094
11/06/15 13:51:31 [17875] resource  is now up
11/06/15 13:51:31 [17875] (16.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
11/06/15 13:51:32 [17875] (16.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
11/06/15 13:51:32 [17875] (16.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
11/06/15 13:51:36 [17875] (16.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
11/06/15 13:51:41 [17875] (16.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
11/06/15 13:51:46 [17875] No jobs left, shutting down
11/06/15 13:51:46 [17875] Got SIGTERM. Performing graceful shutdown.
11/06/15 13:51:46 [17875] **** condor_gridmanager (condor_GRIDMANAGER) pid 17875 EXITING WITH STATUS 0
12/03/15 15:39:26 ******************************************************
12/03/15 15:39:26 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/03/15 15:39:26 ** /usr/sbin/condor_gridmanager
12/03/15 15:39:26 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/03/15 15:39:26 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/03/15 15:39:26 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/03/15 15:39:26 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/03/15 15:39:26 ** PID = 31198
12/03/15 15:39:26 ** Log last touched 11/6 13:51:46
12/03/15 15:39:26 ******************************************************
12/03/15 15:39:26 Using config source: /etc/condor-ce/condor_config
12/03/15 15:39:26 Using local config sources: 
12/03/15 15:39:26    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/03/15 15:39:26    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/01-ce-auth.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/01-ce-router.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/01-common-auth.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/02-ce-lsf.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/02-ce-pbs.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/03-managed-fork.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/50-osg-configure.conf
12/03/15 15:39:26    /etc/condor-ce/config.d/99-local.conf
12/03/15 15:39:26    /usr/share/condor-ce/condor_ce_router_defaults|
12/03/15 15:39:26 config Macros = 141, Sorted = 141, StringBytes = 12496, TablesBytes = 5284
12/03/15 15:39:26 CLASSAD_CACHING is ENABLED
12/03/15 15:39:26 Daemon Log is logging: D_ALWAYS D_ERROR
12/03/15 15:39:26 SharedPortEndpoint: waiting for connections to named socket 30838_e91f_1
12/03/15 15:39:26 DaemonCore: command socket at <140.247.179.131:9620?sock=30838_e91f_1>
12/03/15 15:39:26 DaemonCore: private command socket at <140.247.179.131:9620?sock=30838_e91f_1>
12/03/15 15:39:27 [31198] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/03/15 15:39:27 [31198] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/03/15 15:39:27 [31198] Buf::write(): condor_write() failed
12/03/15 15:39:27 [31198] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/03/15 15:39:29 [31198] Found job 18.0 --- inserting
12/03/15 15:39:29 [31198] gahp server not up yet, delaying ping
12/03/15 15:39:29 [31198] (18.0) doEvaluateState called: gmState GM_INIT, remoteState 0
12/03/15 15:39:29 [31198] GAHP server pid = 31545
12/03/15 15:39:34 [31198] resource  is now up
12/03/15 15:39:36 [31198] (18.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/03/15 15:39:39 [31198] (18.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
12/03/15 15:39:39 [31198] (18.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
12/03/15 15:40:06 [31198] (18.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
12/03/15 15:40:11 [31198] (18.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/03/15 15:40:16 [31198] No jobs left, shutting down
12/03/15 15:40:16 [31198] Got SIGTERM. Performing graceful shutdown.
12/03/15 15:40:16 [31198] **** condor_gridmanager (condor_GRIDMANAGER) pid 31198 EXITING WITH STATUS 0
12/03/15 15:44:21 ******************************************************
12/03/15 15:44:21 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/03/15 15:44:21 ** /usr/sbin/condor_gridmanager
12/03/15 15:44:21 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/03/15 15:44:21 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/03/15 15:44:21 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/03/15 15:44:21 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/03/15 15:44:21 ** PID = 29632
12/03/15 15:44:21 ** Log last touched 12/3 15:40:16
12/03/15 15:44:21 ******************************************************
12/03/15 15:44:21 Using config source: /etc/condor-ce/condor_config
12/03/15 15:44:21 Using local config sources: 
12/03/15 15:44:21    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/03/15 15:44:21    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/01-ce-auth.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/01-ce-router.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/01-common-auth.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/02-ce-lsf.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/02-ce-pbs.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/03-managed-fork.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/50-osg-configure.conf
12/03/15 15:44:21    /etc/condor-ce/config.d/99-local.conf
12/03/15 15:44:21    /usr/share/condor-ce/condor_ce_router_defaults|
12/03/15 15:44:21 config Macros = 141, Sorted = 141, StringBytes = 12496, TablesBytes = 5284
12/03/15 15:44:21 CLASSAD_CACHING is ENABLED
12/03/15 15:44:21 Daemon Log is logging: D_ALWAYS D_ERROR
12/03/15 15:44:21 SharedPortEndpoint: waiting for connections to named socket 30838_e91f_2
12/03/15 15:44:21 DaemonCore: command socket at <140.247.179.131:9620?sock=30838_e91f_2>
12/03/15 15:44:21 DaemonCore: private command socket at <140.247.179.131:9620?sock=30838_e91f_2>
12/03/15 15:44:21 [29632] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/03/15 15:44:21 [29632] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/03/15 15:44:21 [29632] Buf::write(): condor_write() failed
12/03/15 15:44:21 [29632] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/03/15 15:44:23 [29632] Failed to get expiration time of proxy /n/atlasgrid/condor/15/0/cluster15.proc0.subproc0/x509up_u556792
12/03/15 15:44:23 [29632] Found job 16.0 --- inserting
12/03/15 15:44:23 [29632] Found job 20.0 --- inserting
12/03/15 15:44:23 [29632] gahp server not up yet, delaying ping
12/03/15 15:44:23 [29632] (16.0) doEvaluateState called: gmState GM_INIT, remoteState -1
12/03/15 15:44:23 [29632] GAHP server pid = 29731
12/03/15 15:44:23 [29632] (20.0) doEvaluateState called: gmState GM_INIT, remoteState 0
12/03/15 15:44:23 [29632] (16.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
12/03/15 15:44:23 [29632] (16.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/15/0/cluster15.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
12/03/15 15:44:28 [29632] resource  is now up
12/03/15 15:44:28 [29632] (20.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/03/15 15:44:29 [29632] (20.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
12/03/15 15:44:29 [29632] (20.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
12/03/15 15:44:33 [29632] (20.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
12/03/15 15:44:38 [29632] (20.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/03/15 15:44:43 [29632] No jobs left, shutting down
12/03/15 15:44:43 [29632] Got SIGTERM. Performing graceful shutdown.
12/03/15 15:44:43 [29632] **** condor_gridmanager (condor_GRIDMANAGER) pid 29632 EXITING WITH STATUS 0
12/04/15 13:22:00 ******************************************************
12/04/15 13:22:00 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/04/15 13:22:00 ** /usr/sbin/condor_gridmanager
12/04/15 13:22:00 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/04/15 13:22:00 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/04/15 13:22:00 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/04/15 13:22:00 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/04/15 13:22:00 ** PID = 13881
12/04/15 13:22:00 ** Log last touched 12/3 15:44:43
12/04/15 13:22:00 ******************************************************
12/04/15 13:22:00 Using config source: /etc/condor-ce/condor_config
12/04/15 13:22:00 Using local config sources: 
12/04/15 13:22:00    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/04/15 13:22:00    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/01-ce-auth.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/01-ce-router.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/01-common-auth.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/02-ce-lsf.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/02-ce-pbs.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/03-managed-fork.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/50-osg-configure.conf
12/04/15 13:22:00    /etc/condor-ce/config.d/99-local.conf
12/04/15 13:22:00    /usr/share/condor-ce/condor_ce_router_defaults|
12/04/15 13:22:00 config Macros = 141, Sorted = 141, StringBytes = 12493, TablesBytes = 5284
12/04/15 13:22:00 CLASSAD_CACHING is ENABLED
12/04/15 13:22:00 Daemon Log is logging: D_ALWAYS D_ERROR
12/04/15 13:22:00 SharedPortEndpoint: waiting for connections to named socket 8941_7e45_1
12/04/15 13:22:00 DaemonCore: command socket at <140.247.179.131:9620?sock=8941_7e45_1>
12/04/15 13:22:00 DaemonCore: private command socket at <140.247.179.131:9620?sock=8941_7e45_1>
12/04/15 13:22:00 [13881] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/04/15 13:22:00 [13881] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/04/15 13:22:00 [13881] Buf::write(): condor_write() failed
12/04/15 13:22:00 [13881] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/04/15 13:22:03 [13881] Found job 22.0 --- inserting
12/04/15 13:22:03 [13881] gahp server not up yet, delaying ping
12/04/15 13:22:03 [13881] (22.0) doEvaluateState called: gmState GM_INIT, remoteState 0
12/04/15 13:22:03 [13881] GAHP server pid = 13893
12/04/15 13:22:08 [13881] resource  is now up
12/04/15 13:22:09 [13881] (22.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/04/15 13:22:11 [13881] (22.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
12/04/15 13:22:11 [13881] (22.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
12/04/15 13:22:21 [13881] (22.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
12/04/15 13:22:26 [13881] (22.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/04/15 13:22:31 [13881] No jobs left, shutting down
12/04/15 13:22:31 [13881] Got SIGTERM. Performing graceful shutdown.
12/04/15 13:22:31 [13881] **** condor_gridmanager (condor_GRIDMANAGER) pid 13881 EXITING WITH STATUS 0
12/09/15 11:21:40 ******************************************************
12/09/15 11:21:40 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/09/15 11:21:40 ** /usr/sbin/condor_gridmanager
12/09/15 11:21:40 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/09/15 11:21:40 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/09/15 11:21:40 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/09/15 11:21:40 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/09/15 11:21:40 ** PID = 6424
12/09/15 11:21:40 ** Log last touched 12/4 13:22:31
12/09/15 11:21:40 ******************************************************
12/09/15 11:21:40 Using config source: /etc/condor-ce/condor_config
12/09/15 11:21:40 Using local config sources: 
12/09/15 11:21:40    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/09/15 11:21:40    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/01-ce-auth.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/01-ce-router.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/01-common-auth.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/02-ce-lsf.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/02-ce-pbs.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/03-managed-fork.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/50-osg-configure.conf
12/09/15 11:21:40    /etc/condor-ce/config.d/99-local.conf
12/09/15 11:21:40    /usr/share/condor-ce/condor_ce_router_defaults|
12/09/15 11:21:40 config Macros = 141, Sorted = 141, StringBytes = 12489, TablesBytes = 5284
12/09/15 11:21:40 CLASSAD_CACHING is ENABLED
12/09/15 11:21:40 Daemon Log is logging: D_ALWAYS D_ERROR
12/09/15 11:21:40 SharedPortEndpoint: waiting for connections to named socket 5207_8d6d_1
12/09/15 11:21:40 DaemonCore: command socket at <140.247.179.131:9620?sock=5207_8d6d_1>
12/09/15 11:21:40 DaemonCore: private command socket at <140.247.179.131:9620?sock=5207_8d6d_1>
12/09/15 11:21:40 [6424] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/09/15 11:21:40 [6424] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/09/15 11:21:40 [6424] Buf::write(): condor_write() failed
12/09/15 11:21:40 [6424] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/09/15 11:21:43 [6424] Found job 24.0 --- inserting
12/09/15 11:21:43 [6424] gahp server not up yet, delaying ping
12/09/15 11:21:43 [6424] (24.0) doEvaluateState called: gmState GM_INIT, remoteState 0
12/09/15 11:21:43 [6424] GAHP server pid = 6431
12/09/15 11:21:48 [6424] resource  is now up
12/09/15 11:21:48 [6424] (24.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/09/15 11:21:50 [6424] (24.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
12/09/15 11:21:50 [6424] (24.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
12/09/15 11:21:53 [6424] (24.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
12/09/15 11:21:58 [6424] (24.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/09/15 11:22:03 [6424] No jobs left, shutting down
12/09/15 11:22:03 [6424] Got SIGTERM. Performing graceful shutdown.
12/09/15 11:22:03 [6424] **** condor_gridmanager (condor_GRIDMANAGER) pid 6424 EXITING WITH STATUS 0
12/09/15 11:24:48 ******************************************************
12/09/15 11:24:48 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/09/15 11:24:48 ** /usr/sbin/condor_gridmanager
12/09/15 11:24:48 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/09/15 11:24:48 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/09/15 11:24:48 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/09/15 11:24:48 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/09/15 11:24:48 ** PID = 12178
12/09/15 11:24:48 ** Log last touched 12/9 11:22:03
12/09/15 11:24:48 ******************************************************
12/09/15 11:24:48 Using config source: /etc/condor-ce/condor_config
12/09/15 11:24:48 Using local config sources: 
12/09/15 11:24:48    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/09/15 11:24:48    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/01-ce-auth.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/01-ce-router.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/01-common-auth.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/02-ce-lsf.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/02-ce-pbs.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/03-managed-fork.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/50-osg-configure.conf
12/09/15 11:24:48    /etc/condor-ce/config.d/99-local.conf
12/09/15 11:24:48    /usr/share/condor-ce/condor_ce_router_defaults|
12/09/15 11:24:48 config Macros = 141, Sorted = 141, StringBytes = 12491, TablesBytes = 5284
12/09/15 11:24:48 CLASSAD_CACHING is ENABLED
12/09/15 11:24:48 Daemon Log is logging: D_ALWAYS D_ERROR
12/09/15 11:24:48 SharedPortEndpoint: waiting for connections to named socket 5207_8d6d_2
12/09/15 11:24:48 DaemonCore: command socket at <140.247.179.131:9620?sock=5207_8d6d_2>
12/09/15 11:24:48 DaemonCore: private command socket at <140.247.179.131:9620?sock=5207_8d6d_2>
12/09/15 11:24:48 [12178] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/09/15 11:24:48 [12178] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/09/15 11:24:48 [12178] Buf::write(): condor_write() failed
12/09/15 11:24:48 [12178] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/09/15 11:24:50 [12178] Failed to get expiration time of proxy /n/atlasgrid/condor/17/0/cluster17.proc0.subproc0/x509up_u556792
12/09/15 11:24:50 [12178] Found job 18.0 --- inserting
12/09/15 11:24:50 [12178] Failed to get expiration time of proxy /n/atlasgrid/condor/19/0/cluster19.proc0.subproc0/x509up_u556792
12/09/15 11:24:50 [12178] Found job 20.0 --- inserting
12/09/15 11:24:50 [12178] Failed to get expiration time of proxy /n/atlasgrid/condor/21/0/cluster21.proc0.subproc0/x509up_u556792
12/09/15 11:24:50 [12178] Found job 22.0 --- inserting
12/09/15 11:24:50 [12178] gahp server not up yet, delaying ping
12/09/15 11:24:50 [12178] (18.0) doEvaluateState called: gmState GM_INIT, remoteState -1
12/09/15 11:24:50 [12178] GAHP server pid = 12183
12/09/15 11:24:50 [12178] (20.0) doEvaluateState called: gmState GM_INIT, remoteState -1
12/09/15 11:24:50 [12178] (22.0) doEvaluateState called: gmState GM_INIT, remoteState -1
12/09/15 11:24:50 [12178] (18.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
12/09/15 11:24:50 [12178] (18.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/17/0/cluster17.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
12/09/15 11:24:51 [12178] (20.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
12/09/15 11:24:51 [12178] (20.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/19/0/cluster19.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
12/09/15 11:24:51 [12178] (22.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
12/09/15 11:24:51 [12178] (22.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/21/0/cluster21.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
12/09/15 11:24:55 [12178] resource  is now up
12/09/15 11:24:55 [12178] No jobs left, shutting down
12/09/15 11:24:55 [12178] Got SIGTERM. Performing graceful shutdown.
12/09/15 11:24:55 [12178] **** condor_gridmanager (condor_GRIDMANAGER) pid 12178 EXITING WITH STATUS 0
12/11/15 16:39:03 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
12/11/15 16:39:03 Using IDs: 16 processors, 8 CPUs, 8 HTs
12/11/15 16:39:03 Enumerating interfaces: lo 127.0.0.1 up
12/11/15 16:39:03 Enumerating interfaces: eth2 10.31.131.202 up
12/11/15 16:39:03 Enumerating interfaces: eth3 140.247.179.131 up
12/11/15 16:39:03 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
12/11/15 16:39:03 Initializing Directory: curr_dir = /etc/condor-ce/config.d
12/11/15 16:39:03 ******************************************************
12/11/15 16:39:03 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/11/15 16:39:03 ** /usr/sbin/condor_gridmanager
12/11/15 16:39:03 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/11/15 16:39:03 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/11/15 16:39:03 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/11/15 16:39:03 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/11/15 16:39:03 ** PID = 7701
12/11/15 16:39:03 ** Log last touched 12/9 11:24:55
12/11/15 16:39:03 ******************************************************
12/11/15 16:39:03 Using config source: /etc/condor-ce/condor_config
12/11/15 16:39:03 Using local config sources: 
12/11/15 16:39:03    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/11/15 16:39:03    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/01-ce-auth.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/01-ce-router.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/01-common-auth.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/02-ce-lsf.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/02-ce-pbs.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/03-managed-fork.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/50-osg-configure.conf
12/11/15 16:39:03    /etc/condor-ce/config.d/99-local.conf
12/11/15 16:39:03    /usr/share/condor-ce/condor_ce_router_defaults|
12/11/15 16:39:03 config Macros = 142, Sorted = 142, StringBytes = 12501, TablesBytes = 5320
12/11/15 16:39:03 CLASSAD_CACHING is ENABLED
12/11/15 16:39:03 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
12/11/15 16:39:03 SharedPortEndpoint: waiting for connections to named socket 6803_142e_1
12/11/15 16:39:03 DaemonCore: command socket at <140.247.179.131:9620?sock=6803_142e_1>
12/11/15 16:39:03 DaemonCore: private command socket at <140.247.179.131:9620?sock=6803_142e_1>
12/11/15 16:39:03 Setting maximum accepts per cycle 8.
12/11/15 16:39:03 Setting maximum reaps per cycle 8.
12/11/15 16:39:03 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/11/15 16:39:03 [7701] Welcome to the all-singing, all dancing, "amazing" GridManager!
12/11/15 16:39:03 [7701] DaemonCore: No more children processes to reap.
12/11/15 16:39:03 [7701] DaemonCore: in SendAliveToParent()
12/11/15 16:39:03 [7701] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4367_6947_12
12/11/15 16:39:03 [7701] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
12/11/15 16:39:03 [7701] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
12/11/15 16:39:03 [7701] IPVERIFY: ip found is 0
12/11/15 16:39:03 [7701] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/11/15 16:39:03 [7701] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/11/15 16:39:03 [7701] Buf::write(): condor_write() failed
12/11/15 16:39:03 [7701] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/11/15 16:39:03 [7701] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4367_6947_12
12/11/15 16:39:03 [7701] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
12/11/15 16:39:03 [7701] DaemonCore: Leaving SendAliveToParent() - success
12/11/15 16:39:03 [7701] Checking proxies
12/11/15 16:39:05 [7701] Received REMOVE_JOBS signal
12/11/15 16:39:05 [7701] in doContactSchedd()
12/11/15 16:39:05 [7701] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4367_6947_12
12/11/15 16:39:05 [7701] querying for new jobs
12/11/15 16:39:05 [7701] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
12/11/15 16:39:06 [7701] Using job type INFNBatch for job 24.0
12/11/15 16:39:06 [7701] (24.0) SetJobLeaseTimers()
12/11/15 16:39:06 [7701] Failed to get expiration time of proxy /n/atlasgrid/condor/23/0/cluster23.proc0.subproc0/x509up_u556792
12/11/15 16:39:06 [7701] Found job 24.0 --- inserting
12/11/15 16:39:06 [7701] Fetched 1 new job ads from schedd
12/11/15 16:39:06 [7701] querying for removed/held jobs
12/11/15 16:39:06 [7701] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/11/15 16:39:06 [7701] Fetched 1 job ads from schedd
12/11/15 16:39:06 [7701] leaving doContactSchedd()
12/11/15 16:39:06 [7701] gahp server not up yet, delaying ping
12/11/15 16:39:06 [7701] *** UpdateLeases called
12/11/15 16:39:06 [7701]     Leases not supported, cancelling timer
12/11/15 16:39:06 [7701] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=6803_142e_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=4367_6947_12>"
CurrentTime = time()
MyCurrentTime = 1449869946
IdleJobs = 0
JobLimit = 10000

12/11/15 16:39:06 [7701] Trying to update collector <10.31.131.202:9619>
12/11/15 16:39:06 [7701] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/11/15 16:39:06 [7701] File descriptor limits: max 4096, safe 3277
12/11/15 16:39:06 [7701] Received ADD_JOBS signal
12/11/15 16:39:06 [7701] (24.0) doEvaluateState called: gmState GM_INIT, remoteState -1
12/11/15 16:39:06 [7701] GAHP server pid = 7752
12/11/15 16:39:06 [7701] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
12/11/15 16:39:06 [7701] GAHP[7752] <- 'COMMANDS'
12/11/15 16:39:06 [7701] GAHP[7752] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
12/11/15 16:39:06 [7701] GAHP[7752] <- 'ASYNC_MODE_ON'
12/11/15 16:39:06 [7701] GAHP[7752] -> 'S' 'Async mode on'
12/11/15 16:39:06 [7701] (24.0) gm state change: GM_INIT -> GM_START
12/11/15 16:39:06 [7701] (24.0) gm state change: GM_START -> GM_TRANSFER_INPUT
12/11/15 16:39:06 [7701] (24.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
12/11/15 16:39:06 [7701] GAHP[7752] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/23/0/cluster23.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/23/0/cluster23.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/23/0/cluster23.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#24.0#1449678098";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
12/11/15 16:39:06 [7701] GAHP[7752] -> 'S'
12/11/15 16:39:06 [7701] GAHP[7752] <- 'RESULTS'
12/11/15 16:39:06 [7701] GAHP[7752] -> 'R'
12/11/15 16:39:06 [7701] GAHP[7752] -> 'S' '1'
12/11/15 16:39:06 [7701] GAHP[7752] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/23/0/cluster23.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
12/11/15 16:39:06 [7701] (24.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
12/11/15 16:39:06 [7701] (24.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/23/0/cluster23.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
12/11/15 16:39:06 [7701] (24.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
12/11/15 16:39:06 [7701] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
12/11/15 16:39:06 [7701] (24.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
12/11/15 16:39:06 [7701] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
12/11/15 16:39:06 [7701] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
12/11/15 16:39:06 [7701] IPVERIFY: ip found is 1
12/11/15 16:39:08 [7701] Evaluating staleness of remote job statuses.
12/11/15 16:39:11 [7701] resource  is now up
12/11/15 16:39:11 [7701] in doContactSchedd()
12/11/15 16:39:11 [7701] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4367_6947_12
12/11/15 16:39:11 [7701] querying for new jobs
12/11/15 16:39:11 [7701] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
12/11/15 16:39:11 [7701] Fetched 0 new job ads from schedd
12/11/15 16:39:11 [7701] querying for removed/held jobs
12/11/15 16:39:11 [7701] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/11/15 16:39:11 [7701] Fetched 1 job ads from schedd
12/11/15 16:39:11 [7701] Updating classad values for 24.0:
12/11/15 16:39:11 [7701]    CurrentStatusUnknown = false
12/11/15 16:39:11 [7701]    GridJobId = undefined
12/11/15 16:39:11 [7701]    LastRemoteStatusUpdate = 0
12/11/15 16:39:11 [7701]    Managed = "ScheddDone"
12/11/15 16:39:11 [7701] Deleting job 24.0 from schedd
12/11/15 16:39:11 [7701] No jobs left, shutting down
12/11/15 16:39:11 [7701] leaving doContactSchedd()
12/11/15 16:39:11 [7701] Got SIGTERM. Performing graceful shutdown.
12/11/15 16:39:11 [7701] Started timer to call main_shutdown_fast in 1800 seconds
12/11/15 16:39:11 [7701] **** condor_gridmanager (condor_GRIDMANAGER) pid 7701 EXITING WITH STATUS 0
12/11/15 16:49:53 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
12/11/15 16:49:53 Using IDs: 16 processors, 8 CPUs, 8 HTs
12/11/15 16:49:53 Enumerating interfaces: lo 127.0.0.1 up
12/11/15 16:49:53 Enumerating interfaces: eth2 10.31.131.202 up
12/11/15 16:49:53 Enumerating interfaces: eth3 140.247.179.131 up
12/11/15 16:49:53 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
12/11/15 16:49:53 Initializing Directory: curr_dir = /etc/condor-ce/config.d
12/11/15 16:49:53 ******************************************************
12/11/15 16:49:53 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/11/15 16:49:53 ** /usr/sbin/condor_gridmanager
12/11/15 16:49:53 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/11/15 16:49:53 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/11/15 16:49:53 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/11/15 16:49:53 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/11/15 16:49:53 ** PID = 30763
12/11/15 16:49:53 ** Log last touched 12/11 16:39:11
12/11/15 16:49:53 ******************************************************
12/11/15 16:49:53 Using config source: /etc/condor-ce/condor_config
12/11/15 16:49:53 Using local config sources: 
12/11/15 16:49:53    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/11/15 16:49:53    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/01-ce-auth.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/01-ce-router.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/01-common-auth.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/02-ce-lsf.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/02-ce-pbs.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/03-managed-fork.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/50-osg-configure.conf
12/11/15 16:49:53    /etc/condor-ce/config.d/99-local.conf
12/11/15 16:49:53    /usr/share/condor-ce/condor_ce_router_defaults|
12/11/15 16:49:53 config Macros = 142, Sorted = 142, StringBytes = 12508, TablesBytes = 5320
12/11/15 16:49:53 CLASSAD_CACHING is ENABLED
12/11/15 16:49:53 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
12/11/15 16:49:53 SharedPortEndpoint: waiting for connections to named socket 30616_7dd6_1
12/11/15 16:49:53 DaemonCore: command socket at <140.247.179.131:9620?sock=30616_7dd6_1>
12/11/15 16:49:53 DaemonCore: private command socket at <140.247.179.131:9620?sock=30616_7dd6_1>
12/11/15 16:49:53 Setting maximum accepts per cycle 8.
12/11/15 16:49:53 Setting maximum reaps per cycle 8.
12/11/15 16:49:53 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/11/15 16:49:53 [30763] Welcome to the all-singing, all dancing, "amazing" GridManager!
12/11/15 16:49:53 [30763] DaemonCore: No more children processes to reap.
12/11/15 16:49:53 [30763] DaemonCore: in SendAliveToParent()
12/11/15 16:49:53 [30763] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 30609_b31f_4
12/11/15 16:49:54 [30763] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
12/11/15 16:49:54 [30763] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
12/11/15 16:49:54 [30763] IPVERIFY: ip found is 0
12/11/15 16:49:54 [30763] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/11/15 16:49:54 [30763] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/11/15 16:49:54 [30763] Buf::write(): condor_write() failed
12/11/15 16:49:54 [30763] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/11/15 16:49:54 [30763] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 30609_b31f_4
12/11/15 16:49:54 [30763] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
12/11/15 16:49:54 [30763] DaemonCore: Leaving SendAliveToParent() - success
12/11/15 16:49:54 [30763] Checking proxies
12/11/15 16:49:56 [30763] Received ADD_JOBS signal
12/11/15 16:49:56 [30763] in doContactSchedd()
12/11/15 16:49:56 [30763] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30609_b31f_4
12/11/15 16:49:56 [30763] querying for new jobs
12/11/15 16:49:56 [30763] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
12/11/15 16:49:56 [30763] Using job type INFNBatch for job 26.0
12/11/15 16:49:56 [30763] (26.0) SetJobLeaseTimers()
12/11/15 16:49:56 [30763] Found job 26.0 --- inserting
12/11/15 16:49:56 [30763] Fetched 1 new job ads from schedd
12/11/15 16:49:56 [30763] querying for removed/held jobs
12/11/15 16:49:56 [30763] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/11/15 16:49:56 [30763] Fetched 0 job ads from schedd
12/11/15 16:49:56 [30763] leaving doContactSchedd()
12/11/15 16:49:56 [30763] gahp server not up yet, delaying ping
12/11/15 16:49:56 [30763] *** UpdateLeases called
12/11/15 16:49:56 [30763]     Leases not supported, cancelling timer
12/11/15 16:49:56 [30763] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=30616_7dd6_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=30609_b31f_4>"
CurrentTime = time()
MyCurrentTime = 1449870596
IdleJobs = 1
JobLimit = 10000

12/11/15 16:49:56 [30763] Trying to update collector <10.31.131.202:9619>
12/11/15 16:49:56 [30763] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/11/15 16:49:56 [30763] File descriptor limits: max 4096, safe 3277
12/11/15 16:49:56 [30763] (26.0) doEvaluateState called: gmState GM_INIT, remoteState 0
12/11/15 16:49:56 [30763] GAHP server pid = 30772
12/11/15 16:49:56 [30763] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
12/11/15 16:49:56 [30763] GAHP[30772] <- 'COMMANDS'
12/11/15 16:49:56 [30763] GAHP[30772] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
12/11/15 16:49:56 [30763] GAHP[30772] <- 'ASYNC_MODE_ON'
12/11/15 16:49:56 [30763] GAHP[30772] -> 'S' 'Async mode on'
12/11/15 16:49:56 [30763] (26.0) gm state change: GM_INIT -> GM_START
12/11/15 16:49:56 [30763] (26.0) gm state change: GM_START -> GM_CLEAR_REQUEST
12/11/15 16:49:56 [30763] (26.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
12/11/15 16:49:56 [30763] (26.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
12/11/15 16:49:56 [30763] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
12/11/15 16:49:56 [30763] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
12/11/15 16:49:56 [30763] IPVERIFY: ip found is 1
12/11/15 16:49:58 [30763] Evaluating staleness of remote job statuses.
12/11/15 16:50:01 [30763] resource  is now up
12/11/15 16:50:01 [30763] in doContactSchedd()
12/11/15 16:50:01 [30763] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30609_b31f_4
12/11/15 16:50:01 [30763] querying for removed/held jobs
12/11/15 16:50:01 [30763] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/11/15 16:50:01 [30763] Fetched 0 job ads from schedd
12/11/15 16:50:01 [30763] Updating classad values for 26.0:
12/11/15 16:50:01 [30763]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#26.0#1449870592"
12/11/15 16:50:01 [30763]    LastRemoteStatusUpdate = 1449870596
12/11/15 16:50:01 [30763] leaving doContactSchedd()
12/11/15 16:50:01 [30763] (26.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/11/15 16:50:01 [30763] (26.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
12/11/15 16:50:01 [30763] (26.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
12/11/15 16:50:01 [30763] GAHP[30772] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#26.0#1449870592";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
12/11/15 16:50:01 [30763] GAHP[30772] -> 'S'
12/11/15 16:50:02 [30763] GAHP[30772] <- 'RESULTS'
12/11/15 16:50:02 [30763] GAHP[30772] -> 'R'
12/11/15 16:50:02 [30763] GAHP[30772] -> 'S' '1'
12/11/15 16:50:02 [30763] GAHP[30772] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
12/11/15 16:50:02 [30763] (26.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
12/11/15 16:50:02 [30763] (26.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
12/11/15 16:50:02 [30763] (26.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
12/11/15 16:50:02 [30763] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
12/11/15 16:50:02 [30763] (26.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
12/11/15 16:50:06 [30763] in doContactSchedd()
12/11/15 16:50:06 [30763] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30609_b31f_4
12/11/15 16:50:06 [30763] querying for removed/held jobs
12/11/15 16:50:06 [30763] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/11/15 16:50:06 [30763] Fetched 0 job ads from schedd
12/11/15 16:50:06 [30763] Updating classad values for 26.0:
12/11/15 16:50:06 [30763]    CurrentStatusUnknown = false
12/11/15 16:50:06 [30763]    GridJobId = undefined
12/11/15 16:50:06 [30763]    LastRemoteStatusUpdate = 0
12/11/15 16:50:06 [30763] leaving doContactSchedd()
12/11/15 16:50:06 [30763] (26.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
12/11/15 16:50:06 [30763] (26.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
12/11/15 16:50:06 [30763] (26.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
12/11/15 16:50:11 [30763] in doContactSchedd()
12/11/15 16:50:11 [30763] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30609_b31f_4
12/11/15 16:50:11 [30763] querying for removed/held jobs
12/11/15 16:50:11 [30763] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/11/15 16:50:11 [30763] Fetched 0 job ads from schedd
12/11/15 16:50:11 [30763] Updating classad values for 26.0:
12/11/15 16:50:11 [30763]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#26.0#1449870592"
12/11/15 16:50:11 [30763]    LastRemoteStatusUpdate = 1449870606
12/11/15 16:50:11 [30763] leaving doContactSchedd()
12/11/15 16:50:11 [30763] (26.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/11/15 16:50:11 [30763] (26.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
12/11/15 16:50:11 [30763] (26.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
12/11/15 16:50:11 [30763] (26.0) gm state change: GM_HOLD -> GM_DELETE
12/11/15 16:50:16 [30763] in doContactSchedd()
12/11/15 16:50:16 [30763] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30609_b31f_4
12/11/15 16:50:16 [30763] querying for removed/held jobs
12/11/15 16:50:16 [30763] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/11/15 16:50:16 [30763] Fetched 0 job ads from schedd
12/11/15 16:50:16 [30763] Updating classad values for 26.0:
12/11/15 16:50:16 [30763]    EnteredCurrentStatus = 1449870611
12/11/15 16:50:16 [30763]    HoldReason = "Attempts to submit failed: "
12/11/15 16:50:16 [30763]    HoldReasonCode = 0
12/11/15 16:50:16 [30763]    HoldReasonSubCode = 0
12/11/15 16:50:16 [30763]    JobStatus = 5
12/11/15 16:50:16 [30763]    LastReleaseReason = "Data files spooled"
12/11/15 16:50:16 [30763]    Managed = "Schedd"
12/11/15 16:50:16 [30763]    NumSystemHolds = 1
12/11/15 16:50:16 [30763]    ReleaseReason = undefined
12/11/15 16:50:16 [30763] No jobs left, shutting down
12/11/15 16:50:16 [30763] leaving doContactSchedd()
12/11/15 16:50:16 [30763] Got SIGTERM. Performing graceful shutdown.
12/11/15 16:50:16 [30763] Started timer to call main_shutdown_fast in 1800 seconds
12/11/15 16:50:16 [30763] **** condor_gridmanager (condor_GRIDMANAGER) pid 30763 EXITING WITH STATUS 0
12/15/15 16:00:46 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
12/15/15 16:00:46 Using IDs: 16 processors, 8 CPUs, 8 HTs
12/15/15 16:00:46 Enumerating interfaces: lo 127.0.0.1 up
12/15/15 16:00:46 Enumerating interfaces: eth2 10.31.131.202 up
12/15/15 16:00:46 Enumerating interfaces: eth3 140.247.179.131 up
12/15/15 16:00:46 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
12/15/15 16:00:46 Initializing Directory: curr_dir = /etc/condor-ce/config.d
12/15/15 16:00:46 ******************************************************
12/15/15 16:00:46 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/15/15 16:00:46 ** /usr/sbin/condor_gridmanager
12/15/15 16:00:46 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/15/15 16:00:46 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/15/15 16:00:46 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/15/15 16:00:46 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/15/15 16:00:46 ** PID = 25111
12/15/15 16:00:46 ** Log last touched 12/11 16:50:16
12/15/15 16:00:46 ******************************************************
12/15/15 16:00:46 Using config source: /etc/condor-ce/condor_config
12/15/15 16:00:46 Using local config sources: 
12/15/15 16:00:46    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/15/15 16:00:46    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/01-ce-auth.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/01-ce-router.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/01-common-auth.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/02-ce-lsf.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/02-ce-pbs.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/03-managed-fork.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/50-osg-configure.conf
12/15/15 16:00:46    /etc/condor-ce/config.d/99-local.conf
12/15/15 16:00:46    /usr/share/condor-ce/condor_ce_router_defaults|
12/15/15 16:00:46 config Macros = 142, Sorted = 142, StringBytes = 12509, TablesBytes = 5320
12/15/15 16:00:46 CLASSAD_CACHING is ENABLED
12/15/15 16:00:46 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
12/15/15 16:00:46 SharedPortEndpoint: waiting for connections to named socket 12183_c688_2
12/15/15 16:00:46 DaemonCore: command socket at <140.247.179.131:9620?sock=12183_c688_2>
12/15/15 16:00:46 DaemonCore: private command socket at <140.247.179.131:9620?sock=12183_c688_2>
12/15/15 16:00:46 Setting maximum accepts per cycle 8.
12/15/15 16:00:46 Setting maximum reaps per cycle 8.
12/15/15 16:00:46 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/15/15 16:00:46 [25111] Welcome to the all-singing, all dancing, "amazing" GridManager!
12/15/15 16:00:46 [25111] DaemonCore: No more children processes to reap.
12/15/15 16:00:46 [25111] DaemonCore: in SendAliveToParent()
12/15/15 16:00:46 [25111] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12175_31b7_4
12/15/15 16:00:46 [25111] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
12/15/15 16:00:46 [25111] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
12/15/15 16:00:46 [25111] IPVERIFY: ip found is 0
12/15/15 16:00:46 [25111] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/15/15 16:00:46 [25111] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/15/15 16:00:46 [25111] Buf::write(): condor_write() failed
12/15/15 16:00:46 [25111] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/15/15 16:00:46 [25111] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12175_31b7_4
12/15/15 16:00:46 [25111] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
12/15/15 16:00:46 [25111] DaemonCore: Leaving SendAliveToParent() - success
12/15/15 16:00:46 [25111] Checking proxies
12/15/15 16:00:48 [25111] Received REMOVE_JOBS signal
12/15/15 16:00:48 [25111] in doContactSchedd()
12/15/15 16:00:48 [25111] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12175_31b7_4
12/15/15 16:00:48 [25111] querying for new jobs
12/15/15 16:00:48 [25111] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
12/15/15 16:00:48 [25111] Using job type INFNBatch for job 26.0
12/15/15 16:00:48 [25111] (26.0) SetJobLeaseTimers()
12/15/15 16:00:48 [25111] Failed to get expiration time of proxy /n/atlasgrid/condor/25/0/cluster25.proc0.subproc0/x509up_u556792
12/15/15 16:00:48 [25111] Found job 26.0 --- inserting
12/15/15 16:00:48 [25111] Fetched 1 new job ads from schedd
12/15/15 16:00:48 [25111] querying for removed/held jobs
12/15/15 16:00:48 [25111] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/15/15 16:00:48 [25111] Fetched 1 job ads from schedd
12/15/15 16:00:48 [25111] leaving doContactSchedd()
12/15/15 16:00:48 [25111] gahp server not up yet, delaying ping
12/15/15 16:00:48 [25111] *** UpdateLeases called
12/15/15 16:00:48 [25111]     Leases not supported, cancelling timer
12/15/15 16:00:48 [25111] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=12183_c688_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=12175_31b7_4>"
CurrentTime = time()
MyCurrentTime = 1450213248
IdleJobs = 0
JobLimit = 10000

12/15/15 16:00:48 [25111] Trying to update collector <10.31.131.202:9619>
12/15/15 16:00:48 [25111] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/15/15 16:00:48 [25111] File descriptor limits: max 4096, safe 3277
12/15/15 16:00:48 [25111] (26.0) doEvaluateState called: gmState GM_INIT, remoteState -1
12/15/15 16:00:48 [25111] GAHP server pid = 25193
12/15/15 16:00:48 [25111] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
12/15/15 16:00:48 [25111] GAHP[25193] <- 'COMMANDS'
12/15/15 16:00:48 [25111] GAHP[25193] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
12/15/15 16:00:48 [25111] GAHP[25193] <- 'ASYNC_MODE_ON'
12/15/15 16:00:48 [25111] GAHP[25193] -> 'S' 'Async mode on'
12/15/15 16:00:48 [25111] (26.0) gm state change: GM_INIT -> GM_START
12/15/15 16:00:48 [25111] (26.0) gm state change: GM_START -> GM_TRANSFER_INPUT
12/15/15 16:00:48 [25111] (26.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
12/15/15 16:00:48 [25111] GAHP[25193] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#26.0#1449870592";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
12/15/15 16:00:48 [25111] GAHP[25193] -> 'S'
12/15/15 16:00:48 [25111] GAHP[25193] <- 'RESULTS'
12/15/15 16:00:48 [25111] GAHP[25193] -> 'R'
12/15/15 16:00:48 [25111] GAHP[25193] -> 'S' '1'
12/15/15 16:00:48 [25111] GAHP[25193] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
12/15/15 16:00:48 [25111] (26.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
12/15/15 16:00:48 [25111] (26.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/25/0/cluster25.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
12/15/15 16:00:48 [25111] (26.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
12/15/15 16:00:48 [25111] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
12/15/15 16:00:48 [25111] (26.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
12/15/15 16:00:48 [25111] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
12/15/15 16:00:48 [25111] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
12/15/15 16:00:48 [25111] IPVERIFY: ip found is 1
12/15/15 16:00:49 [25111] Received ADD_JOBS signal
12/15/15 16:00:51 [25111] Evaluating staleness of remote job statuses.
12/15/15 16:00:53 [25111] resource  is now up
12/15/15 16:00:53 [25111] in doContactSchedd()
12/15/15 16:00:53 [25111] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12175_31b7_4
12/15/15 16:00:53 [25111] querying for new jobs
12/15/15 16:00:53 [25111] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
12/15/15 16:00:53 [25111] Fetched 0 new job ads from schedd
12/15/15 16:00:53 [25111] querying for removed/held jobs
12/15/15 16:00:53 [25111] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/15/15 16:00:53 [25111] Fetched 1 job ads from schedd
12/15/15 16:00:53 [25111] Updating classad values for 26.0:
12/15/15 16:00:53 [25111]    CurrentStatusUnknown = false
12/15/15 16:00:53 [25111]    GridJobId = undefined
12/15/15 16:00:53 [25111]    LastRemoteStatusUpdate = 0
12/15/15 16:00:53 [25111]    Managed = "ScheddDone"
12/15/15 16:00:53 [25111] Deleting job 26.0 from schedd
12/15/15 16:00:53 [25111] No jobs left, shutting down
12/15/15 16:00:53 [25111] leaving doContactSchedd()
12/15/15 16:00:53 [25111] Got SIGTERM. Performing graceful shutdown.
12/15/15 16:00:53 [25111] Started timer to call main_shutdown_fast in 1800 seconds
12/15/15 16:00:53 [25111] **** condor_gridmanager (condor_GRIDMANAGER) pid 25111 EXITING WITH STATUS 0
12/18/15 13:46:03 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
12/18/15 13:46:03 Using IDs: 16 processors, 8 CPUs, 8 HTs
12/18/15 13:46:03 Enumerating interfaces: lo 127.0.0.1 up
12/18/15 13:46:03 Enumerating interfaces: eth2 10.31.131.202 up
12/18/15 13:46:03 Enumerating interfaces: eth3 140.247.179.131 up
12/18/15 13:46:03 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
12/18/15 13:46:03 Initializing Directory: curr_dir = /etc/condor-ce/config.d
12/18/15 13:46:03 ******************************************************
12/18/15 13:46:03 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/18/15 13:46:03 ** /usr/sbin/condor_gridmanager
12/18/15 13:46:03 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/18/15 13:46:03 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/18/15 13:46:03 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/18/15 13:46:03 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/18/15 13:46:03 ** PID = 30992
12/18/15 13:46:03 ** Log last touched 12/15 16:00:53
12/18/15 13:46:03 ******************************************************
12/18/15 13:46:03 Using config source: /etc/condor-ce/condor_config
12/18/15 13:46:03 Using local config sources: 
12/18/15 13:46:03    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/18/15 13:46:03    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/01-ce-auth.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/01-ce-router.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/01-common-auth.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/02-ce-lsf.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/02-ce-pbs.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/03-managed-fork.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/50-osg-configure.conf
12/18/15 13:46:03    /etc/condor-ce/config.d/99-local.conf
12/18/15 13:46:03    /usr/share/condor-ce/condor_ce_router_defaults|
12/18/15 13:46:03 config Macros = 142, Sorted = 142, StringBytes = 12509, TablesBytes = 5320
12/18/15 13:46:03 CLASSAD_CACHING is ENABLED
12/18/15 13:46:03 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
12/18/15 13:46:03 SharedPortEndpoint: waiting for connections to named socket 11624_e8d2_2
12/18/15 13:46:03 DaemonCore: command socket at <140.247.179.131:9620?sock=11624_e8d2_2>
12/18/15 13:46:03 DaemonCore: private command socket at <140.247.179.131:9620?sock=11624_e8d2_2>
12/18/15 13:46:03 Setting maximum accepts per cycle 8.
12/18/15 13:46:03 Setting maximum reaps per cycle 8.
12/18/15 13:46:03 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/18/15 13:46:03 [30992] Welcome to the all-singing, all dancing, "amazing" GridManager!
12/18/15 13:46:03 [30992] DaemonCore: No more children processes to reap.
12/18/15 13:46:03 [30992] DaemonCore: in SendAliveToParent()
12/18/15 13:46:03 [30992] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11488_b686_4
12/18/15 13:46:03 [30992] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
12/18/15 13:46:03 [30992] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
12/18/15 13:46:03 [30992] IPVERIFY: ip found is 0
12/18/15 13:46:03 [30992] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/18/15 13:46:03 [30992] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/18/15 13:46:03 [30992] Buf::write(): condor_write() failed
12/18/15 13:46:03 [30992] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/18/15 13:46:03 [30992] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11488_b686_4
12/18/15 13:46:03 [30992] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
12/18/15 13:46:03 [30992] DaemonCore: Leaving SendAliveToParent() - success
12/18/15 13:46:03 [30992] Checking proxies
12/18/15 13:46:06 [30992] Received ADD_JOBS signal
12/18/15 13:46:06 [30992] in doContactSchedd()
12/18/15 13:46:06 [30992] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11488_b686_4
12/18/15 13:46:06 [30992] querying for new jobs
12/18/15 13:46:06 [30992] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
12/18/15 13:46:06 [30992] Using job type INFNBatch for job 36.0
12/18/15 13:46:06 [30992] (36.0) SetJobLeaseTimers()
12/18/15 13:46:06 [30992] Found job 36.0 --- inserting
12/18/15 13:46:06 [30992] Fetched 1 new job ads from schedd
12/18/15 13:46:06 [30992] querying for removed/held jobs
12/18/15 13:46:06 [30992] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/18/15 13:46:06 [30992] Fetched 0 job ads from schedd
12/18/15 13:46:06 [30992] leaving doContactSchedd()
12/18/15 13:46:06 [30992] gahp server not up yet, delaying ping
12/18/15 13:46:06 [30992] *** UpdateLeases called
12/18/15 13:46:06 [30992]     Leases not supported, cancelling timer
12/18/15 13:46:06 [30992] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=11624_e8d2_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=11488_b686_4>"
CurrentTime = time()
MyCurrentTime = 1450464366
IdleJobs = 1
JobLimit = 10000

12/18/15 13:46:06 [30992] Trying to update collector <10.31.131.202:9619>
12/18/15 13:46:06 [30992] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/18/15 13:46:06 [30992] File descriptor limits: max 4096, safe 3277
12/18/15 13:46:06 [30992] (36.0) doEvaluateState called: gmState GM_INIT, remoteState 0
12/18/15 13:46:06 [30992] GAHP server pid = 31035
12/18/15 13:46:06 [30992] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
12/18/15 13:46:06 [30992] GAHP[31035] <- 'COMMANDS'
12/18/15 13:46:06 [30992] GAHP[31035] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
12/18/15 13:46:06 [30992] GAHP[31035] <- 'ASYNC_MODE_ON'
12/18/15 13:46:06 [30992] GAHP[31035] -> 'S' 'Async mode on'
12/18/15 13:46:06 [30992] (36.0) gm state change: GM_INIT -> GM_START
12/18/15 13:46:06 [30992] (36.0) gm state change: GM_START -> GM_CLEAR_REQUEST
12/18/15 13:46:06 [30992] (36.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
12/18/15 13:46:06 [30992] (36.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
12/18/15 13:46:06 [30992] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
12/18/15 13:46:06 [30992] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
12/18/15 13:46:06 [30992] IPVERIFY: ip found is 1
12/18/15 13:46:08 [30992] Evaluating staleness of remote job statuses.
12/18/15 13:46:11 [30992] resource  is now up
12/18/15 13:46:11 [30992] in doContactSchedd()
12/18/15 13:46:11 [30992] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11488_b686_4
12/18/15 13:46:11 [30992] querying for removed/held jobs
12/18/15 13:46:11 [30992] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/18/15 13:46:11 [30992] Fetched 0 job ads from schedd
12/18/15 13:46:11 [30992] Updating classad values for 36.0:
12/18/15 13:46:11 [30992]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#36.0#1450464064"
12/18/15 13:46:11 [30992]    LastRemoteStatusUpdate = 1450464366
12/18/15 13:46:11 [30992] leaving doContactSchedd()
12/18/15 13:46:11 [30992] (36.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/18/15 13:46:11 [30992] (36.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
12/18/15 13:46:11 [30992] (36.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
12/18/15 13:46:11 [30992] GAHP[31035] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/sbin:/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/share/man:/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ SSH_CONNECTION=10.255.12.9'\ '49388'\ '10.31.131.202'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.9'\ '49388'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/0\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ GRIDMAP=/etc/grid-security/grid-mapfile\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_XPMNxO\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=net2.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/35/0/cluster35.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/35/0/cluster35.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/35/0/cluster35.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#36.0#1450464064";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
12/18/15 13:46:11 [30992] GAHP[31035] -> 'S'
12/18/15 13:46:13 [30992] GAHP[31035] <- 'RESULTS'
12/18/15 13:46:13 [30992] GAHP[31035] -> 'R'
12/18/15 13:46:13 [30992] GAHP[31035] -> 'S' '1'
12/18/15 13:46:13 [30992] GAHP[31035] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
12/18/15 13:46:13 [30992] (36.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
12/18/15 13:46:13 [30992] (36.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
12/18/15 13:46:13 [30992] (36.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
12/18/15 13:46:13 [30992] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
12/18/15 13:46:13 [30992] (36.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
12/18/15 13:46:16 [30992] in doContactSchedd()
12/18/15 13:46:16 [30992] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11488_b686_4
12/18/15 13:46:16 [30992] querying for removed/held jobs
12/18/15 13:46:16 [30992] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/18/15 13:46:16 [30992] Fetched 0 job ads from schedd
12/18/15 13:46:16 [30992] Updating classad values for 36.0:
12/18/15 13:46:16 [30992]    CurrentStatusUnknown = false
12/18/15 13:46:16 [30992]    GridJobId = undefined
12/18/15 13:46:16 [30992]    LastRemoteStatusUpdate = 0
12/18/15 13:46:16 [30992] leaving doContactSchedd()
12/18/15 13:46:16 [30992] (36.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
12/18/15 13:46:16 [30992] (36.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
12/18/15 13:46:16 [30992] (36.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
12/18/15 13:46:21 [30992] in doContactSchedd()
12/18/15 13:46:21 [30992] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11488_b686_4
12/18/15 13:46:21 [30992] querying for removed/held jobs
12/18/15 13:46:21 [30992] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/18/15 13:46:21 [30992] Fetched 0 job ads from schedd
12/18/15 13:46:21 [30992] Updating classad values for 36.0:
12/18/15 13:46:21 [30992]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#36.0#1450464064"
12/18/15 13:46:21 [30992]    LastRemoteStatusUpdate = 1450464376
12/18/15 13:46:21 [30992] leaving doContactSchedd()
12/18/15 13:46:21 [30992] (36.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
12/18/15 13:46:21 [30992] (36.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
12/18/15 13:46:21 [30992] (36.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
12/18/15 13:46:21 [30992] dirscat: dirpath = /tmp
12/18/15 13:46:21 [30992] dirscat: subdir = condorLocks
12/18/15 13:46:21 [30992] directory_util::rec_touch_file: Creating directory /tmp 
12/18/15 13:46:21 [30992] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
12/18/15 13:46:21 [30992] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/69 
12/18/15 13:46:21 [30992] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/69/34 
12/18/15 13:46:21 [30992] FileLock object is updating timestamp on: /tmp/condorLocks/69/34/060497033511185.lockc
12/18/15 13:46:21 [30992] WriteUserLog::initialize: opened /n/atlasgrid/condor/35/0/cluster35.proc0.subproc0/.log_14866_eIBjjH successfully
12/18/15 13:46:21 [30992] (36.0) Writing hold record to user logfile
12/18/15 13:46:21 [30992] FileLock::obtain(1) - @1450464381.376540 lock on /tmp/condorLocks/69/34/060497033511185.lockc now WRITE
12/18/15 13:46:21 [30992] FileLock::obtain(2) - @1450464381.377557 lock on /tmp/condorLocks/69/34/060497033511185.lockc now UNLOCKED
12/18/15 13:46:21 [30992] FileLock::obtain(1) - @1450464381.377678 lock on /tmp/condorLocks/69/34/060497033511185.lockc now WRITE
12/18/15 13:46:21 [30992] directory_util::rec_clean_up: file /tmp/condorLocks/69/34/060497033511185.lockc has been deleted. 
12/18/15 13:46:21 [30992] Lock file /tmp/condorLocks/69/34/060497033511185.lockc has been deleted. 
12/18/15 13:46:21 [30992] FileLock::obtain(2) - @1450464381.377843 lock on /tmp/condorLocks/69/34/060497033511185.lockc now UNLOCKED
12/18/15 13:46:21 [30992] (36.0) gm state change: GM_HOLD -> GM_DELETE
12/18/15 13:46:26 [30992] in doContactSchedd()
12/18/15 13:46:26 [30992] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11488_b686_4
12/18/15 13:46:26 [30992] querying for removed/held jobs
12/18/15 13:46:26 [30992] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/18/15 13:46:26 [30992] Fetched 0 job ads from schedd
12/18/15 13:46:26 [30992] Updating classad values for 36.0:
12/18/15 13:46:26 [30992]    EnteredCurrentStatus = 1450464381
12/18/15 13:46:26 [30992]    HoldReason = "Attempts to submit failed: "
12/18/15 13:46:26 [30992]    HoldReasonCode = 0
12/18/15 13:46:26 [30992]    HoldReasonSubCode = 0
12/18/15 13:46:26 [30992]    JobStatus = 5
12/18/15 13:46:26 [30992]    LastReleaseReason = "Data files spooled"
12/18/15 13:46:26 [30992]    Managed = "Schedd"
12/18/15 13:46:26 [30992]    NumSystemHolds = 1
12/18/15 13:46:26 [30992]    ReleaseReason = undefined
12/18/15 13:46:26 [30992] No jobs left, shutting down
12/18/15 13:46:26 [30992] leaving doContactSchedd()
12/18/15 13:46:26 [30992] Got SIGTERM. Performing graceful shutdown.
12/18/15 13:46:26 [30992] Started timer to call main_shutdown_fast in 1800 seconds
12/18/15 13:46:26 [30992] **** condor_gridmanager (condor_GRIDMANAGER) pid 30992 EXITING WITH STATUS 0
12/19/15 13:48:13 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
12/19/15 13:48:13 Using IDs: 16 processors, 8 CPUs, 8 HTs
12/19/15 13:48:13 Enumerating interfaces: lo 127.0.0.1 up
12/19/15 13:48:13 Enumerating interfaces: eth2 10.31.131.202 up
12/19/15 13:48:13 Enumerating interfaces: eth3 140.247.179.131 up
12/19/15 13:48:13 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
12/19/15 13:48:13 Initializing Directory: curr_dir = /etc/condor-ce/config.d
12/19/15 13:48:13 ******************************************************
12/19/15 13:48:13 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
12/19/15 13:48:13 ** /usr/sbin/condor_gridmanager
12/19/15 13:48:13 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
12/19/15 13:48:13 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
12/19/15 13:48:13 ** $CondorVersion: 8.2.8 Apr 08 2015 $
12/19/15 13:48:13 ** $CondorPlatform: X86_64-CentOS_6.6 $
12/19/15 13:48:13 ** PID = 25764
12/19/15 13:48:13 ** Log last touched 12/18 13:46:26
12/19/15 13:48:13 ******************************************************
12/19/15 13:48:13 Using config source: /etc/condor-ce/condor_config
12/19/15 13:48:13 Using local config sources: 
12/19/15 13:48:13    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
12/19/15 13:48:13    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/01-ce-auth.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/01-ce-router.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/01-common-auth.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/02-ce-lsf.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/02-ce-pbs.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/03-ce-shared-port.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/03-managed-fork.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/10-ce-collector-generated.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/50-osg-configure.conf
12/19/15 13:48:13    /etc/condor-ce/config.d/99-local.conf
12/19/15 13:48:13    /usr/share/condor-ce/condor_ce_router_defaults|
12/19/15 13:48:13 config Macros = 142, Sorted = 142, StringBytes = 12509, TablesBytes = 5320
12/19/15 13:48:13 CLASSAD_CACHING is ENABLED
12/19/15 13:48:13 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
12/19/15 13:48:13 SharedPortEndpoint: waiting for connections to named socket 11624_e8d2_5
12/19/15 13:48:13 DaemonCore: command socket at <140.247.179.131:9620?sock=11624_e8d2_5>
12/19/15 13:48:13 DaemonCore: private command socket at <140.247.179.131:9620?sock=11624_e8d2_5>
12/19/15 13:48:13 Setting maximum accepts per cycle 8.
12/19/15 13:48:13 Setting maximum reaps per cycle 8.
12/19/15 13:48:13 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/19/15 13:48:13 [25764] Welcome to the all-singing, all dancing, "amazing" GridManager!
12/19/15 13:48:13 [25764] DaemonCore: No more children processes to reap.
12/19/15 13:48:13 [25764] DaemonCore: in SendAliveToParent()
12/19/15 13:48:13 [25764] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11488_b686_4
12/19/15 13:48:13 [25764] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
12/19/15 13:48:13 [25764] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
12/19/15 13:48:13 [25764] IPVERIFY: ip found is 0
12/19/15 13:48:13 [25764] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
12/19/15 13:48:13 [25764] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
12/19/15 13:48:13 [25764] Buf::write(): condor_write() failed
12/19/15 13:48:13 [25764] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
12/19/15 13:48:13 [25764] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11488_b686_4
12/19/15 13:48:13 [25764] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
12/19/15 13:48:13 [25764] DaemonCore: Leaving SendAliveToParent() - success
12/19/15 13:48:13 [25764] Checking proxies
12/19/15 13:48:14 [25764] Received REMOVE_JOBS signal
12/19/15 13:48:14 [25764] in doContactSchedd()
12/19/15 13:48:14 [25764] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11488_b686_4
12/19/15 13:48:14 [25764] querying for new jobs
12/19/15 13:48:14 [25764] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
12/19/15 13:48:14 [25764] Using job type INFNBatch for job 36.0
12/19/15 13:48:14 [25764] (36.0) SetJobLeaseTimers()
12/19/15 13:48:14 [25764] Found job 36.0 --- inserting
12/19/15 13:48:14 [25764] Fetched 1 new job ads from schedd
12/19/15 13:48:14 [25764] querying for removed/held jobs
12/19/15 13:48:14 [25764] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/19/15 13:48:14 [25764] Fetched 1 job ads from schedd
12/19/15 13:48:14 [25764] leaving doContactSchedd()
12/19/15 13:48:14 [25764] gahp server not up yet, delaying ping
12/19/15 13:48:14 [25764] *** UpdateLeases called
12/19/15 13:48:14 [25764]     Leases not supported, cancelling timer
12/19/15 13:48:14 [25764] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=11624_e8d2_5>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=11488_b686_4>"
CurrentTime = time()
MyCurrentTime = 1450550894
IdleJobs = 0
JobLimit = 10000

12/19/15 13:48:14 [25764] Trying to update collector <10.31.131.202:9619>
12/19/15 13:48:14 [25764] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
12/19/15 13:48:14 [25764] File descriptor limits: max 4096, safe 3277
12/19/15 13:48:14 [25764] (36.0) doEvaluateState called: gmState GM_INIT, remoteState -1
12/19/15 13:48:14 [25764] GAHP server pid = 25766
12/19/15 13:48:14 [25764] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
12/19/15 13:48:14 [25764] GAHP[25766] <- 'COMMANDS'
12/19/15 13:48:14 [25764] GAHP[25766] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
12/19/15 13:48:14 [25764] GAHP[25766] <- 'ASYNC_MODE_ON'
12/19/15 13:48:14 [25764] GAHP[25766] -> 'S' 'Async mode on'
12/19/15 13:48:14 [25764] (36.0) gm state change: GM_INIT -> GM_START
12/19/15 13:48:14 [25764] (36.0) gm state change: GM_START -> GM_TRANSFER_INPUT
12/19/15 13:48:14 [25764] (36.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
12/19/15 13:48:14 [25764] GAHP[25766] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/sbin:/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/usr/local/openssh-7.1p1_openssl-1.0.2e_tcpwrap/share/man:/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ SSH_CONNECTION=10.255.12.9'\ '49388'\ '10.31.131.202'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.9'\ '49388'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/0\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ GRIDMAP=/etc/grid-security/grid-mapfile\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_XPMNxO\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=net2.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/35/0/cluster35.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/35/0/cluster35.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/35/0/cluster35.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#36.0#1450464064";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
12/19/15 13:48:14 [25764] GAHP[25766] -> 'S'
12/19/15 13:48:14 [25764] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
12/19/15 13:48:14 [25764] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
12/19/15 13:48:14 [25764] IPVERIFY: ip found is 1
12/19/15 13:48:15 [25764] Received ADD_JOBS signal
12/19/15 13:48:15 [25764] GAHP[25766] <- 'RESULTS'
12/19/15 13:48:15 [25764] GAHP[25766] -> 'R'
12/19/15 13:48:15 [25764] GAHP[25766] -> 'S' '1'
12/19/15 13:48:15 [25764] GAHP[25766] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
12/19/15 13:48:15 [25764] (36.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
12/19/15 13:48:15 [25764] (36.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
12/19/15 13:48:15 [25764] (36.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
12/19/15 13:48:15 [25764] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
12/19/15 13:48:15 [25764] (36.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
12/19/15 13:48:15 [25764] dirscat: dirpath = /tmp
12/19/15 13:48:15 [25764] dirscat: subdir = condorLocks
12/19/15 13:48:15 [25764] directory_util::rec_touch_file: Creating directory /tmp 
12/19/15 13:48:15 [25764] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
12/19/15 13:48:15 [25764] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/69 
12/19/15 13:48:15 [25764] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/69/34 
12/19/15 13:48:15 [25764] FileLock object is updating timestamp on: /tmp/condorLocks/69/34/060497033511185.lockc
12/19/15 13:48:15 [25764] WriteUserLog::initialize: opened /n/atlasgrid/condor/35/0/cluster35.proc0.subproc0/.log_14866_eIBjjH successfully
12/19/15 13:48:15 [25764] (36.0) Writing abort record to user logfile
12/19/15 13:48:15 [25764] FileLock::obtain(1) - @1450550895.521466 lock on /tmp/condorLocks/69/34/060497033511185.lockc now WRITE
12/19/15 13:48:15 [25764] FileLock::obtain(2) - @1450550895.522414 lock on /tmp/condorLocks/69/34/060497033511185.lockc now UNLOCKED
12/19/15 13:48:15 [25764] FileLock::obtain(1) - @1450550895.522508 lock on /tmp/condorLocks/69/34/060497033511185.lockc now WRITE
12/19/15 13:48:15 [25764] directory_util::rec_clean_up: file /tmp/condorLocks/69/34/060497033511185.lockc has been deleted. 
12/19/15 13:48:15 [25764] Lock file /tmp/condorLocks/69/34/060497033511185.lockc has been deleted. 
12/19/15 13:48:15 [25764] FileLock::obtain(2) - @1450550895.522691 lock on /tmp/condorLocks/69/34/060497033511185.lockc now UNLOCKED
12/19/15 13:48:18 [25764] Evaluating staleness of remote job statuses.
12/19/15 13:48:19 [25764] resource  is now up
12/19/15 13:48:19 [25764] in doContactSchedd()
12/19/15 13:48:19 [25764] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11488_b686_4
12/19/15 13:48:19 [25764] querying for new jobs
12/19/15 13:48:19 [25764] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
12/19/15 13:48:19 [25764] Fetched 0 new job ads from schedd
12/19/15 13:48:19 [25764] querying for removed/held jobs
12/19/15 13:48:19 [25764] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
12/19/15 13:48:19 [25764] Fetched 1 job ads from schedd
12/19/15 13:48:19 [25764] Updating classad values for 36.0:
12/19/15 13:48:19 [25764]    CurrentStatusUnknown = false
12/19/15 13:48:19 [25764]    GridJobId = undefined
12/19/15 13:48:19 [25764]    LastRemoteStatusUpdate = 0
12/19/15 13:48:19 [25764]    Managed = "ScheddDone"
12/19/15 13:48:19 [25764] Deleting job 36.0 from schedd
12/19/15 13:48:19 [25764] No jobs left, shutting down
12/19/15 13:48:19 [25764] leaving doContactSchedd()
12/19/15 13:48:19 [25764] Got SIGTERM. Performing graceful shutdown.
12/19/15 13:48:19 [25764] Started timer to call main_shutdown_fast in 1800 seconds
12/19/15 13:48:19 [25764] **** condor_gridmanager (condor_GRIDMANAGER) pid 25764 EXITING WITH STATUS 0
01/06/16 09:28:59 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/06/16 09:28:59 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/06/16 09:28:59 Enumerating interfaces: lo 127.0.0.1 up
01/06/16 09:28:59 Enumerating interfaces: eth2 10.31.131.202 up
01/06/16 09:28:59 Enumerating interfaces: eth3 140.247.179.131 up
01/06/16 09:28:59 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/06/16 09:28:59 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/06/16 09:28:59 ******************************************************
01/06/16 09:28:59 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/06/16 09:28:59 ** /usr/sbin/condor_gridmanager
01/06/16 09:28:59 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/06/16 09:28:59 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/06/16 09:28:59 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/06/16 09:28:59 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/06/16 09:28:59 ** PID = 15844
01/06/16 09:28:59 ** Log last touched 12/19 13:48:19
01/06/16 09:28:59 ******************************************************
01/06/16 09:28:59 Using config source: /etc/condor-ce/condor_config
01/06/16 09:28:59 Using local config sources: 
01/06/16 09:28:59    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/06/16 09:28:59    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/01-ce-auth.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/01-ce-router.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/01-common-auth.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/02-ce-lsf.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/02-ce-pbs.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/03-managed-fork.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/50-osg-configure.conf
01/06/16 09:28:59    /etc/condor-ce/config.d/99-local.conf
01/06/16 09:28:59    /usr/share/condor-ce/condor_ce_router_defaults|
01/06/16 09:28:59 config Macros = 142, Sorted = 142, StringBytes = 12505, TablesBytes = 5320
01/06/16 09:28:59 CLASSAD_CACHING is ENABLED
01/06/16 09:28:59 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/06/16 09:28:59 SharedPortEndpoint: waiting for connections to named socket 9210_0730_1
01/06/16 09:28:59 DaemonCore: command socket at <140.247.179.131:9620?sock=9210_0730_1>
01/06/16 09:28:59 DaemonCore: private command socket at <140.247.179.131:9620?sock=9210_0730_1>
01/06/16 09:28:59 Setting maximum accepts per cycle 8.
01/06/16 09:28:59 Setting maximum reaps per cycle 8.
01/06/16 09:28:59 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 09:28:59 [15844] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/06/16 09:28:59 [15844] DaemonCore: No more children processes to reap.
01/06/16 09:28:59 [15844] DaemonCore: in SendAliveToParent()
01/06/16 09:28:59 [15844] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 8955_76ee_4
01/06/16 09:28:59 [15844] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/06/16 09:28:59 [15844] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/06/16 09:28:59 [15844] IPVERIFY: ip found is 0
01/06/16 09:28:59 [15844] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/06/16 09:28:59 [15844] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/06/16 09:28:59 [15844] Buf::write(): condor_write() failed
01/06/16 09:28:59 [15844] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/06/16 09:28:59 [15844] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 8955_76ee_4
01/06/16 09:28:59 [15844] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/06/16 09:28:59 [15844] DaemonCore: Leaving SendAliveToParent() - success
01/06/16 09:28:59 [15844] Checking proxies
01/06/16 09:29:01 [15844] Received ADD_JOBS signal
01/06/16 09:29:01 [15844] in doContactSchedd()
01/06/16 09:29:01 [15844] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 8955_76ee_4
01/06/16 09:29:01 [15844] querying for new jobs
01/06/16 09:29:01 [15844] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/06/16 09:29:01 [15844] Using job type INFNBatch for job 40.0
01/06/16 09:29:01 [15844] (40.0) SetJobLeaseTimers()
01/06/16 09:29:01 [15844] Found job 40.0 --- inserting
01/06/16 09:29:01 [15844] Fetched 1 new job ads from schedd
01/06/16 09:29:01 [15844] querying for removed/held jobs
01/06/16 09:29:01 [15844] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:29:01 [15844] Fetched 0 job ads from schedd
01/06/16 09:29:02 [15844] leaving doContactSchedd()
01/06/16 09:29:02 [15844] gahp server not up yet, delaying ping
01/06/16 09:29:02 [15844] *** UpdateLeases called
01/06/16 09:29:02 [15844]     Leases not supported, cancelling timer
01/06/16 09:29:02 [15844] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=9210_0730_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=8955_76ee_4>"
CurrentTime = time()
MyCurrentTime = 1452090542
IdleJobs = 1
JobLimit = 10000

01/06/16 09:29:02 [15844] Trying to update collector <10.31.131.202:9619>
01/06/16 09:29:02 [15844] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 09:29:02 [15844] File descriptor limits: max 4096, safe 3277
01/06/16 09:29:02 [15844] (40.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/06/16 09:29:02 [15844] GAHP server pid = 16129
01/06/16 09:29:02 [15844] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/06/16 09:29:02 [15844] GAHP[16129] <- 'COMMANDS'
01/06/16 09:29:02 [15844] GAHP[16129] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/06/16 09:29:02 [15844] GAHP[16129] <- 'ASYNC_MODE_ON'
01/06/16 09:29:02 [15844] GAHP[16129] -> 'S' 'Async mode on'
01/06/16 09:29:02 [15844] (40.0) gm state change: GM_INIT -> GM_START
01/06/16 09:29:02 [15844] (40.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/06/16 09:29:02 [15844] (40.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 09:29:02 [15844] (40.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 09:29:02 [15844] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/06/16 09:29:02 [15844] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/06/16 09:29:02 [15844] IPVERIFY: ip found is 1
01/06/16 09:29:04 [15844] Evaluating staleness of remote job statuses.
01/06/16 09:29:07 [15844] resource  is now up
01/06/16 09:29:07 [15844] in doContactSchedd()
01/06/16 09:29:07 [15844] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 8955_76ee_4
01/06/16 09:29:07 [15844] querying for removed/held jobs
01/06/16 09:29:07 [15844] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:29:07 [15844] Fetched 0 job ads from schedd
01/06/16 09:29:07 [15844] Updating classad values for 40.0:
01/06/16 09:29:07 [15844]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#40.0#1452090534"
01/06/16 09:29:07 [15844]    LastRemoteStatusUpdate = 1452090542
01/06/16 09:29:08 [15844] leaving doContactSchedd()
01/06/16 09:29:08 [15844] (40.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 09:29:08 [15844] (40.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 09:29:08 [15844] (40.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/06/16 09:29:08 [15844] GAHP[16129] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/39/0/cluster39.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/39/0/cluster39.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/39/0/cluster39.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#40.0#1452090534";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/06/16 09:29:08 [15844] GAHP[16129] -> 'S'
01/06/16 09:29:11 [15844] GAHP[16129] <- 'RESULTS'
01/06/16 09:29:11 [15844] GAHP[16129] -> 'R'
01/06/16 09:29:11 [15844] GAHP[16129] -> 'S' '1'
01/06/16 09:29:11 [15844] GAHP[16129] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/06/16 09:29:11 [15844] (40.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/06/16 09:29:11 [15844] (40.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/06/16 09:29:11 [15844] (40.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/06/16 09:29:11 [15844] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/06/16 09:29:36 [15844] (40.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/06/16 09:29:36 [15844] in doContactSchedd()
01/06/16 09:29:36 [15844] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 8955_76ee_4
01/06/16 09:29:36 [15844] querying for removed/held jobs
01/06/16 09:29:36 [15844] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:29:36 [15844] Fetched 0 job ads from schedd
01/06/16 09:29:36 [15844] Updating classad values for 40.0:
01/06/16 09:29:36 [15844]    CurrentStatusUnknown = false
01/06/16 09:29:36 [15844]    GridJobId = undefined
01/06/16 09:29:36 [15844]    LastRemoteStatusUpdate = 0
01/06/16 09:29:36 [15844] leaving doContactSchedd()
01/06/16 09:29:36 [15844] (40.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/06/16 09:29:36 [15844] (40.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 09:29:36 [15844] (40.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 09:29:41 [15844] in doContactSchedd()
01/06/16 09:29:41 [15844] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 8955_76ee_4
01/06/16 09:29:41 [15844] querying for removed/held jobs
01/06/16 09:29:41 [15844] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:29:41 [15844] Fetched 0 job ads from schedd
01/06/16 09:29:41 [15844] Updating classad values for 40.0:
01/06/16 09:29:41 [15844]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#40.0#1452090534"
01/06/16 09:29:41 [15844]    LastRemoteStatusUpdate = 1452090576
01/06/16 09:29:41 [15844] leaving doContactSchedd()
01/06/16 09:29:41 [15844] (40.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 09:29:41 [15844] (40.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 09:29:41 [15844] (40.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/06/16 09:29:41 [15844] (40.0) gm state change: GM_HOLD -> GM_DELETE
01/06/16 09:29:46 [15844] in doContactSchedd()
01/06/16 09:29:46 [15844] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 8955_76ee_4
01/06/16 09:29:46 [15844] querying for removed/held jobs
01/06/16 09:29:46 [15844] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:29:46 [15844] Fetched 0 job ads from schedd
01/06/16 09:29:46 [15844] Updating classad values for 40.0:
01/06/16 09:29:46 [15844]    EnteredCurrentStatus = 1452090581
01/06/16 09:29:46 [15844]    HoldReason = "Attempts to submit failed: "
01/06/16 09:29:46 [15844]    HoldReasonCode = 0
01/06/16 09:29:46 [15844]    HoldReasonSubCode = 0
01/06/16 09:29:46 [15844]    JobStatus = 5
01/06/16 09:29:46 [15844]    LastReleaseReason = "Data files spooled"
01/06/16 09:29:46 [15844]    Managed = "Schedd"
01/06/16 09:29:46 [15844]    NumSystemHolds = 1
01/06/16 09:29:46 [15844]    ReleaseReason = undefined
01/06/16 09:29:46 [15844] No jobs left, shutting down
01/06/16 09:29:46 [15844] leaving doContactSchedd()
01/06/16 09:29:46 [15844] Got SIGTERM. Performing graceful shutdown.
01/06/16 09:29:46 [15844] Started timer to call main_shutdown_fast in 1800 seconds
01/06/16 09:29:46 [15844] **** condor_gridmanager (condor_GRIDMANAGER) pid 15844 EXITING WITH STATUS 0
01/06/16 09:45:08 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/06/16 09:45:08 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/06/16 09:45:08 Enumerating interfaces: lo 127.0.0.1 up
01/06/16 09:45:08 Enumerating interfaces: eth2 10.31.131.202 up
01/06/16 09:45:08 Enumerating interfaces: eth3 140.247.179.131 up
01/06/16 09:45:08 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/06/16 09:45:08 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/06/16 09:45:08 ******************************************************
01/06/16 09:45:08 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/06/16 09:45:08 ** /usr/sbin/condor_gridmanager
01/06/16 09:45:08 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/06/16 09:45:08 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/06/16 09:45:08 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/06/16 09:45:08 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/06/16 09:45:08 ** PID = 24624
01/06/16 09:45:08 ** Log last touched 1/6 09:29:46
01/06/16 09:45:08 ******************************************************
01/06/16 09:45:08 Using config source: /etc/condor-ce/condor_config
01/06/16 09:45:08 Using local config sources: 
01/06/16 09:45:08    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/06/16 09:45:08    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/01-ce-auth.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/01-ce-router.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/01-common-auth.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/02-ce-lsf.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/02-ce-pbs.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/03-managed-fork.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/50-osg-configure.conf
01/06/16 09:45:08    /etc/condor-ce/config.d/99-local.conf
01/06/16 09:45:08    /usr/share/condor-ce/condor_ce_router_defaults|
01/06/16 09:45:08 config Macros = 142, Sorted = 142, StringBytes = 12508, TablesBytes = 5320
01/06/16 09:45:08 CLASSAD_CACHING is ENABLED
01/06/16 09:45:08 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/06/16 09:45:08 SharedPortEndpoint: waiting for connections to named socket 13540_e7a0_1
01/06/16 09:45:08 DaemonCore: command socket at <140.247.179.131:9620?sock=13540_e7a0_1>
01/06/16 09:45:08 DaemonCore: private command socket at <140.247.179.131:9620?sock=13540_e7a0_1>
01/06/16 09:45:08 Setting maximum accepts per cycle 8.
01/06/16 09:45:08 Setting maximum reaps per cycle 8.
01/06/16 09:45:08 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 09:45:08 [24624] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/06/16 09:45:08 [24624] DaemonCore: No more children processes to reap.
01/06/16 09:45:08 [24624] DaemonCore: in SendAliveToParent()
01/06/16 09:45:08 [24624] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 09:45:09 [24624] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/06/16 09:45:09 [24624] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/06/16 09:45:09 [24624] IPVERIFY: ip found is 0
01/06/16 09:45:09 [24624] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/06/16 09:45:09 [24624] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/06/16 09:45:09 [24624] Buf::write(): condor_write() failed
01/06/16 09:45:09 [24624] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/06/16 09:45:09 [24624] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 09:45:09 [24624] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/06/16 09:45:09 [24624] DaemonCore: Leaving SendAliveToParent() - success
01/06/16 09:45:09 [24624] Checking proxies
01/06/16 09:45:11 [24624] Received ADD_JOBS signal
01/06/16 09:45:11 [24624] in doContactSchedd()
01/06/16 09:45:11 [24624] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 09:45:11 [24624] querying for new jobs
01/06/16 09:45:11 [24624] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/06/16 09:45:11 [24624] Using job type INFNBatch for job 42.0
01/06/16 09:45:11 [24624] (42.0) SetJobLeaseTimers()
01/06/16 09:45:12 [24624] Found job 42.0 --- inserting
01/06/16 09:45:12 [24624] Fetched 1 new job ads from schedd
01/06/16 09:45:12 [24624] querying for removed/held jobs
01/06/16 09:45:12 [24624] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:45:12 [24624] Fetched 0 job ads from schedd
01/06/16 09:45:12 [24624] leaving doContactSchedd()
01/06/16 09:45:12 [24624] gahp server not up yet, delaying ping
01/06/16 09:45:12 [24624] *** UpdateLeases called
01/06/16 09:45:12 [24624]     Leases not supported, cancelling timer
01/06/16 09:45:12 [24624] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=13540_e7a0_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=13415_dde0_4>"
CurrentTime = time()
MyCurrentTime = 1452091512
IdleJobs = 1
JobLimit = 10000

01/06/16 09:45:12 [24624] Trying to update collector <10.31.131.202:9619>
01/06/16 09:45:12 [24624] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 09:45:12 [24624] File descriptor limits: max 4096, safe 3277
01/06/16 09:45:12 [24624] (42.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/06/16 09:45:12 [24624] GAHP server pid = 24632
01/06/16 09:45:12 [24624] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/06/16 09:45:12 [24624] GAHP[24632] <- 'COMMANDS'
01/06/16 09:45:12 [24624] GAHP[24632] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/06/16 09:45:12 [24624] GAHP[24632] <- 'ASYNC_MODE_ON'
01/06/16 09:45:12 [24624] GAHP[24632] -> 'S' 'Async mode on'
01/06/16 09:45:12 [24624] (42.0) gm state change: GM_INIT -> GM_START
01/06/16 09:45:12 [24624] (42.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/06/16 09:45:12 [24624] (42.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 09:45:12 [24624] (42.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 09:45:12 [24624] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/06/16 09:45:12 [24624] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/06/16 09:45:12 [24624] IPVERIFY: ip found is 1
01/06/16 09:45:13 [24624] Evaluating staleness of remote job statuses.
01/06/16 09:45:17 [24624] resource  is now up
01/06/16 09:45:17 [24624] in doContactSchedd()
01/06/16 09:45:17 [24624] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 09:45:17 [24624] querying for removed/held jobs
01/06/16 09:45:17 [24624] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:45:17 [24624] Fetched 0 job ads from schedd
01/06/16 09:45:17 [24624] Updating classad values for 42.0:
01/06/16 09:45:17 [24624]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#42.0#1452091211"
01/06/16 09:45:17 [24624]    LastRemoteStatusUpdate = 1452091512
01/06/16 09:45:17 [24624] leaving doContactSchedd()
01/06/16 09:45:17 [24624] (42.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 09:45:17 [24624] (42.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 09:45:17 [24624] (42.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/06/16 09:45:17 [24624] GAHP[24632] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#42.0#1452091211";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/06/16 09:45:17 [24624] GAHP[24632] -> 'S'
01/06/16 09:45:18 [24624] GAHP[24632] <- 'RESULTS'
01/06/16 09:45:18 [24624] GAHP[24632] -> 'R'
01/06/16 09:45:18 [24624] GAHP[24632] -> 'S' '1'
01/06/16 09:45:18 [24624] GAHP[24632] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/06/16 09:45:18 [24624] (42.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/06/16 09:45:18 [24624] (42.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/06/16 09:45:18 [24624] (42.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/06/16 09:45:18 [24624] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/06/16 09:45:35 [24624] (42.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/06/16 09:45:35 [24624] in doContactSchedd()
01/06/16 09:45:35 [24624] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 09:45:35 [24624] querying for removed/held jobs
01/06/16 09:45:35 [24624] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:45:35 [24624] Fetched 0 job ads from schedd
01/06/16 09:45:35 [24624] Updating classad values for 42.0:
01/06/16 09:45:35 [24624]    CurrentStatusUnknown = false
01/06/16 09:45:35 [24624]    GridJobId = undefined
01/06/16 09:45:35 [24624]    LastRemoteStatusUpdate = 0
01/06/16 09:45:35 [24624] leaving doContactSchedd()
01/06/16 09:45:35 [24624] (42.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/06/16 09:45:35 [24624] (42.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 09:45:35 [24624] (42.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 09:45:40 [24624] in doContactSchedd()
01/06/16 09:45:40 [24624] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 09:45:40 [24624] querying for removed/held jobs
01/06/16 09:45:40 [24624] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:45:40 [24624] Fetched 0 job ads from schedd
01/06/16 09:45:40 [24624] Updating classad values for 42.0:
01/06/16 09:45:40 [24624]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#42.0#1452091211"
01/06/16 09:45:40 [24624]    LastRemoteStatusUpdate = 1452091535
01/06/16 09:45:40 [24624] leaving doContactSchedd()
01/06/16 09:45:40 [24624] (42.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 09:45:40 [24624] (42.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 09:45:40 [24624] (42.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/06/16 09:45:40 [24624] (42.0) gm state change: GM_HOLD -> GM_DELETE
01/06/16 09:45:45 [24624] in doContactSchedd()
01/06/16 09:45:45 [24624] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 09:45:45 [24624] querying for removed/held jobs
01/06/16 09:45:45 [24624] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 09:45:45 [24624] Fetched 0 job ads from schedd
01/06/16 09:45:45 [24624] Updating classad values for 42.0:
01/06/16 09:45:45 [24624]    EnteredCurrentStatus = 1452091540
01/06/16 09:45:45 [24624]    HoldReason = "Attempts to submit failed: "
01/06/16 09:45:45 [24624]    HoldReasonCode = 0
01/06/16 09:45:45 [24624]    HoldReasonSubCode = 0
01/06/16 09:45:45 [24624]    JobStatus = 5
01/06/16 09:45:45 [24624]    LastReleaseReason = "Data files spooled"
01/06/16 09:45:45 [24624]    Managed = "Schedd"
01/06/16 09:45:45 [24624]    NumSystemHolds = 1
01/06/16 09:45:45 [24624]    ReleaseReason = undefined
01/06/16 09:45:45 [24624] No jobs left, shutting down
01/06/16 09:45:45 [24624] leaving doContactSchedd()
01/06/16 09:45:45 [24624] Got SIGTERM. Performing graceful shutdown.
01/06/16 09:45:45 [24624] Started timer to call main_shutdown_fast in 1800 seconds
01/06/16 09:45:45 [24624] **** condor_gridmanager (condor_GRIDMANAGER) pid 24624 EXITING WITH STATUS 0
01/06/16 10:37:21 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/06/16 10:37:21 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/06/16 10:37:21 Enumerating interfaces: lo 127.0.0.1 up
01/06/16 10:37:21 Enumerating interfaces: eth2 10.31.131.202 up
01/06/16 10:37:21 Enumerating interfaces: eth3 140.247.179.131 up
01/06/16 10:37:21 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/06/16 10:37:21 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/06/16 10:37:21 ******************************************************
01/06/16 10:37:21 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/06/16 10:37:21 ** /usr/sbin/condor_gridmanager
01/06/16 10:37:21 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/06/16 10:37:21 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/06/16 10:37:21 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/06/16 10:37:21 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/06/16 10:37:21 ** PID = 19890
01/06/16 10:37:21 ** Log last touched 1/6 09:45:45
01/06/16 10:37:21 ******************************************************
01/06/16 10:37:21 Using config source: /etc/condor-ce/condor_config
01/06/16 10:37:21 Using local config sources: 
01/06/16 10:37:21    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/06/16 10:37:21    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/01-ce-auth.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/01-ce-router.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/01-common-auth.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/02-ce-lsf.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/02-ce-pbs.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/03-managed-fork.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/50-osg-configure.conf
01/06/16 10:37:21    /etc/condor-ce/config.d/99-local.conf
01/06/16 10:37:21    /usr/share/condor-ce/condor_ce_router_defaults|
01/06/16 10:37:21 config Macros = 142, Sorted = 142, StringBytes = 12508, TablesBytes = 5320
01/06/16 10:37:21 CLASSAD_CACHING is ENABLED
01/06/16 10:37:21 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/06/16 10:37:21 SharedPortEndpoint: waiting for connections to named socket 13540_e7a0_2
01/06/16 10:37:21 DaemonCore: command socket at <140.247.179.131:9620?sock=13540_e7a0_2>
01/06/16 10:37:21 DaemonCore: private command socket at <140.247.179.131:9620?sock=13540_e7a0_2>
01/06/16 10:37:21 Setting maximum accepts per cycle 8.
01/06/16 10:37:21 Setting maximum reaps per cycle 8.
01/06/16 10:37:21 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 10:37:21 [19890] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/06/16 10:37:21 [19890] DaemonCore: No more children processes to reap.
01/06/16 10:37:21 [19890] DaemonCore: in SendAliveToParent()
01/06/16 10:37:21 [19890] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 10:37:21 [19890] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/06/16 10:37:21 [19890] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/06/16 10:37:21 [19890] IPVERIFY: ip found is 0
01/06/16 10:37:21 [19890] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/06/16 10:37:21 [19890] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/06/16 10:37:21 [19890] Buf::write(): condor_write() failed
01/06/16 10:37:21 [19890] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/06/16 10:37:21 [19890] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 10:37:21 [19890] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/06/16 10:37:21 [19890] DaemonCore: Leaving SendAliveToParent() - success
01/06/16 10:37:21 [19890] Checking proxies
01/06/16 10:37:24 [19890] Received ADD_JOBS signal
01/06/16 10:37:24 [19890] in doContactSchedd()
01/06/16 10:37:24 [19890] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 10:37:24 [19890] querying for new jobs
01/06/16 10:37:24 [19890] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/06/16 10:37:24 [19890] Using job type INFNBatch for job 44.0
01/06/16 10:37:24 [19890] (44.0) SetJobLeaseTimers()
01/06/16 10:37:24 [19890] Found job 44.0 --- inserting
01/06/16 10:37:24 [19890] Fetched 1 new job ads from schedd
01/06/16 10:37:24 [19890] querying for removed/held jobs
01/06/16 10:37:24 [19890] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 10:37:24 [19890] Fetched 0 job ads from schedd
01/06/16 10:37:24 [19890] leaving doContactSchedd()
01/06/16 10:37:24 [19890] gahp server not up yet, delaying ping
01/06/16 10:37:24 [19890] *** UpdateLeases called
01/06/16 10:37:24 [19890]     Leases not supported, cancelling timer
01/06/16 10:37:24 [19890] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=13540_e7a0_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=13415_dde0_4>"
CurrentTime = time()
MyCurrentTime = 1452094644
IdleJobs = 1
JobLimit = 10000

01/06/16 10:37:24 [19890] Trying to update collector <10.31.131.202:9619>
01/06/16 10:37:24 [19890] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 10:37:24 [19890] File descriptor limits: max 4096, safe 3277
01/06/16 10:37:24 [19890] (44.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/06/16 10:37:24 [19890] GAHP server pid = 20429
01/06/16 10:37:24 [19890] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/06/16 10:37:24 [19890] GAHP[20429] <- 'COMMANDS'
01/06/16 10:37:24 [19890] GAHP[20429] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/06/16 10:37:24 [19890] GAHP[20429] <- 'ASYNC_MODE_ON'
01/06/16 10:37:24 [19890] GAHP[20429] -> 'S' 'Async mode on'
01/06/16 10:37:24 [19890] (44.0) gm state change: GM_INIT -> GM_START
01/06/16 10:37:24 [19890] (44.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/06/16 10:37:24 [19890] (44.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 10:37:24 [19890] (44.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 10:37:24 [19890] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/06/16 10:37:24 [19890] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/06/16 10:37:24 [19890] IPVERIFY: ip found is 1
01/06/16 10:37:26 [19890] Evaluating staleness of remote job statuses.
01/06/16 10:37:29 [19890] resource  is now up
01/06/16 10:37:29 [19890] in doContactSchedd()
01/06/16 10:37:29 [19890] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 10:37:29 [19890] querying for removed/held jobs
01/06/16 10:37:29 [19890] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 10:37:29 [19890] Fetched 0 job ads from schedd
01/06/16 10:37:29 [19890] Updating classad values for 44.0:
01/06/16 10:37:29 [19890]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#44.0#1452094345"
01/06/16 10:37:29 [19890]    LastRemoteStatusUpdate = 1452094644
01/06/16 10:37:29 [19890] leaving doContactSchedd()
01/06/16 10:37:29 [19890] (44.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 10:37:29 [19890] (44.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 10:37:29 [19890] (44.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/06/16 10:37:29 [19890] GAHP[20429] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#44.0#1452094345";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/06/16 10:37:29 [19890] GAHP[20429] -> 'S'
01/06/16 10:37:30 [19890] GAHP[20429] <- 'RESULTS'
01/06/16 10:37:30 [19890] GAHP[20429] -> 'R'
01/06/16 10:37:30 [19890] GAHP[20429] -> 'S' '1'
01/06/16 10:37:30 [19890] GAHP[20429] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/06/16 10:37:30 [19890] (44.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/06/16 10:37:30 [19890] (44.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/06/16 10:37:30 [19890] (44.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/06/16 10:37:30 [19890] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/06/16 10:37:30 [19890] (44.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/06/16 10:37:34 [19890] in doContactSchedd()
01/06/16 10:37:34 [19890] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 10:37:34 [19890] querying for removed/held jobs
01/06/16 10:37:34 [19890] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 10:37:34 [19890] Fetched 0 job ads from schedd
01/06/16 10:37:34 [19890] Updating classad values for 44.0:
01/06/16 10:37:34 [19890]    CurrentStatusUnknown = false
01/06/16 10:37:34 [19890]    GridJobId = undefined
01/06/16 10:37:34 [19890]    LastRemoteStatusUpdate = 0
01/06/16 10:37:34 [19890] leaving doContactSchedd()
01/06/16 10:37:34 [19890] (44.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/06/16 10:37:34 [19890] (44.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 10:37:34 [19890] (44.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 10:37:39 [19890] in doContactSchedd()
01/06/16 10:37:39 [19890] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 10:37:39 [19890] querying for removed/held jobs
01/06/16 10:37:39 [19890] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 10:37:39 [19890] Fetched 0 job ads from schedd
01/06/16 10:37:39 [19890] Updating classad values for 44.0:
01/06/16 10:37:39 [19890]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#44.0#1452094345"
01/06/16 10:37:39 [19890]    LastRemoteStatusUpdate = 1452094654
01/06/16 10:37:39 [19890] leaving doContactSchedd()
01/06/16 10:37:39 [19890] (44.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 10:37:39 [19890] (44.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 10:37:39 [19890] (44.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/06/16 10:37:39 [19890] (44.0) gm state change: GM_HOLD -> GM_DELETE
01/06/16 10:37:44 [19890] in doContactSchedd()
01/06/16 10:37:44 [19890] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13415_dde0_4
01/06/16 10:37:44 [19890] querying for removed/held jobs
01/06/16 10:37:44 [19890] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 10:37:44 [19890] Fetched 0 job ads from schedd
01/06/16 10:37:44 [19890] Updating classad values for 44.0:
01/06/16 10:37:44 [19890]    EnteredCurrentStatus = 1452094659
01/06/16 10:37:44 [19890]    HoldReason = "Attempts to submit failed: "
01/06/16 10:37:44 [19890]    HoldReasonCode = 0
01/06/16 10:37:44 [19890]    HoldReasonSubCode = 0
01/06/16 10:37:44 [19890]    JobStatus = 5
01/06/16 10:37:44 [19890]    LastReleaseReason = "Data files spooled"
01/06/16 10:37:44 [19890]    Managed = "Schedd"
01/06/16 10:37:44 [19890]    NumSystemHolds = 1
01/06/16 10:37:44 [19890]    ReleaseReason = undefined
01/06/16 10:37:44 [19890] No jobs left, shutting down
01/06/16 10:37:44 [19890] leaving doContactSchedd()
01/06/16 10:37:44 [19890] Got SIGTERM. Performing graceful shutdown.
01/06/16 10:37:44 [19890] Started timer to call main_shutdown_fast in 1800 seconds
01/06/16 10:37:44 [19890] **** condor_gridmanager (condor_GRIDMANAGER) pid 19890 EXITING WITH STATUS 0
01/06/16 11:42:36 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/06/16 11:42:36 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/06/16 11:42:36 Enumerating interfaces: lo 127.0.0.1 up
01/06/16 11:42:36 Enumerating interfaces: eth2 10.31.131.202 up
01/06/16 11:42:36 Enumerating interfaces: eth3 140.247.179.131 up
01/06/16 11:42:36 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/06/16 11:42:36 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/06/16 11:42:36 ******************************************************
01/06/16 11:42:36 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/06/16 11:42:36 ** /usr/sbin/condor_gridmanager
01/06/16 11:42:36 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/06/16 11:42:36 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/06/16 11:42:36 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/06/16 11:42:36 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/06/16 11:42:36 ** PID = 29344
01/06/16 11:42:36 ** Log last touched 1/6 10:37:44
01/06/16 11:42:36 ******************************************************
01/06/16 11:42:36 Using config source: /etc/condor-ce/condor_config
01/06/16 11:42:36 Using local config sources: 
01/06/16 11:42:36    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/06/16 11:42:36    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/01-ce-auth.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/01-ce-router.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/01-common-auth.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/02-ce-lsf.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/02-ce-pbs.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/03-managed-fork.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/50-osg-configure.conf
01/06/16 11:42:36    /etc/condor-ce/config.d/99-local.conf
01/06/16 11:42:36    /usr/share/condor-ce/condor_ce_router_defaults|
01/06/16 11:42:36 config Macros = 142, Sorted = 142, StringBytes = 12508, TablesBytes = 5320
01/06/16 11:42:36 CLASSAD_CACHING is ENABLED
01/06/16 11:42:36 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/06/16 11:42:36 SharedPortEndpoint: waiting for connections to named socket 23588_a138_1
01/06/16 11:42:36 DaemonCore: command socket at <140.247.179.131:9620?sock=23588_a138_1>
01/06/16 11:42:36 DaemonCore: private command socket at <140.247.179.131:9620?sock=23588_a138_1>
01/06/16 11:42:36 Setting maximum accepts per cycle 8.
01/06/16 11:42:36 Setting maximum reaps per cycle 8.
01/06/16 11:42:36 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 11:42:36 [29344] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/06/16 11:42:36 [29344] DaemonCore: No more children processes to reap.
01/06/16 11:42:36 [29344] DaemonCore: in SendAliveToParent()
01/06/16 11:42:36 [29344] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 23549_0f4a_4
01/06/16 11:42:36 [29344] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/06/16 11:42:36 [29344] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/06/16 11:42:36 [29344] IPVERIFY: ip found is 0
01/06/16 11:42:36 [29344] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/06/16 11:42:36 [29344] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/06/16 11:42:36 [29344] Buf::write(): condor_write() failed
01/06/16 11:42:36 [29344] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/06/16 11:42:36 [29344] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 23549_0f4a_4
01/06/16 11:42:36 [29344] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/06/16 11:42:36 [29344] DaemonCore: Leaving SendAliveToParent() - success
01/06/16 11:42:36 [29344] Checking proxies
01/06/16 11:42:39 [29344] Received ADD_JOBS signal
01/06/16 11:42:39 [29344] in doContactSchedd()
01/06/16 11:42:39 [29344] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 23549_0f4a_4
01/06/16 11:42:40 [29344] querying for new jobs
01/06/16 11:42:40 [29344] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/06/16 11:42:40 [29344] Using job type INFNBatch for job 46.0
01/06/16 11:42:40 [29344] (46.0) SetJobLeaseTimers()
01/06/16 11:42:40 [29344] Found job 46.0 --- inserting
01/06/16 11:42:40 [29344] Fetched 1 new job ads from schedd
01/06/16 11:42:40 [29344] querying for removed/held jobs
01/06/16 11:42:40 [29344] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 11:42:40 [29344] Fetched 0 job ads from schedd
01/06/16 11:42:40 [29344] leaving doContactSchedd()
01/06/16 11:42:40 [29344] gahp server not up yet, delaying ping
01/06/16 11:42:40 [29344] *** UpdateLeases called
01/06/16 11:42:40 [29344]     Leases not supported, cancelling timer
01/06/16 11:42:40 [29344] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=23588_a138_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=23549_0f4a_4>"
CurrentTime = time()
MyCurrentTime = 1452098560
IdleJobs = 1
JobLimit = 10000

01/06/16 11:42:40 [29344] Trying to update collector <10.31.131.202:9619>
01/06/16 11:42:40 [29344] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 11:42:40 [29344] File descriptor limits: max 4096, safe 3277
01/06/16 11:42:40 [29344] (46.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/06/16 11:42:40 [29344] GAHP server pid = 29349
01/06/16 11:42:40 [29344] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/06/16 11:42:40 [29344] GAHP[29349] <- 'COMMANDS'
01/06/16 11:42:40 [29344] GAHP[29349] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/06/16 11:42:40 [29344] GAHP[29349] <- 'ASYNC_MODE_ON'
01/06/16 11:42:40 [29344] GAHP[29349] -> 'S' 'Async mode on'
01/06/16 11:42:40 [29344] (46.0) gm state change: GM_INIT -> GM_START
01/06/16 11:42:40 [29344] (46.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/06/16 11:42:40 [29344] (46.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 11:42:40 [29344] (46.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 11:42:40 [29344] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/06/16 11:42:40 [29344] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/06/16 11:42:40 [29344] IPVERIFY: ip found is 1
01/06/16 11:42:41 [29344] Evaluating staleness of remote job statuses.
01/06/16 11:42:45 [29344] resource  is now up
01/06/16 11:42:45 [29344] in doContactSchedd()
01/06/16 11:42:45 [29344] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 23549_0f4a_4
01/06/16 11:42:45 [29344] querying for removed/held jobs
01/06/16 11:42:45 [29344] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 11:42:45 [29344] Fetched 0 job ads from schedd
01/06/16 11:42:45 [29344] Updating classad values for 46.0:
01/06/16 11:42:45 [29344]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#46.0#1452098269"
01/06/16 11:42:45 [29344]    LastRemoteStatusUpdate = 1452098560
01/06/16 11:42:45 [29344] leaving doContactSchedd()
01/06/16 11:42:45 [29344] (46.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 11:42:45 [29344] (46.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 11:42:45 [29344] (46.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/06/16 11:42:45 [29344] GAHP[29349] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#46.0#1452098269";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/06/16 11:42:45 [29344] GAHP[29349] -> 'S'
01/06/16 11:42:46 [29344] GAHP[29349] <- 'RESULTS'
01/06/16 11:42:46 [29344] GAHP[29349] -> 'R'
01/06/16 11:42:46 [29344] GAHP[29349] -> 'S' '1'
01/06/16 11:42:46 [29344] GAHP[29349] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/06/16 11:42:46 [29344] (46.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/06/16 11:42:46 [29344] (46.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/06/16 11:42:46 [29344] (46.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/06/16 11:42:46 [29344] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/06/16 11:42:59 [29344] (46.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/06/16 11:42:59 [29344] in doContactSchedd()
01/06/16 11:42:59 [29344] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 23549_0f4a_4
01/06/16 11:42:59 [29344] querying for removed/held jobs
01/06/16 11:42:59 [29344] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 11:42:59 [29344] Fetched 0 job ads from schedd
01/06/16 11:42:59 [29344] Updating classad values for 46.0:
01/06/16 11:42:59 [29344]    CurrentStatusUnknown = false
01/06/16 11:42:59 [29344]    GridJobId = undefined
01/06/16 11:42:59 [29344]    LastRemoteStatusUpdate = 0
01/06/16 11:42:59 [29344] leaving doContactSchedd()
01/06/16 11:42:59 [29344] (46.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/06/16 11:42:59 [29344] (46.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 11:42:59 [29344] (46.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 11:43:04 [29344] in doContactSchedd()
01/06/16 11:43:04 [29344] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 23549_0f4a_4
01/06/16 11:43:04 [29344] querying for removed/held jobs
01/06/16 11:43:04 [29344] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 11:43:04 [29344] Fetched 0 job ads from schedd
01/06/16 11:43:04 [29344] Updating classad values for 46.0:
01/06/16 11:43:04 [29344]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#46.0#1452098269"
01/06/16 11:43:04 [29344]    LastRemoteStatusUpdate = 1452098579
01/06/16 11:43:04 [29344] leaving doContactSchedd()
01/06/16 11:43:04 [29344] (46.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 11:43:04 [29344] (46.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 11:43:04 [29344] (46.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/06/16 11:43:04 [29344] (46.0) gm state change: GM_HOLD -> GM_DELETE
01/06/16 11:43:09 [29344] in doContactSchedd()
01/06/16 11:43:09 [29344] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 23549_0f4a_4
01/06/16 11:43:09 [29344] querying for removed/held jobs
01/06/16 11:43:09 [29344] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 11:43:09 [29344] Fetched 0 job ads from schedd
01/06/16 11:43:09 [29344] Updating classad values for 46.0:
01/06/16 11:43:09 [29344]    EnteredCurrentStatus = 1452098584
01/06/16 11:43:09 [29344]    HoldReason = "Attempts to submit failed: "
01/06/16 11:43:09 [29344]    HoldReasonCode = 0
01/06/16 11:43:09 [29344]    HoldReasonSubCode = 0
01/06/16 11:43:09 [29344]    JobStatus = 5
01/06/16 11:43:09 [29344]    LastReleaseReason = "Data files spooled"
01/06/16 11:43:09 [29344]    Managed = "Schedd"
01/06/16 11:43:09 [29344]    NumSystemHolds = 1
01/06/16 11:43:09 [29344]    ReleaseReason = undefined
01/06/16 11:43:09 [29344] No jobs left, shutting down
01/06/16 11:43:09 [29344] leaving doContactSchedd()
01/06/16 11:43:09 [29344] Got SIGTERM. Performing graceful shutdown.
01/06/16 11:43:09 [29344] Started timer to call main_shutdown_fast in 1800 seconds
01/06/16 11:43:09 [29344] **** condor_gridmanager (condor_GRIDMANAGER) pid 29344 EXITING WITH STATUS 0
01/06/16 14:29:05 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/06/16 14:29:05 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/06/16 14:29:05 Enumerating interfaces: lo 127.0.0.1 up
01/06/16 14:29:05 Enumerating interfaces: eth2 10.31.131.202 up
01/06/16 14:29:05 Enumerating interfaces: eth3 140.247.179.131 up
01/06/16 14:29:05 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/06/16 14:29:05 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/06/16 14:29:05 ******************************************************
01/06/16 14:29:05 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/06/16 14:29:05 ** /usr/sbin/condor_gridmanager
01/06/16 14:29:05 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/06/16 14:29:05 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/06/16 14:29:05 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/06/16 14:29:05 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/06/16 14:29:05 ** PID = 20934
01/06/16 14:29:05 ** Log last touched 1/6 11:43:09
01/06/16 14:29:05 ******************************************************
01/06/16 14:29:05 Using config source: /etc/condor-ce/condor_config
01/06/16 14:29:05 Using local config sources: 
01/06/16 14:29:05    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/06/16 14:29:05    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/01-ce-auth.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/01-ce-router.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/01-common-auth.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/02-ce-lsf.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/02-ce-pbs.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/03-managed-fork.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/50-osg-configure.conf
01/06/16 14:29:05    /etc/condor-ce/config.d/99-local.conf
01/06/16 14:29:05    /usr/share/condor-ce/condor_ce_router_defaults|
01/06/16 14:29:05 config Macros = 142, Sorted = 142, StringBytes = 12581, TablesBytes = 5320
01/06/16 14:29:05 CLASSAD_CACHING is ENABLED
01/06/16 14:29:05 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/06/16 14:29:05 SharedPortEndpoint: waiting for connections to named socket 16032_be85_1
01/06/16 14:29:05 DaemonCore: command socket at <140.247.179.131:9620?sock=16032_be85_1>
01/06/16 14:29:05 DaemonCore: private command socket at <140.247.179.131:9620?sock=16032_be85_1>
01/06/16 14:29:05 Setting maximum accepts per cycle 8.
01/06/16 14:29:05 Setting maximum reaps per cycle 8.
01/06/16 14:29:05 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 14:29:05 [20934] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/06/16 14:29:05 [20934] DaemonCore: No more children processes to reap.
01/06/16 14:29:05 [20934] DaemonCore: in SendAliveToParent()
01/06/16 14:29:05 [20934] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 16025_35a9_4
01/06/16 14:29:05 [20934] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/06/16 14:29:05 [20934] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/06/16 14:29:05 [20934] IPVERIFY: ip found is 0
01/06/16 14:29:05 [20934] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/06/16 14:29:05 [20934] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/06/16 14:29:05 [20934] Buf::write(): condor_write() failed
01/06/16 14:29:05 [20934] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/06/16 14:29:05 [20934] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 16025_35a9_4
01/06/16 14:29:05 [20934] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/06/16 14:29:05 [20934] DaemonCore: Leaving SendAliveToParent() - success
01/06/16 14:29:05 [20934] Checking proxies
01/06/16 14:29:08 [20934] Received ADD_JOBS signal
01/06/16 14:29:08 [20934] in doContactSchedd()
01/06/16 14:29:08 [20934] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 16025_35a9_4
01/06/16 14:29:08 [20934] querying for new jobs
01/06/16 14:29:08 [20934] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/06/16 14:29:08 [20934] Using job type INFNBatch for job 48.0
01/06/16 14:29:08 [20934] (48.0) SetJobLeaseTimers()
01/06/16 14:29:08 [20934] Found job 48.0 --- inserting
01/06/16 14:29:08 [20934] Fetched 1 new job ads from schedd
01/06/16 14:29:08 [20934] querying for removed/held jobs
01/06/16 14:29:08 [20934] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 14:29:08 [20934] Fetched 0 job ads from schedd
01/06/16 14:29:08 [20934] leaving doContactSchedd()
01/06/16 14:29:08 [20934] gahp server not up yet, delaying ping
01/06/16 14:29:08 [20934] *** UpdateLeases called
01/06/16 14:29:08 [20934]     Leases not supported, cancelling timer
01/06/16 14:29:08 [20934] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=16032_be85_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=16025_35a9_4>"
CurrentTime = time()
MyCurrentTime = 1452108548
IdleJobs = 1
JobLimit = 10000

01/06/16 14:29:08 [20934] Trying to update collector <10.31.131.202:9619>
01/06/16 14:29:08 [20934] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/06/16 14:29:08 [20934] File descriptor limits: max 4096, safe 3277
01/06/16 14:29:08 [20934] (48.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/06/16 14:29:08 [20934] GAHP server pid = 21162
01/06/16 14:29:08 [20934] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/06/16 14:29:08 [20934] GAHP[21162] <- 'COMMANDS'
01/06/16 14:29:08 [20934] GAHP[21162] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/06/16 14:29:08 [20934] GAHP[21162] <- 'ASYNC_MODE_ON'
01/06/16 14:29:08 [20934] GAHP[21162] -> 'S' 'Async mode on'
01/06/16 14:29:08 [20934] (48.0) gm state change: GM_INIT -> GM_START
01/06/16 14:29:08 [20934] (48.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/06/16 14:29:08 [20934] (48.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 14:29:08 [20934] (48.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 14:29:08 [20934] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/06/16 14:29:08 [20934] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/06/16 14:29:08 [20934] IPVERIFY: ip found is 1
01/06/16 14:29:10 [20934] Evaluating staleness of remote job statuses.
01/06/16 14:29:13 [20934] resource  is now up
01/06/16 14:29:13 [20934] in doContactSchedd()
01/06/16 14:29:13 [20934] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 16025_35a9_4
01/06/16 14:29:13 [20934] querying for removed/held jobs
01/06/16 14:29:13 [20934] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 14:29:13 [20934] Fetched 0 job ads from schedd
01/06/16 14:29:13 [20934] Updating classad values for 48.0:
01/06/16 14:29:13 [20934]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#48.0#1452108542"
01/06/16 14:29:13 [20934]    LastRemoteStatusUpdate = 1452108548
01/06/16 14:29:13 [20934] leaving doContactSchedd()
01/06/16 14:29:13 [20934] (48.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 14:29:13 [20934] (48.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 14:29:13 [20934] (48.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/06/16 14:29:13 [20934] GAHP[21162] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/47/0/cluster47.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/47/0/cluster47.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/47/0/cluster47.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#48.0#1452108542";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/06/16 14:29:13 [20934] GAHP[21162] -> 'S'
01/06/16 14:29:14 [20934] GAHP[21162] <- 'RESULTS'
01/06/16 14:29:14 [20934] GAHP[21162] -> 'R'
01/06/16 14:29:14 [20934] GAHP[21162] -> 'S' '1'
01/06/16 14:29:14 [20934] GAHP[21162] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)' 'N/A'
01/06/16 14:29:14 [20934] (48.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/06/16 14:29:14 [20934] (48.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)
01/06/16 14:29:14 [20934] (48.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/06/16 14:29:14 [20934] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/06/16 14:29:22 [20934] (48.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/06/16 14:29:22 [20934] in doContactSchedd()
01/06/16 14:29:22 [20934] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 16025_35a9_4
01/06/16 14:29:22 [20934] querying for removed/held jobs
01/06/16 14:29:22 [20934] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 14:29:22 [20934] Fetched 0 job ads from schedd
01/06/16 14:29:22 [20934] Updating classad values for 48.0:
01/06/16 14:29:22 [20934]    CurrentStatusUnknown = false
01/06/16 14:29:22 [20934]    GridJobId = undefined
01/06/16 14:29:22 [20934]    LastRemoteStatusUpdate = 0
01/06/16 14:29:22 [20934] leaving doContactSchedd()
01/06/16 14:29:22 [20934] (48.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/06/16 14:29:22 [20934] (48.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/06/16 14:29:22 [20934] (48.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/06/16 14:29:27 [20934] in doContactSchedd()
01/06/16 14:29:27 [20934] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 16025_35a9_4
01/06/16 14:29:27 [20934] querying for removed/held jobs
01/06/16 14:29:27 [20934] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 14:29:27 [20934] Fetched 0 job ads from schedd
01/06/16 14:29:27 [20934] Updating classad values for 48.0:
01/06/16 14:29:27 [20934]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#48.0#1452108542"
01/06/16 14:29:27 [20934]    LastRemoteStatusUpdate = 1452108562
01/06/16 14:29:27 [20934] leaving doContactSchedd()
01/06/16 14:29:27 [20934] (48.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/06/16 14:29:27 [20934] (48.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/06/16 14:29:27 [20934] (48.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/06/16 14:29:27 [20934] (48.0) gm state change: GM_HOLD -> GM_DELETE
01/06/16 14:29:32 [20934] in doContactSchedd()
01/06/16 14:29:32 [20934] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 16025_35a9_4
01/06/16 14:29:32 [20934] querying for removed/held jobs
01/06/16 14:29:32 [20934] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/06/16 14:29:32 [20934] Fetched 0 job ads from schedd
01/06/16 14:29:32 [20934] Updating classad values for 48.0:
01/06/16 14:29:32 [20934]    EnteredCurrentStatus = 1452108567
01/06/16 14:29:32 [20934]    HoldReason = "Attempts to submit failed: "
01/06/16 14:29:32 [20934]    HoldReasonCode = 0
01/06/16 14:29:32 [20934]    HoldReasonSubCode = 0
01/06/16 14:29:32 [20934]    JobStatus = 5
01/06/16 14:29:32 [20934]    LastReleaseReason = "Data files spooled"
01/06/16 14:29:32 [20934]    Managed = "Schedd"
01/06/16 14:29:32 [20934]    NumSystemHolds = 1
01/06/16 14:29:32 [20934]    ReleaseReason = undefined
01/06/16 14:29:32 [20934] No jobs left, shutting down
01/06/16 14:29:32 [20934] leaving doContactSchedd()
01/06/16 14:29:32 [20934] Got SIGTERM. Performing graceful shutdown.
01/06/16 14:29:32 [20934] Started timer to call main_shutdown_fast in 1800 seconds
01/06/16 14:29:32 [20934] **** condor_gridmanager (condor_GRIDMANAGER) pid 20934 EXITING WITH STATUS 0
01/07/16 11:01:18 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 11:01:18 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 11:01:18 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 11:01:18 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 11:01:18 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 11:01:18 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 11:01:18 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 11:01:18 ******************************************************
01/07/16 11:01:18 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 11:01:18 ** /usr/sbin/condor_gridmanager
01/07/16 11:01:18 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 11:01:18 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 11:01:18 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 11:01:18 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 11:01:18 ** PID = 7782
01/07/16 11:01:18 ** Log last touched 1/6 14:29:32
01/07/16 11:01:18 ******************************************************
01/07/16 11:01:18 Using config source: /etc/condor-ce/condor_config
01/07/16 11:01:18 Using local config sources: 
01/07/16 11:01:18    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 11:01:18    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 11:01:18    /etc/condor-ce/config.d/99-local.conf
01/07/16 11:01:18    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 11:01:18 config Macros = 142, Sorted = 142, StringBytes = 12571, TablesBytes = 5320
01/07/16 11:01:18 CLASSAD_CACHING is ENABLED
01/07/16 11:01:18 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 11:01:18 SharedPortEndpoint: waiting for connections to named socket 752_9a18_1
01/07/16 11:01:18 DaemonCore: command socket at <140.247.179.131:9620?sock=752_9a18_1>
01/07/16 11:01:18 DaemonCore: private command socket at <140.247.179.131:9620?sock=752_9a18_1>
01/07/16 11:01:18 Setting maximum accepts per cycle 8.
01/07/16 11:01:18 Setting maximum reaps per cycle 8.
01/07/16 11:01:18 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:01:18 [7782] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 11:01:18 [7782] DaemonCore: No more children processes to reap.
01/07/16 11:01:18 [7782] DaemonCore: in SendAliveToParent()
01/07/16 11:01:18 [7782] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 601_d75a_4
01/07/16 11:01:18 [7782] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 11:01:18 [7782] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 11:01:18 [7782] IPVERIFY: ip found is 0
01/07/16 11:01:18 [7782] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 11:01:18 [7782] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 11:01:18 [7782] Buf::write(): condor_write() failed
01/07/16 11:01:18 [7782] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 11:01:18 [7782] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 601_d75a_4
01/07/16 11:01:18 [7782] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 11:01:18 [7782] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 11:01:18 [7782] Checking proxies
01/07/16 11:01:21 [7782] Received ADD_JOBS signal
01/07/16 11:01:21 [7782] in doContactSchedd()
01/07/16 11:01:21 [7782] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 601_d75a_4
01/07/16 11:01:21 [7782] querying for new jobs
01/07/16 11:01:21 [7782] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 11:01:21 [7782] Using job type INFNBatch for job 50.0
01/07/16 11:01:21 [7782] (50.0) SetJobLeaseTimers()
01/07/16 11:01:21 [7782] Found job 50.0 --- inserting
01/07/16 11:01:21 [7782] Fetched 1 new job ads from schedd
01/07/16 11:01:21 [7782] querying for removed/held jobs
01/07/16 11:01:21 [7782] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:01:21 [7782] Fetched 0 job ads from schedd
01/07/16 11:01:21 [7782] leaving doContactSchedd()
01/07/16 11:01:21 [7782] gahp server not up yet, delaying ping
01/07/16 11:01:21 [7782] *** UpdateLeases called
01/07/16 11:01:21 [7782]     Leases not supported, cancelling timer
01/07/16 11:01:21 [7782] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=752_9a18_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=601_d75a_4>"
CurrentTime = time()
MyCurrentTime = 1452182481
IdleJobs = 1
JobLimit = 10000

01/07/16 11:01:21 [7782] Trying to update collector <10.31.131.202:9619>
01/07/16 11:01:21 [7782] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:01:21 [7782] File descriptor limits: max 4096, safe 3277
01/07/16 11:01:21 [7782] (50.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/07/16 11:01:21 [7782] GAHP server pid = 7840
01/07/16 11:01:21 [7782] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 11:01:21 [7782] GAHP[7840] <- 'COMMANDS'
01/07/16 11:01:21 [7782] GAHP[7840] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 11:01:21 [7782] GAHP[7840] <- 'ASYNC_MODE_ON'
01/07/16 11:01:21 [7782] GAHP[7840] -> 'S' 'Async mode on'
01/07/16 11:01:21 [7782] (50.0) gm state change: GM_INIT -> GM_START
01/07/16 11:01:21 [7782] (50.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/07/16 11:01:21 [7782] (50.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 11:01:21 [7782] (50.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 11:01:21 [7782] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 11:01:21 [7782] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 11:01:21 [7782] IPVERIFY: ip found is 1
01/07/16 11:01:23 [7782] Evaluating staleness of remote job statuses.
01/07/16 11:01:26 [7782] resource  is now up
01/07/16 11:01:26 [7782] in doContactSchedd()
01/07/16 11:01:26 [7782] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 601_d75a_4
01/07/16 11:01:26 [7782] querying for removed/held jobs
01/07/16 11:01:26 [7782] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:01:26 [7782] Fetched 0 job ads from schedd
01/07/16 11:01:26 [7782] Updating classad values for 50.0:
01/07/16 11:01:26 [7782]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#50.0#1452182472"
01/07/16 11:01:26 [7782]    LastRemoteStatusUpdate = 1452182481
01/07/16 11:01:26 [7782] leaving doContactSchedd()
01/07/16 11:01:26 [7782] (50.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 11:01:26 [7782] (50.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 11:01:26 [7782] (50.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 11:01:26 [7782] GAHP[7840] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#50.0#1452182472";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 11:01:26 [7782] GAHP[7840] -> 'S'
01/07/16 11:01:28 [7782] GAHP[7840] <- 'RESULTS'
01/07/16 11:01:28 [7782] GAHP[7840] -> 'R'
01/07/16 11:01:28 [7782] GAHP[7840] -> 'S' '1'
01/07/16 11:01:28 [7782] GAHP[7840] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)' 'N/A'
01/07/16 11:01:28 [7782] (50.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/07/16 11:01:28 [7782] (50.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)
01/07/16 11:01:28 [7782] (50.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 11:01:28 [7782] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 11:01:43 [7782] (50.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/07/16 11:01:43 [7782] in doContactSchedd()
01/07/16 11:01:43 [7782] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 601_d75a_4
01/07/16 11:01:43 [7782] querying for removed/held jobs
01/07/16 11:01:43 [7782] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:01:43 [7782] Fetched 0 job ads from schedd
01/07/16 11:01:43 [7782] Updating classad values for 50.0:
01/07/16 11:01:43 [7782]    CurrentStatusUnknown = false
01/07/16 11:01:43 [7782]    GridJobId = undefined
01/07/16 11:01:43 [7782]    LastRemoteStatusUpdate = 0
01/07/16 11:01:43 [7782] leaving doContactSchedd()
01/07/16 11:01:43 [7782] (50.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/07/16 11:01:43 [7782] (50.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 11:01:43 [7782] (50.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 11:01:48 [7782] in doContactSchedd()
01/07/16 11:01:48 [7782] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 601_d75a_4
01/07/16 11:01:48 [7782] querying for removed/held jobs
01/07/16 11:01:48 [7782] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:01:48 [7782] Fetched 0 job ads from schedd
01/07/16 11:01:48 [7782] Updating classad values for 50.0:
01/07/16 11:01:48 [7782]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#50.0#1452182472"
01/07/16 11:01:48 [7782]    LastRemoteStatusUpdate = 1452182503
01/07/16 11:01:49 [7782] leaving doContactSchedd()
01/07/16 11:01:49 [7782] (50.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 11:01:49 [7782] (50.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 11:01:49 [7782] (50.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/07/16 11:01:49 [7782] (50.0) gm state change: GM_HOLD -> GM_DELETE
01/07/16 11:01:54 [7782] in doContactSchedd()
01/07/16 11:01:54 [7782] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 601_d75a_4
01/07/16 11:01:54 [7782] querying for removed/held jobs
01/07/16 11:01:54 [7782] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:01:54 [7782] Fetched 0 job ads from schedd
01/07/16 11:01:54 [7782] Updating classad values for 50.0:
01/07/16 11:01:54 [7782]    EnteredCurrentStatus = 1452182509
01/07/16 11:01:54 [7782]    HoldReason = "Attempts to submit failed: "
01/07/16 11:01:54 [7782]    HoldReasonCode = 0
01/07/16 11:01:54 [7782]    HoldReasonSubCode = 0
01/07/16 11:01:54 [7782]    JobStatus = 5
01/07/16 11:01:54 [7782]    LastReleaseReason = "Data files spooled"
01/07/16 11:01:54 [7782]    Managed = "Schedd"
01/07/16 11:01:54 [7782]    NumSystemHolds = 1
01/07/16 11:01:54 [7782]    ReleaseReason = undefined
01/07/16 11:01:54 [7782] No jobs left, shutting down
01/07/16 11:01:54 [7782] leaving doContactSchedd()
01/07/16 11:01:54 [7782] Got SIGTERM. Performing graceful shutdown.
01/07/16 11:01:54 [7782] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 11:01:54 [7782] **** condor_gridmanager (condor_GRIDMANAGER) pid 7782 EXITING WITH STATUS 0
01/07/16 11:04:21 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 11:04:21 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 11:04:21 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 11:04:21 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 11:04:21 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 11:04:21 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 11:04:21 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 11:04:21 ******************************************************
01/07/16 11:04:21 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 11:04:21 ** /usr/sbin/condor_gridmanager
01/07/16 11:04:21 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 11:04:21 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 11:04:21 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 11:04:21 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 11:04:21 ** PID = 17405
01/07/16 11:04:21 ** Log last touched 1/7 11:01:54
01/07/16 11:04:21 ******************************************************
01/07/16 11:04:21 Using config source: /etc/condor-ce/condor_config
01/07/16 11:04:21 Using local config sources: 
01/07/16 11:04:21    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 11:04:21    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 11:04:21    /etc/condor-ce/config.d/99-local.conf
01/07/16 11:04:21    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 11:04:21 config Macros = 142, Sorted = 142, StringBytes = 12581, TablesBytes = 5320
01/07/16 11:04:21 CLASSAD_CACHING is ENABLED
01/07/16 11:04:21 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 11:04:21 SharedPortEndpoint: waiting for connections to named socket 15406_abac_1
01/07/16 11:04:21 DaemonCore: command socket at <140.247.179.131:9620?sock=15406_abac_1>
01/07/16 11:04:21 DaemonCore: private command socket at <140.247.179.131:9620?sock=15406_abac_1>
01/07/16 11:04:21 Setting maximum accepts per cycle 8.
01/07/16 11:04:21 Setting maximum reaps per cycle 8.
01/07/16 11:04:21 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:04:21 [17405] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 11:04:21 [17405] DaemonCore: No more children processes to reap.
01/07/16 11:04:21 [17405] DaemonCore: in SendAliveToParent()
01/07/16 11:04:21 [17405] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:04:22 [17405] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 11:04:22 [17405] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 11:04:22 [17405] IPVERIFY: ip found is 0
01/07/16 11:04:22 [17405] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 11:04:22 [17405] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 11:04:22 [17405] Buf::write(): condor_write() failed
01/07/16 11:04:22 [17405] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 11:04:22 [17405] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:04:22 [17405] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 11:04:22 [17405] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 11:04:22 [17405] Checking proxies
01/07/16 11:04:24 [17405] Received ADD_JOBS signal
01/07/16 11:04:24 [17405] in doContactSchedd()
01/07/16 11:04:24 [17405] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:04:24 [17405] querying for new jobs
01/07/16 11:04:24 [17405] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 11:04:24 [17405] Using job type INFNBatch for job 52.0
01/07/16 11:04:24 [17405] (52.0) SetJobLeaseTimers()
01/07/16 11:04:24 [17405] Found job 52.0 --- inserting
01/07/16 11:04:24 [17405] Fetched 1 new job ads from schedd
01/07/16 11:04:24 [17405] querying for removed/held jobs
01/07/16 11:04:24 [17405] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:04:24 [17405] Fetched 0 job ads from schedd
01/07/16 11:04:25 [17405] leaving doContactSchedd()
01/07/16 11:04:25 [17405] gahp server not up yet, delaying ping
01/07/16 11:04:25 [17405] *** UpdateLeases called
01/07/16 11:04:25 [17405]     Leases not supported, cancelling timer
01/07/16 11:04:25 [17405] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=15406_abac_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=15325_1517_4>"
CurrentTime = time()
MyCurrentTime = 1452182665
IdleJobs = 1
JobLimit = 10000

01/07/16 11:04:25 [17405] Trying to update collector <10.31.131.202:9619>
01/07/16 11:04:25 [17405] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:04:25 [17405] File descriptor limits: max 4096, safe 3277
01/07/16 11:04:25 [17405] (52.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/07/16 11:04:25 [17405] GAHP server pid = 17471
01/07/16 11:04:25 [17405] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 11:04:25 [17405] GAHP[17471] <- 'COMMANDS'
01/07/16 11:04:25 [17405] GAHP[17471] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 11:04:25 [17405] GAHP[17471] <- 'ASYNC_MODE_ON'
01/07/16 11:04:25 [17405] GAHP[17471] -> 'S' 'Async mode on'
01/07/16 11:04:25 [17405] (52.0) gm state change: GM_INIT -> GM_START
01/07/16 11:04:25 [17405] (52.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/07/16 11:04:25 [17405] (52.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 11:04:25 [17405] (52.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 11:04:25 [17405] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 11:04:25 [17405] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 11:04:25 [17405] IPVERIFY: ip found is 1
01/07/16 11:04:26 [17405] Evaluating staleness of remote job statuses.
01/07/16 11:04:30 [17405] resource  is now up
01/07/16 11:04:30 [17405] in doContactSchedd()
01/07/16 11:04:30 [17405] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:04:30 [17405] querying for removed/held jobs
01/07/16 11:04:30 [17405] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:04:30 [17405] Fetched 0 job ads from schedd
01/07/16 11:04:30 [17405] Updating classad values for 52.0:
01/07/16 11:04:30 [17405]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#52.0#1452182637"
01/07/16 11:04:30 [17405]    LastRemoteStatusUpdate = 1452182665
01/07/16 11:04:30 [17405] leaving doContactSchedd()
01/07/16 11:04:30 [17405] (52.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 11:04:30 [17405] (52.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 11:04:30 [17405] (52.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 11:04:30 [17405] GAHP[17471] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#52.0#1452182637";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 11:04:30 [17405] GAHP[17471] -> 'S'
01/07/16 11:04:35 [17405] GAHP[17471] <- 'RESULTS'
01/07/16 11:04:35 [17405] GAHP[17471] -> 'R'
01/07/16 11:04:35 [17405] GAHP[17471] -> 'S' '1'
01/07/16 11:04:35 [17405] GAHP[17471] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)' 'N/A'
01/07/16 11:04:35 [17405] (52.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/07/16 11:04:35 [17405] (52.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)
01/07/16 11:04:35 [17405] (52.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 11:04:35 [17405] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 11:04:35 [17405] (52.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/07/16 11:04:35 [17405] in doContactSchedd()
01/07/16 11:04:35 [17405] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:04:35 [17405] querying for removed/held jobs
01/07/16 11:04:35 [17405] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:04:35 [17405] Fetched 0 job ads from schedd
01/07/16 11:04:35 [17405] Updating classad values for 52.0:
01/07/16 11:04:35 [17405]    CurrentStatusUnknown = false
01/07/16 11:04:35 [17405]    GridJobId = undefined
01/07/16 11:04:35 [17405]    LastRemoteStatusUpdate = 0
01/07/16 11:04:35 [17405] leaving doContactSchedd()
01/07/16 11:04:35 [17405] (52.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/07/16 11:04:35 [17405] (52.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 11:04:35 [17405] (52.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 11:04:40 [17405] in doContactSchedd()
01/07/16 11:04:40 [17405] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:04:40 [17405] querying for removed/held jobs
01/07/16 11:04:40 [17405] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:04:40 [17405] Fetched 0 job ads from schedd
01/07/16 11:04:40 [17405] Updating classad values for 52.0:
01/07/16 11:04:40 [17405]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#52.0#1452182637"
01/07/16 11:04:40 [17405]    LastRemoteStatusUpdate = 1452182675
01/07/16 11:04:41 [17405] leaving doContactSchedd()
01/07/16 11:04:41 [17405] (52.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 11:04:41 [17405] (52.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 11:04:41 [17405] (52.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/07/16 11:04:41 [17405] (52.0) gm state change: GM_HOLD -> GM_DELETE
01/07/16 11:04:46 [17405] in doContactSchedd()
01/07/16 11:04:46 [17405] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:04:46 [17405] querying for removed/held jobs
01/07/16 11:04:46 [17405] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:04:46 [17405] Fetched 0 job ads from schedd
01/07/16 11:04:46 [17405] Updating classad values for 52.0:
01/07/16 11:04:46 [17405]    EnteredCurrentStatus = 1452182681
01/07/16 11:04:46 [17405]    HoldReason = "Attempts to submit failed: "
01/07/16 11:04:46 [17405]    HoldReasonCode = 0
01/07/16 11:04:46 [17405]    HoldReasonSubCode = 0
01/07/16 11:04:46 [17405]    JobStatus = 5
01/07/16 11:04:46 [17405]    LastReleaseReason = "Data files spooled"
01/07/16 11:04:46 [17405]    Managed = "Schedd"
01/07/16 11:04:46 [17405]    NumSystemHolds = 1
01/07/16 11:04:46 [17405]    ReleaseReason = undefined
01/07/16 11:04:46 [17405] No jobs left, shutting down
01/07/16 11:04:46 [17405] leaving doContactSchedd()
01/07/16 11:04:46 [17405] Got SIGTERM. Performing graceful shutdown.
01/07/16 11:04:46 [17405] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 11:04:46 [17405] **** condor_gridmanager (condor_GRIDMANAGER) pid 17405 EXITING WITH STATUS 0
01/07/16 11:08:37 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 11:08:37 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 11:08:37 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 11:08:37 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 11:08:37 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 11:08:37 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 11:08:37 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 11:08:37 ******************************************************
01/07/16 11:08:37 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 11:08:37 ** /usr/sbin/condor_gridmanager
01/07/16 11:08:37 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 11:08:37 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 11:08:37 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 11:08:37 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 11:08:37 ** PID = 29653
01/07/16 11:08:37 ** Log last touched 1/7 11:04:46
01/07/16 11:08:37 ******************************************************
01/07/16 11:08:37 Using config source: /etc/condor-ce/condor_config
01/07/16 11:08:37 Using local config sources: 
01/07/16 11:08:37    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 11:08:37    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 11:08:37    /etc/condor-ce/config.d/99-local.conf
01/07/16 11:08:37    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 11:08:37 config Macros = 142, Sorted = 142, StringBytes = 12581, TablesBytes = 5320
01/07/16 11:08:37 CLASSAD_CACHING is ENABLED
01/07/16 11:08:37 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 11:08:37 SharedPortEndpoint: waiting for connections to named socket 15406_abac_2
01/07/16 11:08:37 DaemonCore: command socket at <140.247.179.131:9620?sock=15406_abac_2>
01/07/16 11:08:37 DaemonCore: private command socket at <140.247.179.131:9620?sock=15406_abac_2>
01/07/16 11:08:37 Setting maximum accepts per cycle 8.
01/07/16 11:08:37 Setting maximum reaps per cycle 8.
01/07/16 11:08:37 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:08:37 [29653] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 11:08:37 [29653] DaemonCore: No more children processes to reap.
01/07/16 11:08:37 [29653] DaemonCore: in SendAliveToParent()
01/07/16 11:08:37 [29653] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:08:43 [29653] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 11:08:43 [29653] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 11:08:43 [29653] IPVERIFY: ip found is 0
01/07/16 11:08:43 [29653] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 11:08:43 [29653] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 11:08:43 [29653] Buf::write(): condor_write() failed
01/07/16 11:08:43 [29653] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 11:08:43 [29653] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:08:43 [29653] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 11:08:43 [29653] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 11:08:43 [29653] Checking proxies
01/07/16 11:08:43 [29653] Received ADD_JOBS signal
01/07/16 11:08:43 [29653] Received REMOVE_JOBS signal
01/07/16 11:08:43 [29653] Evaluating staleness of remote job statuses.
01/07/16 11:08:43 [29653] in doContactSchedd()
01/07/16 11:08:43 [29653] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:08:43 [29653] querying for new jobs
01/07/16 11:08:43 [29653] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 11:08:43 [29653] Using job type INFNBatch for job 44.0
01/07/16 11:08:43 [29653] (44.0) SetJobLeaseTimers()
01/07/16 11:08:43 [29653] Found job 44.0 --- inserting
01/07/16 11:08:43 [29653] Using job type INFNBatch for job 42.0
01/07/16 11:08:43 [29653] (42.0) SetJobLeaseTimers()
01/07/16 11:08:43 [29653] Failed to get expiration time of proxy /n/atlasgrid/condor/41/0/cluster41.proc0.subproc0/x509up_u556792
01/07/16 11:08:43 [29653] Found job 42.0 --- inserting
01/07/16 11:08:43 [29653] Using job type INFNBatch for job 40.0
01/07/16 11:08:43 [29653] (40.0) SetJobLeaseTimers()
01/07/16 11:08:43 [29653] Found job 40.0 --- inserting
01/07/16 11:08:43 [29653] Fetched 3 new job ads from schedd
01/07/16 11:08:43 [29653] querying for removed/held jobs
01/07/16 11:08:43 [29653] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:08:43 [29653] Fetched 3 job ads from schedd
01/07/16 11:08:43 [29653] leaving doContactSchedd()
01/07/16 11:08:43 [29653] gahp server not up yet, delaying ping
01/07/16 11:08:43 [29653] *** UpdateLeases called
01/07/16 11:08:43 [29653]     Leases not supported, cancelling timer
01/07/16 11:08:43 [29653] BaseResource::UpdateResource: 
NumJobs = 3
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=15406_abac_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=15325_1517_4>"
CurrentTime = time()
MyCurrentTime = 1452182923
IdleJobs = 0
JobLimit = 10000

01/07/16 11:08:43 [29653] Trying to update collector <10.31.131.202:9619>
01/07/16 11:08:43 [29653] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:08:43 [29653] File descriptor limits: max 4096, safe 3277
01/07/16 11:08:43 [29653] (44.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/07/16 11:08:43 [29653] GAHP server pid = 30114
01/07/16 11:08:43 [29653] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 11:08:43 [29653] GAHP[30114] <- 'COMMANDS'
01/07/16 11:08:43 [29653] GAHP[30114] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 11:08:43 [29653] GAHP[30114] <- 'ASYNC_MODE_ON'
01/07/16 11:08:43 [29653] GAHP[30114] -> 'S' 'Async mode on'
01/07/16 11:08:43 [29653] (44.0) gm state change: GM_INIT -> GM_START
01/07/16 11:08:43 [29653] (44.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/07/16 11:08:43 [29653] (44.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 11:08:43 [29653] GAHP[30114] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#44.0#1452094345";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 11:08:43 [29653] GAHP[30114] -> 'S'
01/07/16 11:08:43 [29653] (42.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/07/16 11:08:43 [29653] (42.0) gm state change: GM_INIT -> GM_START
01/07/16 11:08:43 [29653] (42.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/07/16 11:08:43 [29653] (42.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 11:08:43 [29653] GAHP[30114] <- 'BLAH_JOB_SUBMIT 3 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#42.0#1452091211";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 11:08:43 [29653] GAHP[30114] -> 'S'
01/07/16 11:08:43 [29653] (40.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/07/16 11:08:43 [29653] (40.0) gm state change: GM_INIT -> GM_START
01/07/16 11:08:43 [29653] (40.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/07/16 11:08:43 [29653] (40.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 11:08:43 [29653] GAHP[30114] <- 'BLAH_JOB_SUBMIT 4 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/39/0/cluster39.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/39/0/cluster39.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/39/0/cluster39.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#40.0#1452090534";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 11:08:43 [29653] GAHP[30114] -> 'R'
01/07/16 11:08:43 [29653] GAHP[30114] -> 'S'
01/07/16 11:08:43 [29653] GAHP[30114] <- 'RESULTS'
01/07/16 11:08:43 [29653] GAHP[30114] -> 'S' '1'
01/07/16 11:08:43 [29653] GAHP[30114] -> '3' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
01/07/16 11:08:43 [29653] (42.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/07/16 11:08:43 [29653] (42.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/41/0/cluster41.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
01/07/16 11:08:43 [29653] (42.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 11:08:43 [29653] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 11:08:44 [29653] (42.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/07/16 11:08:44 [29653] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 11:08:44 [29653] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 11:08:44 [29653] IPVERIFY: ip found is 1
01/07/16 11:08:44 [29653] GAHP[30114] <- 'RESULTS'
01/07/16 11:08:44 [29653] GAHP[30114] -> 'R'
01/07/16 11:08:44 [29653] GAHP[30114] -> 'S' '1'
01/07/16 11:08:44 [29653] GAHP[30114] -> '2' '1' 'Unable to limit the proxy (Unable to read proxy file (/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0/x509up_u556792): errno=13, Permission denied)' 'N/A'
01/07/16 11:08:44 [29653] (44.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/07/16 11:08:44 [29653] (44.0) blah_job_submit() failed: Unable to limit the proxy (Unable to read proxy file (/n/atlasgrid/condor/43/0/cluster43.proc0.subproc0/x509up_u556792): errno=13, Permission denied)
01/07/16 11:08:44 [29653] (44.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 11:08:44 [29653] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 11:08:44 [29653] (44.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/07/16 11:08:46 [29653] GAHP[30114] <- 'RESULTS'
01/07/16 11:08:46 [29653] GAHP[30114] -> 'R'
01/07/16 11:08:46 [29653] GAHP[30114] -> 'S' '1'
01/07/16 11:08:46 [29653] GAHP[30114] -> '4' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/07/16 11:08:46 [29653] (40.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/07/16 11:08:46 [29653] (40.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/07/16 11:08:46 [29653] (40.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 11:08:46 [29653] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 11:08:46 [29653] (40.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/07/16 11:08:48 [29653] resource  is now up
01/07/16 11:08:48 [29653] in doContactSchedd()
01/07/16 11:08:48 [29653] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:08:49 [29653] querying for removed/held jobs
01/07/16 11:08:49 [29653] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:08:49 [29653] Fetched 3 job ads from schedd
01/07/16 11:08:49 [29653] Updating classad values for 42.0:
01/07/16 11:08:49 [29653]    CurrentStatusUnknown = false
01/07/16 11:08:49 [29653]    GridJobId = undefined
01/07/16 11:08:49 [29653]    LastRemoteStatusUpdate = 0
01/07/16 11:08:49 [29653]    Managed = "ScheddDone"
01/07/16 11:08:49 [29653] Updating classad values for 44.0:
01/07/16 11:08:49 [29653]    CurrentStatusUnknown = false
01/07/16 11:08:49 [29653]    GridJobId = undefined
01/07/16 11:08:49 [29653]    LastRemoteStatusUpdate = 0
01/07/16 11:08:49 [29653]    Managed = "ScheddDone"
01/07/16 11:08:49 [29653] Updating classad values for 40.0:
01/07/16 11:08:49 [29653]    CurrentStatusUnknown = false
01/07/16 11:08:49 [29653]    GridJobId = undefined
01/07/16 11:08:49 [29653]    LastRemoteStatusUpdate = 0
01/07/16 11:08:49 [29653]    Managed = "ScheddDone"
01/07/16 11:08:50 [29653] Deleting job 42.0 from schedd
01/07/16 11:08:50 [29653] Deleting job 44.0 from schedd
01/07/16 11:08:50 [29653] Deleting job 40.0 from schedd
01/07/16 11:08:50 [29653] No jobs left, shutting down
01/07/16 11:08:50 [29653] leaving doContactSchedd()
01/07/16 11:08:50 [29653] Got SIGTERM. Performing graceful shutdown.
01/07/16 11:08:50 [29653] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 11:08:50 [29653] **** condor_gridmanager (condor_GRIDMANAGER) pid 29653 EXITING WITH STATUS 0
01/07/16 11:43:54 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 11:43:54 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 11:43:54 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 11:43:54 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 11:43:54 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 11:43:54 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 11:43:54 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 11:43:54 ******************************************************
01/07/16 11:43:54 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 11:43:54 ** /usr/sbin/condor_gridmanager
01/07/16 11:43:54 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 11:43:54 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 11:43:54 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 11:43:54 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 11:43:54 ** PID = 19124
01/07/16 11:43:54 ** Log last touched 1/7 11:08:50
01/07/16 11:43:54 ******************************************************
01/07/16 11:43:54 Using config source: /etc/condor-ce/condor_config
01/07/16 11:43:54 Using local config sources: 
01/07/16 11:43:54    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 11:43:54    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 11:43:54    /etc/condor-ce/config.d/99-local.conf
01/07/16 11:43:54    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 11:43:54 config Macros = 142, Sorted = 142, StringBytes = 12581, TablesBytes = 5320
01/07/16 11:43:54 CLASSAD_CACHING is ENABLED
01/07/16 11:43:54 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 11:43:54 SharedPortEndpoint: waiting for connections to named socket 15406_abac_3
01/07/16 11:43:54 DaemonCore: command socket at <140.247.179.131:9620?sock=15406_abac_3>
01/07/16 11:43:54 DaemonCore: private command socket at <140.247.179.131:9620?sock=15406_abac_3>
01/07/16 11:43:54 Setting maximum accepts per cycle 8.
01/07/16 11:43:54 Setting maximum reaps per cycle 8.
01/07/16 11:43:54 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:43:54 [19124] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 11:43:54 [19124] DaemonCore: No more children processes to reap.
01/07/16 11:43:54 [19124] DaemonCore: in SendAliveToParent()
01/07/16 11:43:54 [19124] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:43:54 [19124] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 11:43:54 [19124] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 11:43:54 [19124] IPVERIFY: ip found is 0
01/07/16 11:43:54 [19124] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 11:43:54 [19124] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 11:43:54 [19124] Buf::write(): condor_write() failed
01/07/16 11:43:54 [19124] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 11:43:54 [19124] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:43:54 [19124] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 11:43:54 [19124] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 11:43:54 [19124] Checking proxies
01/07/16 11:43:56 [19124] Received REMOVE_JOBS signal
01/07/16 11:43:56 [19124] in doContactSchedd()
01/07/16 11:43:56 [19124] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:43:56 [19124] querying for new jobs
01/07/16 11:43:56 [19124] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 11:43:56 [19124] Using job type INFNBatch for job 46.0
01/07/16 11:43:56 [19124] (46.0) SetJobLeaseTimers()
01/07/16 11:43:56 [19124] Failed to get expiration time of proxy /n/atlasgrid/condor/45/0/cluster45.proc0.subproc0/x509up_u556792
01/07/16 11:43:56 [19124] Found job 46.0 --- inserting
01/07/16 11:43:56 [19124] Fetched 1 new job ads from schedd
01/07/16 11:43:56 [19124] querying for removed/held jobs
01/07/16 11:43:56 [19124] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:43:56 [19124] Fetched 1 job ads from schedd
01/07/16 11:43:56 [19124] leaving doContactSchedd()
01/07/16 11:43:56 [19124] gahp server not up yet, delaying ping
01/07/16 11:43:56 [19124] *** UpdateLeases called
01/07/16 11:43:56 [19124]     Leases not supported, cancelling timer
01/07/16 11:43:56 [19124] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=15406_abac_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=15325_1517_4>"
CurrentTime = time()
MyCurrentTime = 1452185036
IdleJobs = 0
JobLimit = 10000

01/07/16 11:43:56 [19124] Trying to update collector <10.31.131.202:9619>
01/07/16 11:43:56 [19124] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 11:43:56 [19124] File descriptor limits: max 4096, safe 3277
01/07/16 11:43:56 [19124] (46.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/07/16 11:43:56 [19124] GAHP server pid = 19712
01/07/16 11:43:56 [19124] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 11:43:56 [19124] GAHP[19712] <- 'COMMANDS'
01/07/16 11:43:56 [19124] GAHP[19712] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 11:43:56 [19124] GAHP[19712] <- 'ASYNC_MODE_ON'
01/07/16 11:43:56 [19124] GAHP[19712] -> 'S' 'Async mode on'
01/07/16 11:43:56 [19124] (46.0) gm state change: GM_INIT -> GM_START
01/07/16 11:43:56 [19124] (46.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/07/16 11:43:56 [19124] (46.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 11:43:56 [19124] GAHP[19712] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#46.0#1452098269";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 11:43:56 [19124] GAHP[19712] -> 'S'
01/07/16 11:43:56 [19124] GAHP[19712] <- 'RESULTS'
01/07/16 11:43:56 [19124] GAHP[19712] -> 'R'
01/07/16 11:43:56 [19124] GAHP[19712] -> 'S' '1'
01/07/16 11:43:56 [19124] GAHP[19712] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
01/07/16 11:43:56 [19124] (46.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/07/16 11:43:56 [19124] (46.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/45/0/cluster45.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
01/07/16 11:43:56 [19124] (46.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 11:43:56 [19124] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 11:44:02 [19124] (46.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/07/16 11:44:02 [19124] Received ADD_JOBS signal
01/07/16 11:44:02 [19124] Evaluating staleness of remote job statuses.
01/07/16 11:44:02 [19124] resource  is now up
01/07/16 11:44:02 [19124] in doContactSchedd()
01/07/16 11:44:02 [19124] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15325_1517_4
01/07/16 11:44:02 [19124] querying for new jobs
01/07/16 11:44:02 [19124] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
01/07/16 11:44:02 [19124] Fetched 0 new job ads from schedd
01/07/16 11:44:02 [19124] querying for removed/held jobs
01/07/16 11:44:02 [19124] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 11:44:02 [19124] Fetched 1 job ads from schedd
01/07/16 11:44:02 [19124] Updating classad values for 46.0:
01/07/16 11:44:02 [19124]    CurrentStatusUnknown = false
01/07/16 11:44:02 [19124]    GridJobId = undefined
01/07/16 11:44:02 [19124]    LastRemoteStatusUpdate = 0
01/07/16 11:44:02 [19124]    Managed = "ScheddDone"
01/07/16 11:44:02 [19124] Deleting job 46.0 from schedd
01/07/16 11:44:02 [19124] No jobs left, shutting down
01/07/16 11:44:02 [19124] leaving doContactSchedd()
01/07/16 11:44:02 [19124] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 11:44:02 [19124] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 11:44:02 [19124] IPVERIFY: ip found is 1
01/07/16 11:44:02 [19124] Got SIGTERM. Performing graceful shutdown.
01/07/16 11:44:02 [19124] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 11:44:02 [19124] **** condor_gridmanager (condor_GRIDMANAGER) pid 19124 EXITING WITH STATUS 0
01/07/16 12:24:12 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 12:24:12 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 12:24:12 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 12:24:12 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 12:24:12 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 12:24:12 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 12:24:12 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 12:24:12 ******************************************************
01/07/16 12:24:12 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 12:24:12 ** /usr/sbin/condor_gridmanager
01/07/16 12:24:12 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 12:24:12 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 12:24:12 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 12:24:12 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 12:24:12 ** PID = 19357
01/07/16 12:24:12 ** Log last touched 1/7 11:44:02
01/07/16 12:24:12 ******************************************************
01/07/16 12:24:12 Using config source: /etc/condor-ce/condor_config
01/07/16 12:24:12 Using local config sources: 
01/07/16 12:24:12    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 12:24:12    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 12:24:12    /etc/condor-ce/config.d/99-local.conf
01/07/16 12:24:12    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 12:24:12 config Macros = 142, Sorted = 142, StringBytes = 12577, TablesBytes = 5320
01/07/16 12:24:12 CLASSAD_CACHING is ENABLED
01/07/16 12:24:12 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 12:24:12 SharedPortEndpoint: waiting for connections to named socket 7086_7a59_1
01/07/16 12:24:12 DaemonCore: command socket at <140.247.179.131:9620?sock=7086_7a59_1>
01/07/16 12:24:12 DaemonCore: private command socket at <140.247.179.131:9620?sock=7086_7a59_1>
01/07/16 12:24:12 Setting maximum accepts per cycle 8.
01/07/16 12:24:12 Setting maximum reaps per cycle 8.
01/07/16 12:24:12 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 12:24:12 [19357] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 12:24:12 [19357] DaemonCore: No more children processes to reap.
01/07/16 12:24:12 [19357] DaemonCore: in SendAliveToParent()
01/07/16 12:24:12 [19357] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:24:12 [19357] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 12:24:12 [19357] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 12:24:12 [19357] IPVERIFY: ip found is 0
01/07/16 12:24:12 [19357] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 12:24:12 [19357] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 12:24:12 [19357] Buf::write(): condor_write() failed
01/07/16 12:24:12 [19357] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 12:24:12 [19357] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:24:12 [19357] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 12:24:12 [19357] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 12:24:12 [19357] Checking proxies
01/07/16 12:24:15 [19357] Received ADD_JOBS signal
01/07/16 12:24:15 [19357] in doContactSchedd()
01/07/16 12:24:15 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:24:15 [19357] querying for new jobs
01/07/16 12:24:15 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 12:24:15 [19357] Using job type INFNBatch for job 54.0
01/07/16 12:24:15 [19357] (54.0) SetJobLeaseTimers()
01/07/16 12:24:15 [19357] Found job 54.0 --- inserting
01/07/16 12:24:15 [19357] Fetched 1 new job ads from schedd
01/07/16 12:24:15 [19357] querying for removed/held jobs
01/07/16 12:24:15 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:24:15 [19357] Fetched 0 job ads from schedd
01/07/16 12:24:15 [19357] leaving doContactSchedd()
01/07/16 12:24:15 [19357] gahp server not up yet, delaying ping
01/07/16 12:24:15 [19357] *** UpdateLeases called
01/07/16 12:24:15 [19357]     Leases not supported, cancelling timer
01/07/16 12:24:15 [19357] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7086_7a59_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6961_f065_4>"
CurrentTime = time()
MyCurrentTime = 1452187455
IdleJobs = 1
JobLimit = 10000

01/07/16 12:24:15 [19357] Trying to update collector <10.31.131.202:9619>
01/07/16 12:24:15 [19357] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 12:24:15 [19357] File descriptor limits: max 4096, safe 3277
01/07/16 12:24:15 [19357] (54.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/07/16 12:24:15 [19357] GAHP server pid = 19430
01/07/16 12:24:15 [19357] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 12:24:15 [19357] GAHP[19430] <- 'COMMANDS'
01/07/16 12:24:15 [19357] GAHP[19430] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 12:24:15 [19357] GAHP[19430] <- 'ASYNC_MODE_ON'
01/07/16 12:24:15 [19357] GAHP[19430] -> 'S' 'Async mode on'
01/07/16 12:24:15 [19357] (54.0) gm state change: GM_INIT -> GM_START
01/07/16 12:24:15 [19357] (54.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/07/16 12:24:15 [19357] (54.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 12:24:15 [19357] (54.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 12:24:15 [19357] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 12:24:15 [19357] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 12:24:15 [19357] IPVERIFY: ip found is 1
01/07/16 12:24:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:24:20 [19357] resource  is now up
01/07/16 12:24:20 [19357] in doContactSchedd()
01/07/16 12:24:20 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:24:20 [19357] querying for removed/held jobs
01/07/16 12:24:20 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:24:20 [19357] Fetched 0 job ads from schedd
01/07/16 12:24:20 [19357] Updating classad values for 54.0:
01/07/16 12:24:20 [19357]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#54.0#1452187157"
01/07/16 12:24:20 [19357]    LastRemoteStatusUpdate = 1452187455
01/07/16 12:24:20 [19357] leaving doContactSchedd()
01/07/16 12:24:20 [19357] (54.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 12:24:20 [19357] (54.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 12:24:20 [19357] (54.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 12:24:20 [19357] GAHP[19430] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#54.0#1452187157";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 12:24:20 [19357] GAHP[19430] -> 'S'
01/07/16 12:24:31 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:24:31 [19357] GAHP[19430] -> 'R'
01/07/16 12:24:31 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:24:31 [19357] GAHP[19430] -> '2' '0' 'No error' 'lsf/20160107/542935913'
01/07/16 12:24:31 [19357] (54.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/07/16 12:24:31 [19357] (54.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
01/07/16 12:24:31 [19357] in doContactSchedd()
01/07/16 12:24:31 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:24:31 [19357] querying for removed/held jobs
01/07/16 12:24:31 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:24:31 [19357] Fetched 0 job ads from schedd
01/07/16 12:24:31 [19357] Updating classad values for 54.0:
01/07/16 12:24:31 [19357]    DelegatedProxyExpiration = 1452225638
01/07/16 12:24:31 [19357]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#54.0#1452187157 lsf/20160107/542935913"
01/07/16 12:24:31 [19357] leaving doContactSchedd()
01/07/16 12:24:31 [19357] (54.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
01/07/16 12:24:31 [19357] (54.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
01/07/16 12:25:12 [19357] Received CHECK_LEASES signal
01/07/16 12:25:12 [19357] in doContactSchedd()
01/07/16 12:25:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:25:12 [19357] querying for renewed leases
01/07/16 12:25:12 [19357] querying for removed/held jobs
01/07/16 12:25:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:25:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:25:12 [19357] leaving doContactSchedd()
01/07/16 12:25:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:25:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:25:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:25:31 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
01/07/16 12:25:31 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:25:31 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 3 lsf/20160107/542935913'
01/07/16 12:25:31 [19357] GAHP[19430] -> 'S'
01/07/16 12:25:34 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:25:34 [19357] GAHP[19430] -> 'R'
01/07/16 12:25:34 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:25:34 [19357] GAHP[19430] -> '3' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:25:34 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
01/07/16 12:25:34 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:25:34 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:25:34 [19357] in doContactSchedd()
01/07/16 12:25:34 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:25:34 [19357] querying for removed/held jobs
01/07/16 12:25:34 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:25:34 [19357] Fetched 0 job ads from schedd
01/07/16 12:25:34 [19357] Updating classad values for 54.0:
01/07/16 12:25:34 [19357]    GridJobStatus = "IDLE"
01/07/16 12:25:34 [19357]    LastRemoteStatusUpdate = 1452187534
01/07/16 12:25:34 [19357] leaving doContactSchedd()
01/07/16 12:26:12 [19357] Received CHECK_LEASES signal
01/07/16 12:26:12 [19357] in doContactSchedd()
01/07/16 12:26:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:26:12 [19357] querying for renewed leases
01/07/16 12:26:12 [19357] querying for removed/held jobs
01/07/16 12:26:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:26:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:26:12 [19357] leaving doContactSchedd()
01/07/16 12:26:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:26:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:26:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:26:34 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:26:34 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:26:34 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 4 lsf/20160107/542935913'
01/07/16 12:26:34 [19357] GAHP[19430] -> 'S'
01/07/16 12:26:37 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:26:37 [19357] GAHP[19430] -> 'R'
01/07/16 12:26:37 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:26:37 [19357] GAHP[19430] -> '4' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:26:37 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:26:37 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:26:37 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:26:37 [19357] in doContactSchedd()
01/07/16 12:26:37 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:26:37 [19357] querying for removed/held jobs
01/07/16 12:26:37 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:26:37 [19357] Fetched 0 job ads from schedd
01/07/16 12:26:37 [19357] Updating classad values for 54.0:
01/07/16 12:26:37 [19357]    LastRemoteStatusUpdate = 1452187597
01/07/16 12:26:37 [19357] leaving doContactSchedd()
01/07/16 12:27:12 [19357] Received CHECK_LEASES signal
01/07/16 12:27:12 [19357] in doContactSchedd()
01/07/16 12:27:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:27:12 [19357] querying for renewed leases
01/07/16 12:27:12 [19357] querying for removed/held jobs
01/07/16 12:27:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:27:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:27:12 [19357] leaving doContactSchedd()
01/07/16 12:27:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:27:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:27:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:27:37 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:27:37 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:27:37 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 5 lsf/20160107/542935913'
01/07/16 12:27:37 [19357] GAHP[19430] -> 'S'
01/07/16 12:27:40 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:27:40 [19357] GAHP[19430] -> 'R'
01/07/16 12:27:40 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:27:40 [19357] GAHP[19430] -> '5' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:27:40 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:27:40 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:27:40 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:27:40 [19357] in doContactSchedd()
01/07/16 12:27:40 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:27:41 [19357] querying for removed/held jobs
01/07/16 12:27:41 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:27:41 [19357] Fetched 0 job ads from schedd
01/07/16 12:27:41 [19357] Updating classad values for 54.0:
01/07/16 12:27:41 [19357]    LastRemoteStatusUpdate = 1452187660
01/07/16 12:27:41 [19357] leaving doContactSchedd()
01/07/16 12:28:12 [19357] Received CHECK_LEASES signal
01/07/16 12:28:12 [19357] in doContactSchedd()
01/07/16 12:28:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:28:12 [19357] querying for renewed leases
01/07/16 12:28:12 [19357] querying for removed/held jobs
01/07/16 12:28:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:28:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:28:12 [19357] leaving doContactSchedd()
01/07/16 12:28:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:28:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:28:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:28:40 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:28:40 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:28:40 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 6 lsf/20160107/542935913'
01/07/16 12:28:40 [19357] GAHP[19430] -> 'S'
01/07/16 12:28:42 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:28:42 [19357] GAHP[19430] -> 'R'
01/07/16 12:28:42 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:28:42 [19357] GAHP[19430] -> '6' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:28:42 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:28:42 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:28:42 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:28:42 [19357] in doContactSchedd()
01/07/16 12:28:42 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:28:42 [19357] querying for removed/held jobs
01/07/16 12:28:42 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:28:42 [19357] Fetched 0 job ads from schedd
01/07/16 12:28:42 [19357] Updating classad values for 54.0:
01/07/16 12:28:42 [19357]    LastRemoteStatusUpdate = 1452187722
01/07/16 12:28:42 [19357] leaving doContactSchedd()
01/07/16 12:29:12 [19357] Evaluating periodic job policy expressions.
01/07/16 12:29:12 [19357] Received CHECK_LEASES signal
01/07/16 12:29:12 [19357] in doContactSchedd()
01/07/16 12:29:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:29:12 [19357] querying for renewed leases
01/07/16 12:29:12 [19357] querying for removed/held jobs
01/07/16 12:29:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:29:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:29:12 [19357] leaving doContactSchedd()
01/07/16 12:29:15 [19357] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 1
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7086_7a59_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6961_f065_4>"
CurrentTime = time()
MyCurrentTime = 1452187755
IdleJobs = 1
JobLimit = 10000

01/07/16 12:29:15 [19357] Trying to update collector <10.31.131.202:9619>
01/07/16 12:29:15 [19357] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 12:29:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:29:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:29:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:29:42 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:29:42 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:29:42 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 7 lsf/20160107/542935913'
01/07/16 12:29:42 [19357] GAHP[19430] -> 'S'
01/07/16 12:29:45 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:29:45 [19357] GAHP[19430] -> 'R'
01/07/16 12:29:45 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:29:45 [19357] GAHP[19430] -> '7' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:29:45 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:29:45 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:29:45 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:29:45 [19357] in doContactSchedd()
01/07/16 12:29:45 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:29:45 [19357] querying for removed/held jobs
01/07/16 12:29:45 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:29:45 [19357] Fetched 0 job ads from schedd
01/07/16 12:29:45 [19357] Updating classad values for 54.0:
01/07/16 12:29:45 [19357]    LastRemoteStatusUpdate = 1452187785
01/07/16 12:29:45 [19357]    RemoteWallClockTime = 0.0
01/07/16 12:29:45 [19357] leaving doContactSchedd()
01/07/16 12:30:12 [19357] Received CHECK_LEASES signal
01/07/16 12:30:12 [19357] in doContactSchedd()
01/07/16 12:30:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:30:12 [19357] querying for renewed leases
01/07/16 12:30:12 [19357] querying for removed/held jobs
01/07/16 12:30:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:30:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:30:12 [19357] leaving doContactSchedd()
01/07/16 12:30:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:30:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:30:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:30:45 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:30:45 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:30:45 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 8 lsf/20160107/542935913'
01/07/16 12:30:45 [19357] GAHP[19430] -> 'S'
01/07/16 12:30:48 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:30:48 [19357] GAHP[19430] -> 'R'
01/07/16 12:30:48 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:30:48 [19357] GAHP[19430] -> '8' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:30:48 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:30:48 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:30:48 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:30:48 [19357] in doContactSchedd()
01/07/16 12:30:48 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:30:48 [19357] querying for removed/held jobs
01/07/16 12:30:48 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:30:48 [19357] Fetched 0 job ads from schedd
01/07/16 12:30:48 [19357] Updating classad values for 54.0:
01/07/16 12:30:48 [19357]    LastRemoteStatusUpdate = 1452187848
01/07/16 12:30:48 [19357] leaving doContactSchedd()
01/07/16 12:31:12 [19357] Received CHECK_LEASES signal
01/07/16 12:31:12 [19357] in doContactSchedd()
01/07/16 12:31:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:31:12 [19357] querying for renewed leases
01/07/16 12:31:12 [19357] querying for removed/held jobs
01/07/16 12:31:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:31:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:31:12 [19357] leaving doContactSchedd()
01/07/16 12:31:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:31:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:31:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:31:48 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:31:48 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:31:48 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 9 lsf/20160107/542935913'
01/07/16 12:31:48 [19357] GAHP[19430] -> 'S'
01/07/16 12:31:52 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:31:52 [19357] GAHP[19430] -> 'R'
01/07/16 12:31:52 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:31:52 [19357] GAHP[19430] -> '9' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:31:52 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:31:52 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:31:52 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:31:52 [19357] in doContactSchedd()
01/07/16 12:31:52 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:31:52 [19357] querying for removed/held jobs
01/07/16 12:31:52 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:31:52 [19357] Fetched 0 job ads from schedd
01/07/16 12:31:52 [19357] Updating classad values for 54.0:
01/07/16 12:31:52 [19357]    LastRemoteStatusUpdate = 1452187912
01/07/16 12:31:52 [19357] leaving doContactSchedd()
01/07/16 12:32:12 [19357] Received CHECK_LEASES signal
01/07/16 12:32:12 [19357] in doContactSchedd()
01/07/16 12:32:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:32:12 [19357] querying for renewed leases
01/07/16 12:32:12 [19357] querying for removed/held jobs
01/07/16 12:32:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:32:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:32:12 [19357] leaving doContactSchedd()
01/07/16 12:32:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:32:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:32:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:32:52 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:32:52 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:32:52 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 10 lsf/20160107/542935913'
01/07/16 12:32:52 [19357] GAHP[19430] -> 'S'
01/07/16 12:32:54 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:32:54 [19357] GAHP[19430] -> 'R'
01/07/16 12:32:54 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:32:54 [19357] GAHP[19430] -> '10' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:32:54 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:32:54 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:32:54 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:32:54 [19357] in doContactSchedd()
01/07/16 12:32:54 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:32:54 [19357] querying for removed/held jobs
01/07/16 12:32:54 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:32:54 [19357] Fetched 0 job ads from schedd
01/07/16 12:32:54 [19357] Updating classad values for 54.0:
01/07/16 12:32:54 [19357]    LastRemoteStatusUpdate = 1452187974
01/07/16 12:32:54 [19357] leaving doContactSchedd()
01/07/16 12:33:12 [19357] Received CHECK_LEASES signal
01/07/16 12:33:12 [19357] in doContactSchedd()
01/07/16 12:33:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:33:12 [19357] querying for renewed leases
01/07/16 12:33:12 [19357] querying for removed/held jobs
01/07/16 12:33:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:33:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:33:12 [19357] leaving doContactSchedd()
01/07/16 12:33:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:33:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:33:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:33:54 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:33:54 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:33:54 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 11 lsf/20160107/542935913'
01/07/16 12:33:54 [19357] GAHP[19430] -> 'S'
01/07/16 12:33:59 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:33:59 [19357] GAHP[19430] -> 'R'
01/07/16 12:33:59 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:33:59 [19357] GAHP[19430] -> '11' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:33:59 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:33:59 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:33:59 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:33:59 [19357] in doContactSchedd()
01/07/16 12:33:59 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:33:59 [19357] querying for removed/held jobs
01/07/16 12:33:59 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:33:59 [19357] Fetched 0 job ads from schedd
01/07/16 12:33:59 [19357] Updating classad values for 54.0:
01/07/16 12:33:59 [19357]    LastRemoteStatusUpdate = 1452188039
01/07/16 12:34:00 [19357] leaving doContactSchedd()
01/07/16 12:34:12 [19357] Checking proxies
01/07/16 12:34:12 [19357] Evaluating periodic job policy expressions.
01/07/16 12:34:12 [19357] Received CHECK_LEASES signal
01/07/16 12:34:12 [19357] in doContactSchedd()
01/07/16 12:34:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:34:12 [19357] querying for renewed leases
01/07/16 12:34:12 [19357] querying for removed/held jobs
01/07/16 12:34:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:34:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:34:12 [19357] leaving doContactSchedd()
01/07/16 12:34:15 [19357] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 1
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7086_7a59_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6961_f065_4>"
CurrentTime = time()
MyCurrentTime = 1452188055
IdleJobs = 1
JobLimit = 10000

01/07/16 12:34:15 [19357] Trying to update collector <10.31.131.202:9619>
01/07/16 12:34:15 [19357] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 12:34:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:34:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:34:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:34:59 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:34:59 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:34:59 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 12 lsf/20160107/542935913'
01/07/16 12:34:59 [19357] GAHP[19430] -> 'S'
01/07/16 12:35:02 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:35:02 [19357] GAHP[19430] -> 'R'
01/07/16 12:35:02 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:35:02 [19357] GAHP[19430] -> '12' '0' 'No Error' '1' '[ BatchjobId = "542935913"; JobStatus = 1 ]'
01/07/16 12:35:02 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:35:02 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:35:02 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:35:02 [19357] in doContactSchedd()
01/07/16 12:35:02 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:35:02 [19357] querying for removed/held jobs
01/07/16 12:35:02 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:35:02 [19357] Fetched 0 job ads from schedd
01/07/16 12:35:02 [19357] Updating classad values for 54.0:
01/07/16 12:35:02 [19357]    LastRemoteStatusUpdate = 1452188102
01/07/16 12:35:02 [19357] leaving doContactSchedd()
01/07/16 12:35:12 [19357] Received CHECK_LEASES signal
01/07/16 12:35:12 [19357] in doContactSchedd()
01/07/16 12:35:12 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:35:12 [19357] querying for renewed leases
01/07/16 12:35:12 [19357] querying for removed/held jobs
01/07/16 12:35:12 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:35:12 [19357] Fetched 0 job ads from schedd
01/07/16 12:35:12 [19357] leaving doContactSchedd()
01/07/16 12:35:15 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:35:15 [19357] GAHP[19430] -> 'S' '0'
01/07/16 12:35:17 [19357] Evaluating staleness of remote job statuses.
01/07/16 12:36:02 [19357] (54.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/07/16 12:36:02 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:36:02 [19357] GAHP[19430] <- 'BLAH_JOB_STATUS 13 lsf/20160107/542935913'
01/07/16 12:36:02 [19357] GAHP[19430] -> 'S'
01/07/16 12:36:06 [19357] GAHP[19430] <- 'RESULTS'
01/07/16 12:36:06 [19357] GAHP[19430] -> 'R'
01/07/16 12:36:06 [19357] GAHP[19430] -> 'S' '1'
01/07/16 12:36:06 [19357] GAHP[19430] -> '13' '0' 'No Error' '3' '[ BatchjobId = "542935913"; JobStatus = 3 ]'
01/07/16 12:36:06 [19357] (54.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/07/16 12:36:06 [19357] (54.0) ***ProcessRemoteAd
01/07/16 12:36:06 [19357] (54.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:36:06 [19357] (54.0) gm state change: GM_SUBMITTED -> GM_HOLD
01/07/16 12:36:06 [19357] (54.0) gm state change: GM_HOLD -> GM_DELETE
01/07/16 12:36:06 [19357] in doContactSchedd()
01/07/16 12:36:06 [19357] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:36:06 [19357] querying for removed/held jobs
01/07/16 12:36:06 [19357] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:36:06 [19357] Fetched 0 job ads from schedd
01/07/16 12:36:06 [19357] Updating classad values for 54.0:
01/07/16 12:36:06 [19357]    EnteredCurrentStatus = 1452188166
01/07/16 12:36:06 [19357]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#54.0#1452187157"
01/07/16 12:36:06 [19357]    GridJobStatus = "REMOVED"
01/07/16 12:36:06 [19357]    HoldReason = "Job removed from batch queue manually"
01/07/16 12:36:06 [19357]    HoldReasonCode = 0
01/07/16 12:36:06 [19357]    HoldReasonSubCode = 0
01/07/16 12:36:06 [19357]    JobStatus = 5
01/07/16 12:36:06 [19357]    LastReleaseReason = "Data files spooled"
01/07/16 12:36:06 [19357]    LastRemoteStatusUpdate = 1452188166
01/07/16 12:36:06 [19357]    Managed = "Schedd"
01/07/16 12:36:06 [19357]    NumSystemHolds = 1
01/07/16 12:36:06 [19357]    ReleaseReason = undefined
01/07/16 12:36:07 [19357] No jobs left, shutting down
01/07/16 12:36:07 [19357] leaving doContactSchedd()
01/07/16 12:36:07 [19357] Got SIGTERM. Performing graceful shutdown.
01/07/16 12:36:07 [19357] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 12:36:07 [19357] **** condor_gridmanager (condor_GRIDMANAGER) pid 19357 EXITING WITH STATUS 0
01/07/16 12:44:20 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 12:44:20 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 12:44:20 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 12:44:20 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 12:44:20 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 12:44:20 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 12:44:20 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 12:44:20 ******************************************************
01/07/16 12:44:20 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 12:44:20 ** /usr/sbin/condor_gridmanager
01/07/16 12:44:20 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 12:44:20 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 12:44:20 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 12:44:20 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 12:44:20 ** PID = 22988
01/07/16 12:44:20 ** Log last touched 1/7 12:36:07
01/07/16 12:44:20 ******************************************************
01/07/16 12:44:20 Using config source: /etc/condor-ce/condor_config
01/07/16 12:44:20 Using local config sources: 
01/07/16 12:44:20    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 12:44:20    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 12:44:20    /etc/condor-ce/config.d/99-local.conf
01/07/16 12:44:20    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 12:44:20 config Macros = 142, Sorted = 142, StringBytes = 12577, TablesBytes = 5320
01/07/16 12:44:20 CLASSAD_CACHING is ENABLED
01/07/16 12:44:20 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 12:44:20 SharedPortEndpoint: waiting for connections to named socket 7086_7a59_2
01/07/16 12:44:20 DaemonCore: command socket at <140.247.179.131:9620?sock=7086_7a59_2>
01/07/16 12:44:20 DaemonCore: private command socket at <140.247.179.131:9620?sock=7086_7a59_2>
01/07/16 12:44:20 Setting maximum accepts per cycle 8.
01/07/16 12:44:20 Setting maximum reaps per cycle 8.
01/07/16 12:44:20 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 12:44:20 [22988] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 12:44:20 [22988] DaemonCore: No more children processes to reap.
01/07/16 12:44:20 [22988] DaemonCore: in SendAliveToParent()
01/07/16 12:44:20 [22988] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:44:20 [22988] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 12:44:20 [22988] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 12:44:20 [22988] IPVERIFY: ip found is 0
01/07/16 12:44:20 [22988] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 12:44:20 [22988] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 12:44:20 [22988] Buf::write(): condor_write() failed
01/07/16 12:44:20 [22988] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 12:44:20 [22988] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:44:20 [22988] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 12:44:20 [22988] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 12:44:20 [22988] Checking proxies
01/07/16 12:44:23 [22988] Received ADD_JOBS signal
01/07/16 12:44:23 [22988] in doContactSchedd()
01/07/16 12:44:23 [22988] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:44:23 [22988] querying for new jobs
01/07/16 12:44:23 [22988] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 12:44:23 [22988] Using job type INFNBatch for job 56.0
01/07/16 12:44:23 [22988] (56.0) SetJobLeaseTimers()
01/07/16 12:44:23 [22988] Found job 56.0 --- inserting
01/07/16 12:44:23 [22988] Fetched 1 new job ads from schedd
01/07/16 12:44:23 [22988] querying for removed/held jobs
01/07/16 12:44:23 [22988] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:44:23 [22988] Fetched 0 job ads from schedd
01/07/16 12:44:23 [22988] leaving doContactSchedd()
01/07/16 12:44:23 [22988] gahp server not up yet, delaying ping
01/07/16 12:44:23 [22988] *** UpdateLeases called
01/07/16 12:44:23 [22988]     Leases not supported, cancelling timer
01/07/16 12:44:23 [22988] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7086_7a59_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6961_f065_4>"
CurrentTime = time()
MyCurrentTime = 1452188663
IdleJobs = 1
JobLimit = 10000

01/07/16 12:44:23 [22988] Trying to update collector <10.31.131.202:9619>
01/07/16 12:44:23 [22988] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 12:44:23 [22988] File descriptor limits: max 4096, safe 3277
01/07/16 12:44:23 [22988] (56.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/07/16 12:44:23 [22988] GAHP server pid = 23220
01/07/16 12:44:23 [22988] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 12:44:23 [22988] GAHP[23220] <- 'COMMANDS'
01/07/16 12:44:23 [22988] GAHP[23220] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 12:44:23 [22988] GAHP[23220] <- 'ASYNC_MODE_ON'
01/07/16 12:44:23 [22988] GAHP[23220] -> 'S' 'Async mode on'
01/07/16 12:44:23 [22988] (56.0) gm state change: GM_INIT -> GM_START
01/07/16 12:44:23 [22988] (56.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/07/16 12:44:23 [22988] (56.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 12:44:23 [22988] (56.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 12:44:23 [22988] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 12:44:23 [22988] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 12:44:23 [22988] IPVERIFY: ip found is 1
01/07/16 12:44:25 [22988] Evaluating staleness of remote job statuses.
01/07/16 12:44:28 [22988] resource  is now up
01/07/16 12:44:28 [22988] in doContactSchedd()
01/07/16 12:44:28 [22988] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:44:28 [22988] querying for removed/held jobs
01/07/16 12:44:28 [22988] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:44:28 [22988] Fetched 0 job ads from schedd
01/07/16 12:44:28 [22988] Updating classad values for 56.0:
01/07/16 12:44:28 [22988]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#56.0#1452188659"
01/07/16 12:44:28 [22988]    LastRemoteStatusUpdate = 1452188663
01/07/16 12:44:28 [22988] leaving doContactSchedd()
01/07/16 12:44:28 [22988] (56.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 12:44:28 [22988] (56.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 12:44:28 [22988] (56.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 12:44:28 [22988] GAHP[23220] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/55/0/cluster55.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/55/0/cluster55.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/55/0/cluster55.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#56.0#1452188659";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 12:44:28 [22988] GAHP[23220] -> 'S'
01/07/16 12:44:41 [22988] GAHP[23220] <- 'RESULTS'
01/07/16 12:44:41 [22988] GAHP[23220] -> 'R'
01/07/16 12:44:41 [22988] GAHP[23220] -> 'S' '1'
01/07/16 12:44:41 [22988] GAHP[23220] -> '2' '0' 'No error' 'lsf/20160107/542936014'
01/07/16 12:44:41 [22988] (56.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/07/16 12:44:41 [22988] (56.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
01/07/16 12:44:41 [22988] in doContactSchedd()
01/07/16 12:44:41 [22988] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:44:41 [22988] querying for removed/held jobs
01/07/16 12:44:41 [22988] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:44:41 [22988] Fetched 0 job ads from schedd
01/07/16 12:44:41 [22988] Updating classad values for 56.0:
01/07/16 12:44:41 [22988]    DelegatedProxyExpiration = 1452225638
01/07/16 12:44:41 [22988]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#56.0#1452188659 lsf/20160107/542936014"
01/07/16 12:44:41 [22988] leaving doContactSchedd()
01/07/16 12:44:41 [22988] (56.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
01/07/16 12:44:41 [22988] (56.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
01/07/16 12:45:20 [22988] Received CHECK_LEASES signal
01/07/16 12:45:20 [22988] in doContactSchedd()
01/07/16 12:45:20 [22988] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:45:20 [22988] querying for renewed leases
01/07/16 12:45:20 [22988] querying for removed/held jobs
01/07/16 12:45:20 [22988] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:45:20 [22988] Fetched 0 job ads from schedd
01/07/16 12:45:20 [22988] leaving doContactSchedd()
01/07/16 12:45:23 [22988] GAHP[23220] <- 'RESULTS'
01/07/16 12:45:23 [22988] GAHP[23220] -> 'S' '0'
01/07/16 12:45:25 [22988] Evaluating staleness of remote job statuses.
01/07/16 12:45:41 [22988] (56.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
01/07/16 12:45:41 [22988] (56.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 12:45:41 [22988] GAHP[23220] <- 'BLAH_JOB_STATUS 3 lsf/20160107/542936014'
01/07/16 12:45:41 [22988] GAHP[23220] -> 'S'
01/07/16 12:45:44 [22988] GAHP[23220] <- 'RESULTS'
01/07/16 12:45:44 [22988] GAHP[23220] -> 'R'
01/07/16 12:45:44 [22988] GAHP[23220] -> 'S' '1'
01/07/16 12:45:44 [22988] GAHP[23220] -> '3' '0' 'No Error' '4' '[ BatchjobId = "542936014"; JobStatus = 4; ExitCode = 0; WorkerNode = "camd10" ]'
01/07/16 12:45:44 [22988] (56.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
01/07/16 12:45:44 [22988] (56.0) ***ProcessRemoteAd
01/07/16 12:45:44 [22988] (56.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 12:45:44 [22988] (56.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
01/07/16 12:45:44 [22988] (56.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
01/07/16 12:45:44 [22988] in doContactSchedd()
01/07/16 12:45:44 [22988] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:45:44 [22988] querying for removed/held jobs
01/07/16 12:45:44 [22988] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:45:44 [22988] Fetched 0 job ads from schedd
01/07/16 12:45:44 [22988] Updating classad values for 56.0:
01/07/16 12:45:44 [22988]    EnteredCurrentStatus = 1452188744
01/07/16 12:45:44 [22988]    ExitCode = 0
01/07/16 12:45:44 [22988]    GridJobStatus = "COMPLETED"
01/07/16 12:45:44 [22988]    JobStatus = 4
01/07/16 12:45:44 [22988]    LastRemoteStatusUpdate = 1452188744
01/07/16 12:45:44 [22988]    PeriodicRemove = false
01/07/16 12:45:44 [22988] leaving doContactSchedd()
01/07/16 12:45:44 [22988] (56.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
01/07/16 12:45:44 [22988] (56.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
01/07/16 12:45:44 [22988] (56.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
01/07/16 12:45:44 [22988] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 12:45:48 [22988] (56.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/07/16 12:45:49 [22988] in doContactSchedd()
01/07/16 12:45:49 [22988] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 12:45:49 [22988] querying for removed/held jobs
01/07/16 12:45:49 [22988] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 12:45:49 [22988] Fetched 1 job ads from schedd
01/07/16 12:45:49 [22988] Updating classad values for 56.0:
01/07/16 12:45:49 [22988]    CurrentStatusUnknown = false
01/07/16 12:45:49 [22988]    GridJobId = undefined
01/07/16 12:45:49 [22988]    LastRemoteStatusUpdate = 0
01/07/16 12:45:49 [22988]    Managed = "ScheddDone"
01/07/16 12:45:49 [22988] Deleting job 56.0 from schedd
01/07/16 12:45:49 [22988] No jobs left, shutting down
01/07/16 12:45:49 [22988] leaving doContactSchedd()
01/07/16 12:45:49 [22988] Got SIGTERM. Performing graceful shutdown.
01/07/16 12:45:49 [22988] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 12:45:49 [22988] **** condor_gridmanager (condor_GRIDMANAGER) pid 22988 EXITING WITH STATUS 0
01/07/16 14:33:48 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 14:33:48 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 14:33:48 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 14:33:48 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 14:33:48 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 14:33:48 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 14:33:48 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 14:33:48 ******************************************************
01/07/16 14:33:48 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 14:33:48 ** /usr/sbin/condor_gridmanager
01/07/16 14:33:48 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 14:33:48 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 14:33:48 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 14:33:48 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 14:33:48 ** PID = 23854
01/07/16 14:33:48 ** Log last touched 1/7 12:45:49
01/07/16 14:33:48 ******************************************************
01/07/16 14:33:48 Using config source: /etc/condor-ce/condor_config
01/07/16 14:33:48 Using local config sources: 
01/07/16 14:33:48    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 14:33:48    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 14:33:48    /etc/condor-ce/config.d/99-local.conf
01/07/16 14:33:48    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 14:33:48 config Macros = 142, Sorted = 142, StringBytes = 12577, TablesBytes = 5320
01/07/16 14:33:48 CLASSAD_CACHING is ENABLED
01/07/16 14:33:48 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 14:33:48 SharedPortEndpoint: waiting for connections to named socket 7086_7a59_3
01/07/16 14:33:48 DaemonCore: command socket at <140.247.179.131:9620?sock=7086_7a59_3>
01/07/16 14:33:48 DaemonCore: private command socket at <140.247.179.131:9620?sock=7086_7a59_3>
01/07/16 14:33:48 Setting maximum accepts per cycle 8.
01/07/16 14:33:48 Setting maximum reaps per cycle 8.
01/07/16 14:33:48 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 14:33:48 [23854] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 14:33:48 [23854] DaemonCore: No more children processes to reap.
01/07/16 14:33:48 [23854] DaemonCore: in SendAliveToParent()
01/07/16 14:33:48 [23854] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 14:33:49 [23854] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 14:33:49 [23854] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 14:33:49 [23854] IPVERIFY: ip found is 0
01/07/16 14:33:49 [23854] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 14:33:49 [23854] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 14:33:49 [23854] Buf::write(): condor_write() failed
01/07/16 14:33:49 [23854] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 14:33:49 [23854] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 14:33:49 [23854] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 14:33:49 [23854] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 14:33:49 [23854] Checking proxies
01/07/16 14:33:50 [23854] Received REMOVE_JOBS signal
01/07/16 14:33:50 [23854] in doContactSchedd()
01/07/16 14:33:50 [23854] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 14:33:50 [23854] querying for new jobs
01/07/16 14:33:50 [23854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 14:33:50 [23854] Using job type INFNBatch for job 48.0
01/07/16 14:33:50 [23854] (48.0) SetJobLeaseTimers()
01/07/16 14:33:50 [23854] Found job 48.0 --- inserting
01/07/16 14:33:50 [23854] Fetched 1 new job ads from schedd
01/07/16 14:33:50 [23854] querying for removed/held jobs
01/07/16 14:33:50 [23854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 14:33:50 [23854] Fetched 1 job ads from schedd
01/07/16 14:33:50 [23854] leaving doContactSchedd()
01/07/16 14:33:50 [23854] gahp server not up yet, delaying ping
01/07/16 14:33:50 [23854] *** UpdateLeases called
01/07/16 14:33:50 [23854]     Leases not supported, cancelling timer
01/07/16 14:33:50 [23854] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7086_7a59_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6961_f065_4>"
CurrentTime = time()
MyCurrentTime = 1452195230
IdleJobs = 0
JobLimit = 10000

01/07/16 14:33:50 [23854] Trying to update collector <10.31.131.202:9619>
01/07/16 14:33:50 [23854] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 14:33:50 [23854] File descriptor limits: max 4096, safe 3277
01/07/16 14:33:50 [23854] (48.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/07/16 14:33:50 [23854] GAHP server pid = 24164
01/07/16 14:33:50 [23854] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 14:33:50 [23854] GAHP[24164] <- 'COMMANDS'
01/07/16 14:33:50 [23854] GAHP[24164] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 14:33:50 [23854] GAHP[24164] <- 'ASYNC_MODE_ON'
01/07/16 14:33:50 [23854] GAHP[24164] -> 'S' 'Async mode on'
01/07/16 14:33:50 [23854] (48.0) gm state change: GM_INIT -> GM_START
01/07/16 14:33:50 [23854] (48.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/07/16 14:33:50 [23854] (48.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 14:33:50 [23854] GAHP[24164] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/47/0/cluster47.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/47/0/cluster47.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/47/0/cluster47.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#48.0#1452108542";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 14:33:50 [23854] GAHP[24164] -> 'S'
01/07/16 14:33:51 [23854] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 14:33:51 [23854] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 14:33:51 [23854] IPVERIFY: ip found is 1
01/07/16 14:33:51 [23854] Received ADD_JOBS signal
01/07/16 14:33:52 [23854] GAHP[24164] <- 'RESULTS'
01/07/16 14:33:52 [23854] GAHP[24164] -> 'R'
01/07/16 14:33:52 [23854] GAHP[24164] -> 'S' '1'
01/07/16 14:33:52 [23854] GAHP[24164] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)' 'N/A'
01/07/16 14:33:52 [23854] (48.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/07/16 14:33:52 [23854] (48.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)
01/07/16 14:33:52 [23854] (48.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 14:33:52 [23854] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 14:34:01 [23854] (48.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/07/16 14:34:01 [23854] Evaluating staleness of remote job statuses.
01/07/16 14:34:01 [23854] resource  is now up
01/07/16 14:34:01 [23854] in doContactSchedd()
01/07/16 14:34:01 [23854] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 14:34:01 [23854] querying for new jobs
01/07/16 14:34:01 [23854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
01/07/16 14:34:01 [23854] Fetched 0 new job ads from schedd
01/07/16 14:34:01 [23854] querying for removed/held jobs
01/07/16 14:34:01 [23854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 14:34:01 [23854] Fetched 1 job ads from schedd
01/07/16 14:34:01 [23854] Updating classad values for 48.0:
01/07/16 14:34:01 [23854]    CurrentStatusUnknown = false
01/07/16 14:34:01 [23854]    GridJobId = undefined
01/07/16 14:34:01 [23854]    LastRemoteStatusUpdate = 0
01/07/16 14:34:01 [23854]    Managed = "ScheddDone"
01/07/16 14:34:01 [23854] Deleting job 48.0 from schedd
01/07/16 14:34:01 [23854] No jobs left, shutting down
01/07/16 14:34:01 [23854] leaving doContactSchedd()
01/07/16 14:34:01 [23854] Got SIGTERM. Performing graceful shutdown.
01/07/16 14:34:01 [23854] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 14:34:01 [23854] **** condor_gridmanager (condor_GRIDMANAGER) pid 23854 EXITING WITH STATUS 0
01/07/16 15:47:16 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 15:47:16 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 15:47:16 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 15:47:16 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 15:47:16 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 15:47:16 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 15:47:16 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 15:47:16 ******************************************************
01/07/16 15:47:16 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 15:47:16 ** /usr/sbin/condor_gridmanager
01/07/16 15:47:16 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 15:47:16 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 15:47:16 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 15:47:16 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 15:47:16 ** PID = 1705
01/07/16 15:47:16 ** Log last touched 1/7 14:34:01
01/07/16 15:47:16 ******************************************************
01/07/16 15:47:16 Using config source: /etc/condor-ce/condor_config
01/07/16 15:47:16 Using local config sources: 
01/07/16 15:47:16    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 15:47:16    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 15:47:16    /etc/condor-ce/config.d/99-local.conf
01/07/16 15:47:16    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 15:47:16 config Macros = 142, Sorted = 142, StringBytes = 12575, TablesBytes = 5320
01/07/16 15:47:16 CLASSAD_CACHING is ENABLED
01/07/16 15:47:16 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 15:47:16 SharedPortEndpoint: waiting for connections to named socket 7086_7a59_4
01/07/16 15:47:16 DaemonCore: command socket at <140.247.179.131:9620?sock=7086_7a59_4>
01/07/16 15:47:16 DaemonCore: private command socket at <140.247.179.131:9620?sock=7086_7a59_4>
01/07/16 15:47:16 Setting maximum accepts per cycle 8.
01/07/16 15:47:16 Setting maximum reaps per cycle 8.
01/07/16 15:47:16 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 15:47:16 [1705] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 15:47:16 [1705] DaemonCore: No more children processes to reap.
01/07/16 15:47:16 [1705] DaemonCore: in SendAliveToParent()
01/07/16 15:47:16 [1705] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 15:47:16 [1705] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 15:47:16 [1705] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 15:47:16 [1705] IPVERIFY: ip found is 0
01/07/16 15:47:16 [1705] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 15:47:16 [1705] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 15:47:16 [1705] Buf::write(): condor_write() failed
01/07/16 15:47:16 [1705] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 15:47:16 [1705] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 15:47:16 [1705] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 15:47:16 [1705] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 15:47:16 [1705] Checking proxies
01/07/16 15:47:18 [1705] Received ADD_JOBS signal
01/07/16 15:47:18 [1705] in doContactSchedd()
01/07/16 15:47:18 [1705] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 15:47:18 [1705] querying for new jobs
01/07/16 15:47:18 [1705] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 15:47:18 [1705] Using job type INFNBatch for job 58.0
01/07/16 15:47:18 [1705] (58.0) SetJobLeaseTimers()
01/07/16 15:47:18 [1705] Found job 58.0 --- inserting
01/07/16 15:47:18 [1705] Fetched 1 new job ads from schedd
01/07/16 15:47:18 [1705] querying for removed/held jobs
01/07/16 15:47:18 [1705] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:47:18 [1705] Fetched 0 job ads from schedd
01/07/16 15:47:18 [1705] leaving doContactSchedd()
01/07/16 15:47:18 [1705] gahp server not up yet, delaying ping
01/07/16 15:47:18 [1705] *** UpdateLeases called
01/07/16 15:47:18 [1705]     Leases not supported, cancelling timer
01/07/16 15:47:18 [1705] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7086_7a59_4>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6961_f065_4>"
CurrentTime = time()
MyCurrentTime = 1452199638
IdleJobs = 1
JobLimit = 10000

01/07/16 15:47:18 [1705] Trying to update collector <10.31.131.202:9619>
01/07/16 15:47:18 [1705] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 15:47:18 [1705] File descriptor limits: max 4096, safe 3277
01/07/16 15:47:18 [1705] (58.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/07/16 15:47:18 [1705] GAHP server pid = 2085
01/07/16 15:47:18 [1705] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 15:47:18 [1705] GAHP[2085] <- 'COMMANDS'
01/07/16 15:47:18 [1705] GAHP[2085] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 15:47:18 [1705] GAHP[2085] <- 'ASYNC_MODE_ON'
01/07/16 15:47:18 [1705] GAHP[2085] -> 'S' 'Async mode on'
01/07/16 15:47:18 [1705] (58.0) gm state change: GM_INIT -> GM_START
01/07/16 15:47:18 [1705] (58.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/07/16 15:47:18 [1705] (58.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 15:47:18 [1705] (58.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 15:47:18 [1705] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 15:47:18 [1705] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 15:47:18 [1705] IPVERIFY: ip found is 1
01/07/16 15:47:21 [1705] Evaluating staleness of remote job statuses.
01/07/16 15:47:23 [1705] resource  is now up
01/07/16 15:47:23 [1705] in doContactSchedd()
01/07/16 15:47:23 [1705] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 15:47:23 [1705] querying for removed/held jobs
01/07/16 15:47:23 [1705] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:47:23 [1705] Fetched 0 job ads from schedd
01/07/16 15:47:23 [1705] Updating classad values for 58.0:
01/07/16 15:47:23 [1705]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#58.0#1452199633"
01/07/16 15:47:23 [1705]    LastRemoteStatusUpdate = 1452199638
01/07/16 15:47:23 [1705] leaving doContactSchedd()
01/07/16 15:47:23 [1705] (58.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 15:47:23 [1705] (58.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 15:47:23 [1705] (58.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 15:47:23 [1705] GAHP[2085] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ SSH_CONNECTION=10.255.12.14'\ '51328'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.14'\ '51328'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/0\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_c2ALG5\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#58.0#1452199633";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 15:47:23 [1705] GAHP[2085] -> 'S'
01/07/16 15:47:24 [1705] GAHP[2085] <- 'RESULTS'
01/07/16 15:47:24 [1705] GAHP[2085] -> 'R'
01/07/16 15:47:24 [1705] GAHP[2085] -> 'S' '1'
01/07/16 15:47:24 [1705] GAHP[2085] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)' 'N/A'
01/07/16 15:47:24 [1705] (58.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/07/16 15:47:24 [1705] (58.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:cat: /etc/lsf.conf: No such file or directory-cat: /etc/lsf.conf: No such file or directory-ls_initdebug: Unable to open file lsf.conf-lsb_init: Failed in an LSF library call: Unable to open file lsf.conf. Job not submitted.-)
01/07/16 15:47:24 [1705] (58.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/07/16 15:47:24 [1705] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 15:47:27 [1705] (58.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/07/16 15:47:28 [1705] in doContactSchedd()
01/07/16 15:47:28 [1705] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 15:47:28 [1705] querying for removed/held jobs
01/07/16 15:47:28 [1705] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:47:28 [1705] Fetched 0 job ads from schedd
01/07/16 15:47:28 [1705] Updating classad values for 58.0:
01/07/16 15:47:28 [1705]    CurrentStatusUnknown = false
01/07/16 15:47:28 [1705]    GridJobId = undefined
01/07/16 15:47:28 [1705]    LastRemoteStatusUpdate = 0
01/07/16 15:47:28 [1705] leaving doContactSchedd()
01/07/16 15:47:28 [1705] (58.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/07/16 15:47:28 [1705] (58.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 15:47:28 [1705] (58.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 15:47:33 [1705] in doContactSchedd()
01/07/16 15:47:33 [1705] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 15:47:33 [1705] querying for removed/held jobs
01/07/16 15:47:33 [1705] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:47:33 [1705] Fetched 0 job ads from schedd
01/07/16 15:47:33 [1705] Updating classad values for 58.0:
01/07/16 15:47:33 [1705]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#58.0#1452199633"
01/07/16 15:47:33 [1705]    LastRemoteStatusUpdate = 1452199648
01/07/16 15:47:33 [1705] leaving doContactSchedd()
01/07/16 15:47:33 [1705] (58.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 15:47:33 [1705] (58.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 15:47:33 [1705] (58.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/07/16 15:47:33 [1705] dirscat: dirpath = /tmp
01/07/16 15:47:33 [1705] dirscat: subdir = condorLocks
01/07/16 15:47:33 [1705] directory_util::rec_touch_file: Creating directory /tmp 
01/07/16 15:47:33 [1705] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
01/07/16 15:47:33 [1705] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/13 
01/07/16 15:47:33 [1705] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/13/88 
01/07/16 15:47:33 [1705] FileLock object is updating timestamp on: /tmp/condorLocks/13/88/1955253986372786.lockc
01/07/16 15:47:33 [1705] WriteUserLog::initialize: opened /n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/.log_1466_agzSXL successfully
01/07/16 15:47:33 [1705] (58.0) Writing hold record to user logfile
01/07/16 15:47:33 [1705] FileLock::obtain(1) - @1452199653.920713 lock on /tmp/condorLocks/13/88/1955253986372786.lockc now WRITE
01/07/16 15:47:33 [1705] FileLock::obtain(2) - @1452199653.921881 lock on /tmp/condorLocks/13/88/1955253986372786.lockc now UNLOCKED
01/07/16 15:47:33 [1705] FileLock::obtain(1) - @1452199653.921977 lock on /tmp/condorLocks/13/88/1955253986372786.lockc now WRITE
01/07/16 15:47:33 [1705] directory_util::rec_clean_up: file /tmp/condorLocks/13/88/1955253986372786.lockc has been deleted. 
01/07/16 15:47:33 [1705] directory_util::rec_clean_up: directory /tmp/condorLocks/13 cannot be deleted -- it may not 				be empty and therefore this is not necessarily an error or problem. (Error: Directory not empty) 
01/07/16 15:47:33 [1705] Lock file /tmp/condorLocks/13/88/1955253986372786.lockc cannot be deleted. 
01/07/16 15:47:33 [1705] FileLock::obtain(2) - @1452199653.922202 lock on /tmp/condorLocks/13/88/1955253986372786.lockc now UNLOCKED
01/07/16 15:47:33 [1705] (58.0) gm state change: GM_HOLD -> GM_DELETE
01/07/16 15:47:38 [1705] in doContactSchedd()
01/07/16 15:47:38 [1705] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6961_f065_4
01/07/16 15:47:38 [1705] querying for removed/held jobs
01/07/16 15:47:38 [1705] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:47:38 [1705] Fetched 0 job ads from schedd
01/07/16 15:47:38 [1705] Updating classad values for 58.0:
01/07/16 15:47:38 [1705]    EnteredCurrentStatus = 1452199653
01/07/16 15:47:38 [1705]    HoldReason = "Attempts to submit failed: "
01/07/16 15:47:38 [1705]    HoldReasonCode = 0
01/07/16 15:47:38 [1705]    HoldReasonSubCode = 0
01/07/16 15:47:38 [1705]    JobStatus = 5
01/07/16 15:47:38 [1705]    LastReleaseReason = "Data files spooled"
01/07/16 15:47:38 [1705]    Managed = "Schedd"
01/07/16 15:47:38 [1705]    NumSystemHolds = 1
01/07/16 15:47:38 [1705]    ReleaseReason = undefined
01/07/16 15:47:38 [1705] No jobs left, shutting down
01/07/16 15:47:38 [1705] leaving doContactSchedd()
01/07/16 15:47:38 [1705] Got SIGTERM. Performing graceful shutdown.
01/07/16 15:47:38 [1705] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 15:47:38 [1705] **** condor_gridmanager (condor_GRIDMANAGER) pid 1705 EXITING WITH STATUS 0
01/07/16 15:58:17 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/07/16 15:58:17 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/07/16 15:58:17 Enumerating interfaces: lo 127.0.0.1 up
01/07/16 15:58:17 Enumerating interfaces: eth2 10.31.131.202 up
01/07/16 15:58:17 Enumerating interfaces: eth3 140.247.179.131 up
01/07/16 15:58:17 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/07/16 15:58:17 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/07/16 15:58:17 ******************************************************
01/07/16 15:58:17 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/07/16 15:58:17 ** /usr/sbin/condor_gridmanager
01/07/16 15:58:17 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/07/16 15:58:17 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/07/16 15:58:17 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/07/16 15:58:17 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/07/16 15:58:17 ** PID = 12885
01/07/16 15:58:17 ** Log last touched 1/7 15:47:38
01/07/16 15:58:17 ******************************************************
01/07/16 15:58:17 Using config source: /etc/condor-ce/condor_config
01/07/16 15:58:17 Using local config sources: 
01/07/16 15:58:17    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/07/16 15:58:17    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/01-ce-auth.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/01-ce-router.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/01-common-auth.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/02-ce-lsf.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/02-ce-pbs.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/03-managed-fork.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/50-osg-configure.conf
01/07/16 15:58:17    /etc/condor-ce/config.d/99-local.conf
01/07/16 15:58:17    /usr/share/condor-ce/condor_ce_router_defaults|
01/07/16 15:58:17 config Macros = 142, Sorted = 142, StringBytes = 12581, TablesBytes = 5320
01/07/16 15:58:17 CLASSAD_CACHING is ENABLED
01/07/16 15:58:17 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/07/16 15:58:17 SharedPortEndpoint: waiting for connections to named socket 12095_c585_1
01/07/16 15:58:17 DaemonCore: command socket at <140.247.179.131:9620?sock=12095_c585_1>
01/07/16 15:58:17 DaemonCore: private command socket at <140.247.179.131:9620?sock=12095_c585_1>
01/07/16 15:58:17 Setting maximum accepts per cycle 8.
01/07/16 15:58:17 Setting maximum reaps per cycle 8.
01/07/16 15:58:17 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 15:58:17 [12885] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/07/16 15:58:17 [12885] DaemonCore: No more children processes to reap.
01/07/16 15:58:17 [12885] DaemonCore: in SendAliveToParent()
01/07/16 15:58:17 [12885] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:58:17 [12885] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/07/16 15:58:17 [12885] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/07/16 15:58:17 [12885] IPVERIFY: ip found is 0
01/07/16 15:58:17 [12885] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/07/16 15:58:17 [12885] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/07/16 15:58:17 [12885] Buf::write(): condor_write() failed
01/07/16 15:58:17 [12885] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/07/16 15:58:17 [12885] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:58:17 [12885] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/07/16 15:58:17 [12885] DaemonCore: Leaving SendAliveToParent() - success
01/07/16 15:58:17 [12885] Checking proxies
01/07/16 15:58:20 [12885] Received ADD_JOBS signal
01/07/16 15:58:20 [12885] in doContactSchedd()
01/07/16 15:58:20 [12885] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:58:20 [12885] querying for new jobs
01/07/16 15:58:20 [12885] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/07/16 15:58:20 [12885] Using job type INFNBatch for job 60.0
01/07/16 15:58:20 [12885] (60.0) SetJobLeaseTimers()
01/07/16 15:58:20 [12885] Found job 60.0 --- inserting
01/07/16 15:58:20 [12885] Fetched 1 new job ads from schedd
01/07/16 15:58:20 [12885] querying for removed/held jobs
01/07/16 15:58:20 [12885] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:58:20 [12885] Fetched 0 job ads from schedd
01/07/16 15:58:20 [12885] leaving doContactSchedd()
01/07/16 15:58:20 [12885] gahp server not up yet, delaying ping
01/07/16 15:58:20 [12885] *** UpdateLeases called
01/07/16 15:58:20 [12885]     Leases not supported, cancelling timer
01/07/16 15:58:20 [12885] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=12095_c585_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=12052_ef4a_4>"
CurrentTime = time()
MyCurrentTime = 1452200300
IdleJobs = 1
JobLimit = 10000

01/07/16 15:58:20 [12885] Trying to update collector <10.31.131.202:9619>
01/07/16 15:58:20 [12885] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/07/16 15:58:20 [12885] File descriptor limits: max 4096, safe 3277
01/07/16 15:58:20 [12885] (60.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/07/16 15:58:20 [12885] GAHP server pid = 13020
01/07/16 15:58:20 [12885] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/07/16 15:58:20 [12885] GAHP[13020] <- 'COMMANDS'
01/07/16 15:58:20 [12885] GAHP[13020] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/07/16 15:58:20 [12885] GAHP[13020] <- 'ASYNC_MODE_ON'
01/07/16 15:58:20 [12885] GAHP[13020] -> 'S' 'Async mode on'
01/07/16 15:58:20 [12885] (60.0) gm state change: GM_INIT -> GM_START
01/07/16 15:58:20 [12885] (60.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/07/16 15:58:20 [12885] (60.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/07/16 15:58:20 [12885] (60.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/07/16 15:58:20 [12885] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/07/16 15:58:20 [12885] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/07/16 15:58:20 [12885] IPVERIFY: ip found is 1
01/07/16 15:58:22 [12885] Evaluating staleness of remote job statuses.
01/07/16 15:58:25 [12885] resource  is now up
01/07/16 15:58:25 [12885] in doContactSchedd()
01/07/16 15:58:25 [12885] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:58:25 [12885] querying for removed/held jobs
01/07/16 15:58:25 [12885] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:58:25 [12885] Fetched 0 job ads from schedd
01/07/16 15:58:25 [12885] Updating classad values for 60.0:
01/07/16 15:58:25 [12885]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#60.0#1452200295"
01/07/16 15:58:25 [12885]    LastRemoteStatusUpdate = 1452200300
01/07/16 15:58:25 [12885] leaving doContactSchedd()
01/07/16 15:58:25 [12885] (60.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/07/16 15:58:25 [12885] (60.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/07/16 15:58:25 [12885] (60.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/07/16 15:58:25 [12885] GAHP[13020] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ SSH_CONNECTION=10.255.12.14'\ '51328'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.14'\ '51328'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/0\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_c2ALG5\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/59/0/cluster59.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/59/0/cluster59.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/59/0/cluster59.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#60.0#1452200295";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/07/16 15:58:25 [12885] GAHP[13020] -> 'S'
01/07/16 15:58:38 [12885] GAHP[13020] <- 'RESULTS'
01/07/16 15:58:38 [12885] GAHP[13020] -> 'R'
01/07/16 15:58:38 [12885] GAHP[13020] -> 'S' '1'
01/07/16 15:58:38 [12885] GAHP[13020] -> '2' '0' 'No error' 'lsf/20160107/542937047'
01/07/16 15:58:38 [12885] (60.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/07/16 15:58:38 [12885] dirscat: dirpath = /tmp
01/07/16 15:58:38 [12885] dirscat: subdir = condorLocks
01/07/16 15:58:38 [12885] directory_util::rec_touch_file: Creating directory /tmp 
01/07/16 15:58:38 [12885] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
01/07/16 15:58:38 [12885] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/11 
01/07/16 15:58:38 [12885] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/11/29 
01/07/16 15:58:38 [12885] FileLock object is updating timestamp on: /tmp/condorLocks/11/29/3562875875906761.lockc
01/07/16 15:58:38 [12885] WriteUserLog::initialize: opened /n/atlasgrid/condor/59/0/cluster59.proc0.subproc0/.log_1817_uNGqIv successfully
01/07/16 15:58:38 [12885] (60.0) Writing grid submit record to user logfile
01/07/16 15:58:38 [12885] FileLock::obtain(1) - @1452200318.863398 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now WRITE
01/07/16 15:58:38 [12885] FileLock::obtain(2) - @1452200318.864006 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now UNLOCKED
01/07/16 15:58:38 [12885] FileLock::obtain(1) - @1452200318.864093 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now WRITE
01/07/16 15:58:38 [12885] directory_util::rec_clean_up: file /tmp/condorLocks/11/29/3562875875906761.lockc has been deleted. 
01/07/16 15:58:38 [12885] Lock file /tmp/condorLocks/11/29/3562875875906761.lockc has been deleted. 
01/07/16 15:58:38 [12885] FileLock::obtain(2) - @1452200318.864255 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now UNLOCKED
01/07/16 15:58:38 [12885] (60.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
01/07/16 15:58:38 [12885] in doContactSchedd()
01/07/16 15:58:38 [12885] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:58:38 [12885] querying for removed/held jobs
01/07/16 15:58:38 [12885] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:58:38 [12885] Fetched 0 job ads from schedd
01/07/16 15:58:38 [12885] Updating classad values for 60.0:
01/07/16 15:58:38 [12885]    DelegatedProxyExpiration = 1452225638
01/07/16 15:58:38 [12885]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#60.0#1452200295 lsf/20160107/542937047"
01/07/16 15:58:38 [12885] leaving doContactSchedd()
01/07/16 15:58:38 [12885] (60.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
01/07/16 15:58:38 [12885] (60.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
01/07/16 15:59:17 [12885] Received CHECK_LEASES signal
01/07/16 15:59:17 [12885] in doContactSchedd()
01/07/16 15:59:17 [12885] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:59:17 [12885] querying for renewed leases
01/07/16 15:59:17 [12885] querying for removed/held jobs
01/07/16 15:59:17 [12885] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:59:17 [12885] Fetched 0 job ads from schedd
01/07/16 15:59:17 [12885] leaving doContactSchedd()
01/07/16 15:59:20 [12885] GAHP[13020] <- 'RESULTS'
01/07/16 15:59:20 [12885] GAHP[13020] -> 'S' '0'
01/07/16 15:59:22 [12885] Evaluating staleness of remote job statuses.
01/07/16 15:59:38 [12885] (60.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
01/07/16 15:59:38 [12885] (60.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/07/16 15:59:38 [12885] GAHP[13020] <- 'BLAH_JOB_STATUS 3 lsf/20160107/542937047'
01/07/16 15:59:38 [12885] GAHP[13020] -> 'S'
01/07/16 15:59:41 [12885] GAHP[13020] <- 'RESULTS'
01/07/16 15:59:41 [12885] GAHP[13020] -> 'R'
01/07/16 15:59:41 [12885] GAHP[13020] -> 'S' '1'
01/07/16 15:59:41 [12885] GAHP[13020] -> '3' '0' 'No Error' '4' '[ BatchjobId = "542937047"; JobStatus = 4; ExitCode = 0; WorkerNode = "camd10" ]'
01/07/16 15:59:41 [12885] (60.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
01/07/16 15:59:41 [12885] (60.0) ***ProcessRemoteAd
01/07/16 15:59:41 [12885] (60.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/07/16 15:59:41 [12885] (60.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
01/07/16 15:59:41 [12885] (60.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
01/07/16 15:59:41 [12885] in doContactSchedd()
01/07/16 15:59:41 [12885] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:59:41 [12885] querying for removed/held jobs
01/07/16 15:59:41 [12885] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:59:41 [12885] Fetched 0 job ads from schedd
01/07/16 15:59:41 [12885] Updating classad values for 60.0:
01/07/16 15:59:41 [12885]    EnteredCurrentStatus = 1452200381
01/07/16 15:59:41 [12885]    ExitCode = 0
01/07/16 15:59:41 [12885]    GridJobStatus = "COMPLETED"
01/07/16 15:59:41 [12885]    JobStatus = 4
01/07/16 15:59:41 [12885]    LastRemoteStatusUpdate = 1452200381
01/07/16 15:59:41 [12885]    PeriodicRemove = false
01/07/16 15:59:41 [12885] leaving doContactSchedd()
01/07/16 15:59:41 [12885] (60.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
01/07/16 15:59:41 [12885] (60.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
01/07/16 15:59:41 [12885] (60.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
01/07/16 15:59:41 [12885] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/07/16 15:59:41 [12885] (60.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/07/16 15:59:41 [12885] dirscat: dirpath = /tmp
01/07/16 15:59:41 [12885] dirscat: subdir = condorLocks
01/07/16 15:59:41 [12885] directory_util::rec_touch_file: Creating directory /tmp 
01/07/16 15:59:41 [12885] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
01/07/16 15:59:41 [12885] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/11 
01/07/16 15:59:41 [12885] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/11/29 
01/07/16 15:59:41 [12885] FileLock object is updating timestamp on: /tmp/condorLocks/11/29/3562875875906761.lockc
01/07/16 15:59:41 [12885] WriteUserLog::initialize: opened /n/atlasgrid/condor/59/0/cluster59.proc0.subproc0/.log_1817_uNGqIv successfully
01/07/16 15:59:41 [12885] (60.0) Writing terminate record to user logfile
01/07/16 15:59:41 [12885] FileLock::obtain(1) - @1452200381.723797 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now WRITE
01/07/16 15:59:41 [12885] FileLock::obtain(2) - @1452200381.724669 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now UNLOCKED
01/07/16 15:59:41 [12885] FileLock::obtain(1) - @1452200381.724765 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now WRITE
01/07/16 15:59:41 [12885] directory_util::rec_clean_up: file /tmp/condorLocks/11/29/3562875875906761.lockc has been deleted. 
01/07/16 15:59:41 [12885] Lock file /tmp/condorLocks/11/29/3562875875906761.lockc has been deleted. 
01/07/16 15:59:41 [12885] FileLock::obtain(2) - @1452200381.724931 lock on /tmp/condorLocks/11/29/3562875875906761.lockc now UNLOCKED
01/07/16 15:59:46 [12885] in doContactSchedd()
01/07/16 15:59:46 [12885] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12052_ef4a_4
01/07/16 15:59:46 [12885] querying for removed/held jobs
01/07/16 15:59:46 [12885] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/07/16 15:59:46 [12885] Fetched 1 job ads from schedd
01/07/16 15:59:46 [12885] Updating classad values for 60.0:
01/07/16 15:59:46 [12885]    CurrentStatusUnknown = false
01/07/16 15:59:46 [12885]    GridJobId = undefined
01/07/16 15:59:46 [12885]    LastRemoteStatusUpdate = 0
01/07/16 15:59:46 [12885]    Managed = "ScheddDone"
01/07/16 15:59:46 [12885] Deleting job 60.0 from schedd
01/07/16 15:59:46 [12885] No jobs left, shutting down
01/07/16 15:59:46 [12885] leaving doContactSchedd()
01/07/16 15:59:46 [12885] Got SIGTERM. Performing graceful shutdown.
01/07/16 15:59:46 [12885] Started timer to call main_shutdown_fast in 1800 seconds
01/07/16 15:59:46 [12885] **** condor_gridmanager (condor_GRIDMANAGER) pid 12885 EXITING WITH STATUS 0
01/13/16 13:41:28 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/13/16 13:41:28 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/13/16 13:41:28 Enumerating interfaces: lo 127.0.0.1 up
01/13/16 13:41:28 Enumerating interfaces: eth2 10.31.131.202 up
01/13/16 13:41:28 Enumerating interfaces: eth3 140.247.179.131 up
01/13/16 13:41:28 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/13/16 13:41:28 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/13/16 13:41:28 ******************************************************
01/13/16 13:41:28 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/13/16 13:41:28 ** /usr/sbin/condor_gridmanager
01/13/16 13:41:28 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/13/16 13:41:28 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/13/16 13:41:28 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/13/16 13:41:28 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/13/16 13:41:28 ** PID = 1229
01/13/16 13:41:28 ** Log last touched 1/7 15:59:46
01/13/16 13:41:28 ******************************************************
01/13/16 13:41:28 Using config source: /etc/condor-ce/condor_config
01/13/16 13:41:28 Using local config sources: 
01/13/16 13:41:28    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/13/16 13:41:28    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/01-ce-auth.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/01-ce-router.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/01-common-auth.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/02-ce-lsf.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/02-ce-pbs.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/03-managed-fork.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/50-osg-configure.conf
01/13/16 13:41:28    /etc/condor-ce/config.d/99-local.conf
01/13/16 13:41:28    /usr/share/condor-ce/condor_ce_router_defaults|
01/13/16 13:41:28 config Macros = 142, Sorted = 142, StringBytes = 12584, TablesBytes = 5320
01/13/16 13:41:28 CLASSAD_CACHING is ENABLED
01/13/16 13:41:28 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/13/16 13:41:28 SharedPortEndpoint: waiting for connections to named socket 20074_b2e9_1
01/13/16 13:41:28 DaemonCore: command socket at <140.247.179.131:9620?sock=20074_b2e9_1>
01/13/16 13:41:28 DaemonCore: private command socket at <140.247.179.131:9620?sock=20074_b2e9_1>
01/13/16 13:41:28 Setting maximum accepts per cycle 8.
01/13/16 13:41:28 Setting maximum reaps per cycle 8.
01/13/16 13:41:28 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 13:41:28 [1229] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/13/16 13:41:28 [1229] DaemonCore: No more children processes to reap.
01/13/16 13:41:28 [1229] DaemonCore: in SendAliveToParent()
01/13/16 13:41:28 [1229] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:41:28 [1229] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/13/16 13:41:28 [1229] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/13/16 13:41:28 [1229] IPVERIFY: ip found is 0
01/13/16 13:41:28 [1229] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/13/16 13:41:28 [1229] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/13/16 13:41:28 [1229] Buf::write(): condor_write() failed
01/13/16 13:41:28 [1229] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/13/16 13:41:28 [1229] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:41:28 [1229] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/13/16 13:41:28 [1229] DaemonCore: Leaving SendAliveToParent() - success
01/13/16 13:41:28 [1229] Checking proxies
01/13/16 13:41:31 [1229] Received ADD_JOBS signal
01/13/16 13:41:31 [1229] in doContactSchedd()
01/13/16 13:41:31 [1229] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:41:31 [1229] querying for new jobs
01/13/16 13:41:31 [1229] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/13/16 13:41:31 [1229] Using job type INFNBatch for job 62.0
01/13/16 13:41:31 [1229] (62.0) SetJobLeaseTimers()
01/13/16 13:41:31 [1229] Found job 62.0 --- inserting
01/13/16 13:41:31 [1229] Fetched 1 new job ads from schedd
01/13/16 13:41:31 [1229] querying for removed/held jobs
01/13/16 13:41:31 [1229] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 13:41:31 [1229] Fetched 0 job ads from schedd
01/13/16 13:41:31 [1229] leaving doContactSchedd()
01/13/16 13:41:31 [1229] gahp server not up yet, delaying ping
01/13/16 13:41:31 [1229] *** UpdateLeases called
01/13/16 13:41:31 [1229]     Leases not supported, cancelling timer
01/13/16 13:41:31 [1229] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch SLURM"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=20074_b2e9_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=19943_9fcb_4>"
CurrentTime = time()
MyCurrentTime = 1452710491
IdleJobs = 1
JobLimit = 10000

01/13/16 13:41:31 [1229] Trying to update collector <10.31.131.202:9619>
01/13/16 13:41:31 [1229] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 13:41:31 [1229] File descriptor limits: max 4096, safe 3277
01/13/16 13:41:31 [1229] (62.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/13/16 13:41:31 [1229] GAHP server pid = 1676
01/13/16 13:41:31 [1229] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/13/16 13:41:31 [1229] GAHP[1676] <- 'COMMANDS'
01/13/16 13:41:31 [1229] GAHP[1676] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/13/16 13:41:31 [1229] GAHP[1676] <- 'ASYNC_MODE_ON'
01/13/16 13:41:31 [1229] GAHP[1676] -> 'S' 'Async mode on'
01/13/16 13:41:31 [1229] (62.0) gm state change: GM_INIT -> GM_START
01/13/16 13:41:31 [1229] (62.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/13/16 13:41:31 [1229] (62.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/13/16 13:41:31 [1229] (62.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/13/16 13:41:31 [1229] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/13/16 13:41:31 [1229] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/13/16 13:41:31 [1229] IPVERIFY: ip found is 1
01/13/16 13:41:33 [1229] Evaluating staleness of remote job statuses.
01/13/16 13:41:36 [1229] resource  is now up
01/13/16 13:41:36 [1229] in doContactSchedd()
01/13/16 13:41:36 [1229] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:41:36 [1229] querying for removed/held jobs
01/13/16 13:41:36 [1229] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 13:41:36 [1229] Fetched 0 job ads from schedd
01/13/16 13:41:36 [1229] Updating classad values for 62.0:
01/13/16 13:41:36 [1229]    GridJobId = "batch slurm net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#62.0#1452710484"
01/13/16 13:41:36 [1229]    LastRemoteStatusUpdate = 1452710491
01/13/16 13:41:36 [1229] leaving doContactSchedd()
01/13/16 13:41:36 [1229] (62.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/13/16 13:41:36 [1229] (62.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/13/16 13:41:36 [1229] (62.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 13:41:36 [1229] GAHP[1676] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ slurm";\ x509userproxy\ =\ "/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#62.0#1452710484";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 13:41:36 [1229] GAHP[1676] -> 'S'
01/13/16 13:41:36 [1229] GAHP[1676] <- 'RESULTS'
01/13/16 13:41:36 [1229] GAHP[1676] -> 'R'
01/13/16 13:41:36 [1229] GAHP[1676] -> 'S' '1'
01/13/16 13:41:36 [1229] GAHP[1676] -> '2' '2' 'submission command failed (exit code = 2) (stdout:) (stderr:/usr/libexec/blahp/slurm_submit.sh: No such file or directory)' 'N/A'
01/13/16 13:41:36 [1229] (62.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/13/16 13:41:36 [1229] (62.0) blah_job_submit() failed: submission command failed (exit code = 2) (stdout:) (stderr:/usr/libexec/blahp/slurm_submit.sh: No such file or directory)
01/13/16 13:41:36 [1229] (62.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/13/16 13:41:36 [1229] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 13:41:39 [1229] (62.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/13/16 13:41:41 [1229] in doContactSchedd()
01/13/16 13:41:41 [1229] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:41:41 [1229] querying for removed/held jobs
01/13/16 13:41:41 [1229] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 13:41:41 [1229] Fetched 0 job ads from schedd
01/13/16 13:41:41 [1229] Updating classad values for 62.0:
01/13/16 13:41:41 [1229]    CurrentStatusUnknown = false
01/13/16 13:41:41 [1229]    GridJobId = undefined
01/13/16 13:41:41 [1229]    LastRemoteStatusUpdate = 0
01/13/16 13:41:41 [1229] leaving doContactSchedd()
01/13/16 13:41:41 [1229] (62.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/13/16 13:41:41 [1229] (62.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/13/16 13:41:41 [1229] (62.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/13/16 13:41:46 [1229] in doContactSchedd()
01/13/16 13:41:46 [1229] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:41:46 [1229] querying for removed/held jobs
01/13/16 13:41:46 [1229] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 13:41:46 [1229] Fetched 0 job ads from schedd
01/13/16 13:41:46 [1229] Updating classad values for 62.0:
01/13/16 13:41:46 [1229]    GridJobId = "batch slurm net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#62.0#1452710484"
01/13/16 13:41:46 [1229]    LastRemoteStatusUpdate = 1452710501
01/13/16 13:41:46 [1229] leaving doContactSchedd()
01/13/16 13:41:46 [1229] (62.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/13/16 13:41:46 [1229] (62.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/13/16 13:41:46 [1229] (62.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/13/16 13:41:46 [1229] (62.0) gm state change: GM_HOLD -> GM_DELETE
01/13/16 13:41:51 [1229] in doContactSchedd()
01/13/16 13:41:51 [1229] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:41:51 [1229] querying for removed/held jobs
01/13/16 13:41:51 [1229] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 13:41:51 [1229] Fetched 0 job ads from schedd
01/13/16 13:41:51 [1229] Updating classad values for 62.0:
01/13/16 13:41:51 [1229]    EnteredCurrentStatus = 1452710506
01/13/16 13:41:51 [1229]    HoldReason = "Attempts to submit failed: "
01/13/16 13:41:51 [1229]    HoldReasonCode = 0
01/13/16 13:41:51 [1229]    HoldReasonSubCode = 0
01/13/16 13:41:51 [1229]    JobStatus = 5
01/13/16 13:41:51 [1229]    LastReleaseReason = "Data files spooled"
01/13/16 13:41:51 [1229]    Managed = "Schedd"
01/13/16 13:41:51 [1229]    NumSystemHolds = 1
01/13/16 13:41:51 [1229]    ReleaseReason = undefined
01/13/16 13:41:51 [1229] No jobs left, shutting down
01/13/16 13:41:51 [1229] leaving doContactSchedd()
01/13/16 13:41:51 [1229] Got SIGTERM. Performing graceful shutdown.
01/13/16 13:41:51 [1229] Started timer to call main_shutdown_fast in 1800 seconds
01/13/16 13:41:51 [1229] **** condor_gridmanager (condor_GRIDMANAGER) pid 1229 EXITING WITH STATUS 0
01/13/16 13:45:04 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/13/16 13:45:04 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/13/16 13:45:04 Enumerating interfaces: lo 127.0.0.1 up
01/13/16 13:45:04 Enumerating interfaces: eth2 10.31.131.202 up
01/13/16 13:45:04 Enumerating interfaces: eth3 140.247.179.131 up
01/13/16 13:45:04 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/13/16 13:45:04 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/13/16 13:45:04 ******************************************************
01/13/16 13:45:04 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/13/16 13:45:04 ** /usr/sbin/condor_gridmanager
01/13/16 13:45:04 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/13/16 13:45:04 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/13/16 13:45:04 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/13/16 13:45:04 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/13/16 13:45:04 ** PID = 25903
01/13/16 13:45:04 ** Log last touched 1/13 13:41:51
01/13/16 13:45:04 ******************************************************
01/13/16 13:45:04 Using config source: /etc/condor-ce/condor_config
01/13/16 13:45:04 Using local config sources: 
01/13/16 13:45:04    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/13/16 13:45:04    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/01-ce-auth.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/01-ce-router.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/01-common-auth.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/02-ce-lsf.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/02-ce-pbs.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/03-managed-fork.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/50-osg-configure.conf
01/13/16 13:45:04    /etc/condor-ce/config.d/99-local.conf
01/13/16 13:45:04    /usr/share/condor-ce/condor_ce_router_defaults|
01/13/16 13:45:04 config Macros = 142, Sorted = 142, StringBytes = 12586, TablesBytes = 5320
01/13/16 13:45:04 CLASSAD_CACHING is ENABLED
01/13/16 13:45:04 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/13/16 13:45:04 SharedPortEndpoint: waiting for connections to named socket 20074_b2e9_2
01/13/16 13:45:04 DaemonCore: command socket at <140.247.179.131:9620?sock=20074_b2e9_2>
01/13/16 13:45:04 DaemonCore: private command socket at <140.247.179.131:9620?sock=20074_b2e9_2>
01/13/16 13:45:04 Setting maximum accepts per cycle 8.
01/13/16 13:45:04 Setting maximum reaps per cycle 8.
01/13/16 13:45:04 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 13:45:04 [25903] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/13/16 13:45:04 [25903] DaemonCore: No more children processes to reap.
01/13/16 13:45:04 [25903] DaemonCore: in SendAliveToParent()
01/13/16 13:45:04 [25903] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:45:04 [25903] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/13/16 13:45:04 [25903] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/13/16 13:45:04 [25903] IPVERIFY: ip found is 0
01/13/16 13:45:04 [25903] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/13/16 13:45:04 [25903] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/13/16 13:45:04 [25903] Buf::write(): condor_write() failed
01/13/16 13:45:04 [25903] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/13/16 13:45:04 [25903] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:45:04 [25903] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/13/16 13:45:04 [25903] DaemonCore: Leaving SendAliveToParent() - success
01/13/16 13:45:04 [25903] Checking proxies
01/13/16 13:45:06 [25903] Received REMOVE_JOBS signal
01/13/16 13:45:06 [25903] in doContactSchedd()
01/13/16 13:45:06 [25903] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:45:06 [25903] querying for new jobs
01/13/16 13:45:06 [25903] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/13/16 13:45:06 [25903] Using job type INFNBatch for job 50.0
01/13/16 13:45:06 [25903] (50.0) SetJobLeaseTimers()
01/13/16 13:45:06 [25903] Failed to get expiration time of proxy /n/atlasgrid/condor/49/0/cluster49.proc0.subproc0/x509up_u556792
01/13/16 13:45:06 [25903] Found job 50.0 --- inserting
01/13/16 13:45:06 [25903] Using job type INFNBatch for job 54.0
01/13/16 13:45:06 [25903] (54.0) SetJobLeaseTimers()
01/13/16 13:45:06 [25903] Failed to get expiration time of proxy /n/atlasgrid/condor/53/0/cluster53.proc0.subproc0/x509up_u556792
01/13/16 13:45:06 [25903] Found job 54.0 --- inserting
01/13/16 13:45:06 [25903] Using job type INFNBatch for job 58.0
01/13/16 13:45:06 [25903] (58.0) SetJobLeaseTimers()
01/13/16 13:45:06 [25903] Found job 58.0 --- inserting
01/13/16 13:45:06 [25903] Using job type INFNBatch for job 52.0
01/13/16 13:45:06 [25903] (52.0) SetJobLeaseTimers()
01/13/16 13:45:06 [25903] Failed to get expiration time of proxy /n/atlasgrid/condor/51/0/cluster51.proc0.subproc0/x509up_u556792
01/13/16 13:45:06 [25903] Found job 52.0 --- inserting
01/13/16 13:45:06 [25903] Fetched 4 new job ads from schedd
01/13/16 13:45:06 [25903] querying for removed/held jobs
01/13/16 13:45:06 [25903] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 13:45:06 [25903] Fetched 4 job ads from schedd
01/13/16 13:45:06 [25903] leaving doContactSchedd()
01/13/16 13:45:06 [25903] gahp server not up yet, delaying ping
01/13/16 13:45:06 [25903] *** UpdateLeases called
01/13/16 13:45:06 [25903]     Leases not supported, cancelling timer
01/13/16 13:45:06 [25903] BaseResource::UpdateResource: 
NumJobs = 4
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=20074_b2e9_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=19943_9fcb_4>"
CurrentTime = time()
MyCurrentTime = 1452710706
IdleJobs = 0
JobLimit = 10000

01/13/16 13:45:06 [25903] Trying to update collector <10.31.131.202:9619>
01/13/16 13:45:06 [25903] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 13:45:06 [25903] File descriptor limits: max 4096, safe 3277
01/13/16 13:45:06 [25903] (50.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/13/16 13:45:06 [25903] GAHP server pid = 25907
01/13/16 13:45:06 [25903] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/13/16 13:45:06 [25903] GAHP[25907] <- 'COMMANDS'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/13/16 13:45:06 [25903] GAHP[25907] <- 'ASYNC_MODE_ON'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S' 'Async mode on'
01/13/16 13:45:06 [25903] (50.0) gm state change: GM_INIT -> GM_START
01/13/16 13:45:06 [25903] (50.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/13/16 13:45:06 [25903] (50.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 13:45:06 [25903] GAHP[25907] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#50.0#1452182472";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S'
01/13/16 13:45:06 [25903] (54.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/13/16 13:45:06 [25903] (54.0) gm state change: GM_INIT -> GM_START
01/13/16 13:45:06 [25903] (54.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/13/16 13:45:06 [25903] (54.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 13:45:06 [25903] GAHP[25907] <- 'BLAH_JOB_SUBMIT 3 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#54.0#1452187157";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'R'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S'
01/13/16 13:45:06 [25903] (58.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/13/16 13:45:06 [25903] (58.0) gm state change: GM_INIT -> GM_START
01/13/16 13:45:06 [25903] (58.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/13/16 13:45:06 [25903] (58.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 13:45:06 [25903] GAHP[25907] <- 'BLAH_JOB_SUBMIT 4 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ SSH_CONNECTION=10.255.12.14'\ '51328'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.14'\ '51328'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/0\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_c2ALG5\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#58.0#1452199633";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S'
01/13/16 13:45:06 [25903] (52.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/13/16 13:45:06 [25903] (52.0) gm state change: GM_INIT -> GM_START
01/13/16 13:45:06 [25903] (52.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/13/16 13:45:06 [25903] (52.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 13:45:06 [25903] GAHP[25907] <- 'BLAH_JOB_SUBMIT 5 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#52.0#1452182637";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S'
01/13/16 13:45:06 [25903] GAHP[25907] <- 'RESULTS'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S' '3'
01/13/16 13:45:06 [25903] GAHP[25907] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
01/13/16 13:45:06 [25903] GAHP[25907] -> '3' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
01/13/16 13:45:06 [25903] GAHP[25907] -> '4' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/x509up_u556792.lmt): errno=13, Permission denied)' 'N/A'
01/13/16 13:45:06 [25903] (50.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/13/16 13:45:06 [25903] (50.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/49/0/cluster49.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
01/13/16 13:45:06 [25903] (50.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/13/16 13:45:06 [25903] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 13:45:06 [25903] (50.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/13/16 13:45:06 [25903] (54.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/13/16 13:45:06 [25903] (54.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/53/0/cluster53.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
01/13/16 13:45:06 [25903] (54.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/13/16 13:45:06 [25903] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 13:45:06 [25903] (54.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/13/16 13:45:06 [25903] (58.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/13/16 13:45:06 [25903] (58.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/x509up_u556792.lmt): errno=13, Permission denied)
01/13/16 13:45:06 [25903] (58.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/13/16 13:45:06 [25903] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 13:45:06 [25903] (58.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/13/16 13:45:06 [25903] WriteUserLog::initialize: safe_open_wrapper("/n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/.log_1466_agzSXL") failed - errno 13 (Permission denied)
01/13/16 13:45:06 [25903] WriteUserLog::initialize: failed to open file /n/atlasgrid/condor/57/0/cluster57.proc0.subproc0/.log_1466_agzSXL
01/13/16 13:45:06 [25903] (58.0) Writing abort record to user logfile
01/13/16 13:45:06 [25903] WriteUserLog: not initialized @ writeEvent()
01/13/16 13:45:06 [25903] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/13/16 13:45:06 [25903] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/13/16 13:45:06 [25903] IPVERIFY: ip found is 1
01/13/16 13:45:06 [25903] GAHP[25907] <- 'RESULTS'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'R'
01/13/16 13:45:06 [25903] GAHP[25907] -> 'S' '1'
01/13/16 13:45:06 [25903] GAHP[25907] -> '5' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
01/13/16 13:45:06 [25903] (52.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/13/16 13:45:06 [25903] (52.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/51/0/cluster51.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
01/13/16 13:45:06 [25903] (52.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/13/16 13:45:06 [25903] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 13:45:06 [25903] (52.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/13/16 13:45:07 [25903] Received ADD_JOBS signal
01/13/16 13:45:09 [25903] Evaluating staleness of remote job statuses.
01/13/16 13:45:11 [25903] resource  is now up
01/13/16 13:45:11 [25903] in doContactSchedd()
01/13/16 13:45:11 [25903] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 19943_9fcb_4
01/13/16 13:45:11 [25903] querying for new jobs
01/13/16 13:45:11 [25903] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
01/13/16 13:45:11 [25903] Fetched 0 new job ads from schedd
01/13/16 13:45:11 [25903] querying for removed/held jobs
01/13/16 13:45:11 [25903] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 13:45:11 [25903] Fetched 4 job ads from schedd
01/13/16 13:45:11 [25903] Updating classad values for 50.0:
01/13/16 13:45:11 [25903]    CurrentStatusUnknown = false
01/13/16 13:45:11 [25903]    GridJobId = undefined
01/13/16 13:45:11 [25903]    LastRemoteStatusUpdate = 0
01/13/16 13:45:11 [25903]    Managed = "ScheddDone"
01/13/16 13:45:11 [25903] Updating classad values for 58.0:
01/13/16 13:45:11 [25903]    CurrentStatusUnknown = false
01/13/16 13:45:11 [25903]    GridJobId = undefined
01/13/16 13:45:11 [25903]    LastRemoteStatusUpdate = 0
01/13/16 13:45:11 [25903]    Managed = "ScheddDone"
01/13/16 13:45:11 [25903] Updating classad values for 52.0:
01/13/16 13:45:11 [25903]    CurrentStatusUnknown = false
01/13/16 13:45:11 [25903]    GridJobId = undefined
01/13/16 13:45:11 [25903]    LastRemoteStatusUpdate = 0
01/13/16 13:45:11 [25903]    Managed = "ScheddDone"
01/13/16 13:45:11 [25903] Updating classad values for 54.0:
01/13/16 13:45:11 [25903]    CurrentStatusUnknown = false
01/13/16 13:45:11 [25903]    GridJobId = undefined
01/13/16 13:45:11 [25903]    LastRemoteStatusUpdate = 0
01/13/16 13:45:11 [25903]    Managed = "ScheddDone"
01/13/16 13:45:12 [25903] Deleting job 50.0 from schedd
01/13/16 13:45:12 [25903] Deleting job 58.0 from schedd
01/13/16 13:45:12 [25903] Deleting job 52.0 from schedd
01/13/16 13:45:12 [25903] Deleting job 54.0 from schedd
01/13/16 13:45:12 [25903] No jobs left, shutting down
01/13/16 13:45:12 [25903] leaving doContactSchedd()
01/13/16 13:45:12 [25903] Got SIGTERM. Performing graceful shutdown.
01/13/16 13:45:12 [25903] Started timer to call main_shutdown_fast in 1800 seconds
01/13/16 13:45:12 [25903] **** condor_gridmanager (condor_GRIDMANAGER) pid 25903 EXITING WITH STATUS 0
01/13/16 14:40:28 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/13/16 14:40:28 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/13/16 14:40:28 Enumerating interfaces: lo 127.0.0.1 up
01/13/16 14:40:28 Enumerating interfaces: eth2 10.31.131.202 up
01/13/16 14:40:28 Enumerating interfaces: eth3 140.247.179.131 up
01/13/16 14:40:28 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/13/16 14:40:28 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/13/16 14:40:28 ******************************************************
01/13/16 14:40:28 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/13/16 14:40:28 ** /usr/sbin/condor_gridmanager
01/13/16 14:40:28 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/13/16 14:40:28 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/13/16 14:40:28 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/13/16 14:40:28 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/13/16 14:40:28 ** PID = 12110
01/13/16 14:40:28 ** Log last touched 1/13 13:45:12
01/13/16 14:40:28 ******************************************************
01/13/16 14:40:28 Using config source: /etc/condor-ce/condor_config
01/13/16 14:40:28 Using local config sources: 
01/13/16 14:40:28    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/13/16 14:40:28    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/01-ce-auth.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/01-ce-router.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/01-common-auth.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/02-ce-lsf.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/02-ce-pbs.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/03-managed-fork.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/50-osg-configure.conf
01/13/16 14:40:28    /etc/condor-ce/config.d/99-local.conf
01/13/16 14:40:28    /usr/share/condor-ce/condor_ce_router_defaults|
01/13/16 14:40:28 config Macros = 142, Sorted = 142, StringBytes = 12510, TablesBytes = 5320
01/13/16 14:40:28 CLASSAD_CACHING is ENABLED
01/13/16 14:40:28 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/13/16 14:40:28 SharedPortEndpoint: waiting for connections to named socket 30975_7085_1
01/13/16 14:40:28 DaemonCore: command socket at <140.247.179.131:9620?sock=30975_7085_1>
01/13/16 14:40:28 DaemonCore: private command socket at <140.247.179.131:9620?sock=30975_7085_1>
01/13/16 14:40:28 Setting maximum accepts per cycle 8.
01/13/16 14:40:28 Setting maximum reaps per cycle 8.
01/13/16 14:40:28 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 14:40:28 [12110] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/13/16 14:40:28 [12110] DaemonCore: No more children processes to reap.
01/13/16 14:40:28 [12110] DaemonCore: in SendAliveToParent()
01/13/16 14:40:28 [12110] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 14:40:28 [12110] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/13/16 14:40:28 [12110] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/13/16 14:40:28 [12110] IPVERIFY: ip found is 0
01/13/16 14:40:28 [12110] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/13/16 14:40:28 [12110] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/13/16 14:40:28 [12110] Buf::write(): condor_write() failed
01/13/16 14:40:28 [12110] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/13/16 14:40:28 [12110] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 14:40:28 [12110] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/13/16 14:40:28 [12110] DaemonCore: Leaving SendAliveToParent() - success
01/13/16 14:40:28 [12110] Checking proxies
01/13/16 14:40:31 [12110] Received ADD_JOBS signal
01/13/16 14:40:31 [12110] in doContactSchedd()
01/13/16 14:40:31 [12110] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 14:40:31 [12110] querying for new jobs
01/13/16 14:40:31 [12110] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/13/16 14:40:31 [12110] Using job type INFNBatch for job 64.0
01/13/16 14:40:31 [12110] (64.0) SetJobLeaseTimers()
01/13/16 14:40:31 [12110] Found job 64.0 --- inserting
01/13/16 14:40:31 [12110] Fetched 1 new job ads from schedd
01/13/16 14:40:31 [12110] querying for removed/held jobs
01/13/16 14:40:31 [12110] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 14:40:31 [12110] Fetched 0 job ads from schedd
01/13/16 14:40:31 [12110] leaving doContactSchedd()
01/13/16 14:40:31 [12110] gahp server not up yet, delaying ping
01/13/16 14:40:31 [12110] *** UpdateLeases called
01/13/16 14:40:31 [12110]     Leases not supported, cancelling timer
01/13/16 14:40:31 [12110] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=30975_7085_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=30906_049e_4>"
CurrentTime = time()
MyCurrentTime = 1452714031
IdleJobs = 1
JobLimit = 10000

01/13/16 14:40:31 [12110] Trying to update collector <10.31.131.202:9619>
01/13/16 14:40:31 [12110] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 14:40:31 [12110] File descriptor limits: max 4096, safe 3277
01/13/16 14:40:31 [12110] (64.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/13/16 14:40:31 [12110] GAHP server pid = 12527
01/13/16 14:40:31 [12110] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/13/16 14:40:31 [12110] GAHP[12527] <- 'COMMANDS'
01/13/16 14:40:31 [12110] GAHP[12527] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/13/16 14:40:31 [12110] GAHP[12527] <- 'ASYNC_MODE_ON'
01/13/16 14:40:31 [12110] GAHP[12527] -> 'S' 'Async mode on'
01/13/16 14:40:31 [12110] (64.0) gm state change: GM_INIT -> GM_START
01/13/16 14:40:31 [12110] (64.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/13/16 14:40:31 [12110] (64.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/13/16 14:40:31 [12110] (64.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/13/16 14:40:31 [12110] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/13/16 14:40:31 [12110] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/13/16 14:40:31 [12110] IPVERIFY: ip found is 1
01/13/16 14:40:33 [12110] Evaluating staleness of remote job statuses.
01/13/16 14:40:36 [12110] resource  is now up
01/13/16 14:40:36 [12110] in doContactSchedd()
01/13/16 14:40:36 [12110] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 14:40:36 [12110] querying for removed/held jobs
01/13/16 14:40:36 [12110] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 14:40:36 [12110] Fetched 0 job ads from schedd
01/13/16 14:40:36 [12110] Updating classad values for 64.0:
01/13/16 14:40:36 [12110]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#64.0#1452713736"
01/13/16 14:40:36 [12110]    LastRemoteStatusUpdate = 1452714031
01/13/16 14:40:36 [12110] leaving doContactSchedd()
01/13/16 14:40:36 [12110] (64.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/13/16 14:40:36 [12110] (64.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/13/16 14:40:36 [12110] (64.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 14:40:36 [12110] GAHP[12527] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/63/0/cluster63.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/63/0/cluster63.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/63/0/cluster63.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#64.0#1452713736";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 14:40:36 [12110] GAHP[12527] -> 'S'
01/13/16 14:40:37 [12110] GAHP[12527] <- 'RESULTS'
01/13/16 14:40:37 [12110] GAHP[12527] -> 'R'
01/13/16 14:40:37 [12110] GAHP[12527] -> 'S' '1'
01/13/16 14:40:37 [12110] GAHP[12527] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/13/16 14:40:37 [12110] (64.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/13/16 14:40:37 [12110] (64.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/13/16 14:40:37 [12110] (64.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/13/16 14:40:37 [12110] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 14:40:50 [12110] (64.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/13/16 14:40:50 [12110] in doContactSchedd()
01/13/16 14:40:50 [12110] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 14:40:50 [12110] querying for removed/held jobs
01/13/16 14:40:50 [12110] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 14:40:50 [12110] Fetched 0 job ads from schedd
01/13/16 14:40:50 [12110] Updating classad values for 64.0:
01/13/16 14:40:50 [12110]    CurrentStatusUnknown = false
01/13/16 14:40:50 [12110]    GridJobId = undefined
01/13/16 14:40:50 [12110]    LastRemoteStatusUpdate = 0
01/13/16 14:40:50 [12110] leaving doContactSchedd()
01/13/16 14:40:50 [12110] (64.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/13/16 14:40:50 [12110] (64.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/13/16 14:40:50 [12110] (64.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/13/16 14:40:55 [12110] in doContactSchedd()
01/13/16 14:40:55 [12110] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 14:40:55 [12110] querying for removed/held jobs
01/13/16 14:40:55 [12110] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 14:40:55 [12110] Fetched 0 job ads from schedd
01/13/16 14:40:55 [12110] Updating classad values for 64.0:
01/13/16 14:40:55 [12110]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#64.0#1452713736"
01/13/16 14:40:55 [12110]    LastRemoteStatusUpdate = 1452714050
01/13/16 14:40:55 [12110] leaving doContactSchedd()
01/13/16 14:40:55 [12110] (64.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/13/16 14:40:55 [12110] (64.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/13/16 14:40:55 [12110] (64.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/13/16 14:40:55 [12110] (64.0) gm state change: GM_HOLD -> GM_DELETE
01/13/16 14:41:00 [12110] in doContactSchedd()
01/13/16 14:41:00 [12110] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 14:41:00 [12110] querying for removed/held jobs
01/13/16 14:41:00 [12110] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 14:41:00 [12110] Fetched 0 job ads from schedd
01/13/16 14:41:00 [12110] Updating classad values for 64.0:
01/13/16 14:41:00 [12110]    EnteredCurrentStatus = 1452714055
01/13/16 14:41:00 [12110]    HoldReason = "Attempts to submit failed: "
01/13/16 14:41:00 [12110]    HoldReasonCode = 0
01/13/16 14:41:00 [12110]    HoldReasonSubCode = 0
01/13/16 14:41:00 [12110]    JobStatus = 5
01/13/16 14:41:00 [12110]    LastReleaseReason = "Data files spooled"
01/13/16 14:41:00 [12110]    Managed = "Schedd"
01/13/16 14:41:00 [12110]    NumSystemHolds = 1
01/13/16 14:41:00 [12110]    ReleaseReason = undefined
01/13/16 14:41:00 [12110] No jobs left, shutting down
01/13/16 14:41:00 [12110] leaving doContactSchedd()
01/13/16 14:41:00 [12110] Got SIGTERM. Performing graceful shutdown.
01/13/16 14:41:00 [12110] Started timer to call main_shutdown_fast in 1800 seconds
01/13/16 14:41:00 [12110] **** condor_gridmanager (condor_GRIDMANAGER) pid 12110 EXITING WITH STATUS 0
01/13/16 16:23:29 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/13/16 16:23:29 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/13/16 16:23:29 Enumerating interfaces: lo 127.0.0.1 up
01/13/16 16:23:29 Enumerating interfaces: eth2 10.31.131.202 up
01/13/16 16:23:29 Enumerating interfaces: eth3 140.247.179.131 up
01/13/16 16:23:29 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/13/16 16:23:29 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/13/16 16:23:29 ******************************************************
01/13/16 16:23:29 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/13/16 16:23:29 ** /usr/sbin/condor_gridmanager
01/13/16 16:23:29 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/13/16 16:23:29 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/13/16 16:23:29 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/13/16 16:23:29 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/13/16 16:23:29 ** PID = 26795
01/13/16 16:23:29 ** Log last touched 1/13 14:41:00
01/13/16 16:23:29 ******************************************************
01/13/16 16:23:29 Using config source: /etc/condor-ce/condor_config
01/13/16 16:23:29 Using local config sources: 
01/13/16 16:23:29    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/13/16 16:23:29    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/01-ce-auth.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/01-ce-router.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/01-common-auth.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/02-ce-lsf.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/02-ce-pbs.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/03-managed-fork.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/50-osg-configure.conf
01/13/16 16:23:29    /etc/condor-ce/config.d/99-local.conf
01/13/16 16:23:29    /usr/share/condor-ce/condor_ce_router_defaults|
01/13/16 16:23:29 config Macros = 142, Sorted = 142, StringBytes = 12580, TablesBytes = 5320
01/13/16 16:23:29 CLASSAD_CACHING is ENABLED
01/13/16 16:23:29 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/13/16 16:23:29 SharedPortEndpoint: waiting for connections to named socket 30975_7085_2
01/13/16 16:23:29 DaemonCore: command socket at <140.247.179.131:9620?sock=30975_7085_2>
01/13/16 16:23:29 DaemonCore: private command socket at <140.247.179.131:9620?sock=30975_7085_2>
01/13/16 16:23:29 Setting maximum accepts per cycle 8.
01/13/16 16:23:29 Setting maximum reaps per cycle 8.
01/13/16 16:23:29 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 16:23:29 [26795] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/13/16 16:23:29 [26795] DaemonCore: No more children processes to reap.
01/13/16 16:23:29 [26795] DaemonCore: in SendAliveToParent()
01/13/16 16:23:29 [26795] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 16:23:29 [26795] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/13/16 16:23:29 [26795] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/13/16 16:23:29 [26795] IPVERIFY: ip found is 0
01/13/16 16:23:29 [26795] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/13/16 16:23:29 [26795] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/13/16 16:23:29 [26795] Buf::write(): condor_write() failed
01/13/16 16:23:29 [26795] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/13/16 16:23:29 [26795] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 16:23:29 [26795] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/13/16 16:23:29 [26795] DaemonCore: Leaving SendAliveToParent() - success
01/13/16 16:23:29 [26795] Checking proxies
01/13/16 16:23:31 [26795] Received ADD_JOBS signal
01/13/16 16:23:32 [26795] in doContactSchedd()
01/13/16 16:23:32 [26795] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 16:23:32 [26795] querying for new jobs
01/13/16 16:23:32 [26795] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/13/16 16:23:32 [26795] Using job type INFNBatch for job 66.0
01/13/16 16:23:32 [26795] (66.0) SetJobLeaseTimers()
01/13/16 16:23:32 [26795] Found job 66.0 --- inserting
01/13/16 16:23:32 [26795] Fetched 1 new job ads from schedd
01/13/16 16:23:32 [26795] querying for removed/held jobs
01/13/16 16:23:32 [26795] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:23:32 [26795] Fetched 0 job ads from schedd
01/13/16 16:23:32 [26795] leaving doContactSchedd()
01/13/16 16:23:32 [26795] gahp server not up yet, delaying ping
01/13/16 16:23:32 [26795] *** UpdateLeases called
01/13/16 16:23:32 [26795]     Leases not supported, cancelling timer
01/13/16 16:23:32 [26795] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=30975_7085_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=30906_049e_4>"
CurrentTime = time()
MyCurrentTime = 1452720212
IdleJobs = 1
JobLimit = 10000

01/13/16 16:23:32 [26795] Trying to update collector <10.31.131.202:9619>
01/13/16 16:23:32 [26795] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 16:23:32 [26795] File descriptor limits: max 4096, safe 3277
01/13/16 16:23:32 [26795] (66.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/13/16 16:23:32 [26795] GAHP server pid = 26922
01/13/16 16:23:32 [26795] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/13/16 16:23:32 [26795] GAHP[26922] <- 'COMMANDS'
01/13/16 16:23:32 [26795] GAHP[26922] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/13/16 16:23:32 [26795] GAHP[26922] <- 'ASYNC_MODE_ON'
01/13/16 16:23:32 [26795] GAHP[26922] -> 'S' 'Async mode on'
01/13/16 16:23:32 [26795] (66.0) gm state change: GM_INIT -> GM_START
01/13/16 16:23:32 [26795] (66.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/13/16 16:23:32 [26795] (66.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/13/16 16:23:32 [26795] (66.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/13/16 16:23:32 [26795] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/13/16 16:23:32 [26795] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/13/16 16:23:32 [26795] IPVERIFY: ip found is 1
01/13/16 16:23:34 [26795] Evaluating staleness of remote job statuses.
01/13/16 16:23:37 [26795] resource  is now up
01/13/16 16:23:37 [26795] in doContactSchedd()
01/13/16 16:23:37 [26795] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 16:23:37 [26795] querying for removed/held jobs
01/13/16 16:23:37 [26795] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:23:37 [26795] Fetched 0 job ads from schedd
01/13/16 16:23:37 [26795] Updating classad values for 66.0:
01/13/16 16:23:37 [26795]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#66.0#1452720204"
01/13/16 16:23:37 [26795]    LastRemoteStatusUpdate = 1452720212
01/13/16 16:23:37 [26795] leaving doContactSchedd()
01/13/16 16:23:37 [26795] (66.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/13/16 16:23:37 [26795] (66.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/13/16 16:23:37 [26795] (66.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 16:23:37 [26795] GAHP[26922] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/65/0/cluster65.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/65/0/cluster65.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/65/0/cluster65.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#66.0#1452720204";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 16:23:37 [26795] GAHP[26922] -> 'S'
01/13/16 16:23:40 [26795] GAHP[26922] <- 'RESULTS'
01/13/16 16:23:40 [26795] GAHP[26922] -> 'R'
01/13/16 16:23:40 [26795] GAHP[26922] -> 'S' '1'
01/13/16 16:23:40 [26795] GAHP[26922] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/13/16 16:23:40 [26795] (66.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/13/16 16:23:40 [26795] (66.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/13/16 16:23:40 [26795] (66.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/13/16 16:23:40 [26795] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 16:23:47 [26795] (66.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/13/16 16:23:47 [26795] in doContactSchedd()
01/13/16 16:23:47 [26795] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 16:23:47 [26795] querying for removed/held jobs
01/13/16 16:23:47 [26795] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:23:47 [26795] Fetched 0 job ads from schedd
01/13/16 16:23:47 [26795] Updating classad values for 66.0:
01/13/16 16:23:47 [26795]    CurrentStatusUnknown = false
01/13/16 16:23:47 [26795]    GridJobId = undefined
01/13/16 16:23:47 [26795]    LastRemoteStatusUpdate = 0
01/13/16 16:23:47 [26795] leaving doContactSchedd()
01/13/16 16:23:47 [26795] (66.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/13/16 16:23:47 [26795] (66.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/13/16 16:23:47 [26795] (66.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/13/16 16:23:52 [26795] in doContactSchedd()
01/13/16 16:23:52 [26795] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 16:23:52 [26795] querying for removed/held jobs
01/13/16 16:23:52 [26795] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:23:52 [26795] Fetched 0 job ads from schedd
01/13/16 16:23:52 [26795] Updating classad values for 66.0:
01/13/16 16:23:52 [26795]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#66.0#1452720204"
01/13/16 16:23:52 [26795]    LastRemoteStatusUpdate = 1452720227
01/13/16 16:23:52 [26795] leaving doContactSchedd()
01/13/16 16:23:52 [26795] (66.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/13/16 16:23:52 [26795] (66.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/13/16 16:23:52 [26795] (66.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/13/16 16:23:52 [26795] (66.0) gm state change: GM_HOLD -> GM_DELETE
01/13/16 16:23:57 [26795] in doContactSchedd()
01/13/16 16:23:57 [26795] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 30906_049e_4
01/13/16 16:23:57 [26795] querying for removed/held jobs
01/13/16 16:23:57 [26795] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:23:57 [26795] Fetched 0 job ads from schedd
01/13/16 16:23:57 [26795] Updating classad values for 66.0:
01/13/16 16:23:57 [26795]    EnteredCurrentStatus = 1452720232
01/13/16 16:23:57 [26795]    HoldReason = "Attempts to submit failed: "
01/13/16 16:23:57 [26795]    HoldReasonCode = 0
01/13/16 16:23:57 [26795]    HoldReasonSubCode = 0
01/13/16 16:23:57 [26795]    JobStatus = 5
01/13/16 16:23:57 [26795]    LastReleaseReason = "Data files spooled"
01/13/16 16:23:57 [26795]    Managed = "Schedd"
01/13/16 16:23:57 [26795]    NumSystemHolds = 1
01/13/16 16:23:57 [26795]    ReleaseReason = undefined
01/13/16 16:23:57 [26795] No jobs left, shutting down
01/13/16 16:23:57 [26795] leaving doContactSchedd()
01/13/16 16:23:57 [26795] Got SIGTERM. Performing graceful shutdown.
01/13/16 16:23:57 [26795] Started timer to call main_shutdown_fast in 1800 seconds
01/13/16 16:23:57 [26795] **** condor_gridmanager (condor_GRIDMANAGER) pid 26795 EXITING WITH STATUS 0
01/13/16 16:38:27 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/13/16 16:38:27 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/13/16 16:38:27 Enumerating interfaces: lo 127.0.0.1 up
01/13/16 16:38:27 Enumerating interfaces: eth2 10.31.131.202 up
01/13/16 16:38:27 Enumerating interfaces: eth3 140.247.179.131 up
01/13/16 16:38:27 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/13/16 16:38:27 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/13/16 16:38:27 ******************************************************
01/13/16 16:38:27 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/13/16 16:38:27 ** /usr/sbin/condor_gridmanager
01/13/16 16:38:27 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/13/16 16:38:27 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/13/16 16:38:27 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/13/16 16:38:27 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/13/16 16:38:27 ** PID = 16033
01/13/16 16:38:27 ** Log last touched 1/13 16:23:57
01/13/16 16:38:27 ******************************************************
01/13/16 16:38:27 Using config source: /etc/condor-ce/condor_config
01/13/16 16:38:27 Using local config sources: 
01/13/16 16:38:27    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/13/16 16:38:27    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/01-ce-auth.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/01-ce-router.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/01-common-auth.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/02-ce-lsf.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/02-ce-pbs.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/03-managed-fork.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/50-osg-configure.conf
01/13/16 16:38:27    /etc/condor-ce/config.d/99-local.conf
01/13/16 16:38:27    /usr/share/condor-ce/condor_ce_router_defaults|
01/13/16 16:38:27 config Macros = 142, Sorted = 142, StringBytes = 12577, TablesBytes = 5320
01/13/16 16:38:27 CLASSAD_CACHING is ENABLED
01/13/16 16:38:27 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/13/16 16:38:27 SharedPortEndpoint: waiting for connections to named socket 4391_9b4c_1
01/13/16 16:38:27 DaemonCore: command socket at <140.247.179.131:9620?sock=4391_9b4c_1>
01/13/16 16:38:27 DaemonCore: private command socket at <140.247.179.131:9620?sock=4391_9b4c_1>
01/13/16 16:38:27 Setting maximum accepts per cycle 8.
01/13/16 16:38:27 Setting maximum reaps per cycle 8.
01/13/16 16:38:27 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 16:38:27 [16033] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/13/16 16:38:27 [16033] DaemonCore: No more children processes to reap.
01/13/16 16:38:27 [16033] DaemonCore: in SendAliveToParent()
01/13/16 16:38:27 [16033] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:38:27 [16033] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/13/16 16:38:27 [16033] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/13/16 16:38:27 [16033] IPVERIFY: ip found is 0
01/13/16 16:38:27 [16033] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/13/16 16:38:27 [16033] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/13/16 16:38:27 [16033] Buf::write(): condor_write() failed
01/13/16 16:38:27 [16033] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/13/16 16:38:27 [16033] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:38:27 [16033] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/13/16 16:38:27 [16033] DaemonCore: Leaving SendAliveToParent() - success
01/13/16 16:38:27 [16033] Checking proxies
01/13/16 16:38:30 [16033] Received ADD_JOBS signal
01/13/16 16:38:30 [16033] in doContactSchedd()
01/13/16 16:38:30 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:38:30 [16033] querying for new jobs
01/13/16 16:38:30 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/13/16 16:38:30 [16033] Using job type INFNBatch for job 68.0
01/13/16 16:38:30 [16033] (68.0) SetJobLeaseTimers()
01/13/16 16:38:30 [16033] Found job 68.0 --- inserting
01/13/16 16:38:30 [16033] Fetched 1 new job ads from schedd
01/13/16 16:38:30 [16033] querying for removed/held jobs
01/13/16 16:38:30 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:38:30 [16033] Fetched 0 job ads from schedd
01/13/16 16:38:30 [16033] leaving doContactSchedd()
01/13/16 16:38:30 [16033] gahp server not up yet, delaying ping
01/13/16 16:38:30 [16033] *** UpdateLeases called
01/13/16 16:38:30 [16033]     Leases not supported, cancelling timer
01/13/16 16:38:30 [16033] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch LSF"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=4391_9b4c_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=4032_0830_4>"
CurrentTime = time()
MyCurrentTime = 1452721110
IdleJobs = 1
JobLimit = 10000

01/13/16 16:38:30 [16033] Trying to update collector <10.31.131.202:9619>
01/13/16 16:38:30 [16033] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/13/16 16:38:30 [16033] File descriptor limits: max 4096, safe 3277
01/13/16 16:38:30 [16033] (68.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/13/16 16:38:30 [16033] GAHP server pid = 16105
01/13/16 16:38:30 [16033] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/13/16 16:38:30 [16033] GAHP[16105] <- 'COMMANDS'
01/13/16 16:38:30 [16033] GAHP[16105] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/13/16 16:38:30 [16033] GAHP[16105] <- 'ASYNC_MODE_ON'
01/13/16 16:38:30 [16033] GAHP[16105] -> 'S' 'Async mode on'
01/13/16 16:38:30 [16033] (68.0) gm state change: GM_INIT -> GM_START
01/13/16 16:38:30 [16033] (68.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/13/16 16:38:30 [16033] (68.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/13/16 16:38:30 [16033] (68.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/13/16 16:38:30 [16033] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/13/16 16:38:30 [16033] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/13/16 16:38:30 [16033] IPVERIFY: ip found is 1
01/13/16 16:38:32 [16033] Evaluating staleness of remote job statuses.
01/13/16 16:38:35 [16033] resource  is now up
01/13/16 16:38:35 [16033] in doContactSchedd()
01/13/16 16:38:35 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:38:35 [16033] querying for removed/held jobs
01/13/16 16:38:35 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:38:35 [16033] Fetched 0 job ads from schedd
01/13/16 16:38:35 [16033] Updating classad values for 68.0:
01/13/16 16:38:35 [16033]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#68.0#1452720811"
01/13/16 16:38:35 [16033]    LastRemoteStatusUpdate = 1452721110
01/13/16 16:38:35 [16033] leaving doContactSchedd()
01/13/16 16:38:35 [16033] (68.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/13/16 16:38:35 [16033] (68.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/13/16 16:38:35 [16033] (68.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/13/16 16:38:35 [16033] GAHP[16105] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "lsf";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-lsf'";\ GridResource\ =\ "batch\ lsf";\ x509userproxy\ =\ "/n/atlasgrid/condor/67/0/cluster67.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/67/0/cluster67.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/67/0/cluster67.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#68.0#1452720811";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/13/16 16:38:35 [16033] GAHP[16105] -> 'S'
01/13/16 16:38:46 [16033] GAHP[16105] <- 'RESULTS'
01/13/16 16:38:46 [16033] GAHP[16105] -> 'R'
01/13/16 16:38:46 [16033] GAHP[16105] -> 'S' '1'
01/13/16 16:38:46 [16033] GAHP[16105] -> '2' '0' 'No error' 'lsf/20160113/542961600'
01/13/16 16:38:46 [16033] (68.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/13/16 16:38:46 [16033] (68.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
01/13/16 16:38:46 [16033] in doContactSchedd()
01/13/16 16:38:46 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:38:46 [16033] querying for removed/held jobs
01/13/16 16:38:46 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:38:46 [16033] Fetched 0 job ads from schedd
01/13/16 16:38:46 [16033] Updating classad values for 68.0:
01/13/16 16:38:46 [16033]    DelegatedProxyExpiration = 1452753621
01/13/16 16:38:46 [16033]    GridJobId = "batch lsf net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#68.0#1452720811 lsf/20160113/542961600"
01/13/16 16:38:46 [16033] leaving doContactSchedd()
01/13/16 16:38:46 [16033] (68.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
01/13/16 16:38:46 [16033] (68.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
01/13/16 16:39:27 [16033] Received CHECK_LEASES signal
01/13/16 16:39:27 [16033] in doContactSchedd()
01/13/16 16:39:27 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:39:27 [16033] querying for renewed leases
01/13/16 16:39:27 [16033] querying for removed/held jobs
01/13/16 16:39:27 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:39:27 [16033] Fetched 0 job ads from schedd
01/13/16 16:39:27 [16033] leaving doContactSchedd()
01/13/16 16:39:30 [16033] GAHP[16105] <- 'RESULTS'
01/13/16 16:39:30 [16033] GAHP[16105] -> 'S' '0'
01/13/16 16:39:32 [16033] Evaluating staleness of remote job statuses.
01/13/16 16:39:46 [16033] (68.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
01/13/16 16:39:46 [16033] (68.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/13/16 16:39:46 [16033] GAHP[16105] <- 'BLAH_JOB_STATUS 3 lsf/20160113/542961600'
01/13/16 16:39:46 [16033] GAHP[16105] -> 'S'
01/13/16 16:39:49 [16033] GAHP[16105] <- 'RESULTS'
01/13/16 16:39:49 [16033] GAHP[16105] -> 'R'
01/13/16 16:39:49 [16033] GAHP[16105] -> 'S' '1'
01/13/16 16:39:49 [16033] GAHP[16105] -> '3' '0' 'No Error' '1' '[ BatchjobId = "542961600"; JobStatus = 1 ]'
01/13/16 16:39:49 [16033] (68.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
01/13/16 16:39:49 [16033] (68.0) ***ProcessRemoteAd
01/13/16 16:39:49 [16033] (68.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/13/16 16:39:49 [16033] in doContactSchedd()
01/13/16 16:39:49 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:39:49 [16033] querying for removed/held jobs
01/13/16 16:39:49 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:39:49 [16033] Fetched 0 job ads from schedd
01/13/16 16:39:49 [16033] Updating classad values for 68.0:
01/13/16 16:39:49 [16033]    GridJobStatus = "IDLE"
01/13/16 16:39:49 [16033]    LastRemoteStatusUpdate = 1452721189
01/13/16 16:39:49 [16033] leaving doContactSchedd()
01/13/16 16:40:27 [16033] Received CHECK_LEASES signal
01/13/16 16:40:27 [16033] in doContactSchedd()
01/13/16 16:40:27 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:40:27 [16033] querying for renewed leases
01/13/16 16:40:27 [16033] querying for removed/held jobs
01/13/16 16:40:27 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:40:27 [16033] Fetched 0 job ads from schedd
01/13/16 16:40:27 [16033] leaving doContactSchedd()
01/13/16 16:40:30 [16033] GAHP[16105] <- 'RESULTS'
01/13/16 16:40:30 [16033] GAHP[16105] -> 'S' '0'
01/13/16 16:40:32 [16033] Evaluating staleness of remote job statuses.
01/13/16 16:40:49 [16033] (68.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 1
01/13/16 16:40:49 [16033] (68.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
01/13/16 16:40:49 [16033] GAHP[16105] <- 'BLAH_JOB_STATUS 4 lsf/20160113/542961600'
01/13/16 16:40:49 [16033] GAHP[16105] -> 'S'
01/13/16 16:40:51 [16033] GAHP[16105] <- 'RESULTS'
01/13/16 16:40:51 [16033] GAHP[16105] -> 'R'
01/13/16 16:40:51 [16033] GAHP[16105] -> 'S' '1'
01/13/16 16:40:51 [16033] GAHP[16105] -> '4' '0' 'No Error' '4' '[ BatchjobId = "542961600"; JobStatus = 4; ExitCode = 0; WorkerNode = "camd11" ]'
01/13/16 16:40:51 [16033] (68.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 1
01/13/16 16:40:51 [16033] (68.0) ***ProcessRemoteAd
01/13/16 16:40:51 [16033] (68.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
01/13/16 16:40:51 [16033] (68.0) gm state change: GM_SUBMITTED -> GM_TRANSFER_OUTPUT
01/13/16 16:40:51 [16033] (68.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE
01/13/16 16:40:51 [16033] in doContactSchedd()
01/13/16 16:40:51 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:40:51 [16033] querying for removed/held jobs
01/13/16 16:40:51 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:40:51 [16033] Fetched 0 job ads from schedd
01/13/16 16:40:51 [16033] Updating classad values for 68.0:
01/13/16 16:40:51 [16033]    EnteredCurrentStatus = 1452721251
01/13/16 16:40:51 [16033]    ExitCode = 0
01/13/16 16:40:51 [16033]    GridJobStatus = "COMPLETED"
01/13/16 16:40:51 [16033]    JobStatus = 4
01/13/16 16:40:51 [16033]    LastRemoteStatusUpdate = 1452721251
01/13/16 16:40:52 [16033] leaving doContactSchedd()
01/13/16 16:40:52 [16033] (68.0) doEvaluateState called: gmState GM_DONE_SAVE, remoteState 4
01/13/16 16:40:52 [16033] (68.0) gm state change: GM_DONE_SAVE -> GM_DONE_COMMIT
01/13/16 16:40:52 [16033] (68.0) gm state change: GM_DONE_COMMIT -> GM_DELETE_SANDBOX
01/13/16 16:40:52 [16033] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/13/16 16:40:52 [16033] (68.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/13/16 16:40:57 [16033] in doContactSchedd()
01/13/16 16:40:57 [16033] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4032_0830_4
01/13/16 16:40:57 [16033] querying for removed/held jobs
01/13/16 16:40:57 [16033] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/13/16 16:40:57 [16033] Fetched 1 job ads from schedd
01/13/16 16:40:57 [16033] Updating classad values for 68.0:
01/13/16 16:40:57 [16033]    CurrentStatusUnknown = false
01/13/16 16:40:57 [16033]    GridJobId = undefined
01/13/16 16:40:57 [16033]    LastRemoteStatusUpdate = 0
01/13/16 16:40:57 [16033]    Managed = "ScheddDone"
01/13/16 16:40:57 [16033] Deleting job 68.0 from schedd
01/13/16 16:40:57 [16033] No jobs left, shutting down
01/13/16 16:40:57 [16033] leaving doContactSchedd()
01/13/16 16:40:57 [16033] Got SIGTERM. Performing graceful shutdown.
01/13/16 16:40:57 [16033] Started timer to call main_shutdown_fast in 1800 seconds
01/13/16 16:40:57 [16033] **** condor_gridmanager (condor_GRIDMANAGER) pid 16033 EXITING WITH STATUS 0
01/14/16 10:09:53 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/14/16 10:09:53 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/14/16 10:09:53 Enumerating interfaces: lo 127.0.0.1 up
01/14/16 10:09:53 Enumerating interfaces: eth2 10.31.131.202 up
01/14/16 10:09:53 Enumerating interfaces: eth3 140.247.179.131 up
01/14/16 10:09:53 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/14/16 10:09:53 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/14/16 10:09:53 ******************************************************
01/14/16 10:09:53 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/14/16 10:09:53 ** /usr/sbin/condor_gridmanager
01/14/16 10:09:53 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/14/16 10:09:53 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/14/16 10:09:53 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/14/16 10:09:53 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/14/16 10:09:53 ** PID = 23548
01/14/16 10:09:53 ** Log last touched 1/13 16:40:57
01/14/16 10:09:53 ******************************************************
01/14/16 10:09:53 Using config source: /etc/condor-ce/condor_config
01/14/16 10:09:53 Using local config sources: 
01/14/16 10:09:53    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/14/16 10:09:53    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/01-ce-auth.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/01-ce-router.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/01-common-auth.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/02-ce-lsf.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/02-ce-pbs.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/03-managed-fork.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/50-osg-configure.conf
01/14/16 10:09:53    /etc/condor-ce/config.d/99-local.conf
01/14/16 10:09:53    /usr/share/condor-ce/condor_ce_router_defaults|
01/14/16 10:09:53 config Macros = 142, Sorted = 142, StringBytes = 12510, TablesBytes = 5320
01/14/16 10:09:53 CLASSAD_CACHING is ENABLED
01/14/16 10:09:53 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/14/16 10:09:53 SharedPortEndpoint: waiting for connections to named socket 10881_6614_1
01/14/16 10:09:53 DaemonCore: command socket at <140.247.179.131:9620?sock=10881_6614_1>
01/14/16 10:09:53 DaemonCore: private command socket at <140.247.179.131:9620?sock=10881_6614_1>
01/14/16 10:09:53 Setting maximum accepts per cycle 8.
01/14/16 10:09:53 Setting maximum reaps per cycle 8.
01/14/16 10:09:53 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/14/16 10:09:53 [23548] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/14/16 10:09:53 [23548] DaemonCore: No more children processes to reap.
01/14/16 10:09:53 [23548] DaemonCore: in SendAliveToParent()
01/14/16 10:09:53 [23548] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 10697_c58b_4
01/14/16 10:10:01 [23548] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/14/16 10:10:01 [23548] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/14/16 10:10:01 [23548] IPVERIFY: ip found is 0
01/14/16 10:10:01 [23548] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/14/16 10:10:01 [23548] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/14/16 10:10:01 [23548] Buf::write(): condor_write() failed
01/14/16 10:10:01 [23548] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/14/16 10:10:01 [23548] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 10697_c58b_4
01/14/16 10:10:01 [23548] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/14/16 10:10:01 [23548] DaemonCore: Leaving SendAliveToParent() - success
01/14/16 10:10:01 [23548] Checking proxies
01/14/16 10:10:01 [23548] Received ADD_JOBS signal
01/14/16 10:10:01 [23548] Evaluating staleness of remote job statuses.
01/14/16 10:10:01 [23548] in doContactSchedd()
01/14/16 10:10:01 [23548] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 10697_c58b_4
01/14/16 10:10:01 [23548] querying for new jobs
01/14/16 10:10:01 [23548] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/14/16 10:10:01 [23548] Using job type INFNBatch for job 70.0
01/14/16 10:10:01 [23548] (70.0) SetJobLeaseTimers()
01/14/16 10:10:01 [23548] Found job 70.0 --- inserting
01/14/16 10:10:01 [23548] Fetched 1 new job ads from schedd
01/14/16 10:10:01 [23548] querying for removed/held jobs
01/14/16 10:10:01 [23548] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/14/16 10:10:01 [23548] Fetched 0 job ads from schedd
01/14/16 10:10:01 [23548] leaving doContactSchedd()
01/14/16 10:10:01 [23548] gahp server not up yet, delaying ping
01/14/16 10:10:01 [23548] *** UpdateLeases called
01/14/16 10:10:01 [23548]     Leases not supported, cancelling timer
01/14/16 10:10:01 [23548] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=10881_6614_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=10697_c58b_4>"
CurrentTime = time()
MyCurrentTime = 1452784201
IdleJobs = 1
JobLimit = 10000

01/14/16 10:10:01 [23548] Trying to update collector <10.31.131.202:9619>
01/14/16 10:10:01 [23548] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/14/16 10:10:01 [23548] File descriptor limits: max 4096, safe 3277
01/14/16 10:10:01 [23548] (70.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/14/16 10:10:01 [23548] GAHP server pid = 23930
01/14/16 10:10:01 [23548] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/14/16 10:10:01 [23548] GAHP[23930] <- 'COMMANDS'
01/14/16 10:10:01 [23548] GAHP[23930] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/14/16 10:10:01 [23548] GAHP[23930] <- 'ASYNC_MODE_ON'
01/14/16 10:10:01 [23548] GAHP[23930] -> 'S' 'Async mode on'
01/14/16 10:10:01 [23548] (70.0) gm state change: GM_INIT -> GM_START
01/14/16 10:10:01 [23548] (70.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/14/16 10:10:01 [23548] (70.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/14/16 10:10:01 [23548] (70.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/14/16 10:10:01 [23548] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/14/16 10:10:01 [23548] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/14/16 10:10:01 [23548] IPVERIFY: ip found is 1
01/14/16 10:10:06 [23548] resource  is now up
01/14/16 10:10:06 [23548] in doContactSchedd()
01/14/16 10:10:06 [23548] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 10697_c58b_4
01/14/16 10:10:06 [23548] querying for removed/held jobs
01/14/16 10:10:06 [23548] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/14/16 10:10:06 [23548] Fetched 0 job ads from schedd
01/14/16 10:10:06 [23548] Updating classad values for 70.0:
01/14/16 10:10:06 [23548]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#70.0#1452784191"
01/14/16 10:10:06 [23548]    LastRemoteStatusUpdate = 1452784201
01/14/16 10:10:06 [23548] leaving doContactSchedd()
01/14/16 10:10:06 [23548] (70.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/14/16 10:10:06 [23548] (70.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/14/16 10:10:06 [23548] (70.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/14/16 10:10:06 [23548] GAHP[23930] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#70.0#1452784191";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/14/16 10:10:06 [23548] GAHP[23930] -> 'S'
01/14/16 10:10:07 [23548] GAHP[23930] <- 'RESULTS'
01/14/16 10:10:07 [23548] GAHP[23930] -> 'R'
01/14/16 10:10:07 [23548] GAHP[23930] -> 'S' '1'
01/14/16 10:10:07 [23548] GAHP[23930] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/14/16 10:10:07 [23548] (70.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/14/16 10:10:07 [23548] (70.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/14/16 10:10:07 [23548] (70.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/14/16 10:10:07 [23548] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/14/16 10:10:11 [23548] (70.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/14/16 10:10:11 [23548] in doContactSchedd()
01/14/16 10:10:11 [23548] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 10697_c58b_4
01/14/16 10:10:11 [23548] querying for removed/held jobs
01/14/16 10:10:11 [23548] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/14/16 10:10:11 [23548] Fetched 0 job ads from schedd
01/14/16 10:10:11 [23548] Updating classad values for 70.0:
01/14/16 10:10:11 [23548]    CurrentStatusUnknown = false
01/14/16 10:10:11 [23548]    GridJobId = undefined
01/14/16 10:10:11 [23548]    LastRemoteStatusUpdate = 0
01/14/16 10:10:11 [23548] leaving doContactSchedd()
01/14/16 10:10:11 [23548] (70.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/14/16 10:10:11 [23548] (70.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/14/16 10:10:11 [23548] (70.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/14/16 10:10:16 [23548] in doContactSchedd()
01/14/16 10:10:16 [23548] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 10697_c58b_4
01/14/16 10:10:16 [23548] querying for removed/held jobs
01/14/16 10:10:16 [23548] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/14/16 10:10:16 [23548] Fetched 0 job ads from schedd
01/14/16 10:10:16 [23548] Updating classad values for 70.0:
01/14/16 10:10:16 [23548]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#70.0#1452784191"
01/14/16 10:10:16 [23548]    LastRemoteStatusUpdate = 1452784211
01/14/16 10:10:16 [23548] leaving doContactSchedd()
01/14/16 10:10:16 [23548] (70.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/14/16 10:10:16 [23548] (70.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/14/16 10:10:16 [23548] (70.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/14/16 10:10:16 [23548] (70.0) gm state change: GM_HOLD -> GM_DELETE
01/14/16 10:10:21 [23548] in doContactSchedd()
01/14/16 10:10:21 [23548] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 10697_c58b_4
01/14/16 10:10:21 [23548] querying for removed/held jobs
01/14/16 10:10:21 [23548] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/14/16 10:10:21 [23548] Fetched 0 job ads from schedd
01/14/16 10:10:21 [23548] Updating classad values for 70.0:
01/14/16 10:10:21 [23548]    EnteredCurrentStatus = 1452784216
01/14/16 10:10:21 [23548]    HoldReason = "Attempts to submit failed: "
01/14/16 10:10:21 [23548]    HoldReasonCode = 0
01/14/16 10:10:21 [23548]    HoldReasonSubCode = 0
01/14/16 10:10:21 [23548]    JobStatus = 5
01/14/16 10:10:21 [23548]    LastReleaseReason = "Data files spooled"
01/14/16 10:10:21 [23548]    Managed = "Schedd"
01/14/16 10:10:21 [23548]    NumSystemHolds = 1
01/14/16 10:10:21 [23548]    ReleaseReason = undefined
01/14/16 10:10:21 [23548] No jobs left, shutting down
01/14/16 10:10:21 [23548] leaving doContactSchedd()
01/14/16 10:10:21 [23548] Got SIGTERM. Performing graceful shutdown.
01/14/16 10:10:21 [23548] Started timer to call main_shutdown_fast in 1800 seconds
01/14/16 10:10:21 [23548] **** condor_gridmanager (condor_GRIDMANAGER) pid 23548 EXITING WITH STATUS 0
01/15/16 15:24:45 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/15/16 15:24:45 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/15/16 15:24:45 Enumerating interfaces: lo 127.0.0.1 up
01/15/16 15:24:45 Enumerating interfaces: eth2 10.31.131.202 up
01/15/16 15:24:45 Enumerating interfaces: eth3 140.247.179.131 up
01/15/16 15:24:45 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/15/16 15:24:45 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/15/16 15:24:45 ******************************************************
01/15/16 15:24:45 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/15/16 15:24:45 ** /usr/sbin/condor_gridmanager
01/15/16 15:24:45 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/15/16 15:24:45 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/15/16 15:24:45 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/15/16 15:24:45 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/15/16 15:24:45 ** PID = 17172
01/15/16 15:24:45 ** Log last touched 1/14 10:10:21
01/15/16 15:24:45 ******************************************************
01/15/16 15:24:45 Using config source: /etc/condor-ce/condor_config
01/15/16 15:24:45 Using local config sources: 
01/15/16 15:24:45    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/15/16 15:24:45    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/01-ce-auth.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/01-ce-router.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/01-common-auth.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/02-ce-lsf.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/02-ce-pbs.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/03-managed-fork.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/50-osg-configure.conf
01/15/16 15:24:45    /etc/condor-ce/config.d/99-local.conf
01/15/16 15:24:45    /usr/share/condor-ce/condor_ce_router_defaults|
01/15/16 15:24:45 config Macros = 142, Sorted = 142, StringBytes = 12510, TablesBytes = 5320
01/15/16 15:24:45 CLASSAD_CACHING is ENABLED
01/15/16 15:24:45 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/15/16 15:24:45 SharedPortEndpoint: waiting for connections to named socket 11958_3e1f_1
01/15/16 15:24:45 DaemonCore: command socket at <140.247.179.131:9620?sock=11958_3e1f_1>
01/15/16 15:24:45 DaemonCore: private command socket at <140.247.179.131:9620?sock=11958_3e1f_1>
01/15/16 15:24:45 Setting maximum accepts per cycle 8.
01/15/16 15:24:45 Setting maximum reaps per cycle 8.
01/15/16 15:24:45 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/15/16 15:24:45 [17172] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/15/16 15:24:45 [17172] DaemonCore: No more children processes to reap.
01/15/16 15:24:45 [17172] DaemonCore: in SendAliveToParent()
01/15/16 15:24:45 [17172] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/15/16 15:24:46 [17172] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/15/16 15:24:46 [17172] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/15/16 15:24:46 [17172] IPVERIFY: ip found is 0
01/15/16 15:24:46 [17172] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/15/16 15:24:46 [17172] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/15/16 15:24:46 [17172] Buf::write(): condor_write() failed
01/15/16 15:24:46 [17172] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/15/16 15:24:46 [17172] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/15/16 15:24:46 [17172] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/15/16 15:24:46 [17172] DaemonCore: Leaving SendAliveToParent() - success
01/15/16 15:24:46 [17172] Checking proxies
01/15/16 15:24:47 [17172] Received REMOVE_JOBS signal
01/15/16 15:24:47 [17172] in doContactSchedd()
01/15/16 15:24:47 [17172] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/15/16 15:24:47 [17172] querying for new jobs
01/15/16 15:24:47 [17172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/15/16 15:24:47 [17172] Using job type INFNBatch for job 70.0
01/15/16 15:24:47 [17172] (70.0) SetJobLeaseTimers()
01/15/16 15:24:47 [17172] Found job 70.0 --- inserting
01/15/16 15:24:47 [17172] Using job type INFNBatch for job 62.0
01/15/16 15:24:47 [17172] (62.0) SetJobLeaseTimers()
01/15/16 15:24:47 [17172] Failed to get expiration time of proxy /n/atlasgrid/condor/61/0/cluster61.proc0.subproc0/x509up_u556792
01/15/16 15:24:47 [17172] Found job 62.0 --- inserting
01/15/16 15:24:47 [17172] Using job type INFNBatch for job 66.0
01/15/16 15:24:47 [17172] (66.0) SetJobLeaseTimers()
01/15/16 15:24:47 [17172] Found job 66.0 --- inserting
01/15/16 15:24:47 [17172] Using job type INFNBatch for job 72.0
01/15/16 15:24:47 [17172] (72.0) SetJobLeaseTimers()
01/15/16 15:24:47 [17172] Found job 72.0 --- inserting
01/15/16 15:24:47 [17172] Using job type INFNBatch for job 64.0
01/15/16 15:24:47 [17172] (64.0) SetJobLeaseTimers()
01/15/16 15:24:47 [17172] Found job 64.0 --- inserting
01/15/16 15:24:47 [17172] Fetched 5 new job ads from schedd
01/15/16 15:24:47 [17172] querying for removed/held jobs
01/15/16 15:24:47 [17172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/15/16 15:24:47 [17172] Fetched 4 job ads from schedd
01/15/16 15:24:47 [17172] leaving doContactSchedd()
01/15/16 15:24:47 [17172] gahp server not up yet, delaying ping
01/15/16 15:24:47 [17172] *** UpdateLeases called
01/15/16 15:24:47 [17172]     Leases not supported, cancelling timer
01/15/16 15:24:47 [17172] BaseResource::UpdateResource: 
NumJobs = 4
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=11958_3e1f_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=11951_c84b_4>"
CurrentTime = time()
MyCurrentTime = 1452889487
IdleJobs = 1
JobLimit = 10000

01/15/16 15:24:47 [17172] Trying to update collector <10.31.131.202:9619>
01/15/16 15:24:47 [17172] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/15/16 15:24:47 [17172] File descriptor limits: max 4096, safe 3277
01/15/16 15:24:47 [17172] (70.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/15/16 15:24:47 [17172] GAHP server pid = 17179
01/15/16 15:24:47 [17172] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/15/16 15:24:47 [17172] GAHP[17179] <- 'COMMANDS'
01/15/16 15:24:47 [17172] GAHP[17179] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/15/16 15:24:47 [17172] GAHP[17179] <- 'ASYNC_MODE_ON'
01/15/16 15:24:47 [17172] GAHP[17179] -> 'S' 'Async mode on'
01/15/16 15:24:47 [17172] (70.0) gm state change: GM_INIT -> GM_START
01/15/16 15:24:47 [17172] (70.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/15/16 15:24:47 [17172] (70.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/15/16 15:24:47 [17172] GAHP[17179] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#70.0#1452784191";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/15/16 15:24:47 [17172] GAHP[17179] -> 'S'
01/15/16 15:24:47 [17172] gahp server not up yet, delaying ping
01/15/16 15:24:47 [17172] *** UpdateLeases called
01/15/16 15:24:47 [17172]     Leases not supported, cancelling timer
01/15/16 15:24:47 [17172] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch SLURM"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=11958_3e1f_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=11951_c84b_4>"
CurrentTime = time()
MyCurrentTime = 1452889487
IdleJobs = 0
JobLimit = 10000

01/15/16 15:24:47 [17172] Trying to update collector <10.31.131.202:9619>
01/15/16 15:24:47 [17172] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/15/16 15:24:47 [17172] (62.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/15/16 15:24:47 [17172] GAHP server pid = 17188
01/15/16 15:24:48 [17172] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/15/16 15:24:48 [17172] GAHP[17188] <- 'COMMANDS'
01/15/16 15:24:48 [17172] GAHP[17188] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/15/16 15:24:48 [17172] GAHP[17188] <- 'ASYNC_MODE_ON'
01/15/16 15:24:48 [17172] GAHP[17188] -> 'S' 'Async mode on'
01/15/16 15:24:48 [17172] (62.0) gm state change: GM_INIT -> GM_START
01/15/16 15:24:48 [17172] (62.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/15/16 15:24:48 [17172] (62.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/15/16 15:24:48 [17172] GAHP[17188] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "slurm";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ slurm";\ x509userproxy\ =\ "/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#62.0#1452710484";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/15/16 15:24:48 [17172] GAHP[17188] -> 'S'
01/15/16 15:24:48 [17172] Received ADD_JOBS signal
01/15/16 15:24:48 [17172] (66.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/15/16 15:24:48 [17172] (66.0) gm state change: GM_INIT -> GM_START
01/15/16 15:24:48 [17172] (66.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/15/16 15:24:48 [17172] (66.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/15/16 15:24:48 [17172] GAHP[17179] <- 'BLAH_JOB_SUBMIT 3 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/65/0/cluster65.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/65/0/cluster65.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/65/0/cluster65.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#66.0#1452720204";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/15/16 15:24:48 [17172] GAHP[17179] -> 'R'
01/15/16 15:24:48 [17172] GAHP[17179] -> 'S'
01/15/16 15:24:48 [17172] (72.0) doEvaluateState called: gmState GM_INIT, remoteState 0
01/15/16 15:24:48 [17172] (72.0) gm state change: GM_INIT -> GM_START
01/15/16 15:24:48 [17172] (72.0) gm state change: GM_START -> GM_CLEAR_REQUEST
01/15/16 15:24:48 [17172] (72.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/15/16 15:24:48 [17172] (72.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/15/16 15:24:48 [17172] (64.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/15/16 15:24:48 [17172] (64.0) gm state change: GM_INIT -> GM_START
01/15/16 15:24:48 [17172] (64.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/15/16 15:24:48 [17172] (64.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/15/16 15:24:48 [17172] GAHP[17179] <- 'BLAH_JOB_SUBMIT 4 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/63/0/cluster63.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/63/0/cluster63.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/63/0/cluster63.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#64.0#1452713736";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/15/16 15:24:48 [17172] GAHP[17179] -> 'S'
01/15/16 15:24:48 [17172] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/15/16 15:24:48 [17172] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/15/16 15:24:48 [17172] IPVERIFY: ip found is 1
01/15/16 15:24:48 [17172] GAHP[17179] <- 'RESULTS'
01/15/16 15:24:48 [17172] GAHP[17179] -> 'S' '1'
01/15/16 15:24:48 [17172] GAHP[17179] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0/x509up_u556792.lmt): errno=13, Permission denied)' 'N/A'
01/15/16 15:24:48 [17172] GAHP[17188] <- 'RESULTS'
01/15/16 15:24:48 [17172] GAHP[17188] -> 'R'
01/15/16 15:24:48 [17172] GAHP[17188] -> 'S' '1'
01/15/16 15:24:48 [17172] GAHP[17188] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
01/15/16 15:24:48 [17172] (70.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/15/16 15:24:48 [17172] (70.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/69/0/cluster69.proc0.subproc0/x509up_u556792.lmt): errno=13, Permission denied)
01/15/16 15:24:48 [17172] (70.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/15/16 15:24:48 [17172] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/15/16 15:24:51 [17172] (70.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/15/16 15:24:51 [17172] (62.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/15/16 15:24:51 [17172] (62.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/61/0/cluster61.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
01/15/16 15:24:51 [17172] (62.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/15/16 15:24:51 [17172] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/15/16 15:24:51 [17172] (62.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/15/16 15:24:51 [17172] Evaluating staleness of remote job statuses.
01/15/16 15:24:51 [17172] GAHP[17179] <- 'RESULTS'
01/15/16 15:24:51 [17172] GAHP[17179] -> 'R'
01/15/16 15:24:51 [17172] GAHP[17179] -> 'S' '2'
01/15/16 15:24:51 [17172] GAHP[17179] -> '4' '1' 'Unable to limit the proxy' 'N/A'
01/15/16 15:24:51 [17172] GAHP[17179] -> '3' '1' 'submission command failed (exit code = 1) (stdout:Error-) (stderr:/usr/libexec/blahp/blah_common_submit_functions.sh: line 743: cd: /n/atlasgrid/condor/65/0/cluster65.proc0.subproc0: No such file or directory-Failed to CD to Initial Working Directory.-)' 'N/A'
01/15/16 15:24:51 [17172] (64.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/15/16 15:24:51 [17172] (64.0) blah_job_submit() failed: Unable to limit the proxy
01/15/16 15:24:51 [17172] (64.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/15/16 15:24:51 [17172] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/15/16 15:24:51 [17172] (64.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/15/16 15:24:51 [17172] (66.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/15/16 15:24:51 [17172] (66.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:Error-) (stderr:/usr/libexec/blahp/blah_common_submit_functions.sh: line 743: cd: /n/atlasgrid/condor/65/0/cluster65.proc0.subproc0: No such file or directory-Failed to CD to Initial Working Directory.-)
01/15/16 15:24:51 [17172] (66.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/15/16 15:24:51 [17172] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/15/16 15:24:51 [17172] (66.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/15/16 15:24:52 [17172] resource  is now up
01/15/16 15:24:52 [17172] resource  is now up
01/15/16 15:24:52 [17172] in doContactSchedd()
01/15/16 15:24:52 [17172] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/15/16 15:24:52 [17172] querying for new jobs
01/15/16 15:24:52 [17172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
01/15/16 15:24:52 [17172] Fetched 0 new job ads from schedd
01/15/16 15:24:52 [17172] querying for removed/held jobs
01/15/16 15:24:52 [17172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/15/16 15:24:52 [17172] Fetched 4 job ads from schedd
01/15/16 15:24:52 [17172] Updating classad values for 70.0:
01/15/16 15:24:52 [17172]    CurrentStatusUnknown = false
01/15/16 15:24:52 [17172]    GridJobId = undefined
01/15/16 15:24:52 [17172]    LastRemoteStatusUpdate = 0
01/15/16 15:24:52 [17172]    Managed = "ScheddDone"
01/15/16 15:24:52 [17172] Updating classad values for 64.0:
01/15/16 15:24:52 [17172]    CurrentStatusUnknown = false
01/15/16 15:24:52 [17172]    GridJobId = undefined
01/15/16 15:24:52 [17172]    LastRemoteStatusUpdate = 0
01/15/16 15:24:52 [17172]    Managed = "ScheddDone"
01/15/16 15:24:52 [17172] Updating classad values for 72.0:
01/15/16 15:24:52 [17172]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#72.0#1452889214"
01/15/16 15:24:52 [17172]    LastRemoteStatusUpdate = 1452889488
01/15/16 15:24:52 [17172] Updating classad values for 66.0:
01/15/16 15:24:52 [17172]    CurrentStatusUnknown = false
01/15/16 15:24:52 [17172]    GridJobId = undefined
01/15/16 15:24:52 [17172]    LastRemoteStatusUpdate = 0
01/15/16 15:24:52 [17172]    Managed = "ScheddDone"
01/15/16 15:24:52 [17172] Updating classad values for 62.0:
01/15/16 15:24:52 [17172]    CurrentStatusUnknown = false
01/15/16 15:24:52 [17172]    GridJobId = undefined
01/15/16 15:24:52 [17172]    LastRemoteStatusUpdate = 0
01/15/16 15:24:52 [17172]    Managed = "ScheddDone"
01/15/16 15:24:52 [17172] Deleting job 70.0 from schedd
01/15/16 15:24:52 [17172] Deleting job 64.0 from schedd
01/15/16 15:24:52 [17172] Deleting job 66.0 from schedd
01/15/16 15:24:52 [17172] Deleting job 62.0 from schedd
01/15/16 15:24:52 [17172] leaving doContactSchedd()
01/15/16 15:24:52 [17172] (72.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/15/16 15:24:52 [17172] (72.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/15/16 15:24:52 [17172] (72.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/15/16 15:24:52 [17172] GAHP[17179] <- 'BLAH_JOB_SUBMIT 5 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/71/0/cluster71.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/71/0/cluster71.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/71/0/cluster71.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#72.0#1452889214";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/15/16 15:24:52 [17172] GAHP[17179] -> 'S'
01/15/16 15:24:53 [17172] GAHP[17179] <- 'RESULTS'
01/15/16 15:24:53 [17172] GAHP[17179] -> 'R'
01/15/16 15:24:53 [17172] GAHP[17179] -> 'S' '1'
01/15/16 15:24:53 [17172] GAHP[17179] -> '5' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/15/16 15:24:53 [17172] (72.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
01/15/16 15:24:53 [17172] (72.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/15/16 15:24:53 [17172] (72.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/15/16 15:24:53 [17172] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/15/16 15:24:53 [17172] (72.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
01/15/16 15:24:57 [17172] in doContactSchedd()
01/15/16 15:24:57 [17172] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/15/16 15:24:57 [17172] querying for removed/held jobs
01/15/16 15:24:57 [17172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/15/16 15:24:57 [17172] Fetched 0 job ads from schedd
01/15/16 15:24:57 [17172] Updating classad values for 72.0:
01/15/16 15:24:57 [17172]    CurrentStatusUnknown = false
01/15/16 15:24:57 [17172]    GridJobId = undefined
01/15/16 15:24:57 [17172]    LastRemoteStatusUpdate = 0
01/15/16 15:24:57 [17172] leaving doContactSchedd()
01/15/16 15:24:57 [17172] (72.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
01/15/16 15:24:57 [17172] (72.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
01/15/16 15:24:57 [17172] (72.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
01/15/16 15:25:02 [17172] in doContactSchedd()
01/15/16 15:25:02 [17172] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/15/16 15:25:02 [17172] querying for removed/held jobs
01/15/16 15:25:02 [17172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/15/16 15:25:02 [17172] Fetched 0 job ads from schedd
01/15/16 15:25:02 [17172] Updating classad values for 72.0:
01/15/16 15:25:02 [17172]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#72.0#1452889214"
01/15/16 15:25:02 [17172]    LastRemoteStatusUpdate = 1452889497
01/15/16 15:25:02 [17172] leaving doContactSchedd()
01/15/16 15:25:02 [17172] (72.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
01/15/16 15:25:02 [17172] (72.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
01/15/16 15:25:02 [17172] (72.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
01/15/16 15:25:02 [17172] (72.0) gm state change: GM_HOLD -> GM_DELETE
01/15/16 15:25:07 [17172] in doContactSchedd()
01/15/16 15:25:07 [17172] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/15/16 15:25:07 [17172] querying for removed/held jobs
01/15/16 15:25:07 [17172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/15/16 15:25:07 [17172] Fetched 0 job ads from schedd
01/15/16 15:25:07 [17172] Updating classad values for 72.0:
01/15/16 15:25:07 [17172]    EnteredCurrentStatus = 1452889502
01/15/16 15:25:07 [17172]    HoldReason = "Attempts to submit failed: "
01/15/16 15:25:07 [17172]    HoldReasonCode = 0
01/15/16 15:25:07 [17172]    HoldReasonSubCode = 0
01/15/16 15:25:07 [17172]    JobStatus = 5
01/15/16 15:25:07 [17172]    LastReleaseReason = "Data files spooled"
01/15/16 15:25:07 [17172]    Managed = "Schedd"
01/15/16 15:25:07 [17172]    NumSystemHolds = 1
01/15/16 15:25:07 [17172]    ReleaseReason = undefined
01/15/16 15:25:07 [17172] No jobs left, shutting down
01/15/16 15:25:07 [17172] leaving doContactSchedd()
01/15/16 15:25:07 [17172] Got SIGTERM. Performing graceful shutdown.
01/15/16 15:25:07 [17172] Started timer to call main_shutdown_fast in 1800 seconds
01/15/16 15:25:07 [17172] **** condor_gridmanager (condor_GRIDMANAGER) pid 17172 EXITING WITH STATUS 0
01/16/16 15:27:06 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
01/16/16 15:27:06 Using IDs: 16 processors, 8 CPUs, 8 HTs
01/16/16 15:27:06 Enumerating interfaces: lo 127.0.0.1 up
01/16/16 15:27:06 Enumerating interfaces: eth2 10.31.131.202 up
01/16/16 15:27:06 Enumerating interfaces: eth3 140.247.179.131 up
01/16/16 15:27:06 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
01/16/16 15:27:06 Initializing Directory: curr_dir = /etc/condor-ce/config.d
01/16/16 15:27:06 ******************************************************
01/16/16 15:27:06 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
01/16/16 15:27:06 ** /usr/sbin/condor_gridmanager
01/16/16 15:27:06 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
01/16/16 15:27:06 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
01/16/16 15:27:06 ** $CondorVersion: 8.2.8 Apr 08 2015 $
01/16/16 15:27:06 ** $CondorPlatform: X86_64-CentOS_6.6 $
01/16/16 15:27:06 ** PID = 5929
01/16/16 15:27:06 ** Log last touched 1/15 15:25:07
01/16/16 15:27:06 ******************************************************
01/16/16 15:27:06 Using config source: /etc/condor-ce/condor_config
01/16/16 15:27:06 Using local config sources: 
01/16/16 15:27:06    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
01/16/16 15:27:06    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/01-ce-auth.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/01-ce-router.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/01-common-auth.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/02-ce-lsf.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/02-ce-pbs.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/03-ce-shared-port.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/03-managed-fork.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/10-ce-collector-generated.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/50-osg-configure.conf
01/16/16 15:27:06    /etc/condor-ce/config.d/99-local.conf
01/16/16 15:27:06    /usr/share/condor-ce/condor_ce_router_defaults|
01/16/16 15:27:06 config Macros = 142, Sorted = 142, StringBytes = 12508, TablesBytes = 5320
01/16/16 15:27:06 CLASSAD_CACHING is ENABLED
01/16/16 15:27:06 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
01/16/16 15:27:06 SharedPortEndpoint: waiting for connections to named socket 11958_3e1f_2
01/16/16 15:27:06 DaemonCore: command socket at <140.247.179.131:9620?sock=11958_3e1f_2>
01/16/16 15:27:06 DaemonCore: private command socket at <140.247.179.131:9620?sock=11958_3e1f_2>
01/16/16 15:27:06 Setting maximum accepts per cycle 8.
01/16/16 15:27:06 Setting maximum reaps per cycle 8.
01/16/16 15:27:06 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/16/16 15:27:06 [5929] Welcome to the all-singing, all dancing, "amazing" GridManager!
01/16/16 15:27:06 [5929] DaemonCore: No more children processes to reap.
01/16/16 15:27:06 [5929] DaemonCore: in SendAliveToParent()
01/16/16 15:27:06 [5929] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/16/16 15:27:06 [5929] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
01/16/16 15:27:06 [5929] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
01/16/16 15:27:06 [5929] IPVERIFY: ip found is 0
01/16/16 15:27:06 [5929] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
01/16/16 15:27:06 [5929] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
01/16/16 15:27:06 [5929] Buf::write(): condor_write() failed
01/16/16 15:27:06 [5929] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
01/16/16 15:27:06 [5929] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/16/16 15:27:06 [5929] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
01/16/16 15:27:06 [5929] DaemonCore: Leaving SendAliveToParent() - success
01/16/16 15:27:06 [5929] Checking proxies
01/16/16 15:27:08 [5929] Received REMOVE_JOBS signal
01/16/16 15:27:08 [5929] in doContactSchedd()
01/16/16 15:27:08 [5929] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/16/16 15:27:08 [5929] querying for new jobs
01/16/16 15:27:08 [5929] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
01/16/16 15:27:08 [5929] Using job type INFNBatch for job 72.0
01/16/16 15:27:08 [5929] (72.0) SetJobLeaseTimers()
01/16/16 15:27:09 [5929] Found job 72.0 --- inserting
01/16/16 15:27:09 [5929] Fetched 1 new job ads from schedd
01/16/16 15:27:09 [5929] querying for removed/held jobs
01/16/16 15:27:09 [5929] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/16/16 15:27:09 [5929] Fetched 1 job ads from schedd
01/16/16 15:27:09 [5929] leaving doContactSchedd()
01/16/16 15:27:09 [5929] gahp server not up yet, delaying ping
01/16/16 15:27:09 [5929] *** UpdateLeases called
01/16/16 15:27:09 [5929]     Leases not supported, cancelling timer
01/16/16 15:27:09 [5929] Received ADD_JOBS signal
01/16/16 15:27:09 [5929] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=11958_3e1f_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=11951_c84b_4>"
CurrentTime = time()
MyCurrentTime = 1452976029
IdleJobs = 0
JobLimit = 10000

01/16/16 15:27:09 [5929] Trying to update collector <10.31.131.202:9619>
01/16/16 15:27:09 [5929] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
01/16/16 15:27:09 [5929] File descriptor limits: max 4096, safe 3277
01/16/16 15:27:09 [5929] (72.0) doEvaluateState called: gmState GM_INIT, remoteState -1
01/16/16 15:27:09 [5929] GAHP server pid = 5932
01/16/16 15:27:09 [5929] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
01/16/16 15:27:09 [5929] GAHP[5932] <- 'COMMANDS'
01/16/16 15:27:09 [5929] GAHP[5932] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
01/16/16 15:27:09 [5929] GAHP[5932] <- 'ASYNC_MODE_ON'
01/16/16 15:27:09 [5929] GAHP[5932] -> 'S' 'Async mode on'
01/16/16 15:27:09 [5929] (72.0) gm state change: GM_INIT -> GM_START
01/16/16 15:27:09 [5929] (72.0) gm state change: GM_START -> GM_TRANSFER_INPUT
01/16/16 15:27:09 [5929] (72.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
01/16/16 15:27:09 [5929] GAHP[5932] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/71/0/cluster71.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/71/0/cluster71.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/71/0/cluster71.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#72.0#1452889214";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
01/16/16 15:27:09 [5929] GAHP[5932] -> 'S'
01/16/16 15:27:09 [5929] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
01/16/16 15:27:09 [5929] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
01/16/16 15:27:09 [5929] IPVERIFY: ip found is 1
01/16/16 15:27:10 [5929] GAHP[5932] <- 'RESULTS'
01/16/16 15:27:10 [5929] GAHP[5932] -> 'R'
01/16/16 15:27:10 [5929] GAHP[5932] -> 'S' '1'
01/16/16 15:27:10 [5929] GAHP[5932] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
01/16/16 15:27:10 [5929] (72.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
01/16/16 15:27:10 [5929] (72.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
01/16/16 15:27:10 [5929] (72.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
01/16/16 15:27:10 [5929] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
01/16/16 15:27:17 [5929] (72.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
01/16/16 15:27:17 [5929] Evaluating staleness of remote job statuses.
01/16/16 15:27:17 [5929] resource  is now up
01/16/16 15:27:17 [5929] in doContactSchedd()
01/16/16 15:27:17 [5929] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11951_c84b_4
01/16/16 15:27:17 [5929] querying for new jobs
01/16/16 15:27:17 [5929] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
01/16/16 15:27:17 [5929] Fetched 0 new job ads from schedd
01/16/16 15:27:17 [5929] querying for removed/held jobs
01/16/16 15:27:17 [5929] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
01/16/16 15:27:17 [5929] Fetched 1 job ads from schedd
01/16/16 15:27:17 [5929] Updating classad values for 72.0:
01/16/16 15:27:17 [5929]    CurrentStatusUnknown = false
01/16/16 15:27:17 [5929]    GridJobId = undefined
01/16/16 15:27:17 [5929]    LastRemoteStatusUpdate = 0
01/16/16 15:27:17 [5929]    Managed = "ScheddDone"
01/16/16 15:27:17 [5929] Deleting job 72.0 from schedd
01/16/16 15:27:17 [5929] No jobs left, shutting down
01/16/16 15:27:17 [5929] leaving doContactSchedd()
01/16/16 15:27:17 [5929] Got SIGTERM. Performing graceful shutdown.
01/16/16 15:27:17 [5929] Started timer to call main_shutdown_fast in 1800 seconds
01/16/16 15:27:17 [5929] **** condor_gridmanager (condor_GRIDMANAGER) pid 5929 EXITING WITH STATUS 0
02/10/16 10:24:47 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/10/16 10:24:47 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/10/16 10:24:47 Enumerating interfaces: lo 127.0.0.1 up
02/10/16 10:24:47 Enumerating interfaces: eth2 10.31.131.202 up
02/10/16 10:24:47 Enumerating interfaces: eth3 140.247.179.131 up
02/10/16 10:24:47 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/10/16 10:24:47 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/10/16 10:24:47 ******************************************************
02/10/16 10:24:47 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/10/16 10:24:47 ** /usr/sbin/condor_gridmanager
02/10/16 10:24:47 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/10/16 10:24:47 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/10/16 10:24:47 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/10/16 10:24:47 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/10/16 10:24:47 ** PID = 11442
02/10/16 10:24:47 ** Log last touched 1/16 15:27:17
02/10/16 10:24:47 ******************************************************
02/10/16 10:24:47 Using config source: /etc/condor-ce/condor_config
02/10/16 10:24:47 Using local config sources: 
02/10/16 10:24:47    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/10/16 10:24:47    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/01-ce-auth.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/01-ce-router.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/01-common-auth.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/02-ce-lsf.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/02-ce-pbs.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/03-managed-fork.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/50-osg-configure.conf
02/10/16 10:24:47    /etc/condor-ce/config.d/99-local.conf
02/10/16 10:24:47    /usr/share/condor-ce/condor_ce_router_defaults|
02/10/16 10:24:47 config Macros = 142, Sorted = 142, StringBytes = 12507, TablesBytes = 5320
02/10/16 10:24:47 CLASSAD_CACHING is ENABLED
02/10/16 10:24:47 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/10/16 10:24:47 SharedPortEndpoint: waiting for connections to named socket 7649_2990_1
02/10/16 10:24:47 DaemonCore: command socket at <140.247.179.131:9620?sock=7649_2990_1>
02/10/16 10:24:47 DaemonCore: private command socket at <140.247.179.131:9620?sock=7649_2990_1>
02/10/16 10:24:47 Setting maximum accepts per cycle 8.
02/10/16 10:24:47 Setting maximum reaps per cycle 8.
02/10/16 10:24:47 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 10:24:47 [11442] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/10/16 10:24:47 [11442] DaemonCore: No more children processes to reap.
02/10/16 10:24:47 [11442] DaemonCore: in SendAliveToParent()
02/10/16 10:24:47 [11442] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 10:24:47 [11442] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/10/16 10:24:47 [11442] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/10/16 10:24:47 [11442] IPVERIFY: ip found is 0
02/10/16 10:24:47 [11442] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/10/16 10:24:47 [11442] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/10/16 10:24:47 [11442] Buf::write(): condor_write() failed
02/10/16 10:24:47 [11442] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/10/16 10:24:47 [11442] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 10:24:47 [11442] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/10/16 10:24:47 [11442] DaemonCore: Leaving SendAliveToParent() - success
02/10/16 10:24:47 [11442] Checking proxies
02/10/16 10:24:50 [11442] Received ADD_JOBS signal
02/10/16 10:24:50 [11442] in doContactSchedd()
02/10/16 10:24:50 [11442] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 10:24:50 [11442] querying for new jobs
02/10/16 10:24:50 [11442] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/10/16 10:24:50 [11442] Using job type INFNBatch for job 74.0
02/10/16 10:24:50 [11442] (74.0) SetJobLeaseTimers()
02/10/16 10:24:50 [11442] Found job 74.0 --- inserting
02/10/16 10:24:50 [11442] Fetched 1 new job ads from schedd
02/10/16 10:24:50 [11442] querying for removed/held jobs
02/10/16 10:24:50 [11442] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 10:24:50 [11442] Fetched 0 job ads from schedd
02/10/16 10:24:50 [11442] leaving doContactSchedd()
02/10/16 10:24:50 [11442] gahp server not up yet, delaying ping
02/10/16 10:24:50 [11442] *** UpdateLeases called
02/10/16 10:24:50 [11442]     Leases not supported, cancelling timer
02/10/16 10:24:50 [11442] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7649_2990_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=7608_f991_4>"
CurrentTime = time()
MyCurrentTime = 1455117890
IdleJobs = 1
JobLimit = 10000

02/10/16 10:24:50 [11442] Trying to update collector <10.31.131.202:9619>
02/10/16 10:24:50 [11442] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 10:24:50 [11442] File descriptor limits: max 4096, safe 3277
02/10/16 10:24:50 [11442] (74.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/10/16 10:24:50 [11442] GAHP server pid = 11452
02/10/16 10:24:50 [11442] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/10/16 10:24:50 [11442] GAHP[11452] <- 'COMMANDS'
02/10/16 10:24:50 [11442] GAHP[11452] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/10/16 10:24:50 [11442] GAHP[11452] <- 'ASYNC_MODE_ON'
02/10/16 10:24:50 [11442] GAHP[11452] -> 'S' 'Async mode on'
02/10/16 10:24:50 [11442] (74.0) gm state change: GM_INIT -> GM_START
02/10/16 10:24:50 [11442] (74.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/10/16 10:24:50 [11442] (74.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 10:24:50 [11442] (74.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 10:24:50 [11442] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/10/16 10:24:50 [11442] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/10/16 10:24:50 [11442] IPVERIFY: ip found is 1
02/10/16 10:24:52 [11442] Evaluating staleness of remote job statuses.
02/10/16 10:24:55 [11442] resource  is now up
02/10/16 10:24:55 [11442] in doContactSchedd()
02/10/16 10:24:55 [11442] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 10:24:55 [11442] querying for removed/held jobs
02/10/16 10:24:55 [11442] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 10:24:55 [11442] Fetched 0 job ads from schedd
02/10/16 10:24:55 [11442] Updating classad values for 74.0:
02/10/16 10:24:55 [11442]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#74.0#1455117872"
02/10/16 10:24:55 [11442]    LastRemoteStatusUpdate = 1455117890
02/10/16 10:24:55 [11442] leaving doContactSchedd()
02/10/16 10:24:55 [11442] (74.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 10:24:55 [11442] (74.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 10:24:55 [11442] (74.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/10/16 10:24:55 [11442] GAHP[11452] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#74.0#1455117872";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/10/16 10:24:55 [11442] GAHP[11452] -> 'S'
02/10/16 10:24:57 [11442] GAHP[11452] <- 'RESULTS'
02/10/16 10:24:57 [11442] GAHP[11452] -> 'R'
02/10/16 10:24:57 [11442] GAHP[11452] -> 'S' '1'
02/10/16 10:24:57 [11442] GAHP[11452] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)' 'N/A'
02/10/16 10:24:57 [11442] (74.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/10/16 10:24:57 [11442] (74.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:/usr/libexec/blahp/pbs_submit.sh: line 207: /qsub: No such file or directory-)
02/10/16 10:24:57 [11442] (74.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/10/16 10:24:57 [11442] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/10/16 10:24:57 [11442] (74.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/10/16 10:25:00 [11442] in doContactSchedd()
02/10/16 10:25:00 [11442] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 10:25:00 [11442] querying for removed/held jobs
02/10/16 10:25:00 [11442] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 10:25:00 [11442] Fetched 0 job ads from schedd
02/10/16 10:25:00 [11442] Updating classad values for 74.0:
02/10/16 10:25:00 [11442]    CurrentStatusUnknown = false
02/10/16 10:25:00 [11442]    GridJobId = undefined
02/10/16 10:25:00 [11442]    LastRemoteStatusUpdate = 0
02/10/16 10:25:00 [11442] leaving doContactSchedd()
02/10/16 10:25:00 [11442] (74.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/10/16 10:25:00 [11442] (74.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 10:25:00 [11442] (74.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 10:25:05 [11442] in doContactSchedd()
02/10/16 10:25:05 [11442] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 10:25:05 [11442] querying for removed/held jobs
02/10/16 10:25:05 [11442] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 10:25:05 [11442] Fetched 0 job ads from schedd
02/10/16 10:25:05 [11442] Updating classad values for 74.0:
02/10/16 10:25:05 [11442]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#74.0#1455117872"
02/10/16 10:25:05 [11442]    LastRemoteStatusUpdate = 1455117900
02/10/16 10:25:05 [11442] leaving doContactSchedd()
02/10/16 10:25:05 [11442] (74.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 10:25:05 [11442] (74.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 10:25:05 [11442] (74.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/10/16 10:25:05 [11442] (74.0) gm state change: GM_HOLD -> GM_DELETE
02/10/16 10:25:10 [11442] in doContactSchedd()
02/10/16 10:25:10 [11442] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 10:25:10 [11442] querying for removed/held jobs
02/10/16 10:25:10 [11442] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 10:25:10 [11442] Fetched 0 job ads from schedd
02/10/16 10:25:10 [11442] Updating classad values for 74.0:
02/10/16 10:25:10 [11442]    EnteredCurrentStatus = 1455117905
02/10/16 10:25:10 [11442]    HoldReason = "Attempts to submit failed: "
02/10/16 10:25:10 [11442]    HoldReasonCode = 0
02/10/16 10:25:10 [11442]    HoldReasonSubCode = 0
02/10/16 10:25:10 [11442]    JobStatus = 5
02/10/16 10:25:10 [11442]    LastReleaseReason = "Data files spooled"
02/10/16 10:25:10 [11442]    Managed = "Schedd"
02/10/16 10:25:10 [11442]    NumSystemHolds = 1
02/10/16 10:25:10 [11442]    ReleaseReason = undefined
02/10/16 10:25:10 [11442] No jobs left, shutting down
02/10/16 10:25:10 [11442] leaving doContactSchedd()
02/10/16 10:25:10 [11442] Got SIGTERM. Performing graceful shutdown.
02/10/16 10:25:10 [11442] Started timer to call main_shutdown_fast in 1800 seconds
02/10/16 10:25:10 [11442] **** condor_gridmanager (condor_GRIDMANAGER) pid 11442 EXITING WITH STATUS 0
02/10/16 13:44:08 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/10/16 13:44:08 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/10/16 13:44:08 Enumerating interfaces: lo 127.0.0.1 up
02/10/16 13:44:08 Enumerating interfaces: eth2 10.31.131.202 up
02/10/16 13:44:08 Enumerating interfaces: eth3 140.247.179.131 up
02/10/16 13:44:08 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/10/16 13:44:08 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/10/16 13:44:08 ******************************************************
02/10/16 13:44:08 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/10/16 13:44:08 ** /usr/sbin/condor_gridmanager
02/10/16 13:44:08 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/10/16 13:44:08 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/10/16 13:44:08 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/10/16 13:44:08 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/10/16 13:44:08 ** PID = 15977
02/10/16 13:44:08 ** Log last touched 2/10 10:25:10
02/10/16 13:44:08 ******************************************************
02/10/16 13:44:08 Using config source: /etc/condor-ce/condor_config
02/10/16 13:44:08 Using local config sources: 
02/10/16 13:44:08    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/10/16 13:44:08    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/01-ce-auth.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/01-ce-router.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/01-common-auth.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/02-ce-lsf.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/02-ce-pbs.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/03-managed-fork.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/50-osg-configure.conf
02/10/16 13:44:08    /etc/condor-ce/config.d/99-local.conf
02/10/16 13:44:08    /usr/share/condor-ce/condor_ce_router_defaults|
02/10/16 13:44:08 config Macros = 142, Sorted = 142, StringBytes = 12507, TablesBytes = 5320
02/10/16 13:44:08 CLASSAD_CACHING is ENABLED
02/10/16 13:44:08 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/10/16 13:44:08 SharedPortEndpoint: waiting for connections to named socket 7649_2990_2
02/10/16 13:44:08 DaemonCore: command socket at <140.247.179.131:9620?sock=7649_2990_2>
02/10/16 13:44:08 DaemonCore: private command socket at <140.247.179.131:9620?sock=7649_2990_2>
02/10/16 13:44:08 Setting maximum accepts per cycle 8.
02/10/16 13:44:08 Setting maximum reaps per cycle 8.
02/10/16 13:44:08 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 13:44:08 [15977] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/10/16 13:44:08 [15977] DaemonCore: No more children processes to reap.
02/10/16 13:44:08 [15977] DaemonCore: in SendAliveToParent()
02/10/16 13:44:08 [15977] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 13:44:08 [15977] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/10/16 13:44:08 [15977] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/10/16 13:44:08 [15977] IPVERIFY: ip found is 0
02/10/16 13:44:08 [15977] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/10/16 13:44:08 [15977] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/10/16 13:44:08 [15977] Buf::write(): condor_write() failed
02/10/16 13:44:08 [15977] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/10/16 13:44:08 [15977] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 13:44:08 [15977] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/10/16 13:44:08 [15977] DaemonCore: Leaving SendAliveToParent() - success
02/10/16 13:44:08 [15977] Checking proxies
02/10/16 13:44:10 [15977] Received ADD_JOBS signal
02/10/16 13:44:10 [15977] in doContactSchedd()
02/10/16 13:44:10 [15977] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 13:44:10 [15977] querying for new jobs
02/10/16 13:44:10 [15977] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/10/16 13:44:10 [15977] Using job type INFNBatch for job 76.0
02/10/16 13:44:10 [15977] (76.0) SetJobLeaseTimers()
02/10/16 13:44:10 [15977] Found job 76.0 --- inserting
02/10/16 13:44:10 [15977] Fetched 1 new job ads from schedd
02/10/16 13:44:10 [15977] querying for removed/held jobs
02/10/16 13:44:10 [15977] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 13:44:10 [15977] Fetched 0 job ads from schedd
02/10/16 13:44:10 [15977] leaving doContactSchedd()
02/10/16 13:44:10 [15977] gahp server not up yet, delaying ping
02/10/16 13:44:10 [15977] *** UpdateLeases called
02/10/16 13:44:10 [15977]     Leases not supported, cancelling timer
02/10/16 13:44:10 [15977] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=7649_2990_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=7608_f991_4>"
CurrentTime = time()
MyCurrentTime = 1455129850
IdleJobs = 1
JobLimit = 10000

02/10/16 13:44:10 [15977] Trying to update collector <10.31.131.202:9619>
02/10/16 13:44:10 [15977] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 13:44:10 [15977] File descriptor limits: max 4096, safe 3277
02/10/16 13:44:10 [15977] (76.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/10/16 13:44:10 [15977] GAHP server pid = 15985
02/10/16 13:44:10 [15977] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/10/16 13:44:10 [15977] GAHP[15985] <- 'COMMANDS'
02/10/16 13:44:10 [15977] GAHP[15985] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/10/16 13:44:10 [15977] GAHP[15985] <- 'ASYNC_MODE_ON'
02/10/16 13:44:10 [15977] GAHP[15985] -> 'S' 'Async mode on'
02/10/16 13:44:10 [15977] (76.0) gm state change: GM_INIT -> GM_START
02/10/16 13:44:10 [15977] (76.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/10/16 13:44:10 [15977] (76.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 13:44:10 [15977] (76.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 13:44:10 [15977] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/10/16 13:44:10 [15977] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/10/16 13:44:10 [15977] IPVERIFY: ip found is 1
02/10/16 13:44:13 [15977] Evaluating staleness of remote job statuses.
02/10/16 13:44:15 [15977] resource  is now up
02/10/16 13:44:15 [15977] in doContactSchedd()
02/10/16 13:44:15 [15977] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 13:44:15 [15977] querying for removed/held jobs
02/10/16 13:44:15 [15977] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 13:44:15 [15977] Fetched 0 job ads from schedd
02/10/16 13:44:15 [15977] Updating classad values for 76.0:
02/10/16 13:44:15 [15977]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#76.0#1455129835"
02/10/16 13:44:15 [15977]    LastRemoteStatusUpdate = 1455129850
02/10/16 13:44:15 [15977] leaving doContactSchedd()
02/10/16 13:44:15 [15977] (76.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 13:44:15 [15977] (76.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 13:44:15 [15977] (76.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/10/16 13:44:15 [15977] GAHP[15985] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/75/0/cluster75.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/75/0/cluster75.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/75/0/cluster75.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#76.0#1455129835";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/10/16 13:44:15 [15977] GAHP[15985] -> 'S'
02/10/16 13:44:18 [15977] GAHP[15985] <- 'RESULTS'
02/10/16 13:44:18 [15977] GAHP[15985] -> 'R'
02/10/16 13:44:18 [15977] GAHP[15985] -> 'S' '1'
02/10/16 13:44:18 [15977] GAHP[15985] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/10/16 13:44:18 [15977] (76.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/10/16 13:44:18 [15977] (76.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/10/16 13:44:18 [15977] (76.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/10/16 13:44:18 [15977] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/10/16 13:44:22 [15977] (76.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/10/16 13:44:22 [15977] in doContactSchedd()
02/10/16 13:44:22 [15977] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 13:44:22 [15977] querying for removed/held jobs
02/10/16 13:44:22 [15977] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 13:44:22 [15977] Fetched 0 job ads from schedd
02/10/16 13:44:22 [15977] Updating classad values for 76.0:
02/10/16 13:44:22 [15977]    CurrentStatusUnknown = false
02/10/16 13:44:22 [15977]    GridJobId = undefined
02/10/16 13:44:22 [15977]    LastRemoteStatusUpdate = 0
02/10/16 13:44:22 [15977] leaving doContactSchedd()
02/10/16 13:44:22 [15977] (76.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/10/16 13:44:22 [15977] (76.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 13:44:22 [15977] (76.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 13:44:27 [15977] in doContactSchedd()
02/10/16 13:44:27 [15977] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 13:44:27 [15977] querying for removed/held jobs
02/10/16 13:44:27 [15977] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 13:44:27 [15977] Fetched 0 job ads from schedd
02/10/16 13:44:27 [15977] Updating classad values for 76.0:
02/10/16 13:44:27 [15977]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#76.0#1455129835"
02/10/16 13:44:27 [15977]    LastRemoteStatusUpdate = 1455129862
02/10/16 13:44:29 [15977] leaving doContactSchedd()
02/10/16 13:44:29 [15977] (76.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 13:44:29 [15977] (76.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 13:44:29 [15977] (76.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/10/16 13:44:29 [15977] (76.0) gm state change: GM_HOLD -> GM_DELETE
02/10/16 13:44:34 [15977] in doContactSchedd()
02/10/16 13:44:34 [15977] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 7608_f991_4
02/10/16 13:44:34 [15977] querying for removed/held jobs
02/10/16 13:44:34 [15977] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 13:44:34 [15977] Fetched 0 job ads from schedd
02/10/16 13:44:34 [15977] Updating classad values for 76.0:
02/10/16 13:44:34 [15977]    EnteredCurrentStatus = 1455129869
02/10/16 13:44:34 [15977]    HoldReason = "Attempts to submit failed: "
02/10/16 13:44:34 [15977]    HoldReasonCode = 0
02/10/16 13:44:34 [15977]    HoldReasonSubCode = 0
02/10/16 13:44:34 [15977]    JobStatus = 5
02/10/16 13:44:34 [15977]    LastReleaseReason = "Data files spooled"
02/10/16 13:44:34 [15977]    Managed = "Schedd"
02/10/16 13:44:34 [15977]    NumSystemHolds = 1
02/10/16 13:44:34 [15977]    ReleaseReason = undefined
02/10/16 13:44:34 [15977] No jobs left, shutting down
02/10/16 13:44:34 [15977] leaving doContactSchedd()
02/10/16 13:44:34 [15977] Got SIGTERM. Performing graceful shutdown.
02/10/16 13:44:34 [15977] Started timer to call main_shutdown_fast in 1800 seconds
02/10/16 13:44:34 [15977] **** condor_gridmanager (condor_GRIDMANAGER) pid 15977 EXITING WITH STATUS 0
02/10/16 14:00:25 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/10/16 14:00:25 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/10/16 14:00:25 Enumerating interfaces: lo 127.0.0.1 up
02/10/16 14:00:25 Enumerating interfaces: eth2 10.31.131.202 up
02/10/16 14:00:25 Enumerating interfaces: eth3 140.247.179.131 up
02/10/16 14:00:25 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/10/16 14:00:25 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/10/16 14:00:25 ******************************************************
02/10/16 14:00:25 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/10/16 14:00:25 ** /usr/sbin/condor_gridmanager
02/10/16 14:00:25 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/10/16 14:00:25 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/10/16 14:00:25 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/10/16 14:00:25 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/10/16 14:00:25 ** PID = 16200
02/10/16 14:00:25 ** Log last touched 2/10 13:44:34
02/10/16 14:00:25 ******************************************************
02/10/16 14:00:25 Using config source: /etc/condor-ce/condor_config
02/10/16 14:00:25 Using local config sources: 
02/10/16 14:00:25    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/10/16 14:00:25    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/01-ce-auth.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/01-ce-router.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/01-common-auth.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/02-ce-lsf.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/02-ce-pbs.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/03-managed-fork.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/50-osg-configure.conf
02/10/16 14:00:25    /etc/condor-ce/config.d/99-local.conf
02/10/16 14:00:25    /usr/share/condor-ce/condor_ce_router_defaults|
02/10/16 14:00:25 config Macros = 142, Sorted = 142, StringBytes = 12510, TablesBytes = 5320
02/10/16 14:00:25 CLASSAD_CACHING is ENABLED
02/10/16 14:00:25 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/10/16 14:00:25 SharedPortEndpoint: waiting for connections to named socket 24494_fd1d_1
02/10/16 14:00:25 DaemonCore: command socket at <140.247.179.131:9620?sock=24494_fd1d_1>
02/10/16 14:00:25 DaemonCore: private command socket at <140.247.179.131:9620?sock=24494_fd1d_1>
02/10/16 14:00:25 Setting maximum accepts per cycle 8.
02/10/16 14:00:25 Setting maximum reaps per cycle 8.
02/10/16 14:00:25 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 14:00:25 [16200] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/10/16 14:00:25 [16200] DaemonCore: No more children processes to reap.
02/10/16 14:00:25 [16200] DaemonCore: in SendAliveToParent()
02/10/16 14:00:25 [16200] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:00:25 [16200] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/10/16 14:00:25 [16200] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/10/16 14:00:25 [16200] IPVERIFY: ip found is 0
02/10/16 14:00:25 [16200] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/10/16 14:00:25 [16200] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/10/16 14:00:25 [16200] Buf::write(): condor_write() failed
02/10/16 14:00:25 [16200] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/10/16 14:00:25 [16200] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:00:25 [16200] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/10/16 14:00:25 [16200] DaemonCore: Leaving SendAliveToParent() - success
02/10/16 14:00:25 [16200] Checking proxies
02/10/16 14:00:28 [16200] Received ADD_JOBS signal
02/10/16 14:00:28 [16200] in doContactSchedd()
02/10/16 14:00:28 [16200] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:00:28 [16200] querying for new jobs
02/10/16 14:00:28 [16200] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/10/16 14:00:28 [16200] Using job type INFNBatch for job 78.0
02/10/16 14:00:28 [16200] (78.0) SetJobLeaseTimers()
02/10/16 14:00:28 [16200] Found job 78.0 --- inserting
02/10/16 14:00:28 [16200] Fetched 1 new job ads from schedd
02/10/16 14:00:28 [16200] querying for removed/held jobs
02/10/16 14:00:28 [16200] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:00:28 [16200] Fetched 0 job ads from schedd
02/10/16 14:00:28 [16200] leaving doContactSchedd()
02/10/16 14:00:28 [16200] gahp server not up yet, delaying ping
02/10/16 14:00:28 [16200] *** UpdateLeases called
02/10/16 14:00:28 [16200]     Leases not supported, cancelling timer
02/10/16 14:00:28 [16200] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=24494_fd1d_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=24479_0745_4>"
CurrentTime = time()
MyCurrentTime = 1455130828
IdleJobs = 1
JobLimit = 10000

02/10/16 14:00:28 [16200] Trying to update collector <10.31.131.202:9619>
02/10/16 14:00:28 [16200] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 14:00:28 [16200] File descriptor limits: max 4096, safe 3277
02/10/16 14:00:28 [16200] (78.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/10/16 14:00:28 [16200] GAHP server pid = 16448
02/10/16 14:00:28 [16200] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/10/16 14:00:28 [16200] GAHP[16448] <- 'COMMANDS'
02/10/16 14:00:28 [16200] GAHP[16448] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/10/16 14:00:28 [16200] GAHP[16448] <- 'ASYNC_MODE_ON'
02/10/16 14:00:28 [16200] GAHP[16448] -> 'S' 'Async mode on'
02/10/16 14:00:28 [16200] (78.0) gm state change: GM_INIT -> GM_START
02/10/16 14:00:28 [16200] (78.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/10/16 14:00:28 [16200] (78.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 14:00:28 [16200] (78.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 14:00:28 [16200] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/10/16 14:00:28 [16200] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/10/16 14:00:28 [16200] IPVERIFY: ip found is 1
02/10/16 14:00:30 [16200] Evaluating staleness of remote job statuses.
02/10/16 14:00:33 [16200] resource  is now up
02/10/16 14:00:33 [16200] in doContactSchedd()
02/10/16 14:00:33 [16200] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:00:33 [16200] querying for removed/held jobs
02/10/16 14:00:33 [16200] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:00:33 [16200] Fetched 0 job ads from schedd
02/10/16 14:00:33 [16200] Updating classad values for 78.0:
02/10/16 14:00:33 [16200]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#78.0#1455130529"
02/10/16 14:00:33 [16200]    LastRemoteStatusUpdate = 1455130828
02/10/16 14:00:33 [16200] leaving doContactSchedd()
02/10/16 14:00:33 [16200] (78.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 14:00:33 [16200] (78.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 14:00:33 [16200] (78.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/10/16 14:00:33 [16200] GAHP[16448] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/77/0/cluster77.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/77/0/cluster77.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/77/0/cluster77.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#78.0#1455130529";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/10/16 14:00:33 [16200] GAHP[16448] -> 'S'
02/10/16 14:00:34 [16200] GAHP[16448] <- 'RESULTS'
02/10/16 14:00:34 [16200] GAHP[16448] -> 'R'
02/10/16 14:00:34 [16200] GAHP[16448] -> 'S' '1'
02/10/16 14:00:34 [16200] GAHP[16448] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/10/16 14:00:34 [16200] (78.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/10/16 14:00:34 [16200] (78.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/10/16 14:00:34 [16200] (78.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/10/16 14:00:34 [16200] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/10/16 14:00:34 [16200] (78.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/10/16 14:00:38 [16200] in doContactSchedd()
02/10/16 14:00:38 [16200] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:00:38 [16200] querying for removed/held jobs
02/10/16 14:00:38 [16200] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:00:38 [16200] Fetched 0 job ads from schedd
02/10/16 14:00:38 [16200] Updating classad values for 78.0:
02/10/16 14:00:38 [16200]    CurrentStatusUnknown = false
02/10/16 14:00:38 [16200]    GridJobId = undefined
02/10/16 14:00:38 [16200]    LastRemoteStatusUpdate = 0
02/10/16 14:00:38 [16200] leaving doContactSchedd()
02/10/16 14:00:38 [16200] (78.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/10/16 14:00:38 [16200] (78.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 14:00:38 [16200] (78.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 14:00:43 [16200] in doContactSchedd()
02/10/16 14:00:43 [16200] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:00:43 [16200] querying for removed/held jobs
02/10/16 14:00:43 [16200] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:00:43 [16200] Fetched 0 job ads from schedd
02/10/16 14:00:43 [16200] Updating classad values for 78.0:
02/10/16 14:00:43 [16200]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#78.0#1455130529"
02/10/16 14:00:43 [16200]    LastRemoteStatusUpdate = 1455130838
02/10/16 14:00:45 [16200] leaving doContactSchedd()
02/10/16 14:00:45 [16200] (78.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 14:00:45 [16200] (78.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 14:00:45 [16200] (78.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/10/16 14:00:45 [16200] (78.0) gm state change: GM_HOLD -> GM_DELETE
02/10/16 14:00:50 [16200] in doContactSchedd()
02/10/16 14:00:50 [16200] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:00:50 [16200] querying for removed/held jobs
02/10/16 14:00:50 [16200] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:00:50 [16200] Fetched 0 job ads from schedd
02/10/16 14:00:50 [16200] Updating classad values for 78.0:
02/10/16 14:00:50 [16200]    EnteredCurrentStatus = 1455130845
02/10/16 14:00:50 [16200]    HoldReason = "Attempts to submit failed: "
02/10/16 14:00:50 [16200]    HoldReasonCode = 0
02/10/16 14:00:50 [16200]    HoldReasonSubCode = 0
02/10/16 14:00:50 [16200]    JobStatus = 5
02/10/16 14:00:50 [16200]    LastReleaseReason = "Data files spooled"
02/10/16 14:00:50 [16200]    Managed = "Schedd"
02/10/16 14:00:50 [16200]    NumSystemHolds = 1
02/10/16 14:00:50 [16200]    ReleaseReason = undefined
02/10/16 14:00:50 [16200] No jobs left, shutting down
02/10/16 14:00:50 [16200] leaving doContactSchedd()
02/10/16 14:00:50 [16200] Got SIGTERM. Performing graceful shutdown.
02/10/16 14:00:50 [16200] Started timer to call main_shutdown_fast in 1800 seconds
02/10/16 14:00:50 [16200] **** condor_gridmanager (condor_GRIDMANAGER) pid 16200 EXITING WITH STATUS 0
02/10/16 14:42:58 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/10/16 14:42:58 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/10/16 14:42:58 Enumerating interfaces: lo 127.0.0.1 up
02/10/16 14:42:58 Enumerating interfaces: eth2 10.31.131.202 up
02/10/16 14:42:58 Enumerating interfaces: eth3 140.247.179.131 up
02/10/16 14:42:58 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/10/16 14:42:58 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/10/16 14:42:58 ******************************************************
02/10/16 14:42:58 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/10/16 14:42:58 ** /usr/sbin/condor_gridmanager
02/10/16 14:42:58 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/10/16 14:42:58 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/10/16 14:42:58 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/10/16 14:42:58 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/10/16 14:42:58 ** PID = 27099
02/10/16 14:42:58 ** Log last touched 2/10 14:00:50
02/10/16 14:42:58 ******************************************************
02/10/16 14:42:58 Using config source: /etc/condor-ce/condor_config
02/10/16 14:42:58 Using local config sources: 
02/10/16 14:42:58    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/10/16 14:42:58    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/01-ce-auth.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/01-ce-router.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/01-common-auth.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/02-ce-lsf.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/02-ce-pbs.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/03-managed-fork.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/50-osg-configure.conf
02/10/16 14:42:58    /etc/condor-ce/config.d/99-local.conf
02/10/16 14:42:58    /usr/share/condor-ce/condor_ce_router_defaults|
02/10/16 14:42:58 config Macros = 142, Sorted = 142, StringBytes = 12510, TablesBytes = 5320
02/10/16 14:42:58 CLASSAD_CACHING is ENABLED
02/10/16 14:42:58 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/10/16 14:42:58 SharedPortEndpoint: waiting for connections to named socket 24494_fd1d_2
02/10/16 14:42:58 DaemonCore: command socket at <140.247.179.131:9620?sock=24494_fd1d_2>
02/10/16 14:42:58 DaemonCore: private command socket at <140.247.179.131:9620?sock=24494_fd1d_2>
02/10/16 14:42:58 Setting maximum accepts per cycle 8.
02/10/16 14:42:58 Setting maximum reaps per cycle 8.
02/10/16 14:42:58 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 14:42:58 [27099] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/10/16 14:42:58 [27099] DaemonCore: No more children processes to reap.
02/10/16 14:42:58 [27099] DaemonCore: in SendAliveToParent()
02/10/16 14:42:58 [27099] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:42:58 [27099] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/10/16 14:42:58 [27099] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/10/16 14:42:58 [27099] IPVERIFY: ip found is 0
02/10/16 14:42:58 [27099] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/10/16 14:42:58 [27099] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/10/16 14:42:58 [27099] Buf::write(): condor_write() failed
02/10/16 14:42:58 [27099] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/10/16 14:42:58 [27099] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:42:58 [27099] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/10/16 14:42:58 [27099] DaemonCore: Leaving SendAliveToParent() - success
02/10/16 14:42:58 [27099] Checking proxies
02/10/16 14:43:01 [27099] Received ADD_JOBS signal
02/10/16 14:43:01 [27099] in doContactSchedd()
02/10/16 14:43:01 [27099] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:43:01 [27099] querying for new jobs
02/10/16 14:43:01 [27099] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/10/16 14:43:01 [27099] Using job type INFNBatch for job 80.0
02/10/16 14:43:01 [27099] (80.0) SetJobLeaseTimers()
02/10/16 14:43:01 [27099] Found job 80.0 --- inserting
02/10/16 14:43:01 [27099] Fetched 1 new job ads from schedd
02/10/16 14:43:01 [27099] querying for removed/held jobs
02/10/16 14:43:01 [27099] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:43:01 [27099] Fetched 0 job ads from schedd
02/10/16 14:43:01 [27099] leaving doContactSchedd()
02/10/16 14:43:01 [27099] gahp server not up yet, delaying ping
02/10/16 14:43:01 [27099] *** UpdateLeases called
02/10/16 14:43:01 [27099]     Leases not supported, cancelling timer
02/10/16 14:43:01 [27099] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=24494_fd1d_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=24479_0745_4>"
CurrentTime = time()
MyCurrentTime = 1455133381
IdleJobs = 1
JobLimit = 10000

02/10/16 14:43:01 [27099] Trying to update collector <10.31.131.202:9619>
02/10/16 14:43:01 [27099] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 14:43:01 [27099] File descriptor limits: max 4096, safe 3277
02/10/16 14:43:01 [27099] (80.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/10/16 14:43:01 [27099] GAHP server pid = 27403
02/10/16 14:43:01 [27099] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/10/16 14:43:01 [27099] GAHP[27403] <- 'COMMANDS'
02/10/16 14:43:01 [27099] GAHP[27403] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/10/16 14:43:01 [27099] GAHP[27403] <- 'ASYNC_MODE_ON'
02/10/16 14:43:01 [27099] GAHP[27403] -> 'S' 'Async mode on'
02/10/16 14:43:01 [27099] (80.0) gm state change: GM_INIT -> GM_START
02/10/16 14:43:01 [27099] (80.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/10/16 14:43:01 [27099] (80.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 14:43:01 [27099] (80.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 14:43:01 [27099] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/10/16 14:43:01 [27099] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/10/16 14:43:01 [27099] IPVERIFY: ip found is 1
02/10/16 14:43:03 [27099] Evaluating staleness of remote job statuses.
02/10/16 14:43:06 [27099] resource  is now up
02/10/16 14:43:06 [27099] in doContactSchedd()
02/10/16 14:43:06 [27099] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:43:06 [27099] querying for removed/held jobs
02/10/16 14:43:06 [27099] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:43:06 [27099] Fetched 0 job ads from schedd
02/10/16 14:43:06 [27099] Updating classad values for 80.0:
02/10/16 14:43:06 [27099]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#80.0#1455133375"
02/10/16 14:43:06 [27099]    LastRemoteStatusUpdate = 1455133381
02/10/16 14:43:06 [27099] leaving doContactSchedd()
02/10/16 14:43:06 [27099] (80.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 14:43:06 [27099] (80.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 14:43:06 [27099] (80.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/10/16 14:43:06 [27099] GAHP[27403] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/79/0/cluster79.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/79/0/cluster79.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/79/0/cluster79.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#80.0#1455133375";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/10/16 14:43:06 [27099] GAHP[27403] -> 'S'
02/10/16 14:43:08 [27099] GAHP[27403] <- 'RESULTS'
02/10/16 14:43:08 [27099] GAHP[27403] -> 'R'
02/10/16 14:43:08 [27099] GAHP[27403] -> 'S' '1'
02/10/16 14:43:08 [27099] GAHP[27403] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/10/16 14:43:08 [27099] (80.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/10/16 14:43:08 [27099] (80.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/10/16 14:43:08 [27099] (80.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/10/16 14:43:08 [27099] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/10/16 14:43:08 [27099] (80.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/10/16 14:43:11 [27099] in doContactSchedd()
02/10/16 14:43:11 [27099] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:43:11 [27099] querying for removed/held jobs
02/10/16 14:43:11 [27099] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:43:11 [27099] Fetched 0 job ads from schedd
02/10/16 14:43:11 [27099] Updating classad values for 80.0:
02/10/16 14:43:11 [27099]    CurrentStatusUnknown = false
02/10/16 14:43:11 [27099]    GridJobId = undefined
02/10/16 14:43:11 [27099]    LastRemoteStatusUpdate = 0
02/10/16 14:43:11 [27099] leaving doContactSchedd()
02/10/16 14:43:11 [27099] (80.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/10/16 14:43:11 [27099] (80.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 14:43:11 [27099] (80.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 14:43:16 [27099] in doContactSchedd()
02/10/16 14:43:16 [27099] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:43:16 [27099] querying for removed/held jobs
02/10/16 14:43:16 [27099] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:43:16 [27099] Fetched 0 job ads from schedd
02/10/16 14:43:16 [27099] Updating classad values for 80.0:
02/10/16 14:43:16 [27099]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#80.0#1455133375"
02/10/16 14:43:16 [27099]    LastRemoteStatusUpdate = 1455133391
02/10/16 14:43:16 [27099] leaving doContactSchedd()
02/10/16 14:43:16 [27099] (80.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 14:43:16 [27099] (80.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 14:43:16 [27099] (80.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/10/16 14:43:16 [27099] (80.0) gm state change: GM_HOLD -> GM_DELETE
02/10/16 14:43:21 [27099] in doContactSchedd()
02/10/16 14:43:21 [27099] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 14:43:21 [27099] querying for removed/held jobs
02/10/16 14:43:21 [27099] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 14:43:21 [27099] Fetched 0 job ads from schedd
02/10/16 14:43:21 [27099] Updating classad values for 80.0:
02/10/16 14:43:21 [27099]    EnteredCurrentStatus = 1455133396
02/10/16 14:43:21 [27099]    HoldReason = "Attempts to submit failed: "
02/10/16 14:43:21 [27099]    HoldReasonCode = 0
02/10/16 14:43:21 [27099]    HoldReasonSubCode = 0
02/10/16 14:43:21 [27099]    JobStatus = 5
02/10/16 14:43:21 [27099]    LastReleaseReason = "Data files spooled"
02/10/16 14:43:21 [27099]    Managed = "Schedd"
02/10/16 14:43:21 [27099]    NumSystemHolds = 1
02/10/16 14:43:21 [27099]    ReleaseReason = undefined
02/10/16 14:43:21 [27099] No jobs left, shutting down
02/10/16 14:43:21 [27099] leaving doContactSchedd()
02/10/16 14:43:21 [27099] Got SIGTERM. Performing graceful shutdown.
02/10/16 14:43:21 [27099] Started timer to call main_shutdown_fast in 1800 seconds
02/10/16 14:43:21 [27099] **** condor_gridmanager (condor_GRIDMANAGER) pid 27099 EXITING WITH STATUS 0
02/10/16 15:43:30 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/10/16 15:43:30 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/10/16 15:43:30 Enumerating interfaces: lo 127.0.0.1 up
02/10/16 15:43:30 Enumerating interfaces: eth2 10.31.131.202 up
02/10/16 15:43:30 Enumerating interfaces: eth3 140.247.179.131 up
02/10/16 15:43:30 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/10/16 15:43:30 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/10/16 15:43:30 ******************************************************
02/10/16 15:43:30 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/10/16 15:43:30 ** /usr/sbin/condor_gridmanager
02/10/16 15:43:30 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/10/16 15:43:30 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/10/16 15:43:30 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/10/16 15:43:30 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/10/16 15:43:30 ** PID = 6814
02/10/16 15:43:30 ** Log last touched 2/10 14:43:21
02/10/16 15:43:30 ******************************************************
02/10/16 15:43:30 Using config source: /etc/condor-ce/condor_config
02/10/16 15:43:30 Using local config sources: 
02/10/16 15:43:30    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/10/16 15:43:30    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/01-ce-auth.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/01-ce-router.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/01-common-auth.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/02-ce-lsf.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/02-ce-pbs.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/03-managed-fork.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/50-osg-configure.conf
02/10/16 15:43:30    /etc/condor-ce/config.d/99-local.conf
02/10/16 15:43:30    /usr/share/condor-ce/condor_ce_router_defaults|
02/10/16 15:43:30 config Macros = 142, Sorted = 142, StringBytes = 12508, TablesBytes = 5320
02/10/16 15:43:30 CLASSAD_CACHING is ENABLED
02/10/16 15:43:30 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/10/16 15:43:30 SharedPortEndpoint: waiting for connections to named socket 24494_fd1d_3
02/10/16 15:43:30 DaemonCore: command socket at <140.247.179.131:9620?sock=24494_fd1d_3>
02/10/16 15:43:30 DaemonCore: private command socket at <140.247.179.131:9620?sock=24494_fd1d_3>
02/10/16 15:43:30 Setting maximum accepts per cycle 8.
02/10/16 15:43:30 Setting maximum reaps per cycle 8.
02/10/16 15:43:30 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 15:43:30 [6814] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/10/16 15:43:30 [6814] DaemonCore: No more children processes to reap.
02/10/16 15:43:30 [6814] DaemonCore: in SendAliveToParent()
02/10/16 15:43:30 [6814] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 15:43:31 [6814] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/10/16 15:43:31 [6814] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/10/16 15:43:31 [6814] IPVERIFY: ip found is 0
02/10/16 15:43:31 [6814] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/10/16 15:43:31 [6814] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/10/16 15:43:31 [6814] Buf::write(): condor_write() failed
02/10/16 15:43:31 [6814] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/10/16 15:43:31 [6814] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 15:43:31 [6814] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/10/16 15:43:31 [6814] DaemonCore: Leaving SendAliveToParent() - success
02/10/16 15:43:31 [6814] Checking proxies
02/10/16 15:43:33 [6814] Received ADD_JOBS signal
02/10/16 15:43:33 [6814] in doContactSchedd()
02/10/16 15:43:33 [6814] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 15:43:33 [6814] querying for new jobs
02/10/16 15:43:33 [6814] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/10/16 15:43:33 [6814] Using job type INFNBatch for job 82.0
02/10/16 15:43:33 [6814] (82.0) SetJobLeaseTimers()
02/10/16 15:43:33 [6814] Found job 82.0 --- inserting
02/10/16 15:43:33 [6814] Fetched 1 new job ads from schedd
02/10/16 15:43:33 [6814] querying for removed/held jobs
02/10/16 15:43:33 [6814] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 15:43:33 [6814] Fetched 0 job ads from schedd
02/10/16 15:43:34 [6814] leaving doContactSchedd()
02/10/16 15:43:34 [6814] gahp server not up yet, delaying ping
02/10/16 15:43:34 [6814] *** UpdateLeases called
02/10/16 15:43:34 [6814]     Leases not supported, cancelling timer
02/10/16 15:43:34 [6814] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=24494_fd1d_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=24479_0745_4>"
CurrentTime = time()
MyCurrentTime = 1455137014
IdleJobs = 1
JobLimit = 10000

02/10/16 15:43:34 [6814] Trying to update collector <10.31.131.202:9619>
02/10/16 15:43:34 [6814] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/10/16 15:43:34 [6814] File descriptor limits: max 4096, safe 3277
02/10/16 15:43:34 [6814] (82.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/10/16 15:43:34 [6814] GAHP server pid = 7052
02/10/16 15:43:34 [6814] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/10/16 15:43:34 [6814] GAHP[7052] <- 'COMMANDS'
02/10/16 15:43:34 [6814] GAHP[7052] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/10/16 15:43:34 [6814] GAHP[7052] <- 'ASYNC_MODE_ON'
02/10/16 15:43:34 [6814] GAHP[7052] -> 'S' 'Async mode on'
02/10/16 15:43:34 [6814] (82.0) gm state change: GM_INIT -> GM_START
02/10/16 15:43:34 [6814] (82.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/10/16 15:43:34 [6814] (82.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 15:43:34 [6814] (82.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 15:43:34 [6814] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/10/16 15:43:34 [6814] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/10/16 15:43:34 [6814] IPVERIFY: ip found is 1
02/10/16 15:43:35 [6814] Evaluating staleness of remote job statuses.
02/10/16 15:43:39 [6814] resource  is now up
02/10/16 15:43:39 [6814] in doContactSchedd()
02/10/16 15:43:39 [6814] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 15:43:39 [6814] querying for removed/held jobs
02/10/16 15:43:39 [6814] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 15:43:39 [6814] Fetched 0 job ads from schedd
02/10/16 15:43:39 [6814] Updating classad values for 82.0:
02/10/16 15:43:39 [6814]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#82.0#1455137001"
02/10/16 15:43:39 [6814]    LastRemoteStatusUpdate = 1455137014
02/10/16 15:43:39 [6814] leaving doContactSchedd()
02/10/16 15:43:39 [6814] (82.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 15:43:39 [6814] (82.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 15:43:39 [6814] (82.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/10/16 15:43:39 [6814] GAHP[7052] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#82.0#1455137001";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/10/16 15:43:39 [6814] GAHP[7052] -> 'S'
02/10/16 15:43:43 [6814] GAHP[7052] <- 'RESULTS'
02/10/16 15:43:43 [6814] GAHP[7052] -> 'R'
02/10/16 15:43:43 [6814] GAHP[7052] -> 'S' '1'
02/10/16 15:43:43 [6814] GAHP[7052] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/10/16 15:43:43 [6814] (82.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/10/16 15:43:43 [6814] (82.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/10/16 15:43:43 [6814] (82.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/10/16 15:43:43 [6814] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/10/16 15:43:58 [6814] (82.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/10/16 15:43:58 [6814] in doContactSchedd()
02/10/16 15:43:58 [6814] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 15:43:58 [6814] querying for removed/held jobs
02/10/16 15:43:58 [6814] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 15:43:58 [6814] Fetched 0 job ads from schedd
02/10/16 15:43:58 [6814] Updating classad values for 82.0:
02/10/16 15:43:58 [6814]    CurrentStatusUnknown = false
02/10/16 15:43:58 [6814]    GridJobId = undefined
02/10/16 15:43:58 [6814]    LastRemoteStatusUpdate = 0
02/10/16 15:43:58 [6814] leaving doContactSchedd()
02/10/16 15:43:58 [6814] (82.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/10/16 15:43:58 [6814] (82.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/10/16 15:43:58 [6814] (82.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/10/16 15:44:03 [6814] in doContactSchedd()
02/10/16 15:44:03 [6814] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 15:44:03 [6814] querying for removed/held jobs
02/10/16 15:44:03 [6814] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 15:44:03 [6814] Fetched 0 job ads from schedd
02/10/16 15:44:03 [6814] Updating classad values for 82.0:
02/10/16 15:44:03 [6814]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#82.0#1455137001"
02/10/16 15:44:03 [6814]    LastRemoteStatusUpdate = 1455137038
02/10/16 15:44:03 [6814] leaving doContactSchedd()
02/10/16 15:44:03 [6814] (82.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/10/16 15:44:03 [6814] (82.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/10/16 15:44:03 [6814] (82.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/10/16 15:44:03 [6814] (82.0) gm state change: GM_HOLD -> GM_DELETE
02/10/16 15:44:08 [6814] in doContactSchedd()
02/10/16 15:44:08 [6814] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 24479_0745_4
02/10/16 15:44:08 [6814] querying for removed/held jobs
02/10/16 15:44:08 [6814] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/10/16 15:44:08 [6814] Fetched 0 job ads from schedd
02/10/16 15:44:08 [6814] Updating classad values for 82.0:
02/10/16 15:44:08 [6814]    EnteredCurrentStatus = 1455137043
02/10/16 15:44:08 [6814]    HoldReason = "Attempts to submit failed: "
02/10/16 15:44:08 [6814]    HoldReasonCode = 0
02/10/16 15:44:08 [6814]    HoldReasonSubCode = 0
02/10/16 15:44:08 [6814]    JobStatus = 5
02/10/16 15:44:08 [6814]    LastReleaseReason = "Data files spooled"
02/10/16 15:44:08 [6814]    Managed = "Schedd"
02/10/16 15:44:08 [6814]    NumSystemHolds = 1
02/10/16 15:44:08 [6814]    ReleaseReason = undefined
02/10/16 15:44:09 [6814] No jobs left, shutting down
02/10/16 15:44:09 [6814] leaving doContactSchedd()
02/10/16 15:44:09 [6814] Got SIGTERM. Performing graceful shutdown.
02/10/16 15:44:09 [6814] Started timer to call main_shutdown_fast in 1800 seconds
02/10/16 15:44:09 [6814] **** condor_gridmanager (condor_GRIDMANAGER) pid 6814 EXITING WITH STATUS 0
02/11/16 10:40:31 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/11/16 10:40:31 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/11/16 10:40:31 Enumerating interfaces: lo 127.0.0.1 up
02/11/16 10:40:31 Enumerating interfaces: eth2 10.31.131.202 up
02/11/16 10:40:31 Enumerating interfaces: eth3 140.247.179.131 up
02/11/16 10:40:31 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/11/16 10:40:31 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/11/16 10:40:31 ******************************************************
02/11/16 10:40:31 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/11/16 10:40:31 ** /usr/sbin/condor_gridmanager
02/11/16 10:40:31 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/11/16 10:40:31 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/11/16 10:40:31 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/11/16 10:40:31 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/11/16 10:40:31 ** PID = 31144
02/11/16 10:40:31 ** Log last touched 2/10 15:44:09
02/11/16 10:40:31 ******************************************************
02/11/16 10:40:31 Using config source: /etc/condor-ce/condor_config
02/11/16 10:40:31 Using local config sources: 
02/11/16 10:40:31    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/11/16 10:40:31    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/01-ce-auth.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/01-ce-router.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/01-common-auth.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/02-ce-lsf.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/02-ce-pbs.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/03-managed-fork.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/50-osg-configure.conf
02/11/16 10:40:31    /etc/condor-ce/config.d/99-local.conf
02/11/16 10:40:31    /usr/share/condor-ce/condor_ce_router_defaults|
02/11/16 10:40:31 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
02/11/16 10:40:31 CLASSAD_CACHING is ENABLED
02/11/16 10:40:31 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/11/16 10:40:31 SharedPortEndpoint: waiting for connections to named socket 13134_d74f_1
02/11/16 10:40:31 DaemonCore: command socket at <140.247.179.131:9620?sock=13134_d74f_1>
02/11/16 10:40:31 DaemonCore: private command socket at <140.247.179.131:9620?sock=13134_d74f_1>
02/11/16 10:40:31 Setting maximum accepts per cycle 8.
02/11/16 10:40:31 Setting maximum reaps per cycle 8.
02/11/16 10:40:31 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 10:40:31 [31144] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/11/16 10:40:31 [31144] DaemonCore: No more children processes to reap.
02/11/16 10:40:31 [31144] DaemonCore: in SendAliveToParent()
02/11/16 10:40:31 [31144] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13127_31c2_4
02/11/16 10:40:32 [31144] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/11/16 10:40:32 [31144] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/11/16 10:40:32 [31144] IPVERIFY: ip found is 0
02/11/16 10:40:32 [31144] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/11/16 10:40:32 [31144] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/11/16 10:40:32 [31144] Buf::write(): condor_write() failed
02/11/16 10:40:32 [31144] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/11/16 10:40:32 [31144] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13127_31c2_4
02/11/16 10:40:32 [31144] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/11/16 10:40:32 [31144] DaemonCore: Leaving SendAliveToParent() - success
02/11/16 10:40:32 [31144] Checking proxies
02/11/16 10:40:33 [31144] Received REMOVE_JOBS signal
02/11/16 10:40:33 [31144] in doContactSchedd()
02/11/16 10:40:33 [31144] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13127_31c2_4
02/11/16 10:40:33 [31144] querying for new jobs
02/11/16 10:40:33 [31144] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/11/16 10:40:33 [31144] Using job type INFNBatch for job 74.0
02/11/16 10:40:33 [31144] (74.0) SetJobLeaseTimers()
02/11/16 10:40:33 [31144] Failed to get expiration time of proxy /n/atlasgrid/condor/73/0/cluster73.proc0.subproc0/x509up_u556792
02/11/16 10:40:33 [31144] Found job 74.0 --- inserting
02/11/16 10:40:33 [31144] Using job type INFNBatch for job 84.0
02/11/16 10:40:33 [31144] (84.0) SetJobLeaseTimers()
02/11/16 10:40:33 [31144] Found job 84.0 --- inserting
02/11/16 10:40:33 [31144] Fetched 2 new job ads from schedd
02/11/16 10:40:33 [31144] querying for removed/held jobs
02/11/16 10:40:33 [31144] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 10:40:33 [31144] Fetched 1 job ads from schedd
02/11/16 10:40:33 [31144] leaving doContactSchedd()
02/11/16 10:40:33 [31144] gahp server not up yet, delaying ping
02/11/16 10:40:33 [31144] *** UpdateLeases called
02/11/16 10:40:33 [31144]     Leases not supported, cancelling timer
02/11/16 10:40:33 [31144] BaseResource::UpdateResource: 
NumJobs = 2
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=13134_d74f_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=13127_31c2_4>"
CurrentTime = time()
MyCurrentTime = 1455205233
IdleJobs = 1
JobLimit = 10000

02/11/16 10:40:33 [31144] Trying to update collector <10.31.131.202:9619>
02/11/16 10:40:33 [31144] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 10:40:33 [31144] File descriptor limits: max 4096, safe 3277
02/11/16 10:40:33 [31144] (74.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/11/16 10:40:34 [31144] GAHP server pid = 31319
02/11/16 10:40:34 [31144] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/11/16 10:40:34 [31144] GAHP[31319] <- 'COMMANDS'
02/11/16 10:40:34 [31144] GAHP[31319] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/11/16 10:40:34 [31144] GAHP[31319] <- 'ASYNC_MODE_ON'
02/11/16 10:40:34 [31144] GAHP[31319] -> 'S' 'Async mode on'
02/11/16 10:40:34 [31144] (74.0) gm state change: GM_INIT -> GM_START
02/11/16 10:40:34 [31144] (74.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/11/16 10:40:34 [31144] (74.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/11/16 10:40:34 [31144] GAHP[31319] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#74.0#1455117872";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/11/16 10:40:34 [31144] GAHP[31319] -> 'S'
02/11/16 10:40:34 [31144] (84.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/11/16 10:40:34 [31144] (84.0) gm state change: GM_INIT -> GM_START
02/11/16 10:40:34 [31144] (84.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/11/16 10:40:34 [31144] (84.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/11/16 10:40:34 [31144] (84.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/11/16 10:40:34 [31144] GAHP[31319] <- 'RESULTS'
02/11/16 10:40:34 [31144] GAHP[31319] -> 'R'
02/11/16 10:40:34 [31144] GAHP[31319] -> 'S' '1'
02/11/16 10:40:34 [31144] GAHP[31319] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
02/11/16 10:40:34 [31144] (74.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/11/16 10:40:34 [31144] (74.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/73/0/cluster73.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
02/11/16 10:40:34 [31144] (74.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/11/16 10:40:34 [31144] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/11/16 10:40:34 [31144] (74.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/11/16 10:40:34 [31144] Received ADD_JOBS signal
02/11/16 10:40:34 [31144] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/11/16 10:40:34 [31144] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/11/16 10:40:34 [31144] IPVERIFY: ip found is 1
02/11/16 10:40:36 [31144] Evaluating staleness of remote job statuses.
02/11/16 10:40:38 [31144] resource  is now up
02/11/16 10:40:38 [31144] in doContactSchedd()
02/11/16 10:40:38 [31144] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13127_31c2_4
02/11/16 10:40:38 [31144] querying for new jobs
02/11/16 10:40:38 [31144] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/11/16 10:40:38 [31144] Fetched 0 new job ads from schedd
02/11/16 10:40:38 [31144] querying for removed/held jobs
02/11/16 10:40:38 [31144] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 10:40:38 [31144] Fetched 1 job ads from schedd
02/11/16 10:40:38 [31144] Updating classad values for 84.0:
02/11/16 10:40:38 [31144]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#84.0#1455205061"
02/11/16 10:40:38 [31144]    LastRemoteStatusUpdate = 1455205234
02/11/16 10:40:38 [31144] Updating classad values for 74.0:
02/11/16 10:40:38 [31144]    CurrentStatusUnknown = false
02/11/16 10:40:38 [31144]    GridJobId = undefined
02/11/16 10:40:38 [31144]    LastRemoteStatusUpdate = 0
02/11/16 10:40:38 [31144]    Managed = "ScheddDone"
02/11/16 10:40:38 [31144] Deleting job 74.0 from schedd
02/11/16 10:40:38 [31144] leaving doContactSchedd()
02/11/16 10:40:38 [31144] (84.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/11/16 10:40:38 [31144] (84.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/11/16 10:40:38 [31144] (84.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/11/16 10:40:38 [31144] GAHP[31319] <- 'BLAH_JOB_SUBMIT 3 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#84.0#1455205061";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/11/16 10:40:38 [31144] GAHP[31319] -> 'S'
02/11/16 10:40:41 [31144] GAHP[31319] <- 'RESULTS'
02/11/16 10:40:41 [31144] GAHP[31319] -> 'R'
02/11/16 10:40:41 [31144] GAHP[31319] -> 'S' '1'
02/11/16 10:40:41 [31144] GAHP[31319] -> '3' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/11/16 10:40:41 [31144] (84.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/11/16 10:40:41 [31144] (84.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/11/16 10:40:41 [31144] (84.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/11/16 10:40:41 [31144] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/11/16 10:40:41 [31144] (84.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/11/16 10:40:43 [31144] in doContactSchedd()
02/11/16 10:40:43 [31144] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13127_31c2_4
02/11/16 10:40:43 [31144] querying for removed/held jobs
02/11/16 10:40:43 [31144] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 10:40:43 [31144] Fetched 0 job ads from schedd
02/11/16 10:40:43 [31144] Updating classad values for 84.0:
02/11/16 10:40:43 [31144]    CurrentStatusUnknown = false
02/11/16 10:40:43 [31144]    GridJobId = undefined
02/11/16 10:40:43 [31144]    LastRemoteStatusUpdate = 0
02/11/16 10:40:43 [31144] leaving doContactSchedd()
02/11/16 10:40:43 [31144] (84.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/11/16 10:40:43 [31144] (84.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/11/16 10:40:43 [31144] (84.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/11/16 10:40:48 [31144] in doContactSchedd()
02/11/16 10:40:48 [31144] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13127_31c2_4
02/11/16 10:40:48 [31144] querying for removed/held jobs
02/11/16 10:40:48 [31144] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 10:40:48 [31144] Fetched 0 job ads from schedd
02/11/16 10:40:48 [31144] Updating classad values for 84.0:
02/11/16 10:40:48 [31144]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#84.0#1455205061"
02/11/16 10:40:48 [31144]    LastRemoteStatusUpdate = 1455205243
02/11/16 10:40:48 [31144] leaving doContactSchedd()
02/11/16 10:40:48 [31144] (84.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/11/16 10:40:48 [31144] (84.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/11/16 10:40:48 [31144] (84.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/11/16 10:40:48 [31144] (84.0) gm state change: GM_HOLD -> GM_DELETE
02/11/16 10:40:53 [31144] in doContactSchedd()
02/11/16 10:40:53 [31144] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13127_31c2_4
02/11/16 10:40:53 [31144] querying for removed/held jobs
02/11/16 10:40:53 [31144] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 10:40:53 [31144] Fetched 0 job ads from schedd
02/11/16 10:40:53 [31144] Updating classad values for 84.0:
02/11/16 10:40:53 [31144]    EnteredCurrentStatus = 1455205248
02/11/16 10:40:53 [31144]    HoldReason = "Attempts to submit failed: "
02/11/16 10:40:53 [31144]    HoldReasonCode = 0
02/11/16 10:40:53 [31144]    HoldReasonSubCode = 0
02/11/16 10:40:53 [31144]    JobStatus = 5
02/11/16 10:40:53 [31144]    LastReleaseReason = "Data files spooled"
02/11/16 10:40:53 [31144]    Managed = "Schedd"
02/11/16 10:40:53 [31144]    NumSystemHolds = 1
02/11/16 10:40:53 [31144]    ReleaseReason = undefined
02/11/16 10:40:53 [31144] No jobs left, shutting down
02/11/16 10:40:53 [31144] leaving doContactSchedd()
02/11/16 10:40:53 [31144] Got SIGTERM. Performing graceful shutdown.
02/11/16 10:40:53 [31144] Started timer to call main_shutdown_fast in 1800 seconds
02/11/16 10:40:53 [31144] **** condor_gridmanager (condor_GRIDMANAGER) pid 31144 EXITING WITH STATUS 0
02/11/16 13:46:01 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/11/16 13:46:01 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/11/16 13:46:01 Enumerating interfaces: lo 127.0.0.1 up
02/11/16 13:46:01 Enumerating interfaces: eth2 10.31.131.202 up
02/11/16 13:46:01 Enumerating interfaces: eth3 140.247.179.131 up
02/11/16 13:46:01 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/11/16 13:46:01 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/11/16 13:46:01 ******************************************************
02/11/16 13:46:01 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/11/16 13:46:01 ** /usr/sbin/condor_gridmanager
02/11/16 13:46:01 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/11/16 13:46:01 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/11/16 13:46:01 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/11/16 13:46:01 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/11/16 13:46:01 ** PID = 27738
02/11/16 13:46:01 ** Log last touched 2/11 10:40:53
02/11/16 13:46:01 ******************************************************
02/11/16 13:46:01 Using config source: /etc/condor-ce/condor_config
02/11/16 13:46:01 Using local config sources: 
02/11/16 13:46:01    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/11/16 13:46:01    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/01-ce-auth.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/01-ce-router.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/01-common-auth.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/02-ce-lsf.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/02-ce-pbs.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/03-managed-fork.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/50-osg-configure.conf
02/11/16 13:46:01    /etc/condor-ce/config.d/99-local.conf
02/11/16 13:46:01    /usr/share/condor-ce/condor_ce_router_defaults|
02/11/16 13:46:01 config Macros = 144, Sorted = 144, StringBytes = 12554, TablesBytes = 5392
02/11/16 13:46:01 CLASSAD_CACHING is ENABLED
02/11/16 13:46:01 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/11/16 13:46:01 SharedPortEndpoint: waiting for connections to named socket 5579_a8fc_1
02/11/16 13:46:01 DaemonCore: command socket at <140.247.179.131:9620?sock=5579_a8fc_1>
02/11/16 13:46:01 DaemonCore: private command socket at <140.247.179.131:9620?sock=5579_a8fc_1>
02/11/16 13:46:01 Setting maximum accepts per cycle 8.
02/11/16 13:46:01 Setting maximum reaps per cycle 8.
02/11/16 13:46:01 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 13:46:01 [27738] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/11/16 13:46:01 [27738] DaemonCore: No more children processes to reap.
02/11/16 13:46:01 [27738] DaemonCore: in SendAliveToParent()
02/11/16 13:46:01 [27738] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 13:46:02 [27738] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/11/16 13:46:02 [27738] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/11/16 13:46:02 [27738] IPVERIFY: ip found is 0
02/11/16 13:46:02 [27738] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/11/16 13:46:02 [27738] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/11/16 13:46:02 [27738] Buf::write(): condor_write() failed
02/11/16 13:46:02 [27738] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/11/16 13:46:02 [27738] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 13:46:02 [27738] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/11/16 13:46:02 [27738] DaemonCore: Leaving SendAliveToParent() - success
02/11/16 13:46:02 [27738] Checking proxies
02/11/16 13:46:03 [27738] Received REMOVE_JOBS signal
02/11/16 13:46:03 [27738] in doContactSchedd()
02/11/16 13:46:03 [27738] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 13:46:03 [27738] querying for new jobs
02/11/16 13:46:03 [27738] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/11/16 13:46:03 [27738] Using job type INFNBatch for job 76.0
02/11/16 13:46:03 [27738] (76.0) SetJobLeaseTimers()
02/11/16 13:46:03 [27738] Found job 76.0 --- inserting
02/11/16 13:46:03 [27738] Fetched 1 new job ads from schedd
02/11/16 13:46:03 [27738] querying for removed/held jobs
02/11/16 13:46:03 [27738] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 13:46:03 [27738] Fetched 1 job ads from schedd
02/11/16 13:46:03 [27738] leaving doContactSchedd()
02/11/16 13:46:03 [27738] gahp server not up yet, delaying ping
02/11/16 13:46:03 [27738] *** UpdateLeases called
02/11/16 13:46:03 [27738]     Leases not supported, cancelling timer
02/11/16 13:46:03 [27738] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=5579_a8fc_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=5440_cf3f_4>"
CurrentTime = time()
MyCurrentTime = 1455216363
IdleJobs = 0
JobLimit = 10000

02/11/16 13:46:03 [27738] Trying to update collector <10.31.131.202:9619>
02/11/16 13:46:03 [27738] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 13:46:03 [27738] File descriptor limits: max 4096, safe 3277
02/11/16 13:46:03 [27738] (76.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/11/16 13:46:03 [27738] GAHP server pid = 27749
02/11/16 13:46:03 [27738] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/11/16 13:46:03 [27738] GAHP[27749] <- 'COMMANDS'
02/11/16 13:46:03 [27738] GAHP[27749] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/11/16 13:46:03 [27738] GAHP[27749] <- 'ASYNC_MODE_ON'
02/11/16 13:46:03 [27738] GAHP[27749] -> 'S' 'Async mode on'
02/11/16 13:46:03 [27738] (76.0) gm state change: GM_INIT -> GM_START
02/11/16 13:46:03 [27738] (76.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/11/16 13:46:03 [27738] (76.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/11/16 13:46:03 [27738] GAHP[27749] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/75/0/cluster75.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/75/0/cluster75.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/75/0/cluster75.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#76.0#1455129835";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/11/16 13:46:03 [27738] GAHP[27749] -> 'S'
02/11/16 13:46:03 [27738] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/11/16 13:46:03 [27738] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/11/16 13:46:03 [27738] IPVERIFY: ip found is 1
02/11/16 13:46:03 [27738] GAHP[27749] <- 'RESULTS'
02/11/16 13:46:03 [27738] GAHP[27749] -> 'R'
02/11/16 13:46:03 [27738] GAHP[27749] -> 'S' '1'
02/11/16 13:46:03 [27738] GAHP[27749] -> '2' '1' 'Unable to limit the proxy' 'N/A'
02/11/16 13:46:03 [27738] (76.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/11/16 13:46:03 [27738] (76.0) blah_job_submit() failed: Unable to limit the proxy
02/11/16 13:46:03 [27738] (76.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/11/16 13:46:03 [27738] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/11/16 13:46:03 [27738] (76.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/11/16 13:46:04 [27738] Received ADD_JOBS signal
02/11/16 13:46:06 [27738] Evaluating staleness of remote job statuses.
02/11/16 13:46:08 [27738] resource  is now up
02/11/16 13:46:08 [27738] in doContactSchedd()
02/11/16 13:46:08 [27738] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 13:46:08 [27738] querying for new jobs
02/11/16 13:46:08 [27738] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/11/16 13:46:08 [27738] Fetched 0 new job ads from schedd
02/11/16 13:46:08 [27738] querying for removed/held jobs
02/11/16 13:46:08 [27738] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 13:46:08 [27738] Fetched 1 job ads from schedd
02/11/16 13:46:08 [27738] Updating classad values for 76.0:
02/11/16 13:46:08 [27738]    CurrentStatusUnknown = false
02/11/16 13:46:08 [27738]    GridJobId = undefined
02/11/16 13:46:08 [27738]    LastRemoteStatusUpdate = 0
02/11/16 13:46:08 [27738]    Managed = "ScheddDone"
02/11/16 13:46:08 [27738] Deleting job 76.0 from schedd
02/11/16 13:46:08 [27738] No jobs left, shutting down
02/11/16 13:46:08 [27738] leaving doContactSchedd()
02/11/16 13:46:08 [27738] Got SIGTERM. Performing graceful shutdown.
02/11/16 13:46:08 [27738] Started timer to call main_shutdown_fast in 1800 seconds
02/11/16 13:46:08 [27738] **** condor_gridmanager (condor_GRIDMANAGER) pid 27738 EXITING WITH STATUS 0
02/11/16 14:01:01 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/11/16 14:01:01 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/11/16 14:01:01 Enumerating interfaces: lo 127.0.0.1 up
02/11/16 14:01:01 Enumerating interfaces: eth2 10.31.131.202 up
02/11/16 14:01:01 Enumerating interfaces: eth3 140.247.179.131 up
02/11/16 14:01:01 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/11/16 14:01:01 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/11/16 14:01:01 ******************************************************
02/11/16 14:01:01 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/11/16 14:01:01 ** /usr/sbin/condor_gridmanager
02/11/16 14:01:01 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/11/16 14:01:01 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/11/16 14:01:01 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/11/16 14:01:01 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/11/16 14:01:01 ** PID = 2167
02/11/16 14:01:01 ** Log last touched 2/11 13:46:08
02/11/16 14:01:01 ******************************************************
02/11/16 14:01:01 Using config source: /etc/condor-ce/condor_config
02/11/16 14:01:01 Using local config sources: 
02/11/16 14:01:01    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/11/16 14:01:01    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/01-ce-auth.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/01-ce-router.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/01-common-auth.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/02-ce-lsf.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/02-ce-pbs.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/03-managed-fork.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/50-osg-configure.conf
02/11/16 14:01:01    /etc/condor-ce/config.d/99-local.conf
02/11/16 14:01:01    /usr/share/condor-ce/condor_ce_router_defaults|
02/11/16 14:01:01 config Macros = 144, Sorted = 144, StringBytes = 12552, TablesBytes = 5392
02/11/16 14:01:01 CLASSAD_CACHING is ENABLED
02/11/16 14:01:01 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/11/16 14:01:01 SharedPortEndpoint: waiting for connections to named socket 5579_a8fc_2
02/11/16 14:01:01 DaemonCore: command socket at <140.247.179.131:9620?sock=5579_a8fc_2>
02/11/16 14:01:01 DaemonCore: private command socket at <140.247.179.131:9620?sock=5579_a8fc_2>
02/11/16 14:01:01 Setting maximum accepts per cycle 8.
02/11/16 14:01:01 Setting maximum reaps per cycle 8.
02/11/16 14:01:01 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 14:01:01 [2167] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/11/16 14:01:01 [2167] DaemonCore: No more children processes to reap.
02/11/16 14:01:01 [2167] DaemonCore: in SendAliveToParent()
02/11/16 14:01:01 [2167] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:01:02 [2167] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/11/16 14:01:02 [2167] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/11/16 14:01:02 [2167] IPVERIFY: ip found is 0
02/11/16 14:01:02 [2167] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/11/16 14:01:02 [2167] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/11/16 14:01:02 [2167] Buf::write(): condor_write() failed
02/11/16 14:01:02 [2167] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/11/16 14:01:02 [2167] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:01:02 [2167] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/11/16 14:01:02 [2167] DaemonCore: Leaving SendAliveToParent() - success
02/11/16 14:01:02 [2167] Checking proxies
02/11/16 14:01:03 [2167] Received REMOVE_JOBS signal
02/11/16 14:01:03 [2167] in doContactSchedd()
02/11/16 14:01:03 [2167] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:01:03 [2167] querying for new jobs
02/11/16 14:01:03 [2167] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/11/16 14:01:03 [2167] Using job type INFNBatch for job 78.0
02/11/16 14:01:03 [2167] (78.0) SetJobLeaseTimers()
02/11/16 14:01:04 [2167] Found job 78.0 --- inserting
02/11/16 14:01:04 [2167] Fetched 1 new job ads from schedd
02/11/16 14:01:04 [2167] querying for removed/held jobs
02/11/16 14:01:04 [2167] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 14:01:04 [2167] Fetched 1 job ads from schedd
02/11/16 14:01:04 [2167] leaving doContactSchedd()
02/11/16 14:01:04 [2167] gahp server not up yet, delaying ping
02/11/16 14:01:04 [2167] *** UpdateLeases called
02/11/16 14:01:04 [2167]     Leases not supported, cancelling timer
02/11/16 14:01:04 [2167] Received ADD_JOBS signal
02/11/16 14:01:04 [2167] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=5579_a8fc_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=5440_cf3f_4>"
CurrentTime = time()
MyCurrentTime = 1455217264
IdleJobs = 0
JobLimit = 10000

02/11/16 14:01:04 [2167] Trying to update collector <10.31.131.202:9619>
02/11/16 14:01:04 [2167] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 14:01:04 [2167] File descriptor limits: max 4096, safe 3277
02/11/16 14:01:04 [2167] (78.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/11/16 14:01:04 [2167] GAHP server pid = 2605
02/11/16 14:01:04 [2167] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/11/16 14:01:04 [2167] GAHP[2605] <- 'COMMANDS'
02/11/16 14:01:04 [2167] GAHP[2605] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/11/16 14:01:04 [2167] GAHP[2605] <- 'ASYNC_MODE_ON'
02/11/16 14:01:04 [2167] GAHP[2605] -> 'S' 'Async mode on'
02/11/16 14:01:04 [2167] (78.0) gm state change: GM_INIT -> GM_START
02/11/16 14:01:04 [2167] (78.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/11/16 14:01:04 [2167] (78.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/11/16 14:01:04 [2167] GAHP[2605] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/77/0/cluster77.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/77/0/cluster77.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/77/0/cluster77.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#78.0#1455130529";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/11/16 14:01:04 [2167] GAHP[2605] -> 'S'
02/11/16 14:01:04 [2167] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/11/16 14:01:04 [2167] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/11/16 14:01:04 [2167] IPVERIFY: ip found is 1
02/11/16 14:01:05 [2167] GAHP[2605] <- 'RESULTS'
02/11/16 14:01:05 [2167] GAHP[2605] -> 'R'
02/11/16 14:01:05 [2167] GAHP[2605] -> 'S' '1'
02/11/16 14:01:05 [2167] GAHP[2605] -> '2' '1' 'Unable to limit the proxy' 'N/A'
02/11/16 14:01:05 [2167] (78.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/11/16 14:01:05 [2167] (78.0) blah_job_submit() failed: Unable to limit the proxy
02/11/16 14:01:05 [2167] (78.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/11/16 14:01:05 [2167] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/11/16 14:01:07 [2167] (78.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/11/16 14:01:07 [2167] Evaluating staleness of remote job statuses.
02/11/16 14:01:09 [2167] resource  is now up
02/11/16 14:01:09 [2167] in doContactSchedd()
02/11/16 14:01:09 [2167] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:01:12 [2167] querying for new jobs
02/11/16 14:01:12 [2167] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/11/16 14:01:12 [2167] Fetched 0 new job ads from schedd
02/11/16 14:01:12 [2167] querying for removed/held jobs
02/11/16 14:01:12 [2167] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 14:01:12 [2167] Fetched 1 job ads from schedd
02/11/16 14:01:12 [2167] Updating classad values for 78.0:
02/11/16 14:01:12 [2167]    CurrentStatusUnknown = false
02/11/16 14:01:12 [2167]    GridJobId = undefined
02/11/16 14:01:12 [2167]    LastRemoteStatusUpdate = 0
02/11/16 14:01:12 [2167]    Managed = "ScheddDone"
02/11/16 14:01:12 [2167] Deleting job 78.0 from schedd
02/11/16 14:01:12 [2167] No jobs left, shutting down
02/11/16 14:01:12 [2167] leaving doContactSchedd()
02/11/16 14:01:12 [2167] Got SIGTERM. Performing graceful shutdown.
02/11/16 14:01:12 [2167] Started timer to call main_shutdown_fast in 1800 seconds
02/11/16 14:01:12 [2167] **** condor_gridmanager (condor_GRIDMANAGER) pid 2167 EXITING WITH STATUS 0
02/11/16 14:46:08 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/11/16 14:46:08 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/11/16 14:46:08 Enumerating interfaces: lo 127.0.0.1 up
02/11/16 14:46:08 Enumerating interfaces: eth2 10.31.131.202 up
02/11/16 14:46:08 Enumerating interfaces: eth3 140.247.179.131 up
02/11/16 14:46:08 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/11/16 14:46:08 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/11/16 14:46:08 ******************************************************
02/11/16 14:46:08 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/11/16 14:46:08 ** /usr/sbin/condor_gridmanager
02/11/16 14:46:08 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/11/16 14:46:08 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/11/16 14:46:08 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/11/16 14:46:08 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/11/16 14:46:08 ** PID = 27413
02/11/16 14:46:08 ** Log last touched 2/11 14:01:12
02/11/16 14:46:08 ******************************************************
02/11/16 14:46:08 Using config source: /etc/condor-ce/condor_config
02/11/16 14:46:08 Using local config sources: 
02/11/16 14:46:08    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/11/16 14:46:08    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/01-ce-auth.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/01-ce-router.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/01-common-auth.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/02-ce-lsf.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/02-ce-pbs.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/03-managed-fork.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/50-osg-configure.conf
02/11/16 14:46:08    /etc/condor-ce/config.d/99-local.conf
02/11/16 14:46:08    /usr/share/condor-ce/condor_ce_router_defaults|
02/11/16 14:46:08 config Macros = 144, Sorted = 144, StringBytes = 12554, TablesBytes = 5392
02/11/16 14:46:08 CLASSAD_CACHING is ENABLED
02/11/16 14:46:08 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/11/16 14:46:08 SharedPortEndpoint: waiting for connections to named socket 5579_a8fc_3
02/11/16 14:46:08 DaemonCore: command socket at <140.247.179.131:9620?sock=5579_a8fc_3>
02/11/16 14:46:08 DaemonCore: private command socket at <140.247.179.131:9620?sock=5579_a8fc_3>
02/11/16 14:46:08 Setting maximum accepts per cycle 8.
02/11/16 14:46:08 Setting maximum reaps per cycle 8.
02/11/16 14:46:08 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 14:46:08 [27413] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/11/16 14:46:08 [27413] DaemonCore: No more children processes to reap.
02/11/16 14:46:08 [27413] DaemonCore: in SendAliveToParent()
02/11/16 14:46:08 [27413] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:46:09 [27413] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/11/16 14:46:09 [27413] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/11/16 14:46:09 [27413] IPVERIFY: ip found is 0
02/11/16 14:46:09 [27413] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/11/16 14:46:09 [27413] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/11/16 14:46:09 [27413] Buf::write(): condor_write() failed
02/11/16 14:46:09 [27413] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/11/16 14:46:09 [27413] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:46:09 [27413] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/11/16 14:46:09 [27413] DaemonCore: Leaving SendAliveToParent() - success
02/11/16 14:46:09 [27413] Checking proxies
02/11/16 14:46:10 [27413] Received REMOVE_JOBS signal
02/11/16 14:46:10 [27413] in doContactSchedd()
02/11/16 14:46:10 [27413] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:46:10 [27413] querying for new jobs
02/11/16 14:46:10 [27413] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/11/16 14:46:10 [27413] Using job type INFNBatch for job 80.0
02/11/16 14:46:10 [27413] (80.0) SetJobLeaseTimers()
02/11/16 14:46:10 [27413] Found job 80.0 --- inserting
02/11/16 14:46:10 [27413] Fetched 1 new job ads from schedd
02/11/16 14:46:10 [27413] querying for removed/held jobs
02/11/16 14:46:10 [27413] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 14:46:10 [27413] Fetched 1 job ads from schedd
02/11/16 14:46:10 [27413] leaving doContactSchedd()
02/11/16 14:46:10 [27413] gahp server not up yet, delaying ping
02/11/16 14:46:10 [27413] *** UpdateLeases called
02/11/16 14:46:10 [27413]     Leases not supported, cancelling timer
02/11/16 14:46:10 [27413] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=5579_a8fc_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=5440_cf3f_4>"
CurrentTime = time()
MyCurrentTime = 1455219970
IdleJobs = 0
JobLimit = 10000

02/11/16 14:46:10 [27413] Trying to update collector <10.31.131.202:9619>
02/11/16 14:46:10 [27413] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 14:46:10 [27413] File descriptor limits: max 4096, safe 3277
02/11/16 14:46:10 [27413] (80.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/11/16 14:46:10 [27413] GAHP server pid = 27533
02/11/16 14:46:10 [27413] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/11/16 14:46:10 [27413] GAHP[27533] <- 'COMMANDS'
02/11/16 14:46:10 [27413] GAHP[27533] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/11/16 14:46:10 [27413] GAHP[27533] <- 'ASYNC_MODE_ON'
02/11/16 14:46:10 [27413] GAHP[27533] -> 'S' 'Async mode on'
02/11/16 14:46:10 [27413] (80.0) gm state change: GM_INIT -> GM_START
02/11/16 14:46:10 [27413] (80.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/11/16 14:46:10 [27413] (80.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/11/16 14:46:10 [27413] GAHP[27533] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/79/0/cluster79.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/79/0/cluster79.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/79/0/cluster79.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#80.0#1455133375";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/11/16 14:46:10 [27413] GAHP[27533] -> 'S'
02/11/16 14:46:10 [27413] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/11/16 14:46:10 [27413] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/11/16 14:46:10 [27413] IPVERIFY: ip found is 1
02/11/16 14:46:11 [27413] Received ADD_JOBS signal
02/11/16 14:46:12 [27413] GAHP[27533] <- 'RESULTS'
02/11/16 14:46:12 [27413] GAHP[27533] -> 'R'
02/11/16 14:46:12 [27413] GAHP[27533] -> 'S' '1'
02/11/16 14:46:12 [27413] GAHP[27533] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/11/16 14:46:12 [27413] (80.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/11/16 14:46:12 [27413] (80.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/11/16 14:46:12 [27413] (80.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/11/16 14:46:12 [27413] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/11/16 14:46:27 [27413] (80.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/11/16 14:46:27 [27413] Evaluating staleness of remote job statuses.
02/11/16 14:46:27 [27413] resource  is now up
02/11/16 14:46:27 [27413] in doContactSchedd()
02/11/16 14:46:27 [27413] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 14:46:27 [27413] querying for new jobs
02/11/16 14:46:27 [27413] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/11/16 14:46:27 [27413] Fetched 0 new job ads from schedd
02/11/16 14:46:27 [27413] querying for removed/held jobs
02/11/16 14:46:27 [27413] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 14:46:27 [27413] Fetched 1 job ads from schedd
02/11/16 14:46:27 [27413] Updating classad values for 80.0:
02/11/16 14:46:27 [27413]    CurrentStatusUnknown = false
02/11/16 14:46:27 [27413]    GridJobId = undefined
02/11/16 14:46:27 [27413]    LastRemoteStatusUpdate = 0
02/11/16 14:46:27 [27413]    Managed = "ScheddDone"
02/11/16 14:46:27 [27413] Deleting job 80.0 from schedd
02/11/16 14:46:27 [27413] No jobs left, shutting down
02/11/16 14:46:27 [27413] leaving doContactSchedd()
02/11/16 14:46:27 [27413] Got SIGTERM. Performing graceful shutdown.
02/11/16 14:46:27 [27413] Started timer to call main_shutdown_fast in 1800 seconds
02/11/16 14:46:27 [27413] **** condor_gridmanager (condor_GRIDMANAGER) pid 27413 EXITING WITH STATUS 0
02/11/16 15:46:22 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/11/16 15:46:22 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/11/16 15:46:22 Enumerating interfaces: lo 127.0.0.1 up
02/11/16 15:46:22 Enumerating interfaces: eth2 10.31.131.202 up
02/11/16 15:46:22 Enumerating interfaces: eth3 140.247.179.131 up
02/11/16 15:46:22 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/11/16 15:46:22 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/11/16 15:46:22 ******************************************************
02/11/16 15:46:22 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/11/16 15:46:22 ** /usr/sbin/condor_gridmanager
02/11/16 15:46:22 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/11/16 15:46:22 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/11/16 15:46:22 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/11/16 15:46:22 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/11/16 15:46:22 ** PID = 17350
02/11/16 15:46:22 ** Log last touched 2/11 14:46:27
02/11/16 15:46:22 ******************************************************
02/11/16 15:46:22 Using config source: /etc/condor-ce/condor_config
02/11/16 15:46:22 Using local config sources: 
02/11/16 15:46:22    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/11/16 15:46:22    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/01-ce-auth.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/01-ce-router.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/01-common-auth.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/02-ce-lsf.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/02-ce-pbs.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/03-managed-fork.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/50-osg-configure.conf
02/11/16 15:46:22    /etc/condor-ce/config.d/99-local.conf
02/11/16 15:46:22    /usr/share/condor-ce/condor_ce_router_defaults|
02/11/16 15:46:22 config Macros = 144, Sorted = 144, StringBytes = 12554, TablesBytes = 5392
02/11/16 15:46:22 CLASSAD_CACHING is ENABLED
02/11/16 15:46:22 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/11/16 15:46:22 SharedPortEndpoint: waiting for connections to named socket 5579_a8fc_4
02/11/16 15:46:22 DaemonCore: command socket at <140.247.179.131:9620?sock=5579_a8fc_4>
02/11/16 15:46:22 DaemonCore: private command socket at <140.247.179.131:9620?sock=5579_a8fc_4>
02/11/16 15:46:22 Setting maximum accepts per cycle 8.
02/11/16 15:46:22 Setting maximum reaps per cycle 8.
02/11/16 15:46:22 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 15:46:22 [17350] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/11/16 15:46:22 [17350] DaemonCore: No more children processes to reap.
02/11/16 15:46:22 [17350] DaemonCore: in SendAliveToParent()
02/11/16 15:46:22 [17350] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 15:46:29 [17350] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/11/16 15:46:29 [17350] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/11/16 15:46:29 [17350] IPVERIFY: ip found is 0
02/11/16 15:46:29 [17350] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/11/16 15:46:29 [17350] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/11/16 15:46:29 [17350] Buf::write(): condor_write() failed
02/11/16 15:46:29 [17350] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/11/16 15:46:29 [17350] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 15:46:29 [17350] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/11/16 15:46:29 [17350] DaemonCore: Leaving SendAliveToParent() - success
02/11/16 15:46:29 [17350] Checking proxies
02/11/16 15:46:29 [17350] Received ADD_JOBS signal
02/11/16 15:46:29 [17350] Received REMOVE_JOBS signal
02/11/16 15:46:29 [17350] Evaluating staleness of remote job statuses.
02/11/16 15:46:29 [17350] in doContactSchedd()
02/11/16 15:46:29 [17350] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 15:46:29 [17350] querying for new jobs
02/11/16 15:46:29 [17350] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/11/16 15:46:29 [17350] Using job type INFNBatch for job 82.0
02/11/16 15:46:29 [17350] (82.0) SetJobLeaseTimers()
02/11/16 15:46:29 [17350] Failed to get expiration time of proxy /n/atlasgrid/condor/81/0/cluster81.proc0.subproc0/x509up_u556792
02/11/16 15:46:29 [17350] Found job 82.0 --- inserting
02/11/16 15:46:29 [17350] Fetched 1 new job ads from schedd
02/11/16 15:46:29 [17350] querying for removed/held jobs
02/11/16 15:46:29 [17350] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 15:46:29 [17350] Fetched 1 job ads from schedd
02/11/16 15:46:29 [17350] leaving doContactSchedd()
02/11/16 15:46:29 [17350] gahp server not up yet, delaying ping
02/11/16 15:46:29 [17350] *** UpdateLeases called
02/11/16 15:46:29 [17350]     Leases not supported, cancelling timer
02/11/16 15:46:29 [17350] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=5579_a8fc_4>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=5440_cf3f_4>"
CurrentTime = time()
MyCurrentTime = 1455223589
IdleJobs = 0
JobLimit = 10000

02/11/16 15:46:29 [17350] Trying to update collector <10.31.131.202:9619>
02/11/16 15:46:29 [17350] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/11/16 15:46:29 [17350] File descriptor limits: max 4096, safe 3277
02/11/16 15:46:29 [17350] (82.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/11/16 15:46:29 [17350] GAHP server pid = 17360
02/11/16 15:46:29 [17350] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/11/16 15:46:29 [17350] GAHP[17360] <- 'COMMANDS'
02/11/16 15:46:29 [17350] GAHP[17360] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/11/16 15:46:29 [17350] GAHP[17360] <- 'ASYNC_MODE_ON'
02/11/16 15:46:29 [17350] GAHP[17360] -> 'S' 'Async mode on'
02/11/16 15:46:29 [17350] (82.0) gm state change: GM_INIT -> GM_START
02/11/16 15:46:29 [17350] (82.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/11/16 15:46:29 [17350] (82.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/11/16 15:46:29 [17350] GAHP[17360] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#82.0#1455137001";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/11/16 15:46:29 [17350] GAHP[17360] -> 'S'
02/11/16 15:46:29 [17350] GAHP[17360] <- 'RESULTS'
02/11/16 15:46:29 [17350] GAHP[17360] -> 'R'
02/11/16 15:46:29 [17350] GAHP[17360] -> 'S' '1'
02/11/16 15:46:29 [17350] GAHP[17360] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
02/11/16 15:46:29 [17350] (82.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/11/16 15:46:29 [17350] (82.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/81/0/cluster81.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
02/11/16 15:46:29 [17350] (82.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/11/16 15:46:29 [17350] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/11/16 15:46:33 [17350] (82.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/11/16 15:46:33 [17350] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/11/16 15:46:33 [17350] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/11/16 15:46:33 [17350] IPVERIFY: ip found is 1
02/11/16 15:46:34 [17350] resource  is now up
02/11/16 15:46:34 [17350] in doContactSchedd()
02/11/16 15:46:34 [17350] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/11/16 15:46:34 [17350] querying for removed/held jobs
02/11/16 15:46:34 [17350] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/11/16 15:46:34 [17350] Fetched 1 job ads from schedd
02/11/16 15:46:34 [17350] Updating classad values for 82.0:
02/11/16 15:46:34 [17350]    CurrentStatusUnknown = false
02/11/16 15:46:34 [17350]    GridJobId = undefined
02/11/16 15:46:34 [17350]    LastRemoteStatusUpdate = 0
02/11/16 15:46:34 [17350]    Managed = "ScheddDone"
02/11/16 15:46:34 [17350] Deleting job 82.0 from schedd
02/11/16 15:46:34 [17350] No jobs left, shutting down
02/11/16 15:46:34 [17350] leaving doContactSchedd()
02/11/16 15:46:34 [17350] Got SIGTERM. Performing graceful shutdown.
02/11/16 15:46:34 [17350] Started timer to call main_shutdown_fast in 1800 seconds
02/11/16 15:46:34 [17350] **** condor_gridmanager (condor_GRIDMANAGER) pid 17350 EXITING WITH STATUS 0
02/12/16 10:43:12 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/12/16 10:43:12 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/12/16 10:43:12 Enumerating interfaces: lo 127.0.0.1 up
02/12/16 10:43:12 Enumerating interfaces: eth2 10.31.131.202 up
02/12/16 10:43:12 Enumerating interfaces: eth3 140.247.179.131 up
02/12/16 10:43:12 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/12/16 10:43:12 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/12/16 10:43:12 ******************************************************
02/12/16 10:43:12 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/12/16 10:43:12 ** /usr/sbin/condor_gridmanager
02/12/16 10:43:12 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/12/16 10:43:12 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/12/16 10:43:12 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/12/16 10:43:12 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/12/16 10:43:12 ** PID = 5854
02/12/16 10:43:12 ** Log last touched 2/11 15:46:34
02/12/16 10:43:12 ******************************************************
02/12/16 10:43:12 Using config source: /etc/condor-ce/condor_config
02/12/16 10:43:12 Using local config sources: 
02/12/16 10:43:12    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/12/16 10:43:12    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/01-ce-auth.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/01-ce-router.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/01-common-auth.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/02-ce-lsf.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/02-ce-pbs.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/03-managed-fork.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/50-osg-configure.conf
02/12/16 10:43:12    /etc/condor-ce/config.d/99-local.conf
02/12/16 10:43:12    /usr/share/condor-ce/condor_ce_router_defaults|
02/12/16 10:43:12 config Macros = 144, Sorted = 144, StringBytes = 12552, TablesBytes = 5392
02/12/16 10:43:12 CLASSAD_CACHING is ENABLED
02/12/16 10:43:12 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/12/16 10:43:12 SharedPortEndpoint: waiting for connections to named socket 5579_a8fc_5
02/12/16 10:43:12 DaemonCore: command socket at <140.247.179.131:9620?sock=5579_a8fc_5>
02/12/16 10:43:12 DaemonCore: private command socket at <140.247.179.131:9620?sock=5579_a8fc_5>
02/12/16 10:43:12 Setting maximum accepts per cycle 8.
02/12/16 10:43:12 Setting maximum reaps per cycle 8.
02/12/16 10:43:12 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/12/16 10:43:12 [5854] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/12/16 10:43:12 [5854] DaemonCore: No more children processes to reap.
02/12/16 10:43:12 [5854] DaemonCore: in SendAliveToParent()
02/12/16 10:43:12 [5854] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/12/16 10:43:13 [5854] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/12/16 10:43:13 [5854] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/12/16 10:43:13 [5854] IPVERIFY: ip found is 0
02/12/16 10:43:13 [5854] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/12/16 10:43:13 [5854] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/12/16 10:43:13 [5854] Buf::write(): condor_write() failed
02/12/16 10:43:13 [5854] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/12/16 10:43:13 [5854] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/12/16 10:43:13 [5854] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/12/16 10:43:13 [5854] DaemonCore: Leaving SendAliveToParent() - success
02/12/16 10:43:13 [5854] Checking proxies
02/12/16 10:43:14 [5854] Received REMOVE_JOBS signal
02/12/16 10:43:14 [5854] in doContactSchedd()
02/12/16 10:43:14 [5854] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/12/16 10:43:14 [5854] querying for new jobs
02/12/16 10:43:14 [5854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/12/16 10:43:14 [5854] Using job type INFNBatch for job 84.0
02/12/16 10:43:14 [5854] (84.0) SetJobLeaseTimers()
02/12/16 10:43:14 [5854] Failed to get expiration time of proxy /n/atlasgrid/condor/83/0/cluster83.proc0.subproc0/x509up_u556792
02/12/16 10:43:14 [5854] Found job 84.0 --- inserting
02/12/16 10:43:14 [5854] Fetched 1 new job ads from schedd
02/12/16 10:43:14 [5854] querying for removed/held jobs
02/12/16 10:43:14 [5854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/12/16 10:43:14 [5854] Fetched 1 job ads from schedd
02/12/16 10:43:14 [5854] leaving doContactSchedd()
02/12/16 10:43:14 [5854] gahp server not up yet, delaying ping
02/12/16 10:43:14 [5854] *** UpdateLeases called
02/12/16 10:43:14 [5854]     Leases not supported, cancelling timer
02/12/16 10:43:14 [5854] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=5579_a8fc_5>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=5440_cf3f_4>"
CurrentTime = time()
MyCurrentTime = 1455291794
IdleJobs = 0
JobLimit = 10000

02/12/16 10:43:14 [5854] Trying to update collector <10.31.131.202:9619>
02/12/16 10:43:14 [5854] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/12/16 10:43:14 [5854] File descriptor limits: max 4096, safe 3277
02/12/16 10:43:14 [5854] (84.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/12/16 10:43:14 [5854] GAHP server pid = 5981
02/12/16 10:43:14 [5854] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/12/16 10:43:14 [5854] GAHP[5981] <- 'COMMANDS'
02/12/16 10:43:14 [5854] GAHP[5981] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/12/16 10:43:14 [5854] GAHP[5981] <- 'ASYNC_MODE_ON'
02/12/16 10:43:14 [5854] GAHP[5981] -> 'S' 'Async mode on'
02/12/16 10:43:14 [5854] (84.0) gm state change: GM_INIT -> GM_START
02/12/16 10:43:14 [5854] (84.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/12/16 10:43:14 [5854] (84.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/12/16 10:43:14 [5854] GAHP[5981] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#84.0#1455205061";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/12/16 10:43:14 [5854] GAHP[5981] -> 'S'
02/12/16 10:43:14 [5854] GAHP[5981] <- 'RESULTS'
02/12/16 10:43:14 [5854] GAHP[5981] -> 'R'
02/12/16 10:43:14 [5854] GAHP[5981] -> 'S' '1'
02/12/16 10:43:14 [5854] GAHP[5981] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
02/12/16 10:43:14 [5854] (84.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/12/16 10:43:14 [5854] (84.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/83/0/cluster83.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
02/12/16 10:43:14 [5854] (84.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/12/16 10:43:14 [5854] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/12/16 10:43:16 [5854] (84.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/12/16 10:43:16 [5854] Received ADD_JOBS signal
02/12/16 10:43:16 [5854] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/12/16 10:43:16 [5854] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/12/16 10:43:16 [5854] IPVERIFY: ip found is 1
02/12/16 10:43:17 [5854] Evaluating staleness of remote job statuses.
02/12/16 10:43:19 [5854] resource  is now up
02/12/16 10:43:19 [5854] in doContactSchedd()
02/12/16 10:43:19 [5854] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5440_cf3f_4
02/12/16 10:43:19 [5854] querying for new jobs
02/12/16 10:43:19 [5854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/12/16 10:43:19 [5854] Fetched 0 new job ads from schedd
02/12/16 10:43:19 [5854] querying for removed/held jobs
02/12/16 10:43:19 [5854] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/12/16 10:43:19 [5854] Fetched 1 job ads from schedd
02/12/16 10:43:19 [5854] Updating classad values for 84.0:
02/12/16 10:43:19 [5854]    CurrentStatusUnknown = false
02/12/16 10:43:19 [5854]    GridJobId = undefined
02/12/16 10:43:19 [5854]    LastRemoteStatusUpdate = 0
02/12/16 10:43:19 [5854]    Managed = "ScheddDone"
02/12/16 10:43:19 [5854] Deleting job 84.0 from schedd
02/12/16 10:43:19 [5854] No jobs left, shutting down
02/12/16 10:43:19 [5854] leaving doContactSchedd()
02/12/16 10:43:19 [5854] Got SIGTERM. Performing graceful shutdown.
02/12/16 10:43:19 [5854] Started timer to call main_shutdown_fast in 1800 seconds
02/12/16 10:43:19 [5854] **** condor_gridmanager (condor_GRIDMANAGER) pid 5854 EXITING WITH STATUS 0
02/19/16 11:25:55 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/19/16 11:25:55 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/19/16 11:25:55 Enumerating interfaces: lo 127.0.0.1 up
02/19/16 11:25:55 Enumerating interfaces: eth2 10.31.131.202 up
02/19/16 11:25:55 Enumerating interfaces: eth3 140.247.179.131 up
02/19/16 11:25:55 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/19/16 11:25:55 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/19/16 11:25:55 ******************************************************
02/19/16 11:25:55 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/19/16 11:25:55 ** /usr/sbin/condor_gridmanager
02/19/16 11:25:55 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/19/16 11:25:55 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/19/16 11:25:55 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/19/16 11:25:55 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/19/16 11:25:55 ** PID = 9156
02/19/16 11:25:55 ** Log last touched 2/12 10:43:19
02/19/16 11:25:55 ******************************************************
02/19/16 11:25:55 Using config source: /etc/condor-ce/condor_config
02/19/16 11:25:55 Using local config sources: 
02/19/16 11:25:55    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/19/16 11:25:55    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/01-ce-auth.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/01-ce-router.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/01-common-auth.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/02-ce-lsf.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/02-ce-pbs.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/03-managed-fork.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/50-osg-configure.conf
02/19/16 11:25:55    /etc/condor-ce/config.d/99-local.conf
02/19/16 11:25:55    /usr/share/condor-ce/condor_ce_router_defaults|
02/19/16 11:25:55 config Macros = 144, Sorted = 144, StringBytes = 12556, TablesBytes = 5392
02/19/16 11:25:55 CLASSAD_CACHING is ENABLED
02/19/16 11:25:55 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/19/16 11:25:55 SharedPortEndpoint: waiting for connections to named socket 13911_4ecf_1
02/19/16 11:25:55 DaemonCore: command socket at <140.247.179.131:9620?sock=13911_4ecf_1>
02/19/16 11:25:55 DaemonCore: private command socket at <140.247.179.131:9620?sock=13911_4ecf_1>
02/19/16 11:25:55 Setting maximum accepts per cycle 8.
02/19/16 11:25:55 Setting maximum reaps per cycle 8.
02/19/16 11:25:55 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 11:25:55 [9156] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/19/16 11:25:55 [9156] DaemonCore: No more children processes to reap.
02/19/16 11:25:55 [9156] DaemonCore: in SendAliveToParent()
02/19/16 11:25:55 [9156] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 11:25:55 [9156] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/19/16 11:25:55 [9156] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/19/16 11:25:55 [9156] IPVERIFY: ip found is 0
02/19/16 11:25:55 [9156] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/19/16 11:25:55 [9156] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/19/16 11:25:55 [9156] Buf::write(): condor_write() failed
02/19/16 11:25:55 [9156] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/19/16 11:25:55 [9156] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 11:25:55 [9156] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/19/16 11:25:55 [9156] DaemonCore: Leaving SendAliveToParent() - success
02/19/16 11:25:55 [9156] Checking proxies
02/19/16 11:25:58 [9156] Received ADD_JOBS signal
02/19/16 11:25:58 [9156] in doContactSchedd()
02/19/16 11:25:58 [9156] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 11:25:58 [9156] querying for new jobs
02/19/16 11:25:58 [9156] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/19/16 11:25:58 [9156] Using job type INFNBatch for job 86.0
02/19/16 11:25:58 [9156] (86.0) SetJobLeaseTimers()
02/19/16 11:25:58 [9156] Found job 86.0 --- inserting
02/19/16 11:25:58 [9156] Fetched 1 new job ads from schedd
02/19/16 11:25:58 [9156] querying for removed/held jobs
02/19/16 11:25:58 [9156] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 11:25:58 [9156] Fetched 0 job ads from schedd
02/19/16 11:25:58 [9156] leaving doContactSchedd()
02/19/16 11:25:58 [9156] gahp server not up yet, delaying ping
02/19/16 11:25:58 [9156] *** UpdateLeases called
02/19/16 11:25:58 [9156]     Leases not supported, cancelling timer
02/19/16 11:25:58 [9156] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=13911_4ecf_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=13904_f771_4>"
CurrentTime = time()
MyCurrentTime = 1455899158
IdleJobs = 1
JobLimit = 10000

02/19/16 11:25:58 [9156] Trying to update collector <10.31.131.202:9619>
02/19/16 11:25:58 [9156] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 11:25:58 [9156] File descriptor limits: max 4096, safe 3277
02/19/16 11:25:58 [9156] (86.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/19/16 11:25:58 [9156] GAHP server pid = 9464
02/19/16 11:25:58 [9156] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/19/16 11:25:58 [9156] GAHP[9464] <- 'COMMANDS'
02/19/16 11:25:58 [9156] GAHP[9464] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/19/16 11:25:58 [9156] GAHP[9464] <- 'ASYNC_MODE_ON'
02/19/16 11:25:58 [9156] GAHP[9464] -> 'S' 'Async mode on'
02/19/16 11:25:58 [9156] (86.0) gm state change: GM_INIT -> GM_START
02/19/16 11:25:58 [9156] (86.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/19/16 11:25:58 [9156] (86.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 11:25:58 [9156] (86.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 11:25:58 [9156] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/19/16 11:25:58 [9156] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/19/16 11:25:58 [9156] IPVERIFY: ip found is 1
02/19/16 11:26:00 [9156] Evaluating staleness of remote job statuses.
02/19/16 11:26:03 [9156] resource  is now up
02/19/16 11:26:03 [9156] in doContactSchedd()
02/19/16 11:26:03 [9156] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 11:26:03 [9156] querying for removed/held jobs
02/19/16 11:26:03 [9156] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 11:26:03 [9156] Fetched 0 job ads from schedd
02/19/16 11:26:03 [9156] Updating classad values for 86.0:
02/19/16 11:26:03 [9156]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#86.0#1455899135"
02/19/16 11:26:03 [9156]    LastRemoteStatusUpdate = 1455899158
02/19/16 11:26:03 [9156] leaving doContactSchedd()
02/19/16 11:26:03 [9156] (86.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 11:26:03 [9156] (86.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 11:26:03 [9156] (86.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/19/16 11:26:03 [9156] GAHP[9464] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#86.0#1455899135";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/19/16 11:26:03 [9156] GAHP[9464] -> 'S'
02/19/16 11:26:05 [9156] GAHP[9464] <- 'RESULTS'
02/19/16 11:26:05 [9156] GAHP[9464] -> 'R'
02/19/16 11:26:05 [9156] GAHP[9464] -> 'S' '1'
02/19/16 11:26:05 [9156] GAHP[9464] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/19/16 11:26:05 [9156] (86.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/19/16 11:26:05 [9156] (86.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/19/16 11:26:05 [9156] (86.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/19/16 11:26:05 [9156] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/19/16 11:26:13 [9156] (86.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/19/16 11:26:13 [9156] in doContactSchedd()
02/19/16 11:26:13 [9156] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 11:26:13 [9156] querying for removed/held jobs
02/19/16 11:26:13 [9156] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 11:26:13 [9156] Fetched 0 job ads from schedd
02/19/16 11:26:13 [9156] Updating classad values for 86.0:
02/19/16 11:26:13 [9156]    CurrentStatusUnknown = false
02/19/16 11:26:13 [9156]    GridJobId = undefined
02/19/16 11:26:13 [9156]    LastRemoteStatusUpdate = 0
02/19/16 11:26:13 [9156] leaving doContactSchedd()
02/19/16 11:26:13 [9156] (86.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/19/16 11:26:13 [9156] (86.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 11:26:13 [9156] (86.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 11:26:18 [9156] in doContactSchedd()
02/19/16 11:26:18 [9156] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 11:26:18 [9156] querying for removed/held jobs
02/19/16 11:26:18 [9156] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 11:26:18 [9156] Fetched 0 job ads from schedd
02/19/16 11:26:18 [9156] Updating classad values for 86.0:
02/19/16 11:26:18 [9156]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#86.0#1455899135"
02/19/16 11:26:18 [9156]    LastRemoteStatusUpdate = 1455899173
02/19/16 11:26:19 [9156] leaving doContactSchedd()
02/19/16 11:26:19 [9156] (86.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 11:26:19 [9156] (86.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 11:26:19 [9156] (86.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/19/16 11:26:19 [9156] (86.0) gm state change: GM_HOLD -> GM_DELETE
02/19/16 11:26:24 [9156] in doContactSchedd()
02/19/16 11:26:24 [9156] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 11:26:24 [9156] querying for removed/held jobs
02/19/16 11:26:24 [9156] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 11:26:24 [9156] Fetched 0 job ads from schedd
02/19/16 11:26:24 [9156] Updating classad values for 86.0:
02/19/16 11:26:24 [9156]    EnteredCurrentStatus = 1455899179
02/19/16 11:26:24 [9156]    HoldReason = "Attempts to submit failed: "
02/19/16 11:26:24 [9156]    HoldReasonCode = 0
02/19/16 11:26:24 [9156]    HoldReasonSubCode = 0
02/19/16 11:26:24 [9156]    JobStatus = 5
02/19/16 11:26:24 [9156]    LastReleaseReason = "Data files spooled"
02/19/16 11:26:24 [9156]    Managed = "Schedd"
02/19/16 11:26:24 [9156]    NumSystemHolds = 1
02/19/16 11:26:24 [9156]    ReleaseReason = undefined
02/19/16 11:26:24 [9156] No jobs left, shutting down
02/19/16 11:26:24 [9156] leaving doContactSchedd()
02/19/16 11:26:24 [9156] Got SIGTERM. Performing graceful shutdown.
02/19/16 11:26:24 [9156] Started timer to call main_shutdown_fast in 1800 seconds
02/19/16 11:26:24 [9156] **** condor_gridmanager (condor_GRIDMANAGER) pid 9156 EXITING WITH STATUS 0
02/19/16 12:21:18 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/19/16 12:21:18 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/19/16 12:21:18 Enumerating interfaces: lo 127.0.0.1 up
02/19/16 12:21:18 Enumerating interfaces: eth2 10.31.131.202 up
02/19/16 12:21:18 Enumerating interfaces: eth3 140.247.179.131 up
02/19/16 12:21:18 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/19/16 12:21:18 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/19/16 12:21:18 ******************************************************
02/19/16 12:21:18 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/19/16 12:21:18 ** /usr/sbin/condor_gridmanager
02/19/16 12:21:18 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/19/16 12:21:18 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/19/16 12:21:18 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/19/16 12:21:18 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/19/16 12:21:18 ** PID = 26710
02/19/16 12:21:18 ** Log last touched 2/19 11:26:24
02/19/16 12:21:18 ******************************************************
02/19/16 12:21:18 Using config source: /etc/condor-ce/condor_config
02/19/16 12:21:18 Using local config sources: 
02/19/16 12:21:18    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/19/16 12:21:18    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/01-ce-auth.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/01-ce-router.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/01-common-auth.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/02-ce-lsf.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/02-ce-pbs.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/03-managed-fork.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/50-osg-configure.conf
02/19/16 12:21:18    /etc/condor-ce/config.d/99-local.conf
02/19/16 12:21:18    /usr/share/condor-ce/condor_ce_router_defaults|
02/19/16 12:21:18 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
02/19/16 12:21:18 CLASSAD_CACHING is ENABLED
02/19/16 12:21:18 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/19/16 12:21:18 SharedPortEndpoint: waiting for connections to named socket 13911_4ecf_4
02/19/16 12:21:18 DaemonCore: command socket at <140.247.179.131:9620?sock=13911_4ecf_4>
02/19/16 12:21:18 DaemonCore: private command socket at <140.247.179.131:9620?sock=13911_4ecf_4>
02/19/16 12:21:18 Setting maximum accepts per cycle 8.
02/19/16 12:21:18 Setting maximum reaps per cycle 8.
02/19/16 12:21:18 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 12:21:18 [26710] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/19/16 12:21:18 [26710] DaemonCore: No more children processes to reap.
02/19/16 12:21:18 [26710] DaemonCore: in SendAliveToParent()
02/19/16 12:21:18 [26710] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:21:31 [26710] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/19/16 12:21:31 [26710] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/19/16 12:21:31 [26710] IPVERIFY: ip found is 0
02/19/16 12:21:31 [26710] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/19/16 12:21:31 [26710] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/19/16 12:21:31 [26710] Buf::write(): condor_write() failed
02/19/16 12:21:31 [26710] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/19/16 12:21:31 [26710] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:21:31 [26710] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/19/16 12:21:31 [26710] DaemonCore: Leaving SendAliveToParent() - success
02/19/16 12:21:31 [26710] Checking proxies
02/19/16 12:21:31 [26710] Received ADD_JOBS signal
02/19/16 12:21:31 [26710] Received REMOVE_JOBS signal
02/19/16 12:21:31 [26710] Evaluating staleness of remote job statuses.
02/19/16 12:21:31 [26710] in doContactSchedd()
02/19/16 12:21:31 [26710] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:21:36 [26710] querying for new jobs
02/19/16 12:21:36 [26710] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/19/16 12:21:36 [26710] Using job type INFNBatch for job 86.0
02/19/16 12:21:36 [26710] (86.0) SetJobLeaseTimers()
02/19/16 12:21:36 [26710] Failed to get expiration time of proxy /n/atlasgrid/condor/85/0/cluster85.proc0.subproc0/x509up_u556792
02/19/16 12:21:36 [26710] Found job 86.0 --- inserting
02/19/16 12:21:36 [26710] Fetched 1 new job ads from schedd
02/19/16 12:21:36 [26710] querying for removed/held jobs
02/19/16 12:21:36 [26710] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 12:21:36 [26710] Fetched 1 job ads from schedd
02/19/16 12:21:37 [26710] leaving doContactSchedd()
02/19/16 12:21:37 [26710] gahp server not up yet, delaying ping
02/19/16 12:21:37 [26710] *** UpdateLeases called
02/19/16 12:21:37 [26710]     Leases not supported, cancelling timer
02/19/16 12:21:37 [26710] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=13911_4ecf_4>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=13904_f771_4>"
CurrentTime = time()
MyCurrentTime = 1455902497
IdleJobs = 0
JobLimit = 10000

02/19/16 12:21:37 [26710] Trying to update collector <10.31.131.202:9619>
02/19/16 12:21:37 [26710] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 12:21:37 [26710] File descriptor limits: max 4096, safe 3277
02/19/16 12:21:37 [26710] (86.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/19/16 12:21:37 [26710] GAHP server pid = 26745
02/19/16 12:21:37 [26710] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/19/16 12:21:37 [26710] GAHP[26745] <- 'COMMANDS'
02/19/16 12:21:37 [26710] GAHP[26745] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/19/16 12:21:37 [26710] GAHP[26745] <- 'ASYNC_MODE_ON'
02/19/16 12:21:37 [26710] GAHP[26745] -> 'S' 'Async mode on'
02/19/16 12:21:37 [26710] (86.0) gm state change: GM_INIT -> GM_START
02/19/16 12:21:37 [26710] (86.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/19/16 12:21:37 [26710] (86.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/19/16 12:21:37 [26710] GAHP[26745] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#86.0#1455899135";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/19/16 12:21:37 [26710] GAHP[26745] -> 'S'
02/19/16 12:21:37 [26710] GAHP[26745] <- 'RESULTS'
02/19/16 12:21:37 [26710] GAHP[26745] -> 'R'
02/19/16 12:21:37 [26710] GAHP[26745] -> 'S' '1'
02/19/16 12:21:37 [26710] GAHP[26745] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
02/19/16 12:21:37 [26710] (86.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/19/16 12:21:37 [26710] (86.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/85/0/cluster85.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
02/19/16 12:21:37 [26710] (86.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/19/16 12:21:37 [26710] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/19/16 12:22:02 [26710] (86.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/19/16 12:22:02 [26710] resource  is now up
02/19/16 12:22:02 [26710] in doContactSchedd()
02/19/16 12:22:02 [26710] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:22:02 [26710] querying for removed/held jobs
02/19/16 12:22:02 [26710] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 12:22:02 [26710] Fetched 0 job ads from schedd
02/19/16 12:22:02 [26710] Updating classad values for 86.0:
02/19/16 12:22:02 [26710]    CurrentStatusUnknown = false
02/19/16 12:22:02 [26710] Deleting job 86.0 from schedd
02/19/16 12:22:02 [26710] No jobs left, shutting down
02/19/16 12:22:02 [26710] leaving doContactSchedd()
02/19/16 12:22:02 [26710] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/19/16 12:22:02 [26710] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/19/16 12:22:02 [26710] IPVERIFY: ip found is 1
02/19/16 12:22:02 [26710] Got SIGTERM. Performing graceful shutdown.
02/19/16 12:22:02 [26710] Started timer to call main_shutdown_fast in 1800 seconds
02/19/16 12:22:02 [26710] **** condor_gridmanager (condor_GRIDMANAGER) pid 26710 EXITING WITH STATUS 0
02/19/16 12:29:44 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/19/16 12:29:44 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/19/16 12:29:44 Enumerating interfaces: lo 127.0.0.1 up
02/19/16 12:29:44 Enumerating interfaces: eth2 10.31.131.202 up
02/19/16 12:29:44 Enumerating interfaces: eth3 140.247.179.131 up
02/19/16 12:29:44 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/19/16 12:29:44 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/19/16 12:29:44 ******************************************************
02/19/16 12:29:44 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/19/16 12:29:44 ** /usr/sbin/condor_gridmanager
02/19/16 12:29:44 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/19/16 12:29:44 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/19/16 12:29:44 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/19/16 12:29:44 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/19/16 12:29:44 ** PID = 10717
02/19/16 12:29:44 ** Log last touched 2/19 12:22:02
02/19/16 12:29:44 ******************************************************
02/19/16 12:29:44 Using config source: /etc/condor-ce/condor_config
02/19/16 12:29:44 Using local config sources: 
02/19/16 12:29:44    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/19/16 12:29:44    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/01-ce-auth.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/01-ce-router.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/01-common-auth.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/02-ce-lsf.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/02-ce-pbs.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/03-managed-fork.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/50-osg-configure.conf
02/19/16 12:29:44    /etc/condor-ce/config.d/99-local.conf
02/19/16 12:29:44    /usr/share/condor-ce/condor_ce_router_defaults|
02/19/16 12:29:44 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
02/19/16 12:29:44 CLASSAD_CACHING is ENABLED
02/19/16 12:29:44 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/19/16 12:29:44 SharedPortEndpoint: waiting for connections to named socket 13911_4ecf_5
02/19/16 12:29:44 DaemonCore: command socket at <140.247.179.131:9620?sock=13911_4ecf_5>
02/19/16 12:29:44 DaemonCore: private command socket at <140.247.179.131:9620?sock=13911_4ecf_5>
02/19/16 12:29:44 Setting maximum accepts per cycle 8.
02/19/16 12:29:44 Setting maximum reaps per cycle 8.
02/19/16 12:29:44 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 12:29:44 [10717] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/19/16 12:29:44 [10717] DaemonCore: No more children processes to reap.
02/19/16 12:29:44 [10717] DaemonCore: in SendAliveToParent()
02/19/16 12:29:44 [10717] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:29:44 [10717] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/19/16 12:29:44 [10717] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/19/16 12:29:44 [10717] IPVERIFY: ip found is 0
02/19/16 12:29:44 [10717] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/19/16 12:29:44 [10717] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/19/16 12:29:44 [10717] Buf::write(): condor_write() failed
02/19/16 12:29:44 [10717] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/19/16 12:29:44 [10717] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:29:44 [10717] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/19/16 12:29:44 [10717] DaemonCore: Leaving SendAliveToParent() - success
02/19/16 12:29:44 [10717] Checking proxies
02/19/16 12:29:47 [10717] Received ADD_JOBS signal
02/19/16 12:29:47 [10717] in doContactSchedd()
02/19/16 12:29:47 [10717] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:29:47 [10717] querying for new jobs
02/19/16 12:29:47 [10717] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/19/16 12:29:47 [10717] Using job type INFNBatch for job 90.0
02/19/16 12:29:47 [10717] (90.0) SetJobLeaseTimers()
02/19/16 12:29:47 [10717] Found job 90.0 --- inserting
02/19/16 12:29:47 [10717] Fetched 1 new job ads from schedd
02/19/16 12:29:47 [10717] querying for removed/held jobs
02/19/16 12:29:47 [10717] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 12:29:47 [10717] Fetched 0 job ads from schedd
02/19/16 12:29:48 [10717] leaving doContactSchedd()
02/19/16 12:29:48 [10717] gahp server not up yet, delaying ping
02/19/16 12:29:48 [10717] *** UpdateLeases called
02/19/16 12:29:48 [10717]     Leases not supported, cancelling timer
02/19/16 12:29:48 [10717] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=13911_4ecf_5>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=13904_f771_4>"
CurrentTime = time()
MyCurrentTime = 1455902988
IdleJobs = 1
JobLimit = 10000

02/19/16 12:29:48 [10717] Trying to update collector <10.31.131.202:9619>
02/19/16 12:29:48 [10717] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 12:29:48 [10717] File descriptor limits: max 4096, safe 3277
02/19/16 12:29:48 [10717] (90.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/19/16 12:29:48 [10717] GAHP server pid = 11225
02/19/16 12:29:48 [10717] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/19/16 12:29:48 [10717] GAHP[11225] <- 'COMMANDS'
02/19/16 12:29:48 [10717] GAHP[11225] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/19/16 12:29:48 [10717] GAHP[11225] <- 'ASYNC_MODE_ON'
02/19/16 12:29:48 [10717] GAHP[11225] -> 'S' 'Async mode on'
02/19/16 12:29:48 [10717] (90.0) gm state change: GM_INIT -> GM_START
02/19/16 12:29:48 [10717] (90.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/19/16 12:29:48 [10717] (90.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 12:29:48 [10717] (90.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 12:29:48 [10717] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/19/16 12:29:48 [10717] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/19/16 12:29:48 [10717] IPVERIFY: ip found is 1
02/19/16 12:29:49 [10717] Evaluating staleness of remote job statuses.
02/19/16 12:29:53 [10717] resource  is now up
02/19/16 12:29:53 [10717] in doContactSchedd()
02/19/16 12:29:53 [10717] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:29:53 [10717] querying for removed/held jobs
02/19/16 12:29:53 [10717] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 12:29:53 [10717] Fetched 0 job ads from schedd
02/19/16 12:29:53 [10717] Updating classad values for 90.0:
02/19/16 12:29:53 [10717]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#90.0#1455902688"
02/19/16 12:29:53 [10717]    LastRemoteStatusUpdate = 1455902988
02/19/16 12:29:53 [10717] leaving doContactSchedd()
02/19/16 12:29:53 [10717] (90.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 12:29:53 [10717] (90.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 12:29:53 [10717] (90.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/19/16 12:29:53 [10717] GAHP[11225] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ HISTFILESIZE=20000\ SSH_CONNECTION=10.255.12.11'\ '61414'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.11'\ '61414'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/1\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_jZXZAy\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/89/0/cluster89.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#90.0#1455902688";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/19/16 12:29:53 [10717] GAHP[11225] -> 'S'
02/19/16 12:29:56 [10717] GAHP[11225] <- 'RESULTS'
02/19/16 12:29:56 [10717] GAHP[11225] -> 'R'
02/19/16 12:29:56 [10717] GAHP[11225] -> 'S' '1'
02/19/16 12:29:56 [10717] GAHP[11225] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/19/16 12:29:56 [10717] (90.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/19/16 12:29:56 [10717] (90.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/19/16 12:29:56 [10717] (90.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/19/16 12:29:56 [10717] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/19/16 12:29:56 [10717] (90.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/19/16 12:29:58 [10717] in doContactSchedd()
02/19/16 12:29:58 [10717] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:29:58 [10717] querying for removed/held jobs
02/19/16 12:29:58 [10717] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 12:29:58 [10717] Fetched 0 job ads from schedd
02/19/16 12:29:58 [10717] Updating classad values for 90.0:
02/19/16 12:29:58 [10717]    CurrentStatusUnknown = false
02/19/16 12:29:58 [10717]    GridJobId = undefined
02/19/16 12:29:58 [10717]    LastRemoteStatusUpdate = 0
02/19/16 12:29:58 [10717] leaving doContactSchedd()
02/19/16 12:29:58 [10717] (90.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/19/16 12:29:58 [10717] (90.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 12:29:58 [10717] (90.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 12:30:03 [10717] in doContactSchedd()
02/19/16 12:30:03 [10717] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:30:03 [10717] querying for removed/held jobs
02/19/16 12:30:03 [10717] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 12:30:03 [10717] Fetched 0 job ads from schedd
02/19/16 12:30:03 [10717] Updating classad values for 90.0:
02/19/16 12:30:03 [10717]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#90.0#1455902688"
02/19/16 12:30:03 [10717]    LastRemoteStatusUpdate = 1455902998
02/19/16 12:30:03 [10717] leaving doContactSchedd()
02/19/16 12:30:03 [10717] (90.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 12:30:03 [10717] (90.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 12:30:03 [10717] (90.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/19/16 12:30:03 [10717] dirscat: dirpath = /tmp
02/19/16 12:30:03 [10717] dirscat: subdir = condorLocks
02/19/16 12:30:03 [10717] directory_util::rec_touch_file: Creating directory /tmp 
02/19/16 12:30:03 [10717] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
02/19/16 12:30:03 [10717] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/39 
02/19/16 12:30:03 [10717] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/39/98 
02/19/16 12:30:03 [10717] FileLock object is updating timestamp on: /tmp/condorLocks/39/98/302777985962395.lockc
02/19/16 12:30:03 [10717] WriteUserLog::initialize: opened /n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/.log_14441_GsU0Ko successfully
02/19/16 12:30:03 [10717] (90.0) Writing hold record to user logfile
02/19/16 12:30:03 [10717] FileLock::obtain(1) - @1455903003.611567 lock on /tmp/condorLocks/39/98/302777985962395.lockc now WRITE
02/19/16 12:30:03 [10717] FileLock::obtain(2) - @1455903003.655463 lock on /tmp/condorLocks/39/98/302777985962395.lockc now UNLOCKED
02/19/16 12:30:03 [10717] FileLock::obtain(1) - @1455903003.655563 lock on /tmp/condorLocks/39/98/302777985962395.lockc now WRITE
02/19/16 12:30:03 [10717] directory_util::rec_clean_up: file /tmp/condorLocks/39/98/302777985962395.lockc has been deleted. 
02/19/16 12:30:03 [10717] Lock file /tmp/condorLocks/39/98/302777985962395.lockc has been deleted. 
02/19/16 12:30:03 [10717] FileLock::obtain(2) - @1455903003.655724 lock on /tmp/condorLocks/39/98/302777985962395.lockc now UNLOCKED
02/19/16 12:30:03 [10717] (90.0) gm state change: GM_HOLD -> GM_DELETE
02/19/16 12:30:08 [10717] in doContactSchedd()
02/19/16 12:30:08 [10717] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 13904_f771_4
02/19/16 12:30:08 [10717] querying for removed/held jobs
02/19/16 12:30:08 [10717] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 12:30:08 [10717] Fetched 0 job ads from schedd
02/19/16 12:30:08 [10717] Updating classad values for 90.0:
02/19/16 12:30:08 [10717]    EnteredCurrentStatus = 1455903003
02/19/16 12:30:08 [10717]    HoldReason = "Attempts to submit failed: "
02/19/16 12:30:08 [10717]    HoldReasonCode = 0
02/19/16 12:30:08 [10717]    HoldReasonSubCode = 0
02/19/16 12:30:08 [10717]    JobStatus = 5
02/19/16 12:30:08 [10717]    LastReleaseReason = "Data files spooled"
02/19/16 12:30:08 [10717]    Managed = "Schedd"
02/19/16 12:30:08 [10717]    NumSystemHolds = 1
02/19/16 12:30:08 [10717]    ReleaseReason = undefined
02/19/16 12:30:08 [10717] No jobs left, shutting down
02/19/16 12:30:08 [10717] leaving doContactSchedd()
02/19/16 12:30:08 [10717] Got SIGTERM. Performing graceful shutdown.
02/19/16 12:30:08 [10717] Started timer to call main_shutdown_fast in 1800 seconds
02/19/16 12:30:08 [10717] **** condor_gridmanager (condor_GRIDMANAGER) pid 10717 EXITING WITH STATUS 0
02/19/16 13:02:55 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/19/16 13:02:55 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/19/16 13:02:55 Enumerating interfaces: lo 127.0.0.1 up
02/19/16 13:02:55 Enumerating interfaces: eth2 10.31.131.202 up
02/19/16 13:02:55 Enumerating interfaces: eth3 140.247.179.131 up
02/19/16 13:02:55 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/19/16 13:02:55 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/19/16 13:02:55 ******************************************************
02/19/16 13:02:55 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/19/16 13:02:55 ** /usr/sbin/condor_gridmanager
02/19/16 13:02:55 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/19/16 13:02:55 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/19/16 13:02:55 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/19/16 13:02:55 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/19/16 13:02:55 ** PID = 13347
02/19/16 13:02:55 ** Log last touched 2/19 12:30:08
02/19/16 13:02:55 ******************************************************
02/19/16 13:02:55 Using config source: /etc/condor-ce/condor_config
02/19/16 13:02:55 Using local config sources: 
02/19/16 13:02:55    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/19/16 13:02:55    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/01-ce-auth.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/01-ce-router.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/01-common-auth.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/02-ce-lsf.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/02-ce-pbs.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/03-managed-fork.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/50-osg-configure.conf
02/19/16 13:02:55    /etc/condor-ce/config.d/99-local.conf
02/19/16 13:02:55    /usr/share/condor-ce/condor_ce_router_defaults|
02/19/16 13:02:55 config Macros = 144, Sorted = 144, StringBytes = 12557, TablesBytes = 5392
02/19/16 13:02:55 CLASSAD_CACHING is ENABLED
02/19/16 13:02:55 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/19/16 13:02:55 SharedPortEndpoint: waiting for connections to named socket 28300_a8f8_1
02/19/16 13:02:55 DaemonCore: command socket at <140.247.179.131:9620?sock=28300_a8f8_1>
02/19/16 13:02:55 DaemonCore: private command socket at <140.247.179.131:9620?sock=28300_a8f8_1>
02/19/16 13:02:55 Setting maximum accepts per cycle 8.
02/19/16 13:02:55 Setting maximum reaps per cycle 8.
02/19/16 13:02:55 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 13:02:55 [13347] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/19/16 13:02:55 [13347] DaemonCore: No more children processes to reap.
02/19/16 13:02:55 [13347] DaemonCore: in SendAliveToParent()
02/19/16 13:02:55 [13347] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 28028_3413_4
02/19/16 13:02:55 [13347] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/19/16 13:02:55 [13347] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/19/16 13:02:55 [13347] IPVERIFY: ip found is 0
02/19/16 13:02:55 [13347] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/19/16 13:02:55 [13347] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/19/16 13:02:55 [13347] Buf::write(): condor_write() failed
02/19/16 13:02:55 [13347] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/19/16 13:02:55 [13347] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 28028_3413_4
02/19/16 13:02:55 [13347] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/19/16 13:02:55 [13347] DaemonCore: Leaving SendAliveToParent() - success
02/19/16 13:02:55 [13347] Checking proxies
02/19/16 13:02:58 [13347] Received ADD_JOBS signal
02/19/16 13:02:58 [13347] in doContactSchedd()
02/19/16 13:02:58 [13347] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 28028_3413_4
02/19/16 13:02:58 [13347] querying for new jobs
02/19/16 13:02:58 [13347] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/19/16 13:02:58 [13347] Using job type INFNBatch for job 92.0
02/19/16 13:02:58 [13347] (92.0) SetJobLeaseTimers()
02/19/16 13:02:58 [13347] Found job 92.0 --- inserting
02/19/16 13:02:58 [13347] Fetched 1 new job ads from schedd
02/19/16 13:02:58 [13347] querying for removed/held jobs
02/19/16 13:02:58 [13347] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:02:58 [13347] Fetched 0 job ads from schedd
02/19/16 13:02:59 [13347] leaving doContactSchedd()
02/19/16 13:02:59 [13347] gahp server not up yet, delaying ping
02/19/16 13:02:59 [13347] *** UpdateLeases called
02/19/16 13:02:59 [13347]     Leases not supported, cancelling timer
02/19/16 13:02:59 [13347] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=28300_a8f8_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=28028_3413_4>"
CurrentTime = time()
MyCurrentTime = 1455904979
IdleJobs = 1
JobLimit = 10000

02/19/16 13:02:59 [13347] Trying to update collector <10.31.131.202:9619>
02/19/16 13:02:59 [13347] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 13:02:59 [13347] File descriptor limits: max 4096, safe 3277
02/19/16 13:02:59 [13347] (92.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/19/16 13:02:59 [13347] GAHP server pid = 13773
02/19/16 13:02:59 [13347] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/19/16 13:02:59 [13347] GAHP[13773] <- 'COMMANDS'
02/19/16 13:02:59 [13347] GAHP[13773] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/19/16 13:02:59 [13347] GAHP[13773] <- 'ASYNC_MODE_ON'
02/19/16 13:02:59 [13347] GAHP[13773] -> 'S' 'Async mode on'
02/19/16 13:02:59 [13347] (92.0) gm state change: GM_INIT -> GM_START
02/19/16 13:02:59 [13347] (92.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/19/16 13:02:59 [13347] (92.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 13:02:59 [13347] (92.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 13:02:59 [13347] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/19/16 13:02:59 [13347] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/19/16 13:02:59 [13347] IPVERIFY: ip found is 1
02/19/16 13:03:00 [13347] Evaluating staleness of remote job statuses.
02/19/16 13:03:04 [13347] resource  is now up
02/19/16 13:03:04 [13347] in doContactSchedd()
02/19/16 13:03:04 [13347] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 28028_3413_4
02/19/16 13:03:04 [13347] querying for removed/held jobs
02/19/16 13:03:04 [13347] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:03:04 [13347] Fetched 0 job ads from schedd
02/19/16 13:03:04 [13347] Updating classad values for 92.0:
02/19/16 13:03:04 [13347]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#92.0#1455904953"
02/19/16 13:03:04 [13347]    LastRemoteStatusUpdate = 1455904979
02/19/16 13:03:06 [13347] leaving doContactSchedd()
02/19/16 13:03:06 [13347] (92.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 13:03:06 [13347] (92.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 13:03:06 [13347] (92.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/19/16 13:03:06 [13347] GAHP[13773] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ HISTFILESIZE=20000\ SSH_CONNECTION=10.255.12.11'\ '61414'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.11'\ '61414'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/1\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_jZXZAy\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/91/0/cluster91.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/91/0/cluster91.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/91/0/cluster91.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#92.0#1455904953";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/19/16 13:03:06 [13347] GAHP[13773] -> 'S'
02/19/16 13:03:15 [13347] GAHP[13773] <- 'RESULTS'
02/19/16 13:03:15 [13347] GAHP[13773] -> 'R'
02/19/16 13:03:15 [13347] GAHP[13773] -> 'S' '1'
02/19/16 13:03:15 [13347] GAHP[13773] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/19/16 13:03:15 [13347] (92.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/19/16 13:03:15 [13347] (92.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/19/16 13:03:15 [13347] (92.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/19/16 13:03:15 [13347] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/19/16 13:03:21 [13347] (92.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/19/16 13:03:21 [13347] in doContactSchedd()
02/19/16 13:03:21 [13347] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 28028_3413_4
02/19/16 13:03:21 [13347] querying for removed/held jobs
02/19/16 13:03:21 [13347] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:03:21 [13347] Fetched 0 job ads from schedd
02/19/16 13:03:21 [13347] Updating classad values for 92.0:
02/19/16 13:03:21 [13347]    CurrentStatusUnknown = false
02/19/16 13:03:21 [13347]    GridJobId = undefined
02/19/16 13:03:21 [13347]    LastRemoteStatusUpdate = 0
02/19/16 13:03:21 [13347] leaving doContactSchedd()
02/19/16 13:03:21 [13347] (92.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/19/16 13:03:21 [13347] (92.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 13:03:21 [13347] (92.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 13:03:26 [13347] in doContactSchedd()
02/19/16 13:03:26 [13347] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 28028_3413_4
02/19/16 13:03:26 [13347] querying for removed/held jobs
02/19/16 13:03:26 [13347] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:03:26 [13347] Fetched 0 job ads from schedd
02/19/16 13:03:26 [13347] Updating classad values for 92.0:
02/19/16 13:03:26 [13347]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#92.0#1455904953"
02/19/16 13:03:26 [13347]    LastRemoteStatusUpdate = 1455905001
02/19/16 13:03:26 [13347] leaving doContactSchedd()
02/19/16 13:03:26 [13347] (92.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 13:03:26 [13347] (92.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 13:03:26 [13347] (92.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/19/16 13:03:26 [13347] dirscat: dirpath = /tmp
02/19/16 13:03:26 [13347] dirscat: subdir = condorLocks
02/19/16 13:03:26 [13347] directory_util::rec_touch_file: Creating directory /tmp 
02/19/16 13:03:26 [13347] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
02/19/16 13:03:26 [13347] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/99 
02/19/16 13:03:26 [13347] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/99/98 
02/19/16 13:03:26 [13347] FileLock object is updating timestamp on: /tmp/condorLocks/99/98/540346910294500.lockc
02/19/16 13:03:26 [13347] WriteUserLog::initialize: opened /n/atlasgrid/condor/91/0/cluster91.proc0.subproc0/.log_17088_UaDShk successfully
02/19/16 13:03:26 [13347] (92.0) Writing hold record to user logfile
02/19/16 13:03:26 [13347] FileLock::obtain(1) - @1455905006.842559 lock on /tmp/condorLocks/99/98/540346910294500.lockc now WRITE
02/19/16 13:03:26 [13347] FileLock::obtain(2) - @1455905006.843356 lock on /tmp/condorLocks/99/98/540346910294500.lockc now UNLOCKED
02/19/16 13:03:26 [13347] FileLock::obtain(1) - @1455905006.843452 lock on /tmp/condorLocks/99/98/540346910294500.lockc now WRITE
02/19/16 13:03:26 [13347] directory_util::rec_clean_up: file /tmp/condorLocks/99/98/540346910294500.lockc has been deleted. 
02/19/16 13:03:26 [13347] Lock file /tmp/condorLocks/99/98/540346910294500.lockc has been deleted. 
02/19/16 13:03:26 [13347] FileLock::obtain(2) - @1455905006.843613 lock on /tmp/condorLocks/99/98/540346910294500.lockc now UNLOCKED
02/19/16 13:03:26 [13347] (92.0) gm state change: GM_HOLD -> GM_DELETE
02/19/16 13:03:31 [13347] in doContactSchedd()
02/19/16 13:03:31 [13347] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 28028_3413_4
02/19/16 13:03:31 [13347] querying for removed/held jobs
02/19/16 13:03:31 [13347] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:03:31 [13347] Fetched 0 job ads from schedd
02/19/16 13:03:31 [13347] Updating classad values for 92.0:
02/19/16 13:03:31 [13347]    EnteredCurrentStatus = 1455905006
02/19/16 13:03:31 [13347]    HoldReason = "Attempts to submit failed: "
02/19/16 13:03:31 [13347]    HoldReasonCode = 0
02/19/16 13:03:31 [13347]    HoldReasonSubCode = 0
02/19/16 13:03:31 [13347]    JobStatus = 5
02/19/16 13:03:31 [13347]    LastReleaseReason = "Data files spooled"
02/19/16 13:03:31 [13347]    Managed = "Schedd"
02/19/16 13:03:31 [13347]    NumSystemHolds = 1
02/19/16 13:03:31 [13347]    ReleaseReason = undefined
02/19/16 13:03:32 [13347] No jobs left, shutting down
02/19/16 13:03:32 [13347] leaving doContactSchedd()
02/19/16 13:03:32 [13347] Got SIGTERM. Performing graceful shutdown.
02/19/16 13:03:32 [13347] Started timer to call main_shutdown_fast in 1800 seconds
02/19/16 13:03:32 [13347] **** condor_gridmanager (condor_GRIDMANAGER) pid 13347 EXITING WITH STATUS 0
02/19/16 13:42:32 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/19/16 13:42:32 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/19/16 13:42:32 Enumerating interfaces: lo 127.0.0.1 up
02/19/16 13:42:32 Enumerating interfaces: eth2 10.31.131.202 up
02/19/16 13:42:32 Enumerating interfaces: eth3 140.247.179.131 up
02/19/16 13:42:32 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/19/16 13:42:32 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/19/16 13:42:32 ******************************************************
02/19/16 13:42:32 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/19/16 13:42:32 ** /usr/sbin/condor_gridmanager
02/19/16 13:42:32 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/19/16 13:42:32 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/19/16 13:42:32 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/19/16 13:42:32 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/19/16 13:42:32 ** PID = 22320
02/19/16 13:42:32 ** Log last touched 2/19 13:03:32
02/19/16 13:42:32 ******************************************************
02/19/16 13:42:32 Using config source: /etc/condor-ce/condor_config
02/19/16 13:42:32 Using local config sources: 
02/19/16 13:42:32    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/19/16 13:42:32    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/01-ce-auth.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/01-ce-router.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/01-common-auth.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/02-ce-lsf.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/02-ce-pbs.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/03-managed-fork.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/50-osg-configure.conf
02/19/16 13:42:32    /etc/condor-ce/config.d/99-local.conf
02/19/16 13:42:32    /usr/share/condor-ce/condor_ce_router_defaults|
02/19/16 13:42:32 config Macros = 144, Sorted = 144, StringBytes = 12557, TablesBytes = 5392
02/19/16 13:42:32 CLASSAD_CACHING is ENABLED
02/19/16 13:42:32 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/19/16 13:42:32 SharedPortEndpoint: waiting for connections to named socket 10002_1050_1
02/19/16 13:42:32 DaemonCore: command socket at <140.247.179.131:9620?sock=10002_1050_1>
02/19/16 13:42:32 DaemonCore: private command socket at <140.247.179.131:9620?sock=10002_1050_1>
02/19/16 13:42:32 Setting maximum accepts per cycle 8.
02/19/16 13:42:32 Setting maximum reaps per cycle 8.
02/19/16 13:42:32 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 13:42:32 [22320] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/19/16 13:42:32 [22320] DaemonCore: No more children processes to reap.
02/19/16 13:42:32 [22320] DaemonCore: in SendAliveToParent()
02/19/16 13:42:32 [22320] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 9942_2b28_4
02/19/16 13:42:32 [22320] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/19/16 13:42:32 [22320] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/19/16 13:42:32 [22320] IPVERIFY: ip found is 0
02/19/16 13:42:32 [22320] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/19/16 13:42:32 [22320] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/19/16 13:42:32 [22320] Buf::write(): condor_write() failed
02/19/16 13:42:32 [22320] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/19/16 13:42:32 [22320] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 9942_2b28_4
02/19/16 13:42:32 [22320] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/19/16 13:42:32 [22320] DaemonCore: Leaving SendAliveToParent() - success
02/19/16 13:42:32 [22320] Checking proxies
02/19/16 13:42:35 [22320] Received ADD_JOBS signal
02/19/16 13:42:35 [22320] in doContactSchedd()
02/19/16 13:42:35 [22320] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 9942_2b28_4
02/19/16 13:42:35 [22320] querying for new jobs
02/19/16 13:42:35 [22320] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/19/16 13:42:35 [22320] Using job type INFNBatch for job 94.0
02/19/16 13:42:35 [22320] (94.0) SetJobLeaseTimers()
02/19/16 13:42:35 [22320] Found job 94.0 --- inserting
02/19/16 13:42:35 [22320] Fetched 1 new job ads from schedd
02/19/16 13:42:35 [22320] querying for removed/held jobs
02/19/16 13:42:35 [22320] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:42:35 [22320] Fetched 0 job ads from schedd
02/19/16 13:42:35 [22320] leaving doContactSchedd()
02/19/16 13:42:35 [22320] gahp server not up yet, delaying ping
02/19/16 13:42:35 [22320] *** UpdateLeases called
02/19/16 13:42:35 [22320]     Leases not supported, cancelling timer
02/19/16 13:42:35 [22320] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=10002_1050_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=9942_2b28_4>"
CurrentTime = time()
MyCurrentTime = 1455907355
IdleJobs = 1
JobLimit = 10000

02/19/16 13:42:35 [22320] Trying to update collector <10.31.131.202:9619>
02/19/16 13:42:35 [22320] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 13:42:35 [22320] File descriptor limits: max 4096, safe 3277
02/19/16 13:42:35 [22320] (94.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/19/16 13:42:35 [22320] GAHP server pid = 22430
02/19/16 13:42:35 [22320] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/19/16 13:42:35 [22320] GAHP[22430] <- 'COMMANDS'
02/19/16 13:42:35 [22320] GAHP[22430] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/19/16 13:42:35 [22320] GAHP[22430] <- 'ASYNC_MODE_ON'
02/19/16 13:42:35 [22320] GAHP[22430] -> 'S' 'Async mode on'
02/19/16 13:42:35 [22320] (94.0) gm state change: GM_INIT -> GM_START
02/19/16 13:42:35 [22320] (94.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/19/16 13:42:35 [22320] (94.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 13:42:35 [22320] (94.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 13:42:35 [22320] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/19/16 13:42:35 [22320] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/19/16 13:42:35 [22320] IPVERIFY: ip found is 1
02/19/16 13:42:37 [22320] Evaluating staleness of remote job statuses.
02/19/16 13:42:40 [22320] resource  is now up
02/19/16 13:42:40 [22320] in doContactSchedd()
02/19/16 13:42:40 [22320] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 9942_2b28_4
02/19/16 13:42:40 [22320] querying for removed/held jobs
02/19/16 13:42:40 [22320] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:42:40 [22320] Fetched 0 job ads from schedd
02/19/16 13:42:40 [22320] Updating classad values for 94.0:
02/19/16 13:42:40 [22320]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#94.0#1455907057"
02/19/16 13:42:40 [22320]    LastRemoteStatusUpdate = 1455907355
02/19/16 13:42:40 [22320] leaving doContactSchedd()
02/19/16 13:42:40 [22320] (94.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 13:42:40 [22320] (94.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 13:42:40 [22320] (94.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/19/16 13:42:40 [22320] GAHP[22430] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ HISTFILESIZE=20000\ SSH_CONNECTION=10.255.12.11'\ '61414'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.11'\ '61414'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/1\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_jZXZAy\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/93/0/cluster93.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/93/0/cluster93.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/93/0/cluster93.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#94.0#1455907057";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/19/16 13:42:40 [22320] GAHP[22430] -> 'S'
02/19/16 13:42:42 [22320] GAHP[22430] <- 'RESULTS'
02/19/16 13:42:42 [22320] GAHP[22430] -> 'R'
02/19/16 13:42:42 [22320] GAHP[22430] -> 'S' '1'
02/19/16 13:42:42 [22320] GAHP[22430] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/19/16 13:42:42 [22320] (94.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/19/16 13:42:42 [22320] (94.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/19/16 13:42:42 [22320] (94.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/19/16 13:42:42 [22320] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/19/16 13:42:42 [22320] (94.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/19/16 13:42:45 [22320] in doContactSchedd()
02/19/16 13:42:45 [22320] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 9942_2b28_4
02/19/16 13:42:45 [22320] querying for removed/held jobs
02/19/16 13:42:45 [22320] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:42:45 [22320] Fetched 0 job ads from schedd
02/19/16 13:42:45 [22320] Updating classad values for 94.0:
02/19/16 13:42:45 [22320]    CurrentStatusUnknown = false
02/19/16 13:42:45 [22320]    GridJobId = undefined
02/19/16 13:42:45 [22320]    LastRemoteStatusUpdate = 0
02/19/16 13:42:45 [22320] leaving doContactSchedd()
02/19/16 13:42:45 [22320] (94.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/19/16 13:42:45 [22320] (94.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 13:42:45 [22320] (94.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 13:42:50 [22320] in doContactSchedd()
02/19/16 13:42:50 [22320] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 9942_2b28_4
02/19/16 13:42:50 [22320] querying for removed/held jobs
02/19/16 13:42:50 [22320] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:42:50 [22320] Fetched 0 job ads from schedd
02/19/16 13:42:50 [22320] Updating classad values for 94.0:
02/19/16 13:42:50 [22320]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#94.0#1455907057"
02/19/16 13:42:50 [22320]    LastRemoteStatusUpdate = 1455907365
02/19/16 13:42:50 [22320] leaving doContactSchedd()
02/19/16 13:42:50 [22320] (94.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 13:42:50 [22320] (94.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 13:42:50 [22320] (94.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/19/16 13:42:50 [22320] dirscat: dirpath = /tmp
02/19/16 13:42:50 [22320] dirscat: subdir = condorLocks
02/19/16 13:42:50 [22320] directory_util::rec_touch_file: Creating directory /tmp 
02/19/16 13:42:50 [22320] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
02/19/16 13:42:50 [22320] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/74 
02/19/16 13:42:50 [22320] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/74/86 
02/19/16 13:42:50 [22320] FileLock object is updating timestamp on: /tmp/condorLocks/74/86/704348923219125.lockc
02/19/16 13:42:50 [22320] WriteUserLog::initialize: opened /n/atlasgrid/condor/93/0/cluster93.proc0.subproc0/.log_19324_XjQ1GK successfully
02/19/16 13:42:50 [22320] (94.0) Writing hold record to user logfile
02/19/16 13:42:50 [22320] FileLock::obtain(1) - @1455907370.659041 lock on /tmp/condorLocks/74/86/704348923219125.lockc now WRITE
02/19/16 13:42:50 [22320] FileLock::obtain(2) - @1455907370.659628 lock on /tmp/condorLocks/74/86/704348923219125.lockc now UNLOCKED
02/19/16 13:42:50 [22320] FileLock::obtain(1) - @1455907370.659723 lock on /tmp/condorLocks/74/86/704348923219125.lockc now WRITE
02/19/16 13:42:50 [22320] directory_util::rec_clean_up: file /tmp/condorLocks/74/86/704348923219125.lockc has been deleted. 
02/19/16 13:42:50 [22320] Lock file /tmp/condorLocks/74/86/704348923219125.lockc has been deleted. 
02/19/16 13:42:50 [22320] FileLock::obtain(2) - @1455907370.659885 lock on /tmp/condorLocks/74/86/704348923219125.lockc now UNLOCKED
02/19/16 13:42:50 [22320] (94.0) gm state change: GM_HOLD -> GM_DELETE
02/19/16 13:42:55 [22320] in doContactSchedd()
02/19/16 13:42:55 [22320] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 9942_2b28_4
02/19/16 13:42:55 [22320] querying for removed/held jobs
02/19/16 13:42:55 [22320] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 13:42:55 [22320] Fetched 0 job ads from schedd
02/19/16 13:42:55 [22320] Updating classad values for 94.0:
02/19/16 13:42:55 [22320]    EnteredCurrentStatus = 1455907370
02/19/16 13:42:55 [22320]    HoldReason = "Attempts to submit failed: "
02/19/16 13:42:55 [22320]    HoldReasonCode = 0
02/19/16 13:42:55 [22320]    HoldReasonSubCode = 0
02/19/16 13:42:55 [22320]    JobStatus = 5
02/19/16 13:42:55 [22320]    LastReleaseReason = "Data files spooled"
02/19/16 13:42:55 [22320]    Managed = "Schedd"
02/19/16 13:42:55 [22320]    NumSystemHolds = 1
02/19/16 13:42:55 [22320]    ReleaseReason = undefined
02/19/16 13:42:55 [22320] No jobs left, shutting down
02/19/16 13:42:55 [22320] leaving doContactSchedd()
02/19/16 13:42:55 [22320] Got SIGTERM. Performing graceful shutdown.
02/19/16 13:42:55 [22320] Started timer to call main_shutdown_fast in 1800 seconds
02/19/16 13:42:55 [22320] **** condor_gridmanager (condor_GRIDMANAGER) pid 22320 EXITING WITH STATUS 0
02/19/16 15:53:50 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/19/16 15:53:50 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/19/16 15:53:50 Enumerating interfaces: lo 127.0.0.1 up
02/19/16 15:53:50 Enumerating interfaces: eth2 10.31.131.202 up
02/19/16 15:53:50 Enumerating interfaces: eth3 140.247.179.131 up
02/19/16 15:53:50 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/19/16 15:53:50 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/19/16 15:53:50 ******************************************************
02/19/16 15:53:50 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/19/16 15:53:50 ** /usr/sbin/condor_gridmanager
02/19/16 15:53:50 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/19/16 15:53:50 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/19/16 15:53:50 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/19/16 15:53:50 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/19/16 15:53:50 ** PID = 13382
02/19/16 15:53:50 ** Log last touched 2/19 13:42:55
02/19/16 15:53:50 ******************************************************
02/19/16 15:53:50 Using config source: /etc/condor-ce/condor_config
02/19/16 15:53:50 Using local config sources: 
02/19/16 15:53:50    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/19/16 15:53:50    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/01-ce-auth.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/01-ce-router.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/01-common-auth.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/02-ce-lsf.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/02-ce-pbs.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/03-managed-fork.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/50-osg-configure.conf
02/19/16 15:53:50    /etc/condor-ce/config.d/99-local.conf
02/19/16 15:53:50    /usr/share/condor-ce/condor_ce_router_defaults|
02/19/16 15:53:50 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
02/19/16 15:53:50 CLASSAD_CACHING is ENABLED
02/19/16 15:53:50 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/19/16 15:53:50 SharedPortEndpoint: waiting for connections to named socket 3540_0009_1
02/19/16 15:53:50 DaemonCore: command socket at <140.247.179.131:9620?sock=3540_0009_1>
02/19/16 15:53:50 DaemonCore: private command socket at <140.247.179.131:9620?sock=3540_0009_1>
02/19/16 15:53:50 Setting maximum accepts per cycle 8.
02/19/16 15:53:50 Setting maximum reaps per cycle 8.
02/19/16 15:53:50 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 15:53:50 [13382] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/19/16 15:53:50 [13382] DaemonCore: No more children processes to reap.
02/19/16 15:53:50 [13382] DaemonCore: in SendAliveToParent()
02/19/16 15:53:50 [13382] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/19/16 15:53:50 [13382] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/19/16 15:53:50 [13382] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/19/16 15:53:50 [13382] IPVERIFY: ip found is 0
02/19/16 15:53:50 [13382] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/19/16 15:53:50 [13382] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/19/16 15:53:50 [13382] Buf::write(): condor_write() failed
02/19/16 15:53:50 [13382] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/19/16 15:53:50 [13382] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/19/16 15:53:50 [13382] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/19/16 15:53:50 [13382] DaemonCore: Leaving SendAliveToParent() - success
02/19/16 15:53:50 [13382] Checking proxies
02/19/16 15:53:53 [13382] Received ADD_JOBS signal
02/19/16 15:53:53 [13382] in doContactSchedd()
02/19/16 15:53:53 [13382] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/19/16 15:53:53 [13382] querying for new jobs
02/19/16 15:53:53 [13382] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/19/16 15:53:53 [13382] Using job type INFNBatch for job 96.0
02/19/16 15:53:53 [13382] (96.0) SetJobLeaseTimers()
02/19/16 15:53:53 [13382] Found job 96.0 --- inserting
02/19/16 15:53:53 [13382] Fetched 1 new job ads from schedd
02/19/16 15:53:53 [13382] querying for removed/held jobs
02/19/16 15:53:53 [13382] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 15:53:53 [13382] Fetched 0 job ads from schedd
02/19/16 15:53:53 [13382] leaving doContactSchedd()
02/19/16 15:53:53 [13382] gahp server not up yet, delaying ping
02/19/16 15:53:53 [13382] *** UpdateLeases called
02/19/16 15:53:53 [13382]     Leases not supported, cancelling timer
02/19/16 15:53:53 [13382] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=3540_0009_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=3533_8c32_4>"
CurrentTime = time()
MyCurrentTime = 1455915233
IdleJobs = 1
JobLimit = 10000

02/19/16 15:53:53 [13382] Trying to update collector <10.31.131.202:9619>
02/19/16 15:53:53 [13382] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/19/16 15:53:53 [13382] File descriptor limits: max 4096, safe 3277
02/19/16 15:53:53 [13382] (96.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/19/16 15:53:53 [13382] GAHP server pid = 13446
02/19/16 15:53:53 [13382] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/19/16 15:53:53 [13382] GAHP[13446] <- 'COMMANDS'
02/19/16 15:53:53 [13382] GAHP[13446] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/19/16 15:53:53 [13382] GAHP[13446] <- 'ASYNC_MODE_ON'
02/19/16 15:53:53 [13382] GAHP[13446] -> 'S' 'Async mode on'
02/19/16 15:53:53 [13382] (96.0) gm state change: GM_INIT -> GM_START
02/19/16 15:53:53 [13382] (96.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/19/16 15:53:53 [13382] (96.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 15:53:53 [13382] (96.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 15:53:53 [13382] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/19/16 15:53:53 [13382] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/19/16 15:53:53 [13382] IPVERIFY: ip found is 1
02/19/16 15:53:55 [13382] Evaluating staleness of remote job statuses.
02/19/16 15:53:58 [13382] resource  is now up
02/19/16 15:53:58 [13382] in doContactSchedd()
02/19/16 15:53:58 [13382] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/19/16 15:53:58 [13382] querying for removed/held jobs
02/19/16 15:53:58 [13382] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 15:53:58 [13382] Fetched 0 job ads from schedd
02/19/16 15:53:58 [13382] Updating classad values for 96.0:
02/19/16 15:53:58 [13382]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#96.0#1455915228"
02/19/16 15:53:58 [13382]    LastRemoteStatusUpdate = 1455915233
02/19/16 15:53:58 [13382] leaving doContactSchedd()
02/19/16 15:53:58 [13382] (96.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 15:53:58 [13382] (96.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 15:53:58 [13382] (96.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/19/16 15:53:58 [13382] GAHP[13446] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/95/0/cluster95.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/95/0/cluster95.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/95/0/cluster95.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#96.0#1455915228";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/19/16 15:53:58 [13382] GAHP[13446] -> 'S'
02/19/16 15:53:59 [13382] GAHP[13446] <- 'RESULTS'
02/19/16 15:53:59 [13382] GAHP[13446] -> 'R'
02/19/16 15:53:59 [13382] GAHP[13446] -> 'S' '1'
02/19/16 15:53:59 [13382] GAHP[13446] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/19/16 15:53:59 [13382] (96.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/19/16 15:53:59 [13382] (96.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/19/16 15:53:59 [13382] (96.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/19/16 15:53:59 [13382] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/19/16 15:54:05 [13382] (96.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/19/16 15:54:05 [13382] in doContactSchedd()
02/19/16 15:54:05 [13382] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/19/16 15:54:05 [13382] querying for removed/held jobs
02/19/16 15:54:05 [13382] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 15:54:05 [13382] Fetched 0 job ads from schedd
02/19/16 15:54:05 [13382] Updating classad values for 96.0:
02/19/16 15:54:05 [13382]    CurrentStatusUnknown = false
02/19/16 15:54:05 [13382]    GridJobId = undefined
02/19/16 15:54:05 [13382]    LastRemoteStatusUpdate = 0
02/19/16 15:54:05 [13382] leaving doContactSchedd()
02/19/16 15:54:05 [13382] (96.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/19/16 15:54:05 [13382] (96.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/19/16 15:54:05 [13382] (96.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/19/16 15:54:10 [13382] in doContactSchedd()
02/19/16 15:54:10 [13382] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/19/16 15:54:10 [13382] querying for removed/held jobs
02/19/16 15:54:10 [13382] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 15:54:10 [13382] Fetched 0 job ads from schedd
02/19/16 15:54:10 [13382] Updating classad values for 96.0:
02/19/16 15:54:10 [13382]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#96.0#1455915228"
02/19/16 15:54:10 [13382]    LastRemoteStatusUpdate = 1455915245
02/19/16 15:54:10 [13382] leaving doContactSchedd()
02/19/16 15:54:10 [13382] (96.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/19/16 15:54:10 [13382] (96.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/19/16 15:54:10 [13382] (96.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/19/16 15:54:10 [13382] (96.0) gm state change: GM_HOLD -> GM_DELETE
02/19/16 15:54:15 [13382] in doContactSchedd()
02/19/16 15:54:15 [13382] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/19/16 15:54:15 [13382] querying for removed/held jobs
02/19/16 15:54:15 [13382] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/19/16 15:54:15 [13382] Fetched 0 job ads from schedd
02/19/16 15:54:15 [13382] Updating classad values for 96.0:
02/19/16 15:54:15 [13382]    EnteredCurrentStatus = 1455915250
02/19/16 15:54:15 [13382]    HoldReason = "Attempts to submit failed: "
02/19/16 15:54:15 [13382]    HoldReasonCode = 0
02/19/16 15:54:15 [13382]    HoldReasonSubCode = 0
02/19/16 15:54:15 [13382]    JobStatus = 5
02/19/16 15:54:15 [13382]    LastReleaseReason = "Data files spooled"
02/19/16 15:54:15 [13382]    Managed = "Schedd"
02/19/16 15:54:15 [13382]    NumSystemHolds = 1
02/19/16 15:54:15 [13382]    ReleaseReason = undefined
02/19/16 15:54:15 [13382] No jobs left, shutting down
02/19/16 15:54:15 [13382] leaving doContactSchedd()
02/19/16 15:54:15 [13382] Got SIGTERM. Performing graceful shutdown.
02/19/16 15:54:15 [13382] Started timer to call main_shutdown_fast in 1800 seconds
02/19/16 15:54:15 [13382] **** condor_gridmanager (condor_GRIDMANAGER) pid 13382 EXITING WITH STATUS 0
02/20/16 12:31:35 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/20/16 12:31:35 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/20/16 12:31:35 Enumerating interfaces: lo 127.0.0.1 up
02/20/16 12:31:35 Enumerating interfaces: eth2 10.31.131.202 up
02/20/16 12:31:35 Enumerating interfaces: eth3 140.247.179.131 up
02/20/16 12:31:35 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/20/16 12:31:35 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/20/16 12:31:35 ******************************************************
02/20/16 12:31:35 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/20/16 12:31:35 ** /usr/sbin/condor_gridmanager
02/20/16 12:31:35 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/20/16 12:31:35 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/20/16 12:31:35 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/20/16 12:31:35 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/20/16 12:31:35 ** PID = 24895
02/20/16 12:31:35 ** Log last touched 2/19 15:54:15
02/20/16 12:31:35 ******************************************************
02/20/16 12:31:35 Using config source: /etc/condor-ce/condor_config
02/20/16 12:31:35 Using local config sources: 
02/20/16 12:31:35    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/20/16 12:31:35    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/01-ce-auth.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/01-ce-router.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/01-common-auth.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/02-ce-lsf.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/02-ce-pbs.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/03-managed-fork.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/50-osg-configure.conf
02/20/16 12:31:35    /etc/condor-ce/config.d/99-local.conf
02/20/16 12:31:35    /usr/share/condor-ce/condor_ce_router_defaults|
02/20/16 12:31:35 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
02/20/16 12:31:35 CLASSAD_CACHING is ENABLED
02/20/16 12:31:35 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/20/16 12:31:35 SharedPortEndpoint: waiting for connections to named socket 3540_0009_2
02/20/16 12:31:35 DaemonCore: command socket at <140.247.179.131:9620?sock=3540_0009_2>
02/20/16 12:31:35 DaemonCore: private command socket at <140.247.179.131:9620?sock=3540_0009_2>
02/20/16 12:31:35 Setting maximum accepts per cycle 8.
02/20/16 12:31:35 Setting maximum reaps per cycle 8.
02/20/16 12:31:35 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 12:31:35 [24895] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/20/16 12:31:35 [24895] DaemonCore: No more children processes to reap.
02/20/16 12:31:35 [24895] DaemonCore: in SendAliveToParent()
02/20/16 12:31:35 [24895] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 12:31:36 [24895] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/20/16 12:31:36 [24895] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/20/16 12:31:36 [24895] IPVERIFY: ip found is 0
02/20/16 12:31:36 [24895] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/20/16 12:31:36 [24895] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/20/16 12:31:36 [24895] Buf::write(): condor_write() failed
02/20/16 12:31:36 [24895] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/20/16 12:31:36 [24895] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 12:31:36 [24895] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/20/16 12:31:36 [24895] DaemonCore: Leaving SendAliveToParent() - success
02/20/16 12:31:36 [24895] Checking proxies
02/20/16 12:31:37 [24895] Received REMOVE_JOBS signal
02/20/16 12:31:37 [24895] in doContactSchedd()
02/20/16 12:31:37 [24895] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 12:31:37 [24895] querying for new jobs
02/20/16 12:31:37 [24895] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/20/16 12:31:37 [24895] Using job type INFNBatch for job 90.0
02/20/16 12:31:37 [24895] (90.0) SetJobLeaseTimers()
02/20/16 12:31:38 [24895] Found job 90.0 --- inserting
02/20/16 12:31:38 [24895] Fetched 1 new job ads from schedd
02/20/16 12:31:38 [24895] querying for removed/held jobs
02/20/16 12:31:38 [24895] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 12:31:38 [24895] Fetched 1 job ads from schedd
02/20/16 12:31:38 [24895] leaving doContactSchedd()
02/20/16 12:31:38 [24895] gahp server not up yet, delaying ping
02/20/16 12:31:38 [24895] *** UpdateLeases called
02/20/16 12:31:38 [24895]     Leases not supported, cancelling timer
02/20/16 12:31:38 [24895] Received ADD_JOBS signal
02/20/16 12:31:38 [24895] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=3540_0009_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=3533_8c32_4>"
CurrentTime = time()
MyCurrentTime = 1455989498
IdleJobs = 0
JobLimit = 10000

02/20/16 12:31:38 [24895] Trying to update collector <10.31.131.202:9619>
02/20/16 12:31:38 [24895] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 12:31:38 [24895] File descriptor limits: max 4096, safe 3277
02/20/16 12:31:38 [24895] (90.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/20/16 12:31:38 [24895] GAHP server pid = 25188
02/20/16 12:31:38 [24895] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/20/16 12:31:38 [24895] GAHP[25188] <- 'COMMANDS'
02/20/16 12:31:38 [24895] GAHP[25188] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/20/16 12:31:38 [24895] GAHP[25188] <- 'ASYNC_MODE_ON'
02/20/16 12:31:38 [24895] GAHP[25188] -> 'S' 'Async mode on'
02/20/16 12:31:38 [24895] (90.0) gm state change: GM_INIT -> GM_START
02/20/16 12:31:38 [24895] (90.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/20/16 12:31:38 [24895] (90.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/20/16 12:31:38 [24895] GAHP[25188] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ HISTFILESIZE=20000\ SSH_CONNECTION=10.255.12.11'\ '61414'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.11'\ '61414'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/1\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_jZXZAy\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/89/0/cluster89.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#90.0#1455902688";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/20/16 12:31:38 [24895] GAHP[25188] -> 'S'
02/20/16 12:31:38 [24895] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/20/16 12:31:38 [24895] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/20/16 12:31:38 [24895] IPVERIFY: ip found is 1
02/20/16 12:31:40 [24895] Evaluating staleness of remote job statuses.
02/20/16 12:31:41 [24895] dirscat: dirpath = /tmp
02/20/16 12:31:41 [24895] dirscat: subdir = condorLocks
02/20/16 12:31:41 [24895] directory_util::rec_touch_file: Creating directory /tmp 
02/20/16 12:31:41 [24895] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
02/20/16 12:31:41 [24895] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/39 
02/20/16 12:31:41 [24895] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/39/98 
02/20/16 12:31:41 [24895] FileLock object is updating timestamp on: /tmp/condorLocks/39/98/302777985962395.lockc
02/20/16 12:31:41 [24895] WriteUserLog::initialize: opened /n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/.log_14441_GsU0Ko successfully
02/20/16 12:31:41 [24895] (90.0) Writing job status unknown record to user logfile
02/20/16 12:31:41 [24895] FileLock::obtain(1) - @1455989501.159407 lock on /tmp/condorLocks/39/98/302777985962395.lockc now WRITE
02/20/16 12:31:41 [24895] FileLock::obtain(2) - @1455989501.600546 lock on /tmp/condorLocks/39/98/302777985962395.lockc now UNLOCKED
02/20/16 12:31:41 [24895] FileLock::obtain(1) - @1455989501.600648 lock on /tmp/condorLocks/39/98/302777985962395.lockc now WRITE
02/20/16 12:31:41 [24895] directory_util::rec_clean_up: file /tmp/condorLocks/39/98/302777985962395.lockc has been deleted. 
02/20/16 12:31:41 [24895] Lock file /tmp/condorLocks/39/98/302777985962395.lockc has been deleted. 
02/20/16 12:31:41 [24895] FileLock::obtain(2) - @1455989501.600813 lock on /tmp/condorLocks/39/98/302777985962395.lockc now UNLOCKED
02/20/16 12:31:41 [24895] (90.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/20/16 12:31:43 [24895] resource  is now up
02/20/16 12:31:43 [24895] in doContactSchedd()
02/20/16 12:31:43 [24895] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 12:31:43 [24895] querying for new jobs
02/20/16 12:31:43 [24895] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/20/16 12:31:43 [24895] Fetched 0 new job ads from schedd
02/20/16 12:31:43 [24895] querying for removed/held jobs
02/20/16 12:31:43 [24895] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 12:31:43 [24895] Fetched 1 job ads from schedd
02/20/16 12:31:43 [24895] Updating classad values for 90.0:
02/20/16 12:31:43 [24895]    CurrentStatusUnknown = true
02/20/16 12:31:43 [24895] leaving doContactSchedd()
02/20/16 12:31:43 [24895] (90.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/20/16 12:31:45 [24895] GAHP[25188] <- 'RESULTS'
02/20/16 12:31:45 [24895] GAHP[25188] -> 'R'
02/20/16 12:31:45 [24895] GAHP[25188] -> 'S' '1'
02/20/16 12:31:45 [24895] GAHP[25188] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/20/16 12:31:45 [24895] (90.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/20/16 12:31:45 [24895] (90.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/20/16 12:31:45 [24895] (90.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/20/16 12:31:45 [24895] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/20/16 12:32:18 [24895] (90.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/20/16 12:32:18 [24895] WriteUserLog::initialize: safe_open_wrapper("/n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/.log_14441_GsU0Ko") failed - errno 13 (Permission denied)
02/20/16 12:32:18 [24895] WriteUserLog::initialize: failed to open file /n/atlasgrid/condor/89/0/cluster89.proc0.subproc0/.log_14441_GsU0Ko
02/20/16 12:32:18 [24895] (90.0) Writing abort record to user logfile
02/20/16 12:32:18 [24895] WriteUserLog: not initialized @ writeEvent()
02/20/16 12:32:18 [24895] in doContactSchedd()
02/20/16 12:32:18 [24895] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 12:32:18 [24895] querying for removed/held jobs
02/20/16 12:32:18 [24895] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 12:32:18 [24895] Fetched 1 job ads from schedd
02/20/16 12:32:18 [24895] Updating classad values for 90.0:
02/20/16 12:32:18 [24895]    CurrentStatusUnknown = false
02/20/16 12:32:18 [24895]    GridJobId = undefined
02/20/16 12:32:18 [24895]    LastRemoteStatusUpdate = 0
02/20/16 12:32:18 [24895]    Managed = "ScheddDone"
02/20/16 12:32:18 [24895] Deleting job 90.0 from schedd
02/20/16 12:32:18 [24895] No jobs left, shutting down
02/20/16 12:32:18 [24895] leaving doContactSchedd()
02/20/16 12:32:18 [24895] Got SIGTERM. Performing graceful shutdown.
02/20/16 12:32:18 [24895] Started timer to call main_shutdown_fast in 1800 seconds
02/20/16 12:32:18 [24895] **** condor_gridmanager (condor_GRIDMANAGER) pid 24895 EXITING WITH STATUS 0
02/20/16 13:06:40 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/20/16 13:06:40 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/20/16 13:06:40 Enumerating interfaces: lo 127.0.0.1 up
02/20/16 13:06:40 Enumerating interfaces: eth2 10.31.131.202 up
02/20/16 13:06:40 Enumerating interfaces: eth3 140.247.179.131 up
02/20/16 13:06:40 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/20/16 13:06:40 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/20/16 13:06:40 ******************************************************
02/20/16 13:06:40 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/20/16 13:06:40 ** /usr/sbin/condor_gridmanager
02/20/16 13:06:40 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/20/16 13:06:40 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/20/16 13:06:40 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/20/16 13:06:40 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/20/16 13:06:40 ** PID = 29440
02/20/16 13:06:40 ** Log last touched 2/20 12:32:18
02/20/16 13:06:40 ******************************************************
02/20/16 13:06:40 Using config source: /etc/condor-ce/condor_config
02/20/16 13:06:40 Using local config sources: 
02/20/16 13:06:40    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/20/16 13:06:40    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/01-ce-auth.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/01-ce-router.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/01-common-auth.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/02-ce-lsf.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/02-ce-pbs.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/03-managed-fork.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/50-osg-configure.conf
02/20/16 13:06:40    /etc/condor-ce/config.d/99-local.conf
02/20/16 13:06:40    /usr/share/condor-ce/condor_ce_router_defaults|
02/20/16 13:06:40 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
02/20/16 13:06:40 CLASSAD_CACHING is ENABLED
02/20/16 13:06:40 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/20/16 13:06:40 SharedPortEndpoint: waiting for connections to named socket 3540_0009_3
02/20/16 13:06:40 DaemonCore: command socket at <140.247.179.131:9620?sock=3540_0009_3>
02/20/16 13:06:40 DaemonCore: private command socket at <140.247.179.131:9620?sock=3540_0009_3>
02/20/16 13:06:40 Setting maximum accepts per cycle 8.
02/20/16 13:06:40 Setting maximum reaps per cycle 8.
02/20/16 13:06:40 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 13:06:40 [29440] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/20/16 13:06:40 [29440] DaemonCore: No more children processes to reap.
02/20/16 13:06:40 [29440] DaemonCore: in SendAliveToParent()
02/20/16 13:06:40 [29440] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:06:40 [29440] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/20/16 13:06:40 [29440] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/20/16 13:06:40 [29440] IPVERIFY: ip found is 0
02/20/16 13:06:40 [29440] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/20/16 13:06:40 [29440] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/20/16 13:06:40 [29440] Buf::write(): condor_write() failed
02/20/16 13:06:40 [29440] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/20/16 13:06:40 [29440] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:06:40 [29440] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/20/16 13:06:40 [29440] DaemonCore: Leaving SendAliveToParent() - success
02/20/16 13:06:40 [29440] Checking proxies
02/20/16 13:06:42 [29440] Received REMOVE_JOBS signal
02/20/16 13:06:42 [29440] in doContactSchedd()
02/20/16 13:06:42 [29440] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:06:42 [29440] querying for new jobs
02/20/16 13:06:42 [29440] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/20/16 13:06:42 [29440] Using job type INFNBatch for job 92.0
02/20/16 13:06:42 [29440] (92.0) SetJobLeaseTimers()
02/20/16 13:06:43 [29440] Found job 92.0 --- inserting
02/20/16 13:06:43 [29440] Fetched 1 new job ads from schedd
02/20/16 13:06:43 [29440] querying for removed/held jobs
02/20/16 13:06:43 [29440] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 13:06:43 [29440] Fetched 1 job ads from schedd
02/20/16 13:06:43 [29440] leaving doContactSchedd()
02/20/16 13:06:43 [29440] gahp server not up yet, delaying ping
02/20/16 13:06:43 [29440] *** UpdateLeases called
02/20/16 13:06:43 [29440]     Leases not supported, cancelling timer
02/20/16 13:06:43 [29440] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=3540_0009_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=3533_8c32_4>"
CurrentTime = time()
MyCurrentTime = 1455991603
IdleJobs = 0
JobLimit = 10000

02/20/16 13:06:43 [29440] Trying to update collector <10.31.131.202:9619>
02/20/16 13:06:43 [29440] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 13:06:43 [29440] File descriptor limits: max 4096, safe 3277
02/20/16 13:06:43 [29440] (92.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/20/16 13:06:43 [29440] GAHP server pid = 29447
02/20/16 13:06:43 [29440] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/20/16 13:06:43 [29440] GAHP[29447] <- 'COMMANDS'
02/20/16 13:06:43 [29440] GAHP[29447] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/20/16 13:06:43 [29440] GAHP[29447] <- 'ASYNC_MODE_ON'
02/20/16 13:06:43 [29440] GAHP[29447] -> 'S' 'Async mode on'
02/20/16 13:06:43 [29440] (92.0) gm state change: GM_INIT -> GM_START
02/20/16 13:06:43 [29440] (92.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/20/16 13:06:43 [29440] (92.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/20/16 13:06:43 [29440] GAHP[29447] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ HISTFILESIZE=20000\ SSH_CONNECTION=10.255.12.11'\ '61414'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.11'\ '61414'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/1\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_jZXZAy\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/91/0/cluster91.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/91/0/cluster91.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/91/0/cluster91.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#92.0#1455904953";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/20/16 13:06:43 [29440] GAHP[29447] -> 'S'
02/20/16 13:06:43 [29440] Received ADD_JOBS signal
02/20/16 13:06:43 [29440] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/20/16 13:06:43 [29440] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/20/16 13:06:43 [29440] IPVERIFY: ip found is 1
02/20/16 13:06:45 [29440] Evaluating staleness of remote job statuses.
02/20/16 13:06:45 [29440] dirscat: dirpath = /tmp
02/20/16 13:06:45 [29440] dirscat: subdir = condorLocks
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp 
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/99 
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/99/98 
02/20/16 13:06:45 [29440] FileLock object is updating timestamp on: /tmp/condorLocks/99/98/540346910294500.lockc
02/20/16 13:06:45 [29440] WriteUserLog::initialize: opened /n/atlasgrid/condor/91/0/cluster91.proc0.subproc0/.log_17088_UaDShk successfully
02/20/16 13:06:45 [29440] (92.0) Writing job status unknown record to user logfile
02/20/16 13:06:45 [29440] FileLock::obtain(1) - @1455991605.253206 lock on /tmp/condorLocks/99/98/540346910294500.lockc now WRITE
02/20/16 13:06:45 [29440] FileLock::obtain(2) - @1455991605.262174 lock on /tmp/condorLocks/99/98/540346910294500.lockc now UNLOCKED
02/20/16 13:06:45 [29440] FileLock::obtain(1) - @1455991605.262268 lock on /tmp/condorLocks/99/98/540346910294500.lockc now WRITE
02/20/16 13:06:45 [29440] directory_util::rec_clean_up: file /tmp/condorLocks/99/98/540346910294500.lockc has been deleted. 
02/20/16 13:06:45 [29440] Lock file /tmp/condorLocks/99/98/540346910294500.lockc has been deleted. 
02/20/16 13:06:45 [29440] FileLock::obtain(2) - @1455991605.262428 lock on /tmp/condorLocks/99/98/540346910294500.lockc now UNLOCKED
02/20/16 13:06:45 [29440] GAHP[29447] <- 'RESULTS'
02/20/16 13:06:45 [29440] GAHP[29447] -> 'R'
02/20/16 13:06:45 [29440] GAHP[29447] -> 'S' '1'
02/20/16 13:06:45 [29440] GAHP[29447] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/20/16 13:06:45 [29440] (92.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/20/16 13:06:45 [29440] (92.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/20/16 13:06:45 [29440] (92.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/20/16 13:06:45 [29440] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/20/16 13:06:45 [29440] (92.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/20/16 13:06:45 [29440] dirscat: dirpath = /tmp
02/20/16 13:06:45 [29440] dirscat: subdir = condorLocks
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp 
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/99 
02/20/16 13:06:45 [29440] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/99/98 
02/20/16 13:06:45 [29440] FileLock object is updating timestamp on: /tmp/condorLocks/99/98/540346910294500.lockc
02/20/16 13:06:45 [29440] WriteUserLog::initialize: opened /n/atlasgrid/condor/91/0/cluster91.proc0.subproc0/.log_17088_UaDShk successfully
02/20/16 13:06:45 [29440] (92.0) Writing abort record to user logfile
02/20/16 13:06:45 [29440] FileLock::obtain(1) - @1455991605.298374 lock on /tmp/condorLocks/99/98/540346910294500.lockc now WRITE
02/20/16 13:06:45 [29440] FileLock::obtain(2) - @1455991605.309719 lock on /tmp/condorLocks/99/98/540346910294500.lockc now UNLOCKED
02/20/16 13:06:45 [29440] FileLock::obtain(1) - @1455991605.309812 lock on /tmp/condorLocks/99/98/540346910294500.lockc now WRITE
02/20/16 13:06:45 [29440] directory_util::rec_clean_up: file /tmp/condorLocks/99/98/540346910294500.lockc has been deleted. 
02/20/16 13:06:45 [29440] Lock file /tmp/condorLocks/99/98/540346910294500.lockc has been deleted. 
02/20/16 13:06:45 [29440] FileLock::obtain(2) - @1455991605.309970 lock on /tmp/condorLocks/99/98/540346910294500.lockc now UNLOCKED
02/20/16 13:06:48 [29440] resource  is now up
02/20/16 13:06:48 [29440] in doContactSchedd()
02/20/16 13:06:48 [29440] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:06:48 [29440] querying for new jobs
02/20/16 13:06:48 [29440] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/20/16 13:06:48 [29440] Fetched 0 new job ads from schedd
02/20/16 13:06:48 [29440] querying for removed/held jobs
02/20/16 13:06:48 [29440] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 13:06:48 [29440] Fetched 1 job ads from schedd
02/20/16 13:06:48 [29440] Updating classad values for 92.0:
02/20/16 13:06:48 [29440]    CurrentStatusUnknown = false
02/20/16 13:06:48 [29440]    GridJobId = undefined
02/20/16 13:06:48 [29440]    LastRemoteStatusUpdate = 0
02/20/16 13:06:48 [29440]    Managed = "ScheddDone"
02/20/16 13:06:48 [29440] Deleting job 92.0 from schedd
02/20/16 13:06:48 [29440] No jobs left, shutting down
02/20/16 13:06:48 [29440] leaving doContactSchedd()
02/20/16 13:06:48 [29440] Got SIGTERM. Performing graceful shutdown.
02/20/16 13:06:48 [29440] Started timer to call main_shutdown_fast in 1800 seconds
02/20/16 13:06:48 [29440] **** condor_gridmanager (condor_GRIDMANAGER) pid 29440 EXITING WITH STATUS 0
02/20/16 13:46:44 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/20/16 13:46:44 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/20/16 13:46:44 Enumerating interfaces: lo 127.0.0.1 up
02/20/16 13:46:44 Enumerating interfaces: eth2 10.31.131.202 up
02/20/16 13:46:44 Enumerating interfaces: eth3 140.247.179.131 up
02/20/16 13:46:44 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/20/16 13:46:44 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/20/16 13:46:44 ******************************************************
02/20/16 13:46:44 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/20/16 13:46:44 ** /usr/sbin/condor_gridmanager
02/20/16 13:46:44 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/20/16 13:46:44 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/20/16 13:46:44 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/20/16 13:46:44 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/20/16 13:46:44 ** PID = 18507
02/20/16 13:46:44 ** Log last touched 2/20 13:06:48
02/20/16 13:46:44 ******************************************************
02/20/16 13:46:44 Using config source: /etc/condor-ce/condor_config
02/20/16 13:46:44 Using local config sources: 
02/20/16 13:46:44    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/20/16 13:46:44    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/01-ce-auth.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/01-ce-router.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/01-common-auth.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/02-ce-lsf.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/02-ce-pbs.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/03-managed-fork.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/50-osg-configure.conf
02/20/16 13:46:44    /etc/condor-ce/config.d/99-local.conf
02/20/16 13:46:44    /usr/share/condor-ce/condor_ce_router_defaults|
02/20/16 13:46:44 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
02/20/16 13:46:44 CLASSAD_CACHING is ENABLED
02/20/16 13:46:44 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/20/16 13:46:44 SharedPortEndpoint: waiting for connections to named socket 3540_0009_4
02/20/16 13:46:44 DaemonCore: command socket at <140.247.179.131:9620?sock=3540_0009_4>
02/20/16 13:46:44 DaemonCore: private command socket at <140.247.179.131:9620?sock=3540_0009_4>
02/20/16 13:46:44 Setting maximum accepts per cycle 8.
02/20/16 13:46:44 Setting maximum reaps per cycle 8.
02/20/16 13:46:44 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 13:46:44 [18507] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/20/16 13:46:44 [18507] DaemonCore: No more children processes to reap.
02/20/16 13:46:44 [18507] DaemonCore: in SendAliveToParent()
02/20/16 13:46:44 [18507] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:46:45 [18507] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/20/16 13:46:45 [18507] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/20/16 13:46:45 [18507] IPVERIFY: ip found is 0
02/20/16 13:46:45 [18507] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/20/16 13:46:45 [18507] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/20/16 13:46:45 [18507] Buf::write(): condor_write() failed
02/20/16 13:46:45 [18507] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/20/16 13:46:45 [18507] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:46:45 [18507] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/20/16 13:46:45 [18507] DaemonCore: Leaving SendAliveToParent() - success
02/20/16 13:46:45 [18507] Checking proxies
02/20/16 13:46:46 [18507] Received REMOVE_JOBS signal
02/20/16 13:46:46 [18507] in doContactSchedd()
02/20/16 13:46:46 [18507] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:46:46 [18507] querying for new jobs
02/20/16 13:46:46 [18507] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/20/16 13:46:46 [18507] Using job type INFNBatch for job 94.0
02/20/16 13:46:46 [18507] (94.0) SetJobLeaseTimers()
02/20/16 13:46:46 [18507] Found job 94.0 --- inserting
02/20/16 13:46:46 [18507] Fetched 1 new job ads from schedd
02/20/16 13:46:46 [18507] querying for removed/held jobs
02/20/16 13:46:46 [18507] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 13:46:46 [18507] Fetched 1 job ads from schedd
02/20/16 13:46:46 [18507] leaving doContactSchedd()
02/20/16 13:46:46 [18507] gahp server not up yet, delaying ping
02/20/16 13:46:46 [18507] *** UpdateLeases called
02/20/16 13:46:46 [18507]     Leases not supported, cancelling timer
02/20/16 13:46:46 [18507] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=3540_0009_4>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=3533_8c32_4>"
CurrentTime = time()
MyCurrentTime = 1455994006
IdleJobs = 0
JobLimit = 10000

02/20/16 13:46:46 [18507] Trying to update collector <10.31.131.202:9619>
02/20/16 13:46:46 [18507] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 13:46:46 [18507] File descriptor limits: max 4096, safe 3277
02/20/16 13:46:46 [18507] (94.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/20/16 13:46:46 [18507] GAHP server pid = 18509
02/20/16 13:46:46 [18507] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/20/16 13:46:46 [18507] GAHP[18509] <- 'COMMANDS'
02/20/16 13:46:46 [18507] GAHP[18509] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/20/16 13:46:46 [18507] GAHP[18509] <- 'ASYNC_MODE_ON'
02/20/16 13:46:46 [18507] GAHP[18509] -> 'S' 'Async mode on'
02/20/16 13:46:46 [18507] (94.0) gm state change: GM_INIT -> GM_START
02/20/16 13:46:46 [18507] (94.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/20/16 13:46:46 [18507] (94.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/20/16 13:46:46 [18507] GAHP[18509] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'\ PATH=/odyssey/rc_admin/sw/admin/web/drupal:/odyssey/rc_admin/sw/admin/web:/odyssey/rc_admin/sw/admin/virt:/odyssey/rc_admin/sw/admin/textmanip:/odyssey/rc_admin/sw/admin/scifs/admin:/odyssey/rc_admin/sw/admin/scifs:/odyssey/rc_admin/sw/admin/rt:/odyssey/rc_admin/sw/admin/provisioning:/odyssey/rc_admin/sw/admin/nagios:/odyssey/rc_admin/sw/admin/misc:/odyssey/rc_admin/sw/admin/iltvis:/odyssey/rc_admin/sw/admin/fs/_attic:/odyssey/rc_admin/sw/admin/fs:/odyssey/rc_admin/sw/admin/emc:/odyssey/rc_admin/sw/admin/ad:/n/sw/rc/bin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/lsf/7.0/linux2.6-glibc2.3-x86_64/etc:/lsf/7.0/linux2.6-glibc2.3-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/opt/dell/srvadmin/bin:/n/home_rc/dcaunt/bin\ MAIL=/var/spool/mail/dcaunt\ LD_LIBRARY_PATH=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ XLSF_UIDDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib/uid\ CVS_RSH=ssh\ BASH_ENV=/etc/profile.d/modules.sh\ MANPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/man:/lsf/7.0/man:\ LANG=en_US.UTF-8\ OMP_NUM_THREADS=1\ HISTFILESIZE=20000\ SSH_CONNECTION=10.255.12.11'\ '61414'\ '10.31.130.37'\ '22\ MODULEPATH=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles:\ FASRC_MODULE_FLAVOR=legacy\ DATACENTER=\ QTINC=/usr/lib64/qt-3.3/include\ X509_CERT_DIR=/etc/grid-security/certificates\ SSH_CLIENT=10.255.12.11'\ '61414'\ '22\ SHELL=/bin/bash\ _LMFILES_=/n/sw/odyssey-apps/modules-3.2.6/Modules/modulefiles/hpc/rc\ COBBLER_SERVER=cobbler.rc.fas.harvard.edu\ _=/usr/bin/condor_ce_run\ PWD=/n/home_rc/dcaunt\ QTDIR=/usr/lib64/qt-3.3\ SSH_TTY=/dev/pts/1\ QTLIB=/usr/lib64/qt-3.3/lib\ LSF_SERVERDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/etc\ TOOL_DEBUG=D_FULLDEBUG\ CONDOR_CONFIG=/etc/condor-ce/condor_config\ HISTSIZE=1000\ USER=dcaunt\ LOADEDMODULES=hpc/rc\ G_BROKEN_FILENAMES=1\ LESSOPEN=|/usr/bin/lesspipe.sh'\ '%s\ MODULESHOME=/n/sw/odyssey-apps/modules-3.2.6/Modules\ BINARY_TYPE_HPC=\ KRB5CCNAME=FILE:/tmp/krb5cc_556792_jZXZAy\ SHLVL=1\ LSF_BINDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin\ LSF_LIBDIR=/lsf/7.0/linux2.6-glibc2.3-x86_64/lib\ _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI\ HOSTNAME=atlas5311.rc.fas.harvard.edu\ HOME=/n/home_rc/dcaunt\ TERM=xterm-256color\ INPUTRC=/etc/inputrc\ LSF_ENVDIR=/lsf/conf\ LOGNAME=dcaunt";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/93/0/cluster93.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/93/0/cluster93.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/93/0/cluster93.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#94.0#1455907057";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/20/16 13:46:46 [18507] GAHP[18509] -> 'S'
02/20/16 13:46:46 [18507] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/20/16 13:46:46 [18507] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/20/16 13:46:46 [18507] IPVERIFY: ip found is 1
02/20/16 13:46:47 [18507] Received ADD_JOBS signal
02/20/16 13:46:47 [18507] GAHP[18509] <- 'RESULTS'
02/20/16 13:46:47 [18507] GAHP[18509] -> 'R'
02/20/16 13:46:47 [18507] GAHP[18509] -> 'S' '1'
02/20/16 13:46:47 [18507] GAHP[18509] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/20/16 13:46:47 [18507] (94.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/20/16 13:46:47 [18507] (94.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/20/16 13:46:47 [18507] (94.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/20/16 13:46:47 [18507] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/20/16 13:46:50 [18507] (94.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/20/16 13:46:50 [18507] dirscat: dirpath = /tmp
02/20/16 13:46:50 [18507] dirscat: subdir = condorLocks
02/20/16 13:46:50 [18507] directory_util::rec_touch_file: Creating directory /tmp 
02/20/16 13:46:50 [18507] directory_util::rec_touch_file: Creating directory /tmp/condorLocks 
02/20/16 13:46:50 [18507] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/74 
02/20/16 13:46:50 [18507] directory_util::rec_touch_file: Creating directory /tmp/condorLocks/74/86 
02/20/16 13:46:50 [18507] FileLock object is updating timestamp on: /tmp/condorLocks/74/86/704348923219125.lockc
02/20/16 13:46:50 [18507] WriteUserLog::initialize: opened /n/atlasgrid/condor/93/0/cluster93.proc0.subproc0/.log_19324_XjQ1GK successfully
02/20/16 13:46:50 [18507] (94.0) Writing abort record to user logfile
02/20/16 13:46:50 [18507] FileLock::obtain(1) - @1455994010.936573 lock on /tmp/condorLocks/74/86/704348923219125.lockc now WRITE
02/20/16 13:46:50 [18507] FileLock::obtain(2) - @1455994010.953897 lock on /tmp/condorLocks/74/86/704348923219125.lockc now UNLOCKED
02/20/16 13:46:50 [18507] FileLock::obtain(1) - @1455994010.953993 lock on /tmp/condorLocks/74/86/704348923219125.lockc now WRITE
02/20/16 13:46:50 [18507] directory_util::rec_clean_up: file /tmp/condorLocks/74/86/704348923219125.lockc has been deleted. 
02/20/16 13:46:50 [18507] Lock file /tmp/condorLocks/74/86/704348923219125.lockc has been deleted. 
02/20/16 13:46:50 [18507] FileLock::obtain(2) - @1455994010.954164 lock on /tmp/condorLocks/74/86/704348923219125.lockc now UNLOCKED
02/20/16 13:46:50 [18507] Evaluating staleness of remote job statuses.
02/20/16 13:46:51 [18507] resource  is now up
02/20/16 13:46:51 [18507] in doContactSchedd()
02/20/16 13:46:51 [18507] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 13:46:51 [18507] querying for new jobs
02/20/16 13:46:51 [18507] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/20/16 13:46:51 [18507] Fetched 0 new job ads from schedd
02/20/16 13:46:51 [18507] querying for removed/held jobs
02/20/16 13:46:51 [18507] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 13:46:51 [18507] Fetched 1 job ads from schedd
02/20/16 13:46:51 [18507] Updating classad values for 94.0:
02/20/16 13:46:51 [18507]    CurrentStatusUnknown = false
02/20/16 13:46:51 [18507]    GridJobId = undefined
02/20/16 13:46:51 [18507]    LastRemoteStatusUpdate = 0
02/20/16 13:46:51 [18507]    Managed = "ScheddDone"
02/20/16 13:46:51 [18507] Deleting job 94.0 from schedd
02/20/16 13:46:51 [18507] No jobs left, shutting down
02/20/16 13:46:51 [18507] leaving doContactSchedd()
02/20/16 13:46:51 [18507] Got SIGTERM. Performing graceful shutdown.
02/20/16 13:46:51 [18507] Started timer to call main_shutdown_fast in 1800 seconds
02/20/16 13:46:51 [18507] **** condor_gridmanager (condor_GRIDMANAGER) pid 18507 EXITING WITH STATUS 0
02/20/16 15:56:56 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/20/16 15:56:56 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/20/16 15:56:56 Enumerating interfaces: lo 127.0.0.1 up
02/20/16 15:56:56 Enumerating interfaces: eth2 10.31.131.202 up
02/20/16 15:56:56 Enumerating interfaces: eth3 140.247.179.131 up
02/20/16 15:56:56 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/20/16 15:56:56 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/20/16 15:56:56 ******************************************************
02/20/16 15:56:56 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/20/16 15:56:56 ** /usr/sbin/condor_gridmanager
02/20/16 15:56:56 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/20/16 15:56:56 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/20/16 15:56:56 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/20/16 15:56:56 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/20/16 15:56:56 ** PID = 15847
02/20/16 15:56:56 ** Log last touched 2/20 13:46:51
02/20/16 15:56:56 ******************************************************
02/20/16 15:56:56 Using config source: /etc/condor-ce/condor_config
02/20/16 15:56:56 Using local config sources: 
02/20/16 15:56:56    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/20/16 15:56:56    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/01-ce-auth.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/01-ce-router.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/01-common-auth.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/02-ce-lsf.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/02-ce-pbs.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/03-managed-fork.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/50-osg-configure.conf
02/20/16 15:56:56    /etc/condor-ce/config.d/99-local.conf
02/20/16 15:56:56    /usr/share/condor-ce/condor_ce_router_defaults|
02/20/16 15:56:56 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
02/20/16 15:56:56 CLASSAD_CACHING is ENABLED
02/20/16 15:56:56 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/20/16 15:56:56 SharedPortEndpoint: waiting for connections to named socket 3540_0009_5
02/20/16 15:56:56 DaemonCore: command socket at <140.247.179.131:9620?sock=3540_0009_5>
02/20/16 15:56:56 DaemonCore: private command socket at <140.247.179.131:9620?sock=3540_0009_5>
02/20/16 15:56:56 Setting maximum accepts per cycle 8.
02/20/16 15:56:56 Setting maximum reaps per cycle 8.
02/20/16 15:56:56 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 15:56:56 [15847] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/20/16 15:56:56 [15847] DaemonCore: No more children processes to reap.
02/20/16 15:56:56 [15847] DaemonCore: in SendAliveToParent()
02/20/16 15:56:56 [15847] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 15:56:57 [15847] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/20/16 15:56:57 [15847] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/20/16 15:56:57 [15847] IPVERIFY: ip found is 0
02/20/16 15:56:57 [15847] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/20/16 15:56:57 [15847] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/20/16 15:56:57 [15847] Buf::write(): condor_write() failed
02/20/16 15:56:57 [15847] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/20/16 15:56:57 [15847] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 15:56:57 [15847] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/20/16 15:56:57 [15847] DaemonCore: Leaving SendAliveToParent() - success
02/20/16 15:56:57 [15847] Checking proxies
02/20/16 15:56:58 [15847] Received REMOVE_JOBS signal
02/20/16 15:56:58 [15847] in doContactSchedd()
02/20/16 15:56:58 [15847] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 15:56:58 [15847] querying for new jobs
02/20/16 15:56:58 [15847] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/20/16 15:56:58 [15847] Using job type INFNBatch for job 96.0
02/20/16 15:56:58 [15847] (96.0) SetJobLeaseTimers()
02/20/16 15:56:59 [15847] Found job 96.0 --- inserting
02/20/16 15:56:59 [15847] Fetched 1 new job ads from schedd
02/20/16 15:56:59 [15847] querying for removed/held jobs
02/20/16 15:56:59 [15847] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 15:56:59 [15847] Fetched 1 job ads from schedd
02/20/16 15:57:00 [15847] leaving doContactSchedd()
02/20/16 15:57:00 [15847] gahp server not up yet, delaying ping
02/20/16 15:57:00 [15847] *** UpdateLeases called
02/20/16 15:57:00 [15847]     Leases not supported, cancelling timer
02/20/16 15:57:00 [15847] Received ADD_JOBS signal
02/20/16 15:57:00 [15847] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=3540_0009_5>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=3533_8c32_4>"
CurrentTime = time()
MyCurrentTime = 1456001820
IdleJobs = 0
JobLimit = 10000

02/20/16 15:57:00 [15847] Trying to update collector <10.31.131.202:9619>
02/20/16 15:57:00 [15847] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/20/16 15:57:00 [15847] File descriptor limits: max 4096, safe 3277
02/20/16 15:57:00 [15847] (96.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/20/16 15:57:00 [15847] GAHP server pid = 15996
02/20/16 15:57:00 [15847] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/20/16 15:57:00 [15847] GAHP[15996] <- 'COMMANDS'
02/20/16 15:57:00 [15847] GAHP[15996] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/20/16 15:57:00 [15847] GAHP[15996] <- 'ASYNC_MODE_ON'
02/20/16 15:57:00 [15847] GAHP[15996] -> 'S' 'Async mode on'
02/20/16 15:57:00 [15847] (96.0) gm state change: GM_INIT -> GM_START
02/20/16 15:57:00 [15847] (96.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/20/16 15:57:00 [15847] (96.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/20/16 15:57:00 [15847] GAHP[15996] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/95/0/cluster95.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/95/0/cluster95.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/95/0/cluster95.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#96.0#1455915228";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/20/16 15:57:00 [15847] GAHP[15996] -> 'S'
02/20/16 15:57:00 [15847] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/20/16 15:57:00 [15847] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/20/16 15:57:00 [15847] IPVERIFY: ip found is 1
02/20/16 15:57:01 [15847] Evaluating staleness of remote job statuses.
02/20/16 15:57:01 [15847] (96.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/20/16 15:57:04 [15847] GAHP[15996] <- 'RESULTS'
02/20/16 15:57:04 [15847] GAHP[15996] -> 'R'
02/20/16 15:57:04 [15847] GAHP[15996] -> 'S' '1'
02/20/16 15:57:04 [15847] GAHP[15996] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/20/16 15:57:04 [15847] (96.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/20/16 15:57:04 [15847] (96.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/20/16 15:57:04 [15847] (96.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/20/16 15:57:04 [15847] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/20/16 15:57:21 [15847] (96.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/20/16 15:57:21 [15847] resource  is now up
02/20/16 15:57:21 [15847] in doContactSchedd()
02/20/16 15:57:21 [15847] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3533_8c32_4
02/20/16 15:57:21 [15847] querying for new jobs
02/20/16 15:57:21 [15847] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/20/16 15:57:21 [15847] Fetched 0 new job ads from schedd
02/20/16 15:57:21 [15847] querying for removed/held jobs
02/20/16 15:57:21 [15847] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/20/16 15:57:21 [15847] Fetched 1 job ads from schedd
02/20/16 15:57:21 [15847] Updating classad values for 96.0:
02/20/16 15:57:21 [15847]    CurrentStatusUnknown = false
02/20/16 15:57:21 [15847]    GridJobId = undefined
02/20/16 15:57:21 [15847]    LastRemoteStatusUpdate = 0
02/20/16 15:57:21 [15847]    Managed = "ScheddDone"
02/20/16 15:57:21 [15847] Deleting job 96.0 from schedd
02/20/16 15:57:21 [15847] No jobs left, shutting down
02/20/16 15:57:21 [15847] leaving doContactSchedd()
02/20/16 15:57:21 [15847] Got SIGTERM. Performing graceful shutdown.
02/20/16 15:57:21 [15847] Started timer to call main_shutdown_fast in 1800 seconds
02/20/16 15:57:21 [15847] **** condor_gridmanager (condor_GRIDMANAGER) pid 15847 EXITING WITH STATUS 0
02/22/16 11:04:00 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/22/16 11:04:00 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/22/16 11:04:00 Enumerating interfaces: lo 127.0.0.1 up
02/22/16 11:04:00 Enumerating interfaces: eth2 10.31.131.202 up
02/22/16 11:04:00 Enumerating interfaces: eth3 140.247.179.131 up
02/22/16 11:04:00 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/22/16 11:04:00 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/22/16 11:04:00 ******************************************************
02/22/16 11:04:00 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/22/16 11:04:00 ** /usr/sbin/condor_gridmanager
02/22/16 11:04:00 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/22/16 11:04:00 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/22/16 11:04:00 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/22/16 11:04:00 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/22/16 11:04:00 ** PID = 20082
02/22/16 11:04:00 ** Log last touched 2/20 15:57:21
02/22/16 11:04:00 ******************************************************
02/22/16 11:04:00 Using config source: /etc/condor-ce/condor_config
02/22/16 11:04:00 Using local config sources: 
02/22/16 11:04:00    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/22/16 11:04:00    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/01-ce-auth.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/01-ce-router.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/01-common-auth.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/02-ce-lsf.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/02-ce-pbs.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/03-managed-fork.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/50-osg-configure.conf
02/22/16 11:04:00    /etc/condor-ce/config.d/99-local.conf
02/22/16 11:04:00    /usr/share/condor-ce/condor_ce_router_defaults|
02/22/16 11:04:00 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
02/22/16 11:04:00 CLASSAD_CACHING is ENABLED
02/22/16 11:04:00 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/22/16 11:04:00 SharedPortEndpoint: waiting for connections to named socket 17797_58ad_1
02/22/16 11:04:00 DaemonCore: command socket at <140.247.179.131:9620?sock=17797_58ad_1>
02/22/16 11:04:00 DaemonCore: private command socket at <140.247.179.131:9620?sock=17797_58ad_1>
02/22/16 11:04:00 Setting maximum accepts per cycle 8.
02/22/16 11:04:00 Setting maximum reaps per cycle 8.
02/22/16 11:04:00 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/22/16 11:04:00 [20082] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/22/16 11:04:00 [20082] DaemonCore: No more children processes to reap.
02/22/16 11:04:00 [20082] DaemonCore: in SendAliveToParent()
02/22/16 11:04:00 [20082] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 17789_d3dd_4
02/22/16 11:04:00 [20082] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/22/16 11:04:00 [20082] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/22/16 11:04:00 [20082] IPVERIFY: ip found is 0
02/22/16 11:04:00 [20082] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/22/16 11:04:00 [20082] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/22/16 11:04:00 [20082] Buf::write(): condor_write() failed
02/22/16 11:04:00 [20082] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/22/16 11:04:00 [20082] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 17789_d3dd_4
02/22/16 11:04:00 [20082] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/22/16 11:04:00 [20082] DaemonCore: Leaving SendAliveToParent() - success
02/22/16 11:04:00 [20082] Checking proxies
02/22/16 11:04:03 [20082] Received ADD_JOBS signal
02/22/16 11:04:03 [20082] in doContactSchedd()
02/22/16 11:04:03 [20082] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 17789_d3dd_4
02/22/16 11:04:03 [20082] querying for new jobs
02/22/16 11:04:03 [20082] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/22/16 11:04:03 [20082] Using job type INFNBatch for job 98.0
02/22/16 11:04:03 [20082] (98.0) SetJobLeaseTimers()
02/22/16 11:04:03 [20082] Found job 98.0 --- inserting
02/22/16 11:04:03 [20082] Fetched 1 new job ads from schedd
02/22/16 11:04:03 [20082] querying for removed/held jobs
02/22/16 11:04:03 [20082] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/22/16 11:04:03 [20082] Fetched 0 job ads from schedd
02/22/16 11:04:03 [20082] leaving doContactSchedd()
02/22/16 11:04:03 [20082] gahp server not up yet, delaying ping
02/22/16 11:04:03 [20082] *** UpdateLeases called
02/22/16 11:04:03 [20082]     Leases not supported, cancelling timer
02/22/16 11:04:03 [20082] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=17797_58ad_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=17789_d3dd_4>"
CurrentTime = time()
MyCurrentTime = 1456157043
IdleJobs = 1
JobLimit = 10000

02/22/16 11:04:03 [20082] Trying to update collector <10.31.131.202:9619>
02/22/16 11:04:03 [20082] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/22/16 11:04:03 [20082] File descriptor limits: max 4096, safe 3277
02/22/16 11:04:03 [20082] (98.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/22/16 11:04:03 [20082] GAHP server pid = 20090
02/22/16 11:04:03 [20082] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/22/16 11:04:03 [20082] GAHP[20090] <- 'COMMANDS'
02/22/16 11:04:03 [20082] GAHP[20090] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/22/16 11:04:03 [20082] GAHP[20090] <- 'ASYNC_MODE_ON'
02/22/16 11:04:03 [20082] GAHP[20090] -> 'S' 'Async mode on'
02/22/16 11:04:03 [20082] (98.0) gm state change: GM_INIT -> GM_START
02/22/16 11:04:03 [20082] (98.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/22/16 11:04:03 [20082] (98.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/22/16 11:04:03 [20082] (98.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/22/16 11:04:03 [20082] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/22/16 11:04:03 [20082] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/22/16 11:04:03 [20082] IPVERIFY: ip found is 1
02/22/16 11:04:05 [20082] Evaluating staleness of remote job statuses.
02/22/16 11:04:08 [20082] resource  is now up
02/22/16 11:04:08 [20082] in doContactSchedd()
02/22/16 11:04:08 [20082] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 17789_d3dd_4
02/22/16 11:04:08 [20082] querying for removed/held jobs
02/22/16 11:04:08 [20082] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/22/16 11:04:08 [20082] Fetched 0 job ads from schedd
02/22/16 11:04:08 [20082] Updating classad values for 98.0:
02/22/16 11:04:08 [20082]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#98.0#1456157029"
02/22/16 11:04:08 [20082]    LastRemoteStatusUpdate = 1456157043
02/22/16 11:04:09 [20082] leaving doContactSchedd()
02/22/16 11:04:09 [20082] (98.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/22/16 11:04:09 [20082] (98.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/22/16 11:04:09 [20082] (98.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/22/16 11:04:09 [20082] GAHP[20090] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#98.0#1456157029";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/22/16 11:04:09 [20082] GAHP[20090] -> 'S'
02/22/16 11:04:12 [20082] GAHP[20090] <- 'RESULTS'
02/22/16 11:04:12 [20082] GAHP[20090] -> 'R'
02/22/16 11:04:12 [20082] GAHP[20090] -> 'S' '1'
02/22/16 11:04:12 [20082] GAHP[20090] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/22/16 11:04:12 [20082] (98.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/22/16 11:04:12 [20082] (98.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/22/16 11:04:12 [20082] (98.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/22/16 11:04:12 [20082] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/22/16 11:04:31 [20082] (98.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/22/16 11:04:31 [20082] in doContactSchedd()
02/22/16 11:04:31 [20082] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 17789_d3dd_4
02/22/16 11:04:31 [20082] querying for removed/held jobs
02/22/16 11:04:31 [20082] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/22/16 11:04:31 [20082] Fetched 0 job ads from schedd
02/22/16 11:04:31 [20082] Updating classad values for 98.0:
02/22/16 11:04:31 [20082]    CurrentStatusUnknown = false
02/22/16 11:04:31 [20082]    GridJobId = undefined
02/22/16 11:04:31 [20082]    LastRemoteStatusUpdate = 0
02/22/16 11:04:31 [20082] leaving doContactSchedd()
02/22/16 11:04:31 [20082] (98.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/22/16 11:04:31 [20082] (98.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/22/16 11:04:31 [20082] (98.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/22/16 11:04:36 [20082] in doContactSchedd()
02/22/16 11:04:36 [20082] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 17789_d3dd_4
02/22/16 11:04:36 [20082] querying for removed/held jobs
02/22/16 11:04:36 [20082] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/22/16 11:04:36 [20082] Fetched 0 job ads from schedd
02/22/16 11:04:36 [20082] Updating classad values for 98.0:
02/22/16 11:04:36 [20082]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#98.0#1456157029"
02/22/16 11:04:36 [20082]    LastRemoteStatusUpdate = 1456157071
02/22/16 11:04:37 [20082] leaving doContactSchedd()
02/22/16 11:04:37 [20082] (98.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/22/16 11:04:37 [20082] (98.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/22/16 11:04:37 [20082] (98.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/22/16 11:04:37 [20082] (98.0) gm state change: GM_HOLD -> GM_DELETE
02/22/16 11:04:42 [20082] in doContactSchedd()
02/22/16 11:04:42 [20082] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 17789_d3dd_4
02/22/16 11:04:42 [20082] querying for removed/held jobs
02/22/16 11:04:42 [20082] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/22/16 11:04:42 [20082] Fetched 0 job ads from schedd
02/22/16 11:04:42 [20082] Updating classad values for 98.0:
02/22/16 11:04:42 [20082]    EnteredCurrentStatus = 1456157077
02/22/16 11:04:42 [20082]    HoldReason = "Attempts to submit failed: "
02/22/16 11:04:42 [20082]    HoldReasonCode = 0
02/22/16 11:04:42 [20082]    HoldReasonSubCode = 0
02/22/16 11:04:42 [20082]    JobStatus = 5
02/22/16 11:04:42 [20082]    LastReleaseReason = "Data files spooled"
02/22/16 11:04:42 [20082]    Managed = "Schedd"
02/22/16 11:04:42 [20082]    NumSystemHolds = 1
02/22/16 11:04:42 [20082]    ReleaseReason = undefined
02/22/16 11:04:42 [20082] No jobs left, shutting down
02/22/16 11:04:42 [20082] leaving doContactSchedd()
02/22/16 11:04:42 [20082] Got SIGTERM. Performing graceful shutdown.
02/22/16 11:04:42 [20082] Started timer to call main_shutdown_fast in 1800 seconds
02/22/16 11:04:42 [20082] **** condor_gridmanager (condor_GRIDMANAGER) pid 20082 EXITING WITH STATUS 0
02/23/16 11:22:08 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/23/16 11:22:08 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/23/16 11:22:08 Enumerating interfaces: lo 127.0.0.1 up
02/23/16 11:22:08 Enumerating interfaces: eth2 10.31.131.202 up
02/23/16 11:22:08 Enumerating interfaces: eth3 140.247.179.131 up
02/23/16 11:22:08 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/23/16 11:22:08 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/23/16 11:22:08 ******************************************************
02/23/16 11:22:08 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/23/16 11:22:08 ** /usr/sbin/condor_gridmanager
02/23/16 11:22:08 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/23/16 11:22:08 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/23/16 11:22:08 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/23/16 11:22:08 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/23/16 11:22:08 ** PID = 11212
02/23/16 11:22:08 ** Log last touched 2/22 11:04:42
02/23/16 11:22:08 ******************************************************
02/23/16 11:22:08 Using config source: /etc/condor-ce/condor_config
02/23/16 11:22:08 Using local config sources: 
02/23/16 11:22:08    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/23/16 11:22:08    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/01-ce-auth.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/01-ce-router.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/01-common-auth.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/02-ce-lsf.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/02-ce-pbs.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/03-managed-fork.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/50-osg-configure.conf
02/23/16 11:22:08    /etc/condor-ce/config.d/99-local.conf
02/23/16 11:22:08    /usr/share/condor-ce/condor_ce_router_defaults|
02/23/16 11:22:08 config Macros = 144, Sorted = 144, StringBytes = 12557, TablesBytes = 5392
02/23/16 11:22:08 CLASSAD_CACHING is ENABLED
02/23/16 11:22:08 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/23/16 11:22:08 SharedPortEndpoint: waiting for connections to named socket 26915_cd60_1
02/23/16 11:22:08 DaemonCore: command socket at <140.247.179.131:9620?sock=26915_cd60_1>
02/23/16 11:22:08 DaemonCore: private command socket at <140.247.179.131:9620?sock=26915_cd60_1>
02/23/16 11:22:08 Setting maximum accepts per cycle 8.
02/23/16 11:22:08 Setting maximum reaps per cycle 8.
02/23/16 11:22:08 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/23/16 11:22:08 [11212] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/23/16 11:22:08 [11212] DaemonCore: No more children processes to reap.
02/23/16 11:22:08 [11212] DaemonCore: in SendAliveToParent()
02/23/16 11:22:08 [11212] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 26032_d598_6
02/23/16 11:22:08 [11212] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/23/16 11:22:08 [11212] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/23/16 11:22:08 [11212] IPVERIFY: ip found is 0
02/23/16 11:22:08 [11212] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/23/16 11:22:08 [11212] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/23/16 11:22:08 [11212] Buf::write(): condor_write() failed
02/23/16 11:22:08 [11212] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/23/16 11:22:08 [11212] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 26032_d598_6
02/23/16 11:22:08 [11212] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/23/16 11:22:08 [11212] DaemonCore: Leaving SendAliveToParent() - success
02/23/16 11:22:08 [11212] Checking proxies
02/23/16 11:22:13 [11212] Received ADD_JOBS signal
02/23/16 11:22:13 [11212] Received REMOVE_JOBS signal
02/23/16 11:22:13 [11212] Evaluating staleness of remote job statuses.
02/23/16 11:22:13 [11212] in doContactSchedd()
02/23/16 11:22:13 [11212] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 26032_d598_6
02/23/16 11:22:13 [11212] querying for new jobs
02/23/16 11:22:13 [11212] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/23/16 11:22:13 [11212] Using job type INFNBatch for job 98.0
02/23/16 11:22:13 [11212] (98.0) SetJobLeaseTimers()
02/23/16 11:22:13 [11212] Failed to get expiration time of proxy /n/atlasgrid/condor/97/0/cluster97.proc0.subproc0/x509up_u556792
02/23/16 11:22:13 [11212] Found job 98.0 --- inserting
02/23/16 11:22:13 [11212] Fetched 1 new job ads from schedd
02/23/16 11:22:13 [11212] querying for removed/held jobs
02/23/16 11:22:13 [11212] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/23/16 11:22:13 [11212] Fetched 1 job ads from schedd
02/23/16 11:22:13 [11212] leaving doContactSchedd()
02/23/16 11:22:13 [11212] gahp server not up yet, delaying ping
02/23/16 11:22:13 [11212] *** UpdateLeases called
02/23/16 11:22:13 [11212]     Leases not supported, cancelling timer
02/23/16 11:22:13 [11212] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=26915_cd60_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=26032_d598_6>"
CurrentTime = time()
MyCurrentTime = 1456244533
IdleJobs = 0
JobLimit = 10000

02/23/16 11:22:13 [11212] Trying to update collector <10.31.131.202:9619>
02/23/16 11:22:13 [11212] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/23/16 11:22:13 [11212] File descriptor limits: max 4096, safe 3277
02/23/16 11:22:13 [11212] (98.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/23/16 11:22:13 [11212] GAHP server pid = 11481
02/23/16 11:22:13 [11212] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/23/16 11:22:13 [11212] GAHP[11481] <- 'COMMANDS'
02/23/16 11:22:13 [11212] GAHP[11481] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/23/16 11:22:13 [11212] GAHP[11481] <- 'ASYNC_MODE_ON'
02/23/16 11:22:13 [11212] GAHP[11481] -> 'S' 'Async mode on'
02/23/16 11:22:13 [11212] (98.0) gm state change: GM_INIT -> GM_START
02/23/16 11:22:13 [11212] (98.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/23/16 11:22:13 [11212] (98.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/23/16 11:22:13 [11212] GAHP[11481] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#98.0#1456157029";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/23/16 11:22:13 [11212] GAHP[11481] -> 'S'
02/23/16 11:22:13 [11212] GAHP[11481] <- 'RESULTS'
02/23/16 11:22:13 [11212] GAHP[11481] -> 'R'
02/23/16 11:22:13 [11212] GAHP[11481] -> 'S' '1'
02/23/16 11:22:13 [11212] GAHP[11481] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
02/23/16 11:22:13 [11212] (98.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/23/16 11:22:13 [11212] (98.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/97/0/cluster97.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
02/23/16 11:22:13 [11212] (98.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/23/16 11:22:13 [11212] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/23/16 11:22:15 [11212] (98.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/23/16 11:22:15 [11212] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/23/16 11:22:15 [11212] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/23/16 11:22:15 [11212] IPVERIFY: ip found is 1
02/23/16 11:22:18 [11212] resource  is now up
02/23/16 11:22:18 [11212] in doContactSchedd()
02/23/16 11:22:18 [11212] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 26032_d598_6
02/23/16 11:22:18 [11212] querying for removed/held jobs
02/23/16 11:22:18 [11212] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/23/16 11:22:18 [11212] Fetched 1 job ads from schedd
02/23/16 11:22:18 [11212] Updating classad values for 98.0:
02/23/16 11:22:18 [11212]    CurrentStatusUnknown = false
02/23/16 11:22:18 [11212]    GridJobId = undefined
02/23/16 11:22:18 [11212]    LastRemoteStatusUpdate = 0
02/23/16 11:22:18 [11212]    Managed = "ScheddDone"
02/23/16 11:22:19 [11212] Deleting job 98.0 from schedd
02/23/16 11:22:19 [11212] No jobs left, shutting down
02/23/16 11:22:19 [11212] leaving doContactSchedd()
02/23/16 11:22:19 [11212] Got SIGTERM. Performing graceful shutdown.
02/23/16 11:22:19 [11212] Started timer to call main_shutdown_fast in 1800 seconds
02/23/16 11:22:19 [11212] **** condor_gridmanager (condor_GRIDMANAGER) pid 11212 EXITING WITH STATUS 0
02/24/16 14:40:41 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/24/16 14:40:41 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/24/16 14:40:41 Enumerating interfaces: lo 127.0.0.1 up
02/24/16 14:40:41 Enumerating interfaces: eth2 10.31.131.202 up
02/24/16 14:40:41 Enumerating interfaces: eth3 140.247.179.131 up
02/24/16 14:40:41 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/24/16 14:40:41 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/24/16 14:40:41 ******************************************************
02/24/16 14:40:41 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/24/16 14:40:41 ** /usr/sbin/condor_gridmanager
02/24/16 14:40:41 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/24/16 14:40:41 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/24/16 14:40:41 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/24/16 14:40:41 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/24/16 14:40:41 ** PID = 22826
02/24/16 14:40:41 ** Log last touched 2/23 11:22:19
02/24/16 14:40:41 ******************************************************
02/24/16 14:40:41 Using config source: /etc/condor-ce/condor_config
02/24/16 14:40:41 Using local config sources: 
02/24/16 14:40:41    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/24/16 14:40:41    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/01-ce-auth.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/01-ce-router.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/01-common-auth.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/02-ce-lsf.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/02-ce-pbs.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/03-managed-fork.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/50-osg-configure.conf
02/24/16 14:40:41    /etc/condor-ce/config.d/99-local.conf
02/24/16 14:40:41    /usr/share/condor-ce/condor_ce_router_defaults|
02/24/16 14:40:41 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
02/24/16 14:40:41 CLASSAD_CACHING is ENABLED
02/24/16 14:40:41 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/24/16 14:40:41 SharedPortEndpoint: waiting for connections to named socket 11700_7555_1
02/24/16 14:40:41 DaemonCore: command socket at <140.247.179.131:9620?sock=11700_7555_1>
02/24/16 14:40:41 DaemonCore: private command socket at <140.247.179.131:9620?sock=11700_7555_1>
02/24/16 14:40:41 Setting maximum accepts per cycle 8.
02/24/16 14:40:41 Setting maximum reaps per cycle 8.
02/24/16 14:40:41 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/24/16 14:40:41 [22826] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/24/16 14:40:41 [22826] DaemonCore: No more children processes to reap.
02/24/16 14:40:41 [22826] DaemonCore: in SendAliveToParent()
02/24/16 14:40:41 [22826] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11664_e50d_4
02/24/16 14:40:41 [22826] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/24/16 14:40:41 [22826] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/24/16 14:40:41 [22826] IPVERIFY: ip found is 0
02/24/16 14:40:41 [22826] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/24/16 14:40:41 [22826] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/24/16 14:40:41 [22826] Buf::write(): condor_write() failed
02/24/16 14:40:41 [22826] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/24/16 14:40:41 [22826] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11664_e50d_4
02/24/16 14:40:41 [22826] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/24/16 14:40:41 [22826] DaemonCore: Leaving SendAliveToParent() - success
02/24/16 14:40:41 [22826] Checking proxies
02/24/16 14:40:44 [22826] Received ADD_JOBS signal
02/24/16 14:40:44 [22826] in doContactSchedd()
02/24/16 14:40:44 [22826] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11664_e50d_4
02/24/16 14:40:44 [22826] querying for new jobs
02/24/16 14:40:44 [22826] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/24/16 14:40:44 [22826] Using job type INFNBatch for job 100.0
02/24/16 14:40:44 [22826] (100.0) SetJobLeaseTimers()
02/24/16 14:40:44 [22826] Found job 100.0 --- inserting
02/24/16 14:40:44 [22826] Fetched 1 new job ads from schedd
02/24/16 14:40:44 [22826] querying for removed/held jobs
02/24/16 14:40:44 [22826] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/24/16 14:40:44 [22826] Fetched 0 job ads from schedd
02/24/16 14:40:44 [22826] leaving doContactSchedd()
02/24/16 14:40:44 [22826] gahp server not up yet, delaying ping
02/24/16 14:40:44 [22826] *** UpdateLeases called
02/24/16 14:40:44 [22826]     Leases not supported, cancelling timer
02/24/16 14:40:44 [22826] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=11700_7555_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=11664_e50d_4>"
CurrentTime = time()
MyCurrentTime = 1456342844
IdleJobs = 1
JobLimit = 10000

02/24/16 14:40:44 [22826] Trying to update collector <10.31.131.202:9619>
02/24/16 14:40:44 [22826] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/24/16 14:40:44 [22826] File descriptor limits: max 4096, safe 3277
02/24/16 14:40:44 [22826] (100.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/24/16 14:40:44 [22826] GAHP server pid = 22913
02/24/16 14:40:44 [22826] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/24/16 14:40:44 [22826] GAHP[22913] <- 'COMMANDS'
02/24/16 14:40:44 [22826] GAHP[22913] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/24/16 14:40:44 [22826] GAHP[22913] <- 'ASYNC_MODE_ON'
02/24/16 14:40:44 [22826] GAHP[22913] -> 'S' 'Async mode on'
02/24/16 14:40:44 [22826] (100.0) gm state change: GM_INIT -> GM_START
02/24/16 14:40:44 [22826] (100.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/24/16 14:40:44 [22826] (100.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/24/16 14:40:44 [22826] (100.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/24/16 14:40:44 [22826] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/24/16 14:40:44 [22826] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/24/16 14:40:44 [22826] IPVERIFY: ip found is 1
02/24/16 14:40:46 [22826] Evaluating staleness of remote job statuses.
02/24/16 14:40:49 [22826] resource  is now up
02/24/16 14:40:50 [22826] in doContactSchedd()
02/24/16 14:40:50 [22826] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11664_e50d_4
02/24/16 14:40:50 [22826] querying for removed/held jobs
02/24/16 14:40:50 [22826] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/24/16 14:40:50 [22826] Fetched 0 job ads from schedd
02/24/16 14:40:50 [22826] Updating classad values for 100.0:
02/24/16 14:40:50 [22826]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#100.0#1456342544"
02/24/16 14:40:50 [22826]    LastRemoteStatusUpdate = 1456342844
02/24/16 14:40:50 [22826] leaving doContactSchedd()
02/24/16 14:40:50 [22826] (100.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/24/16 14:40:50 [22826] (100.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/24/16 14:40:50 [22826] (100.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/24/16 14:40:50 [22826] GAHP[22913] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#100.0#1456342544";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/24/16 14:40:50 [22826] GAHP[22913] -> 'S'
02/24/16 14:40:52 [22826] GAHP[22913] <- 'RESULTS'
02/24/16 14:40:52 [22826] GAHP[22913] -> 'R'
02/24/16 14:40:52 [22826] GAHP[22913] -> 'S' '1'
02/24/16 14:40:52 [22826] GAHP[22913] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/24/16 14:40:52 [22826] (100.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/24/16 14:40:52 [22826] (100.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/24/16 14:40:52 [22826] (100.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/24/16 14:40:52 [22826] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/24/16 14:41:04 [22826] (100.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/24/16 14:41:04 [22826] in doContactSchedd()
02/24/16 14:41:04 [22826] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11664_e50d_4
02/24/16 14:41:04 [22826] querying for removed/held jobs
02/24/16 14:41:04 [22826] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/24/16 14:41:04 [22826] Fetched 0 job ads from schedd
02/24/16 14:41:04 [22826] Updating classad values for 100.0:
02/24/16 14:41:04 [22826]    CurrentStatusUnknown = false
02/24/16 14:41:04 [22826]    GridJobId = undefined
02/24/16 14:41:04 [22826]    LastRemoteStatusUpdate = 0
02/24/16 14:41:04 [22826] leaving doContactSchedd()
02/24/16 14:41:04 [22826] (100.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/24/16 14:41:04 [22826] (100.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/24/16 14:41:04 [22826] (100.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/24/16 14:41:09 [22826] in doContactSchedd()
02/24/16 14:41:09 [22826] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11664_e50d_4
02/24/16 14:41:09 [22826] querying for removed/held jobs
02/24/16 14:41:09 [22826] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/24/16 14:41:09 [22826] Fetched 0 job ads from schedd
02/24/16 14:41:09 [22826] Updating classad values for 100.0:
02/24/16 14:41:09 [22826]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#100.0#1456342544"
02/24/16 14:41:09 [22826]    LastRemoteStatusUpdate = 1456342864
02/24/16 14:41:09 [22826] leaving doContactSchedd()
02/24/16 14:41:09 [22826] (100.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/24/16 14:41:09 [22826] (100.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/24/16 14:41:09 [22826] (100.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/24/16 14:41:09 [22826] (100.0) gm state change: GM_HOLD -> GM_DELETE
02/24/16 14:41:14 [22826] in doContactSchedd()
02/24/16 14:41:14 [22826] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11664_e50d_4
02/24/16 14:41:14 [22826] querying for removed/held jobs
02/24/16 14:41:14 [22826] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/24/16 14:41:14 [22826] Fetched 0 job ads from schedd
02/24/16 14:41:14 [22826] Updating classad values for 100.0:
02/24/16 14:41:14 [22826]    EnteredCurrentStatus = 1456342869
02/24/16 14:41:14 [22826]    HoldReason = "Attempts to submit failed: "
02/24/16 14:41:14 [22826]    HoldReasonCode = 0
02/24/16 14:41:14 [22826]    HoldReasonSubCode = 0
02/24/16 14:41:14 [22826]    JobStatus = 5
02/24/16 14:41:14 [22826]    LastReleaseReason = "Data files spooled"
02/24/16 14:41:14 [22826]    Managed = "Schedd"
02/24/16 14:41:14 [22826]    NumSystemHolds = 1
02/24/16 14:41:14 [22826]    ReleaseReason = undefined
02/24/16 14:41:14 [22826] No jobs left, shutting down
02/24/16 14:41:14 [22826] leaving doContactSchedd()
02/24/16 14:41:14 [22826] Got SIGTERM. Performing graceful shutdown.
02/24/16 14:41:14 [22826] Started timer to call main_shutdown_fast in 1800 seconds
02/24/16 14:41:14 [22826] **** condor_gridmanager (condor_GRIDMANAGER) pid 22826 EXITING WITH STATUS 0
02/26/16 12:36:05 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/26/16 12:36:05 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/26/16 12:36:05 Enumerating interfaces: lo 127.0.0.1 up
02/26/16 12:36:05 Enumerating interfaces: eth2 10.31.131.202 up
02/26/16 12:36:05 Enumerating interfaces: eth3 140.247.179.131 up
02/26/16 12:36:05 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/26/16 12:36:05 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/26/16 12:36:05 ******************************************************
02/26/16 12:36:05 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/26/16 12:36:05 ** /usr/sbin/condor_gridmanager
02/26/16 12:36:05 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/26/16 12:36:05 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/26/16 12:36:05 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/26/16 12:36:05 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/26/16 12:36:05 ** PID = 630
02/26/16 12:36:05 ** Log last touched 2/24 14:41:14
02/26/16 12:36:05 ******************************************************
02/26/16 12:36:05 Using config source: /etc/condor-ce/condor_config
02/26/16 12:36:05 Using local config sources: 
02/26/16 12:36:05    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/26/16 12:36:05    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/01-ce-auth.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/01-ce-router.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/01-common-auth.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/02-ce-lsf.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/02-ce-pbs.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/03-managed-fork.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/50-osg-configure.conf
02/26/16 12:36:05    /etc/condor-ce/config.d/99-local.conf
02/26/16 12:36:05    /usr/share/condor-ce/condor_ce_router_defaults|
02/26/16 12:36:05 config Macros = 144, Sorted = 144, StringBytes = 12554, TablesBytes = 5392
02/26/16 12:36:05 CLASSAD_CACHING is ENABLED
02/26/16 12:36:05 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/26/16 12:36:05 SharedPortEndpoint: waiting for connections to named socket 12318_e52e_1
02/26/16 12:36:05 DaemonCore: command socket at <140.247.179.131:9620?sock=12318_e52e_1>
02/26/16 12:36:05 DaemonCore: private command socket at <140.247.179.131:9620?sock=12318_e52e_1>
02/26/16 12:36:05 Setting maximum accepts per cycle 8.
02/26/16 12:36:05 Setting maximum reaps per cycle 8.
02/26/16 12:36:05 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/26/16 12:36:05 [630] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/26/16 12:36:05 [630] DaemonCore: No more children processes to reap.
02/26/16 12:36:05 [630] DaemonCore: in SendAliveToParent()
02/26/16 12:36:05 [630] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 12:36:15 [630] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/26/16 12:36:15 [630] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/26/16 12:36:15 [630] IPVERIFY: ip found is 0
02/26/16 12:36:15 [630] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/26/16 12:36:15 [630] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/26/16 12:36:15 [630] Buf::write(): condor_write() failed
02/26/16 12:36:15 [630] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/26/16 12:36:15 [630] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 12:36:15 [630] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/26/16 12:36:15 [630] DaemonCore: Leaving SendAliveToParent() - success
02/26/16 12:36:15 [630] Checking proxies
02/26/16 12:36:15 [630] Received ADD_JOBS signal
02/26/16 12:36:15 [630] Received REMOVE_JOBS signal
02/26/16 12:36:15 [630] Evaluating staleness of remote job statuses.
02/26/16 12:36:15 [630] in doContactSchedd()
02/26/16 12:36:15 [630] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 12:36:15 [630] querying for new jobs
02/26/16 12:36:15 [630] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/26/16 12:36:15 [630] Fetched 0 new job ads from schedd
02/26/16 12:36:15 [630] querying for removed/held jobs
02/26/16 12:36:15 [630] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 12:36:15 [630] Fetched 0 job ads from schedd
02/26/16 12:36:15 [630] No jobs left, shutting down
02/26/16 12:36:15 [630] leaving doContactSchedd()
02/26/16 12:36:15 [630] Got SIGTERM. Performing graceful shutdown.
02/26/16 12:36:15 [630] Started timer to call main_shutdown_fast in 1800 seconds
02/26/16 12:36:15 [630] **** condor_gridmanager (condor_GRIDMANAGER) pid 630 EXITING WITH STATUS 0
02/26/16 12:46:56 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/26/16 12:46:56 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/26/16 12:46:56 Enumerating interfaces: lo 127.0.0.1 up
02/26/16 12:46:56 Enumerating interfaces: eth2 10.31.131.202 up
02/26/16 12:46:56 Enumerating interfaces: eth3 140.247.179.131 up
02/26/16 12:46:56 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/26/16 12:46:56 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/26/16 12:46:56 ******************************************************
02/26/16 12:46:56 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/26/16 12:46:56 ** /usr/sbin/condor_gridmanager
02/26/16 12:46:56 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/26/16 12:46:56 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/26/16 12:46:56 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/26/16 12:46:56 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/26/16 12:46:56 ** PID = 11282
02/26/16 12:46:56 ** Log last touched 2/26 12:36:15
02/26/16 12:46:56 ******************************************************
02/26/16 12:46:56 Using config source: /etc/condor-ce/condor_config
02/26/16 12:46:56 Using local config sources: 
02/26/16 12:46:56    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/26/16 12:46:56    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/01-ce-auth.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/01-ce-router.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/01-common-auth.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/02-ce-lsf.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/02-ce-pbs.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/03-managed-fork.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/50-osg-configure.conf
02/26/16 12:46:56    /etc/condor-ce/config.d/99-local.conf
02/26/16 12:46:56    /usr/share/condor-ce/condor_ce_router_defaults|
02/26/16 12:46:56 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
02/26/16 12:46:56 CLASSAD_CACHING is ENABLED
02/26/16 12:46:56 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/26/16 12:46:56 SharedPortEndpoint: waiting for connections to named socket 12318_e52e_2
02/26/16 12:46:56 DaemonCore: command socket at <140.247.179.131:9620?sock=12318_e52e_2>
02/26/16 12:46:56 DaemonCore: private command socket at <140.247.179.131:9620?sock=12318_e52e_2>
02/26/16 12:46:56 Setting maximum accepts per cycle 8.
02/26/16 12:46:56 Setting maximum reaps per cycle 8.
02/26/16 12:46:56 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/26/16 12:46:56 [11282] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/26/16 12:46:56 [11282] DaemonCore: No more children processes to reap.
02/26/16 12:46:56 [11282] DaemonCore: in SendAliveToParent()
02/26/16 12:46:56 [11282] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 12:46:56 [11282] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/26/16 12:46:56 [11282] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/26/16 12:46:56 [11282] IPVERIFY: ip found is 0
02/26/16 12:46:56 [11282] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/26/16 12:46:56 [11282] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/26/16 12:46:56 [11282] Buf::write(): condor_write() failed
02/26/16 12:46:56 [11282] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/26/16 12:46:56 [11282] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 12:46:56 [11282] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/26/16 12:46:56 [11282] DaemonCore: Leaving SendAliveToParent() - success
02/26/16 12:46:56 [11282] Checking proxies
02/26/16 12:46:57 [11282] Received REMOVE_JOBS signal
02/26/16 12:46:57 [11282] in doContactSchedd()
02/26/16 12:46:57 [11282] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 12:46:57 [11282] querying for new jobs
02/26/16 12:46:57 [11282] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/26/16 12:46:57 [11282] Using job type INFNBatch for job 100.0
02/26/16 12:46:57 [11282] (100.0) SetJobLeaseTimers()
02/26/16 12:46:57 [11282] Failed to get expiration time of proxy /n/atlasgrid/condor/99/0/cluster99.proc0.subproc0/x509up_u556792
02/26/16 12:46:57 [11282] Found job 100.0 --- inserting
02/26/16 12:46:57 [11282] Fetched 1 new job ads from schedd
02/26/16 12:46:57 [11282] querying for removed/held jobs
02/26/16 12:46:57 [11282] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 12:46:57 [11282] Fetched 1 job ads from schedd
02/26/16 12:46:57 [11282] leaving doContactSchedd()
02/26/16 12:46:57 [11282] gahp server not up yet, delaying ping
02/26/16 12:46:57 [11282] *** UpdateLeases called
02/26/16 12:46:57 [11282]     Leases not supported, cancelling timer
02/26/16 12:46:57 [11282] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=12318_e52e_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=12160_1e15_4>"
CurrentTime = time()
MyCurrentTime = 1456508817
IdleJobs = 0
JobLimit = 10000

02/26/16 12:46:57 [11282] Trying to update collector <10.31.131.202:9619>
02/26/16 12:46:57 [11282] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/26/16 12:46:57 [11282] File descriptor limits: max 4096, safe 3277
02/26/16 12:46:57 [11282] (100.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/26/16 12:46:57 [11282] GAHP server pid = 11284
02/26/16 12:46:57 [11282] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/26/16 12:46:57 [11282] GAHP[11284] <- 'COMMANDS'
02/26/16 12:46:57 [11282] GAHP[11284] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/26/16 12:46:57 [11282] GAHP[11284] <- 'ASYNC_MODE_ON'
02/26/16 12:46:57 [11282] GAHP[11284] -> 'S' 'Async mode on'
02/26/16 12:46:57 [11282] (100.0) gm state change: GM_INIT -> GM_START
02/26/16 12:46:57 [11282] (100.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/26/16 12:46:57 [11282] (100.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/26/16 12:46:57 [11282] GAHP[11284] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#100.0#1456342544";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/26/16 12:46:57 [11282] GAHP[11284] -> 'S'
02/26/16 12:46:57 [11282] GAHP[11284] <- 'RESULTS'
02/26/16 12:46:57 [11282] GAHP[11284] -> 'R'
02/26/16 12:46:57 [11282] GAHP[11284] -> 'S' '1'
02/26/16 12:46:57 [11282] GAHP[11284] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
02/26/16 12:46:57 [11282] (100.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/26/16 12:46:57 [11282] (100.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/99/0/cluster99.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
02/26/16 12:46:57 [11282] (100.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/26/16 12:46:57 [11282] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/26/16 12:47:08 [11282] (100.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/26/16 12:47:08 [11282] Received ADD_JOBS signal
02/26/16 12:47:08 [11282] Evaluating staleness of remote job statuses.
02/26/16 12:47:08 [11282] resource  is now up
02/26/16 12:47:08 [11282] in doContactSchedd()
02/26/16 12:47:08 [11282] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 12:47:08 [11282] querying for new jobs
02/26/16 12:47:08 [11282] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/26/16 12:47:08 [11282] Fetched 0 new job ads from schedd
02/26/16 12:47:08 [11282] querying for removed/held jobs
02/26/16 12:47:08 [11282] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 12:47:08 [11282] Fetched 1 job ads from schedd
02/26/16 12:47:08 [11282] Updating classad values for 100.0:
02/26/16 12:47:08 [11282]    CurrentStatusUnknown = false
02/26/16 12:47:08 [11282]    GridJobId = undefined
02/26/16 12:47:08 [11282]    LastRemoteStatusUpdate = 0
02/26/16 12:47:08 [11282]    Managed = "ScheddDone"
02/26/16 12:47:08 [11282] Deleting job 100.0 from schedd
02/26/16 12:47:08 [11282] No jobs left, shutting down
02/26/16 12:47:08 [11282] leaving doContactSchedd()
02/26/16 12:47:08 [11282] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/26/16 12:47:08 [11282] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/26/16 12:47:08 [11282] IPVERIFY: ip found is 1
02/26/16 12:47:08 [11282] Got SIGTERM. Performing graceful shutdown.
02/26/16 12:47:08 [11282] Started timer to call main_shutdown_fast in 1800 seconds
02/26/16 12:47:08 [11282] **** condor_gridmanager (condor_GRIDMANAGER) pid 11282 EXITING WITH STATUS 0
02/26/16 13:17:53 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/26/16 13:17:53 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/26/16 13:17:53 Enumerating interfaces: lo 127.0.0.1 up
02/26/16 13:17:53 Enumerating interfaces: eth2 10.31.131.202 up
02/26/16 13:17:53 Enumerating interfaces: eth3 140.247.179.131 up
02/26/16 13:17:53 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/26/16 13:17:53 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/26/16 13:17:53 ******************************************************
02/26/16 13:17:53 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/26/16 13:17:53 ** /usr/sbin/condor_gridmanager
02/26/16 13:17:53 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/26/16 13:17:53 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/26/16 13:17:53 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/26/16 13:17:53 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/26/16 13:17:53 ** PID = 7175
02/26/16 13:17:53 ** Log last touched 2/26 12:47:08
02/26/16 13:17:53 ******************************************************
02/26/16 13:17:53 Using config source: /etc/condor-ce/condor_config
02/26/16 13:17:53 Using local config sources: 
02/26/16 13:17:53    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/26/16 13:17:53    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/01-ce-auth.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/01-ce-router.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/01-common-auth.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/02-ce-lsf.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/02-ce-pbs.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/03-managed-fork.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/50-osg-configure.conf
02/26/16 13:17:53    /etc/condor-ce/config.d/99-local.conf
02/26/16 13:17:53    /usr/share/condor-ce/condor_ce_router_defaults|
02/26/16 13:17:53 config Macros = 144, Sorted = 144, StringBytes = 12556, TablesBytes = 5392
02/26/16 13:17:53 CLASSAD_CACHING is ENABLED
02/26/16 13:17:53 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/26/16 13:17:53 SharedPortEndpoint: waiting for connections to named socket 12318_e52e_3
02/26/16 13:17:53 DaemonCore: command socket at <140.247.179.131:9620?sock=12318_e52e_3>
02/26/16 13:17:53 DaemonCore: private command socket at <140.247.179.131:9620?sock=12318_e52e_3>
02/26/16 13:17:53 Setting maximum accepts per cycle 8.
02/26/16 13:17:53 Setting maximum reaps per cycle 8.
02/26/16 13:17:53 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/26/16 13:17:53 [7175] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/26/16 13:17:53 [7175] DaemonCore: No more children processes to reap.
02/26/16 13:17:53 [7175] DaemonCore: in SendAliveToParent()
02/26/16 13:17:53 [7175] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 13:17:54 [7175] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/26/16 13:17:54 [7175] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/26/16 13:17:54 [7175] IPVERIFY: ip found is 0
02/26/16 13:17:54 [7175] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/26/16 13:17:54 [7175] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/26/16 13:17:54 [7175] Buf::write(): condor_write() failed
02/26/16 13:17:54 [7175] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/26/16 13:17:54 [7175] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 13:17:54 [7175] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/26/16 13:17:54 [7175] DaemonCore: Leaving SendAliveToParent() - success
02/26/16 13:17:54 [7175] Checking proxies
02/26/16 13:17:56 [7175] Received ADD_JOBS signal
02/26/16 13:17:56 [7175] in doContactSchedd()
02/26/16 13:17:56 [7175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 13:17:56 [7175] querying for new jobs
02/26/16 13:17:56 [7175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/26/16 13:17:56 [7175] Using job type INFNBatch for job 102.0
02/26/16 13:17:56 [7175] (102.0) SetJobLeaseTimers()
02/26/16 13:17:56 [7175] Found job 102.0 --- inserting
02/26/16 13:17:56 [7175] Fetched 1 new job ads from schedd
02/26/16 13:17:56 [7175] querying for removed/held jobs
02/26/16 13:17:56 [7175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:17:56 [7175] Fetched 0 job ads from schedd
02/26/16 13:17:56 [7175] leaving doContactSchedd()
02/26/16 13:17:56 [7175] gahp server not up yet, delaying ping
02/26/16 13:17:56 [7175] *** UpdateLeases called
02/26/16 13:17:56 [7175]     Leases not supported, cancelling timer
02/26/16 13:17:56 [7175] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=12318_e52e_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=12160_1e15_4>"
CurrentTime = time()
MyCurrentTime = 1456510676
IdleJobs = 1
JobLimit = 10000

02/26/16 13:17:56 [7175] Trying to update collector <10.31.131.202:9619>
02/26/16 13:17:56 [7175] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/26/16 13:17:56 [7175] File descriptor limits: max 4096, safe 3277
02/26/16 13:17:56 [7175] (102.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/26/16 13:17:56 [7175] GAHP server pid = 7300
02/26/16 13:17:56 [7175] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/26/16 13:17:56 [7175] GAHP[7300] <- 'COMMANDS'
02/26/16 13:17:56 [7175] GAHP[7300] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/26/16 13:17:56 [7175] GAHP[7300] <- 'ASYNC_MODE_ON'
02/26/16 13:17:56 [7175] GAHP[7300] -> 'S' 'Async mode on'
02/26/16 13:17:56 [7175] (102.0) gm state change: GM_INIT -> GM_START
02/26/16 13:17:56 [7175] (102.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/26/16 13:17:56 [7175] (102.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/26/16 13:17:56 [7175] (102.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/26/16 13:17:56 [7175] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/26/16 13:17:56 [7175] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/26/16 13:17:56 [7175] IPVERIFY: ip found is 1
02/26/16 13:17:58 [7175] Evaluating staleness of remote job statuses.
02/26/16 13:18:01 [7175] resource  is now up
02/26/16 13:18:01 [7175] in doContactSchedd()
02/26/16 13:18:01 [7175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 13:18:01 [7175] querying for removed/held jobs
02/26/16 13:18:01 [7175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:18:01 [7175] Fetched 0 job ads from schedd
02/26/16 13:18:01 [7175] Updating classad values for 102.0:
02/26/16 13:18:01 [7175]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#102.0#1456510668"
02/26/16 13:18:01 [7175]    LastRemoteStatusUpdate = 1456510676
02/26/16 13:18:01 [7175] leaving doContactSchedd()
02/26/16 13:18:01 [7175] (102.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/26/16 13:18:01 [7175] (102.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/26/16 13:18:01 [7175] (102.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/26/16 13:18:01 [7175] GAHP[7300] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/101/0/cluster101.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/101/0/cluster101.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/101/0/cluster101.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#102.0#1456510668";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/26/16 13:18:01 [7175] GAHP[7300] -> 'S'
02/26/16 13:18:03 [7175] GAHP[7300] <- 'RESULTS'
02/26/16 13:18:03 [7175] GAHP[7300] -> 'R'
02/26/16 13:18:03 [7175] GAHP[7300] -> 'S' '1'
02/26/16 13:18:03 [7175] GAHP[7300] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/26/16 13:18:03 [7175] (102.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/26/16 13:18:03 [7175] (102.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/26/16 13:18:03 [7175] (102.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/26/16 13:18:03 [7175] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/26/16 13:18:03 [7175] (102.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/26/16 13:18:06 [7175] in doContactSchedd()
02/26/16 13:18:06 [7175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 13:18:06 [7175] querying for removed/held jobs
02/26/16 13:18:06 [7175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:18:06 [7175] Fetched 0 job ads from schedd
02/26/16 13:18:06 [7175] Updating classad values for 102.0:
02/26/16 13:18:06 [7175]    CurrentStatusUnknown = false
02/26/16 13:18:06 [7175]    GridJobId = undefined
02/26/16 13:18:06 [7175]    LastRemoteStatusUpdate = 0
02/26/16 13:18:06 [7175] leaving doContactSchedd()
02/26/16 13:18:06 [7175] (102.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/26/16 13:18:06 [7175] (102.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/26/16 13:18:06 [7175] (102.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/26/16 13:18:11 [7175] in doContactSchedd()
02/26/16 13:18:11 [7175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 13:18:11 [7175] querying for removed/held jobs
02/26/16 13:18:11 [7175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:18:11 [7175] Fetched 0 job ads from schedd
02/26/16 13:18:11 [7175] Updating classad values for 102.0:
02/26/16 13:18:11 [7175]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#102.0#1456510668"
02/26/16 13:18:11 [7175]    LastRemoteStatusUpdate = 1456510686
02/26/16 13:18:11 [7175] leaving doContactSchedd()
02/26/16 13:18:11 [7175] (102.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/26/16 13:18:11 [7175] (102.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/26/16 13:18:11 [7175] (102.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/26/16 13:18:11 [7175] (102.0) gm state change: GM_HOLD -> GM_DELETE
02/26/16 13:18:16 [7175] in doContactSchedd()
02/26/16 13:18:16 [7175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 12160_1e15_4
02/26/16 13:18:16 [7175] querying for removed/held jobs
02/26/16 13:18:16 [7175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:18:16 [7175] Fetched 0 job ads from schedd
02/26/16 13:18:16 [7175] Updating classad values for 102.0:
02/26/16 13:18:16 [7175]    EnteredCurrentStatus = 1456510691
02/26/16 13:18:16 [7175]    HoldReason = "Attempts to submit failed: "
02/26/16 13:18:16 [7175]    HoldReasonCode = 0
02/26/16 13:18:16 [7175]    HoldReasonSubCode = 0
02/26/16 13:18:16 [7175]    JobStatus = 5
02/26/16 13:18:16 [7175]    LastReleaseReason = "Data files spooled"
02/26/16 13:18:16 [7175]    Managed = "Schedd"
02/26/16 13:18:16 [7175]    NumSystemHolds = 1
02/26/16 13:18:16 [7175]    ReleaseReason = undefined
02/26/16 13:18:16 [7175] No jobs left, shutting down
02/26/16 13:18:16 [7175] leaving doContactSchedd()
02/26/16 13:18:16 [7175] Got SIGTERM. Performing graceful shutdown.
02/26/16 13:18:16 [7175] Started timer to call main_shutdown_fast in 1800 seconds
02/26/16 13:18:16 [7175] **** condor_gridmanager (condor_GRIDMANAGER) pid 7175 EXITING WITH STATUS 0
02/26/16 13:42:01 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/26/16 13:42:01 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/26/16 13:42:01 Enumerating interfaces: lo 127.0.0.1 up
02/26/16 13:42:01 Enumerating interfaces: eth2 10.31.131.202 up
02/26/16 13:42:01 Enumerating interfaces: eth3 140.247.179.131 up
02/26/16 13:42:01 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/26/16 13:42:01 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/26/16 13:42:01 ******************************************************
02/26/16 13:42:01 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/26/16 13:42:01 ** /usr/sbin/condor_gridmanager
02/26/16 13:42:01 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/26/16 13:42:01 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/26/16 13:42:01 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/26/16 13:42:01 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/26/16 13:42:01 ** PID = 9458
02/26/16 13:42:01 ** Log last touched 2/26 13:18:16
02/26/16 13:42:01 ******************************************************
02/26/16 13:42:01 Using config source: /etc/condor-ce/condor_config
02/26/16 13:42:01 Using local config sources: 
02/26/16 13:42:01    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/26/16 13:42:01    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/01-ce-auth.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/01-ce-router.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/01-common-auth.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/02-ce-lsf.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/02-ce-pbs.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/03-managed-fork.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/50-osg-configure.conf
02/26/16 13:42:01    /etc/condor-ce/config.d/99-local.conf
02/26/16 13:42:01    /usr/share/condor-ce/condor_ce_router_defaults|
02/26/16 13:42:01 config Macros = 144, Sorted = 144, StringBytes = 12551, TablesBytes = 5392
02/26/16 13:42:01 CLASSAD_CACHING is ENABLED
02/26/16 13:42:01 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/26/16 13:42:01 SharedPortEndpoint: waiting for connections to named socket 6929_db73_1
02/26/16 13:42:01 DaemonCore: command socket at <140.247.179.131:9620?sock=6929_db73_1>
02/26/16 13:42:01 DaemonCore: private command socket at <140.247.179.131:9620?sock=6929_db73_1>
02/26/16 13:42:01 Setting maximum accepts per cycle 8.
02/26/16 13:42:01 Setting maximum reaps per cycle 8.
02/26/16 13:42:01 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/26/16 13:42:01 [9458] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/26/16 13:42:01 [9458] DaemonCore: No more children processes to reap.
02/26/16 13:42:01 [9458] DaemonCore: in SendAliveToParent()
02/26/16 13:42:01 [9458] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/26/16 13:42:01 [9458] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/26/16 13:42:01 [9458] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/26/16 13:42:01 [9458] IPVERIFY: ip found is 0
02/26/16 13:42:01 [9458] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/26/16 13:42:01 [9458] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/26/16 13:42:01 [9458] Buf::write(): condor_write() failed
02/26/16 13:42:01 [9458] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/26/16 13:42:01 [9458] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/26/16 13:42:01 [9458] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/26/16 13:42:01 [9458] DaemonCore: Leaving SendAliveToParent() - success
02/26/16 13:42:01 [9458] Checking proxies
02/26/16 13:42:04 [9458] Received ADD_JOBS signal
02/26/16 13:42:04 [9458] in doContactSchedd()
02/26/16 13:42:04 [9458] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/26/16 13:42:04 [9458] querying for new jobs
02/26/16 13:42:04 [9458] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/26/16 13:42:04 [9458] Using job type INFNBatch for job 104.0
02/26/16 13:42:04 [9458] (104.0) SetJobLeaseTimers()
02/26/16 13:42:04 [9458] Found job 104.0 --- inserting
02/26/16 13:42:04 [9458] Fetched 1 new job ads from schedd
02/26/16 13:42:04 [9458] querying for removed/held jobs
02/26/16 13:42:04 [9458] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:42:04 [9458] Fetched 0 job ads from schedd
02/26/16 13:42:04 [9458] leaving doContactSchedd()
02/26/16 13:42:04 [9458] gahp server not up yet, delaying ping
02/26/16 13:42:04 [9458] *** UpdateLeases called
02/26/16 13:42:04 [9458]     Leases not supported, cancelling timer
02/26/16 13:42:04 [9458] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=6929_db73_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6838_1c38_4>"
CurrentTime = time()
MyCurrentTime = 1456512124
IdleJobs = 1
JobLimit = 10000

02/26/16 13:42:04 [9458] Trying to update collector <10.31.131.202:9619>
02/26/16 13:42:04 [9458] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/26/16 13:42:04 [9458] File descriptor limits: max 4096, safe 3277
02/26/16 13:42:04 [9458] (104.0) doEvaluateState called: gmState GM_INIT, remoteState 0
02/26/16 13:42:04 [9458] GAHP server pid = 9805
02/26/16 13:42:04 [9458] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/26/16 13:42:04 [9458] GAHP[9805] <- 'COMMANDS'
02/26/16 13:42:04 [9458] GAHP[9805] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/26/16 13:42:04 [9458] GAHP[9805] <- 'ASYNC_MODE_ON'
02/26/16 13:42:04 [9458] GAHP[9805] -> 'S' 'Async mode on'
02/26/16 13:42:04 [9458] (104.0) gm state change: GM_INIT -> GM_START
02/26/16 13:42:04 [9458] (104.0) gm state change: GM_START -> GM_CLEAR_REQUEST
02/26/16 13:42:04 [9458] (104.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/26/16 13:42:04 [9458] (104.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/26/16 13:42:04 [9458] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/26/16 13:42:04 [9458] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/26/16 13:42:04 [9458] IPVERIFY: ip found is 1
02/26/16 13:42:06 [9458] Evaluating staleness of remote job statuses.
02/26/16 13:42:09 [9458] resource  is now up
02/26/16 13:42:09 [9458] in doContactSchedd()
02/26/16 13:42:09 [9458] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/26/16 13:42:09 [9458] querying for removed/held jobs
02/26/16 13:42:09 [9458] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:42:09 [9458] Fetched 0 job ads from schedd
02/26/16 13:42:09 [9458] Updating classad values for 104.0:
02/26/16 13:42:09 [9458]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#104.0#1456512093"
02/26/16 13:42:09 [9458]    LastRemoteStatusUpdate = 1456512124
02/26/16 13:42:10 [9458] leaving doContactSchedd()
02/26/16 13:42:10 [9458] (104.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/26/16 13:42:10 [9458] (104.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/26/16 13:42:10 [9458] (104.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/26/16 13:42:10 [9458] GAHP[9805] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/103/0/cluster103.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/103/0/cluster103.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/103/0/cluster103.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#104.0#1456512093";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/26/16 13:42:10 [9458] GAHP[9805] -> 'S'
02/26/16 13:42:12 [9458] GAHP[9805] <- 'RESULTS'
02/26/16 13:42:12 [9458] GAHP[9805] -> 'R'
02/26/16 13:42:12 [9458] GAHP[9805] -> 'S' '1'
02/26/16 13:42:12 [9458] GAHP[9805] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/26/16 13:42:12 [9458] (104.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
02/26/16 13:42:12 [9458] (104.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/26/16 13:42:12 [9458] (104.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/26/16 13:42:12 [9458] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/26/16 13:42:43 [9458] (104.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
02/26/16 13:42:43 [9458] in doContactSchedd()
02/26/16 13:42:43 [9458] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/26/16 13:42:43 [9458] querying for removed/held jobs
02/26/16 13:42:43 [9458] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:42:43 [9458] Fetched 0 job ads from schedd
02/26/16 13:42:43 [9458] Updating classad values for 104.0:
02/26/16 13:42:43 [9458]    CurrentStatusUnknown = false
02/26/16 13:42:43 [9458]    GridJobId = undefined
02/26/16 13:42:43 [9458]    LastRemoteStatusUpdate = 0
02/26/16 13:42:43 [9458] leaving doContactSchedd()
02/26/16 13:42:43 [9458] (104.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
02/26/16 13:42:43 [9458] (104.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
02/26/16 13:42:43 [9458] (104.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
02/26/16 13:42:48 [9458] in doContactSchedd()
02/26/16 13:42:48 [9458] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/26/16 13:42:48 [9458] querying for removed/held jobs
02/26/16 13:42:48 [9458] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:42:48 [9458] Fetched 0 job ads from schedd
02/26/16 13:42:48 [9458] Updating classad values for 104.0:
02/26/16 13:42:48 [9458]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#104.0#1456512093"
02/26/16 13:42:48 [9458]    LastRemoteStatusUpdate = 1456512163
02/26/16 13:42:48 [9458] leaving doContactSchedd()
02/26/16 13:42:48 [9458] (104.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
02/26/16 13:42:48 [9458] (104.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
02/26/16 13:42:48 [9458] (104.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
02/26/16 13:42:48 [9458] (104.0) gm state change: GM_HOLD -> GM_DELETE
02/26/16 13:42:53 [9458] in doContactSchedd()
02/26/16 13:42:53 [9458] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/26/16 13:42:53 [9458] querying for removed/held jobs
02/26/16 13:42:53 [9458] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/26/16 13:42:53 [9458] Fetched 0 job ads from schedd
02/26/16 13:42:53 [9458] Updating classad values for 104.0:
02/26/16 13:42:53 [9458]    EnteredCurrentStatus = 1456512168
02/26/16 13:42:53 [9458]    HoldReason = "Attempts to submit failed: "
02/26/16 13:42:53 [9458]    HoldReasonCode = 0
02/26/16 13:42:53 [9458]    HoldReasonSubCode = 0
02/26/16 13:42:53 [9458]    JobStatus = 5
02/26/16 13:42:53 [9458]    LastReleaseReason = "Data files spooled"
02/26/16 13:42:53 [9458]    Managed = "Schedd"
02/26/16 13:42:53 [9458]    NumSystemHolds = 1
02/26/16 13:42:53 [9458]    ReleaseReason = undefined
02/26/16 13:42:54 [9458] No jobs left, shutting down
02/26/16 13:42:54 [9458] leaving doContactSchedd()
02/26/16 13:42:54 [9458] Got SIGTERM. Performing graceful shutdown.
02/26/16 13:42:54 [9458] Started timer to call main_shutdown_fast in 1800 seconds
02/26/16 13:42:54 [9458] **** condor_gridmanager (condor_GRIDMANAGER) pid 9458 EXITING WITH STATUS 0
02/27/16 13:22:41 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/27/16 13:22:41 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/27/16 13:22:41 Enumerating interfaces: lo 127.0.0.1 up
02/27/16 13:22:41 Enumerating interfaces: eth2 10.31.131.202 up
02/27/16 13:22:41 Enumerating interfaces: eth3 140.247.179.131 up
02/27/16 13:22:41 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/27/16 13:22:41 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/27/16 13:22:41 ******************************************************
02/27/16 13:22:41 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/27/16 13:22:41 ** /usr/sbin/condor_gridmanager
02/27/16 13:22:41 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/27/16 13:22:41 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/27/16 13:22:41 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/27/16 13:22:41 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/27/16 13:22:41 ** PID = 32096
02/27/16 13:22:41 ** Log last touched 2/26 13:42:54
02/27/16 13:22:41 ******************************************************
02/27/16 13:22:41 Using config source: /etc/condor-ce/condor_config
02/27/16 13:22:41 Using local config sources: 
02/27/16 13:22:41    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/27/16 13:22:41    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/01-ce-auth.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/01-ce-router.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/01-common-auth.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/02-ce-lsf.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/02-ce-pbs.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/03-managed-fork.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/50-osg-configure.conf
02/27/16 13:22:41    /etc/condor-ce/config.d/99-local.conf
02/27/16 13:22:41    /usr/share/condor-ce/condor_ce_router_defaults|
02/27/16 13:22:41 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
02/27/16 13:22:41 CLASSAD_CACHING is ENABLED
02/27/16 13:22:41 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/27/16 13:22:41 SharedPortEndpoint: waiting for connections to named socket 6929_db73_2
02/27/16 13:22:41 DaemonCore: command socket at <140.247.179.131:9620?sock=6929_db73_2>
02/27/16 13:22:41 DaemonCore: private command socket at <140.247.179.131:9620?sock=6929_db73_2>
02/27/16 13:22:41 Setting maximum accepts per cycle 8.
02/27/16 13:22:41 Setting maximum reaps per cycle 8.
02/27/16 13:22:41 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/27/16 13:22:41 [32096] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/27/16 13:22:41 [32096] DaemonCore: No more children processes to reap.
02/27/16 13:22:41 [32096] DaemonCore: in SendAliveToParent()
02/27/16 13:22:41 [32096] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:22:41 [32096] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/27/16 13:22:41 [32096] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/27/16 13:22:41 [32096] IPVERIFY: ip found is 0
02/27/16 13:22:41 [32096] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/27/16 13:22:41 [32096] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/27/16 13:22:41 [32096] Buf::write(): condor_write() failed
02/27/16 13:22:41 [32096] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/27/16 13:22:41 [32096] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:22:41 [32096] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/27/16 13:22:41 [32096] DaemonCore: Leaving SendAliveToParent() - success
02/27/16 13:22:41 [32096] Checking proxies
02/27/16 13:22:43 [32096] Received REMOVE_JOBS signal
02/27/16 13:22:43 [32096] in doContactSchedd()
02/27/16 13:22:43 [32096] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:22:43 [32096] querying for new jobs
02/27/16 13:22:43 [32096] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/27/16 13:22:43 [32096] Using job type INFNBatch for job 102.0
02/27/16 13:22:43 [32096] (102.0) SetJobLeaseTimers()
02/27/16 13:22:43 [32096] Found job 102.0 --- inserting
02/27/16 13:22:43 [32096] Fetched 1 new job ads from schedd
02/27/16 13:22:43 [32096] querying for removed/held jobs
02/27/16 13:22:43 [32096] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/27/16 13:22:43 [32096] Fetched 1 job ads from schedd
02/27/16 13:22:43 [32096] leaving doContactSchedd()
02/27/16 13:22:43 [32096] gahp server not up yet, delaying ping
02/27/16 13:22:43 [32096] *** UpdateLeases called
02/27/16 13:22:43 [32096]     Leases not supported, cancelling timer
02/27/16 13:22:43 [32096] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=6929_db73_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6838_1c38_4>"
CurrentTime = time()
MyCurrentTime = 1456597363
IdleJobs = 0
JobLimit = 10000

02/27/16 13:22:43 [32096] Trying to update collector <10.31.131.202:9619>
02/27/16 13:22:43 [32096] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/27/16 13:22:43 [32096] File descriptor limits: max 4096, safe 3277
02/27/16 13:22:43 [32096] (102.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/27/16 13:22:43 [32096] GAHP server pid = 32216
02/27/16 13:22:43 [32096] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/27/16 13:22:43 [32096] GAHP[32216] <- 'COMMANDS'
02/27/16 13:22:43 [32096] GAHP[32216] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/27/16 13:22:43 [32096] GAHP[32216] <- 'ASYNC_MODE_ON'
02/27/16 13:22:43 [32096] GAHP[32216] -> 'S' 'Async mode on'
02/27/16 13:22:43 [32096] (102.0) gm state change: GM_INIT -> GM_START
02/27/16 13:22:43 [32096] (102.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/27/16 13:22:43 [32096] (102.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/27/16 13:22:43 [32096] GAHP[32216] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/101/0/cluster101.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/101/0/cluster101.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/101/0/cluster101.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#102.0#1456510668";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/27/16 13:22:43 [32096] GAHP[32216] -> 'S'
02/27/16 13:22:43 [32096] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/27/16 13:22:43 [32096] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/27/16 13:22:43 [32096] IPVERIFY: ip found is 1
02/27/16 13:22:44 [32096] Received ADD_JOBS signal
02/27/16 13:22:45 [32096] GAHP[32216] <- 'RESULTS'
02/27/16 13:22:45 [32096] GAHP[32216] -> 'R'
02/27/16 13:22:45 [32096] GAHP[32216] -> 'S' '1'
02/27/16 13:22:45 [32096] GAHP[32216] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/27/16 13:22:45 [32096] (102.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/27/16 13:22:45 [32096] (102.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/27/16 13:22:45 [32096] (102.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/27/16 13:22:45 [32096] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/27/16 13:22:56 [32096] (102.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/27/16 13:22:56 [32096] Evaluating staleness of remote job statuses.
02/27/16 13:22:56 [32096] resource  is now up
02/27/16 13:22:56 [32096] in doContactSchedd()
02/27/16 13:22:56 [32096] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:22:56 [32096] querying for new jobs
02/27/16 13:22:56 [32096] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/27/16 13:22:56 [32096] Fetched 0 new job ads from schedd
02/27/16 13:22:56 [32096] querying for removed/held jobs
02/27/16 13:22:56 [32096] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/27/16 13:22:56 [32096] Fetched 1 job ads from schedd
02/27/16 13:22:56 [32096] Updating classad values for 102.0:
02/27/16 13:22:56 [32096]    CurrentStatusUnknown = false
02/27/16 13:22:56 [32096]    GridJobId = undefined
02/27/16 13:22:56 [32096]    LastRemoteStatusUpdate = 0
02/27/16 13:22:56 [32096]    Managed = "ScheddDone"
02/27/16 13:22:56 [32096] Deleting job 102.0 from schedd
02/27/16 13:22:56 [32096] No jobs left, shutting down
02/27/16 13:22:56 [32096] leaving doContactSchedd()
02/27/16 13:22:56 [32096] Got SIGTERM. Performing graceful shutdown.
02/27/16 13:22:56 [32096] Started timer to call main_shutdown_fast in 1800 seconds
02/27/16 13:22:56 [32096] **** condor_gridmanager (condor_GRIDMANAGER) pid 32096 EXITING WITH STATUS 0
02/27/16 13:47:43 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
02/27/16 13:47:43 Using IDs: 16 processors, 8 CPUs, 8 HTs
02/27/16 13:47:43 Enumerating interfaces: lo 127.0.0.1 up
02/27/16 13:47:43 Enumerating interfaces: eth2 10.31.131.202 up
02/27/16 13:47:43 Enumerating interfaces: eth3 140.247.179.131 up
02/27/16 13:47:43 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
02/27/16 13:47:43 Initializing Directory: curr_dir = /etc/condor-ce/config.d
02/27/16 13:47:43 ******************************************************
02/27/16 13:47:43 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
02/27/16 13:47:43 ** /usr/sbin/condor_gridmanager
02/27/16 13:47:43 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
02/27/16 13:47:43 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
02/27/16 13:47:43 ** $CondorVersion: 8.2.8 Apr 08 2015 $
02/27/16 13:47:43 ** $CondorPlatform: X86_64-CentOS_6.6 $
02/27/16 13:47:43 ** PID = 32461
02/27/16 13:47:43 ** Log last touched 2/27 13:22:56
02/27/16 13:47:43 ******************************************************
02/27/16 13:47:43 Using config source: /etc/condor-ce/condor_config
02/27/16 13:47:43 Using local config sources: 
02/27/16 13:47:43    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
02/27/16 13:47:43    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/01-ce-auth.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/01-ce-router.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/01-common-auth.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/02-ce-lsf.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/02-ce-pbs.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/03-ce-shared-port.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/03-managed-fork.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/10-ce-collector-generated.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/50-osg-configure.conf
02/27/16 13:47:43    /etc/condor-ce/config.d/99-local.conf
02/27/16 13:47:43    /usr/share/condor-ce/condor_ce_router_defaults|
02/27/16 13:47:43 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
02/27/16 13:47:43 CLASSAD_CACHING is ENABLED
02/27/16 13:47:43 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
02/27/16 13:47:43 SharedPortEndpoint: waiting for connections to named socket 6929_db73_3
02/27/16 13:47:43 DaemonCore: command socket at <140.247.179.131:9620?sock=6929_db73_3>
02/27/16 13:47:43 DaemonCore: private command socket at <140.247.179.131:9620?sock=6929_db73_3>
02/27/16 13:47:43 Setting maximum accepts per cycle 8.
02/27/16 13:47:43 Setting maximum reaps per cycle 8.
02/27/16 13:47:43 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/27/16 13:47:43 [32461] Welcome to the all-singing, all dancing, "amazing" GridManager!
02/27/16 13:47:43 [32461] DaemonCore: No more children processes to reap.
02/27/16 13:47:43 [32461] DaemonCore: in SendAliveToParent()
02/27/16 13:47:43 [32461] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:47:43 [32461] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
02/27/16 13:47:43 [32461] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
02/27/16 13:47:43 [32461] IPVERIFY: ip found is 0
02/27/16 13:47:43 [32461] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
02/27/16 13:47:43 [32461] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
02/27/16 13:47:43 [32461] Buf::write(): condor_write() failed
02/27/16 13:47:43 [32461] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
02/27/16 13:47:43 [32461] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:47:43 [32461] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
02/27/16 13:47:43 [32461] DaemonCore: Leaving SendAliveToParent() - success
02/27/16 13:47:43 [32461] Checking proxies
02/27/16 13:47:45 [32461] Received REMOVE_JOBS signal
02/27/16 13:47:45 [32461] in doContactSchedd()
02/27/16 13:47:45 [32461] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:47:45 [32461] querying for new jobs
02/27/16 13:47:45 [32461] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
02/27/16 13:47:45 [32461] Using job type INFNBatch for job 104.0
02/27/16 13:47:45 [32461] (104.0) SetJobLeaseTimers()
02/27/16 13:47:45 [32461] Found job 104.0 --- inserting
02/27/16 13:47:45 [32461] Fetched 1 new job ads from schedd
02/27/16 13:47:45 [32461] querying for removed/held jobs
02/27/16 13:47:45 [32461] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/27/16 13:47:45 [32461] Fetched 1 job ads from schedd
02/27/16 13:47:45 [32461] leaving doContactSchedd()
02/27/16 13:47:45 [32461] gahp server not up yet, delaying ping
02/27/16 13:47:45 [32461] *** UpdateLeases called
02/27/16 13:47:45 [32461]     Leases not supported, cancelling timer
02/27/16 13:47:45 [32461] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=6929_db73_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=6838_1c38_4>"
CurrentTime = time()
MyCurrentTime = 1456598865
IdleJobs = 0
JobLimit = 10000

02/27/16 13:47:45 [32461] Trying to update collector <10.31.131.202:9619>
02/27/16 13:47:45 [32461] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
02/27/16 13:47:45 [32461] File descriptor limits: max 4096, safe 3277
02/27/16 13:47:45 [32461] (104.0) doEvaluateState called: gmState GM_INIT, remoteState -1
02/27/16 13:47:45 [32461] GAHP server pid = 32603
02/27/16 13:47:45 [32461] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
02/27/16 13:47:45 [32461] GAHP[32603] <- 'COMMANDS'
02/27/16 13:47:45 [32461] GAHP[32603] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
02/27/16 13:47:45 [32461] GAHP[32603] <- 'ASYNC_MODE_ON'
02/27/16 13:47:45 [32461] GAHP[32603] -> 'S' 'Async mode on'
02/27/16 13:47:45 [32461] (104.0) gm state change: GM_INIT -> GM_START
02/27/16 13:47:45 [32461] (104.0) gm state change: GM_START -> GM_TRANSFER_INPUT
02/27/16 13:47:45 [32461] (104.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
02/27/16 13:47:45 [32461] GAHP[32603] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/103/0/cluster103.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/103/0/cluster103.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/103/0/cluster103.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#104.0#1456512093";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
02/27/16 13:47:45 [32461] GAHP[32603] -> 'S'
02/27/16 13:47:45 [32461] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
02/27/16 13:47:45 [32461] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
02/27/16 13:47:45 [32461] IPVERIFY: ip found is 1
02/27/16 13:47:46 [32461] Received ADD_JOBS signal
02/27/16 13:47:47 [32461] GAHP[32603] <- 'RESULTS'
02/27/16 13:47:47 [32461] GAHP[32603] -> 'R'
02/27/16 13:47:47 [32461] GAHP[32603] -> 'S' '1'
02/27/16 13:47:47 [32461] GAHP[32603] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
02/27/16 13:47:47 [32461] (104.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
02/27/16 13:47:47 [32461] (104.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
02/27/16 13:47:47 [32461] (104.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
02/27/16 13:47:47 [32461] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
02/27/16 13:47:47 [32461] (104.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
02/27/16 13:47:48 [32461] Evaluating staleness of remote job statuses.
02/27/16 13:47:50 [32461] resource  is now up
02/27/16 13:47:50 [32461] in doContactSchedd()
02/27/16 13:47:50 [32461] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 6838_1c38_4
02/27/16 13:47:50 [32461] querying for new jobs
02/27/16 13:47:50 [32461] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
02/27/16 13:47:50 [32461] Fetched 0 new job ads from schedd
02/27/16 13:47:50 [32461] querying for removed/held jobs
02/27/16 13:47:50 [32461] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
02/27/16 13:47:50 [32461] Fetched 1 job ads from schedd
02/27/16 13:47:50 [32461] Updating classad values for 104.0:
02/27/16 13:47:50 [32461]    CurrentStatusUnknown = false
02/27/16 13:47:50 [32461]    GridJobId = undefined
02/27/16 13:47:50 [32461]    LastRemoteStatusUpdate = 0
02/27/16 13:47:50 [32461]    Managed = "ScheddDone"
02/27/16 13:47:50 [32461] Deleting job 104.0 from schedd
02/27/16 13:47:50 [32461] No jobs left, shutting down
02/27/16 13:47:50 [32461] leaving doContactSchedd()
02/27/16 13:47:50 [32461] Got SIGTERM. Performing graceful shutdown.
02/27/16 13:47:50 [32461] Started timer to call main_shutdown_fast in 1800 seconds
02/27/16 13:47:50 [32461] **** condor_gridmanager (condor_GRIDMANAGER) pid 32461 EXITING WITH STATUS 0
03/01/16 12:47:01 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/01/16 12:47:01 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/01/16 12:47:01 Enumerating interfaces: lo 127.0.0.1 up
03/01/16 12:47:01 Enumerating interfaces: eth2 10.31.131.202 up
03/01/16 12:47:01 Enumerating interfaces: eth3 140.247.179.131 up
03/01/16 12:47:01 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/01/16 12:47:01 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/01/16 12:47:01 ******************************************************
03/01/16 12:47:01 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/01/16 12:47:01 ** /usr/sbin/condor_gridmanager
03/01/16 12:47:01 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/01/16 12:47:01 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/01/16 12:47:01 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/01/16 12:47:01 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/01/16 12:47:01 ** PID = 18180
03/01/16 12:47:01 ** Log last touched 2/27 13:47:50
03/01/16 12:47:01 ******************************************************
03/01/16 12:47:01 Using config source: /etc/condor-ce/condor_config
03/01/16 12:47:01 Using local config sources: 
03/01/16 12:47:01    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/01/16 12:47:01    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/01-ce-auth.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/01-ce-router.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/01-common-auth.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/02-ce-lsf.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/02-ce-pbs.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/03-managed-fork.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/50-osg-configure.conf
03/01/16 12:47:01    /etc/condor-ce/config.d/99-local.conf
03/01/16 12:47:01    /usr/share/condor-ce/condor_ce_router_defaults|
03/01/16 12:47:01 config Macros = 144, Sorted = 144, StringBytes = 12557, TablesBytes = 5392
03/01/16 12:47:01 CLASSAD_CACHING is ENABLED
03/01/16 12:47:01 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/01/16 12:47:01 SharedPortEndpoint: waiting for connections to named socket 11405_9aa9_1
03/01/16 12:47:01 DaemonCore: command socket at <140.247.179.131:9620?sock=11405_9aa9_1>
03/01/16 12:47:01 DaemonCore: private command socket at <140.247.179.131:9620?sock=11405_9aa9_1>
03/01/16 12:47:01 Setting maximum accepts per cycle 8.
03/01/16 12:47:01 Setting maximum reaps per cycle 8.
03/01/16 12:47:01 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/01/16 12:47:01 [18180] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/01/16 12:47:01 [18180] DaemonCore: No more children processes to reap.
03/01/16 12:47:01 [18180] DaemonCore: in SendAliveToParent()
03/01/16 12:47:01 [18180] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11377_ea26_4
03/01/16 12:47:01 [18180] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/01/16 12:47:01 [18180] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/01/16 12:47:01 [18180] IPVERIFY: ip found is 0
03/01/16 12:47:01 [18180] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/01/16 12:47:01 [18180] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/01/16 12:47:01 [18180] Buf::write(): condor_write() failed
03/01/16 12:47:01 [18180] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/01/16 12:47:01 [18180] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 11377_ea26_4
03/01/16 12:47:01 [18180] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/01/16 12:47:01 [18180] DaemonCore: Leaving SendAliveToParent() - success
03/01/16 12:47:01 [18180] Checking proxies
03/01/16 12:47:04 [18180] Received ADD_JOBS signal
03/01/16 12:47:04 [18180] in doContactSchedd()
03/01/16 12:47:04 [18180] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11377_ea26_4
03/01/16 12:47:04 [18180] querying for new jobs
03/01/16 12:47:04 [18180] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/01/16 12:47:04 [18180] Using job type INFNBatch for job 106.0
03/01/16 12:47:04 [18180] (106.0) SetJobLeaseTimers()
03/01/16 12:47:04 [18180] Found job 106.0 --- inserting
03/01/16 12:47:04 [18180] Fetched 1 new job ads from schedd
03/01/16 12:47:04 [18180] querying for removed/held jobs
03/01/16 12:47:04 [18180] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/01/16 12:47:04 [18180] Fetched 0 job ads from schedd
03/01/16 12:47:05 [18180] leaving doContactSchedd()
03/01/16 12:47:05 [18180] gahp server not up yet, delaying ping
03/01/16 12:47:05 [18180] *** UpdateLeases called
03/01/16 12:47:05 [18180]     Leases not supported, cancelling timer
03/01/16 12:47:05 [18180] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=11405_9aa9_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=11377_ea26_4>"
CurrentTime = time()
MyCurrentTime = 1456854425
IdleJobs = 1
JobLimit = 10000

03/01/16 12:47:05 [18180] Trying to update collector <10.31.131.202:9619>
03/01/16 12:47:05 [18180] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/01/16 12:47:05 [18180] File descriptor limits: max 4096, safe 3277
03/01/16 12:47:05 [18180] (106.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/01/16 12:47:05 [18180] GAHP server pid = 18588
03/01/16 12:47:05 [18180] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/01/16 12:47:05 [18180] GAHP[18588] <- 'COMMANDS'
03/01/16 12:47:05 [18180] GAHP[18588] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/01/16 12:47:05 [18180] GAHP[18588] <- 'ASYNC_MODE_ON'
03/01/16 12:47:05 [18180] GAHP[18588] -> 'S' 'Async mode on'
03/01/16 12:47:05 [18180] (106.0) gm state change: GM_INIT -> GM_START
03/01/16 12:47:05 [18180] (106.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/01/16 12:47:05 [18180] (106.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/01/16 12:47:05 [18180] (106.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/01/16 12:47:05 [18180] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/01/16 12:47:05 [18180] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/01/16 12:47:05 [18180] IPVERIFY: ip found is 1
03/01/16 12:47:06 [18180] Evaluating staleness of remote job statuses.
03/01/16 12:47:10 [18180] resource  is now up
03/01/16 12:47:10 [18180] in doContactSchedd()
03/01/16 12:47:10 [18180] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11377_ea26_4
03/01/16 12:47:10 [18180] querying for removed/held jobs
03/01/16 12:47:10 [18180] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/01/16 12:47:10 [18180] Fetched 0 job ads from schedd
03/01/16 12:47:10 [18180] Updating classad values for 106.0:
03/01/16 12:47:10 [18180]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#106.0#1456854398"
03/01/16 12:47:10 [18180]    LastRemoteStatusUpdate = 1456854425
03/01/16 12:47:11 [18180] leaving doContactSchedd()
03/01/16 12:47:11 [18180] (106.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/01/16 12:47:11 [18180] (106.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/01/16 12:47:11 [18180] (106.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/01/16 12:47:11 [18180] GAHP[18588] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#106.0#1456854398";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/01/16 12:47:11 [18180] GAHP[18588] -> 'S'
03/01/16 12:47:16 [18180] GAHP[18588] <- 'RESULTS'
03/01/16 12:47:16 [18180] GAHP[18588] -> 'R'
03/01/16 12:47:16 [18180] GAHP[18588] -> 'S' '1'
03/01/16 12:47:16 [18180] GAHP[18588] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
03/01/16 12:47:16 [18180] (106.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/01/16 12:47:16 [18180] (106.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
03/01/16 12:47:16 [18180] (106.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/01/16 12:47:16 [18180] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/01/16 12:47:45 [18180] (106.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/01/16 12:47:45 [18180] in doContactSchedd()
03/01/16 12:47:45 [18180] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11377_ea26_4
03/01/16 12:47:45 [18180] querying for removed/held jobs
03/01/16 12:47:45 [18180] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/01/16 12:47:45 [18180] Fetched 0 job ads from schedd
03/01/16 12:47:45 [18180] Updating classad values for 106.0:
03/01/16 12:47:45 [18180]    CurrentStatusUnknown = false
03/01/16 12:47:45 [18180]    GridJobId = undefined
03/01/16 12:47:45 [18180]    LastRemoteStatusUpdate = 0
03/01/16 12:47:45 [18180] leaving doContactSchedd()
03/01/16 12:47:45 [18180] (106.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/01/16 12:47:45 [18180] (106.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/01/16 12:47:45 [18180] (106.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/01/16 12:47:50 [18180] in doContactSchedd()
03/01/16 12:47:50 [18180] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11377_ea26_4
03/01/16 12:47:50 [18180] querying for removed/held jobs
03/01/16 12:47:50 [18180] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/01/16 12:47:50 [18180] Fetched 0 job ads from schedd
03/01/16 12:47:50 [18180] Updating classad values for 106.0:
03/01/16 12:47:50 [18180]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#106.0#1456854398"
03/01/16 12:47:50 [18180]    LastRemoteStatusUpdate = 1456854465
03/01/16 12:47:50 [18180] leaving doContactSchedd()
03/01/16 12:47:50 [18180] (106.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/01/16 12:47:50 [18180] (106.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/01/16 12:47:50 [18180] (106.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/01/16 12:47:50 [18180] (106.0) gm state change: GM_HOLD -> GM_DELETE
03/01/16 12:47:55 [18180] in doContactSchedd()
03/01/16 12:47:55 [18180] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 11377_ea26_4
03/01/16 12:47:55 [18180] querying for removed/held jobs
03/01/16 12:47:55 [18180] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/01/16 12:47:55 [18180] Fetched 0 job ads from schedd
03/01/16 12:47:55 [18180] Updating classad values for 106.0:
03/01/16 12:47:55 [18180]    EnteredCurrentStatus = 1456854470
03/01/16 12:47:55 [18180]    HoldReason = "Attempts to submit failed: "
03/01/16 12:47:55 [18180]    HoldReasonCode = 0
03/01/16 12:47:55 [18180]    HoldReasonSubCode = 0
03/01/16 12:47:55 [18180]    JobStatus = 5
03/01/16 12:47:55 [18180]    LastReleaseReason = "Data files spooled"
03/01/16 12:47:55 [18180]    Managed = "Schedd"
03/01/16 12:47:55 [18180]    NumSystemHolds = 1
03/01/16 12:47:55 [18180]    ReleaseReason = undefined
03/01/16 12:47:55 [18180] No jobs left, shutting down
03/01/16 12:47:55 [18180] leaving doContactSchedd()
03/01/16 12:47:55 [18180] Got SIGTERM. Performing graceful shutdown.
03/01/16 12:47:55 [18180] Started timer to call main_shutdown_fast in 1800 seconds
03/01/16 12:47:55 [18180] **** condor_gridmanager (condor_GRIDMANAGER) pid 18180 EXITING WITH STATUS 0
03/02/16 15:07:20 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/02/16 15:07:20 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/02/16 15:07:20 Enumerating interfaces: lo 127.0.0.1 up
03/02/16 15:07:20 Enumerating interfaces: eth2 10.31.131.202 up
03/02/16 15:07:20 Enumerating interfaces: eth3 140.247.179.131 up
03/02/16 15:07:20 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/02/16 15:07:20 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/02/16 15:07:20 ******************************************************
03/02/16 15:07:20 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/02/16 15:07:20 ** /usr/sbin/condor_gridmanager
03/02/16 15:07:20 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/02/16 15:07:20 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/02/16 15:07:20 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/02/16 15:07:20 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/02/16 15:07:20 ** PID = 7746
03/02/16 15:07:20 ** Log last touched 3/1 12:47:55
03/02/16 15:07:20 ******************************************************
03/02/16 15:07:20 Using config source: /etc/condor-ce/condor_config
03/02/16 15:07:20 Using local config sources: 
03/02/16 15:07:20    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/02/16 15:07:20    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/01-ce-auth.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/01-ce-router.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/01-common-auth.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/02-ce-lsf.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/02-ce-pbs.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/03-managed-fork.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/50-osg-configure.conf
03/02/16 15:07:20    /etc/condor-ce/config.d/99-local.conf
03/02/16 15:07:20    /usr/share/condor-ce/condor_ce_router_defaults|
03/02/16 15:07:20 config Macros = 144, Sorted = 144, StringBytes = 12551, TablesBytes = 5392
03/02/16 15:07:20 CLASSAD_CACHING is ENABLED
03/02/16 15:07:20 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/02/16 15:07:20 SharedPortEndpoint: waiting for connections to named socket 4189_4783_1
03/02/16 15:07:20 DaemonCore: command socket at <140.247.179.131:9620?sock=4189_4783_1>
03/02/16 15:07:20 DaemonCore: private command socket at <140.247.179.131:9620?sock=4189_4783_1>
03/02/16 15:07:20 Setting maximum accepts per cycle 8.
03/02/16 15:07:20 Setting maximum reaps per cycle 8.
03/02/16 15:07:20 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/02/16 15:07:20 [7746] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/02/16 15:07:20 [7746] DaemonCore: No more children processes to reap.
03/02/16 15:07:20 [7746] DaemonCore: in SendAliveToParent()
03/02/16 15:07:20 [7746] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:07:20 [7746] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/02/16 15:07:20 [7746] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/02/16 15:07:20 [7746] IPVERIFY: ip found is 0
03/02/16 15:07:20 [7746] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/02/16 15:07:20 [7746] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/02/16 15:07:20 [7746] Buf::write(): condor_write() failed
03/02/16 15:07:20 [7746] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/02/16 15:07:20 [7746] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:07:20 [7746] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/02/16 15:07:20 [7746] DaemonCore: Leaving SendAliveToParent() - success
03/02/16 15:07:20 [7746] Checking proxies
03/02/16 15:07:23 [7746] Received ADD_JOBS signal
03/02/16 15:07:23 [7746] in doContactSchedd()
03/02/16 15:07:23 [7746] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:07:23 [7746] querying for new jobs
03/02/16 15:07:23 [7746] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/02/16 15:07:23 [7746] Using job type INFNBatch for job 108.0
03/02/16 15:07:23 [7746] (108.0) SetJobLeaseTimers()
03/02/16 15:07:23 [7746] Found job 108.0 --- inserting
03/02/16 15:07:23 [7746] Fetched 1 new job ads from schedd
03/02/16 15:07:23 [7746] querying for removed/held jobs
03/02/16 15:07:23 [7746] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/02/16 15:07:23 [7746] Fetched 0 job ads from schedd
03/02/16 15:07:23 [7746] leaving doContactSchedd()
03/02/16 15:07:23 [7746] gahp server not up yet, delaying ping
03/02/16 15:07:23 [7746] *** UpdateLeases called
03/02/16 15:07:23 [7746]     Leases not supported, cancelling timer
03/02/16 15:07:23 [7746] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=4189_4783_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=4182_cc7f_4>"
CurrentTime = time()
MyCurrentTime = 1456949243
IdleJobs = 1
JobLimit = 10000

03/02/16 15:07:23 [7746] Trying to update collector <10.31.131.202:9619>
03/02/16 15:07:23 [7746] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/02/16 15:07:23 [7746] File descriptor limits: max 4096, safe 3277
03/02/16 15:07:23 [7746] (108.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/02/16 15:07:23 [7746] GAHP server pid = 7752
03/02/16 15:07:23 [7746] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/02/16 15:07:23 [7746] GAHP[7752] <- 'COMMANDS'
03/02/16 15:07:23 [7746] GAHP[7752] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/02/16 15:07:23 [7746] GAHP[7752] <- 'ASYNC_MODE_ON'
03/02/16 15:07:23 [7746] GAHP[7752] -> 'S' 'Async mode on'
03/02/16 15:07:23 [7746] (108.0) gm state change: GM_INIT -> GM_START
03/02/16 15:07:23 [7746] (108.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/02/16 15:07:23 [7746] (108.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/02/16 15:07:23 [7746] (108.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/02/16 15:07:23 [7746] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/02/16 15:07:23 [7746] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/02/16 15:07:23 [7746] IPVERIFY: ip found is 1
03/02/16 15:07:25 [7746] Evaluating staleness of remote job statuses.
03/02/16 15:07:28 [7746] resource  is now up
03/02/16 15:07:28 [7746] in doContactSchedd()
03/02/16 15:07:28 [7746] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:07:28 [7746] querying for removed/held jobs
03/02/16 15:07:28 [7746] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/02/16 15:07:28 [7746] Fetched 0 job ads from schedd
03/02/16 15:07:28 [7746] Updating classad values for 108.0:
03/02/16 15:07:28 [7746]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#108.0#1456949227"
03/02/16 15:07:28 [7746]    LastRemoteStatusUpdate = 1456949243
03/02/16 15:07:28 [7746] leaving doContactSchedd()
03/02/16 15:07:28 [7746] (108.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/02/16 15:07:28 [7746] (108.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/02/16 15:07:28 [7746] (108.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/02/16 15:07:28 [7746] GAHP[7752] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#108.0#1456949227";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/02/16 15:07:28 [7746] GAHP[7752] -> 'S'
03/02/16 15:07:30 [7746] GAHP[7752] <- 'RESULTS'
03/02/16 15:07:30 [7746] GAHP[7752] -> 'R'
03/02/16 15:07:30 [7746] GAHP[7752] -> 'S' '1'
03/02/16 15:07:30 [7746] GAHP[7752] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
03/02/16 15:07:30 [7746] (108.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/02/16 15:07:30 [7746] (108.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
03/02/16 15:07:30 [7746] (108.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/02/16 15:07:30 [7746] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/02/16 15:07:39 [7746] (108.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/02/16 15:07:39 [7746] in doContactSchedd()
03/02/16 15:07:39 [7746] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:07:39 [7746] querying for removed/held jobs
03/02/16 15:07:39 [7746] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/02/16 15:07:39 [7746] Fetched 0 job ads from schedd
03/02/16 15:07:39 [7746] Updating classad values for 108.0:
03/02/16 15:07:39 [7746]    CurrentStatusUnknown = false
03/02/16 15:07:39 [7746]    GridJobId = undefined
03/02/16 15:07:39 [7746]    LastRemoteStatusUpdate = 0
03/02/16 15:07:39 [7746] leaving doContactSchedd()
03/02/16 15:07:39 [7746] (108.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/02/16 15:07:39 [7746] (108.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/02/16 15:07:39 [7746] (108.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/02/16 15:07:44 [7746] in doContactSchedd()
03/02/16 15:07:44 [7746] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:07:44 [7746] querying for removed/held jobs
03/02/16 15:07:44 [7746] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/02/16 15:07:44 [7746] Fetched 0 job ads from schedd
03/02/16 15:07:44 [7746] Updating classad values for 108.0:
03/02/16 15:07:44 [7746]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#108.0#1456949227"
03/02/16 15:07:44 [7746]    LastRemoteStatusUpdate = 1456949259
03/02/16 15:07:46 [7746] leaving doContactSchedd()
03/02/16 15:07:46 [7746] (108.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/02/16 15:07:46 [7746] (108.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/02/16 15:07:46 [7746] (108.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/02/16 15:07:46 [7746] (108.0) gm state change: GM_HOLD -> GM_DELETE
03/02/16 15:07:51 [7746] in doContactSchedd()
03/02/16 15:07:51 [7746] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:07:51 [7746] querying for removed/held jobs
03/02/16 15:07:51 [7746] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/02/16 15:07:51 [7746] Fetched 0 job ads from schedd
03/02/16 15:07:51 [7746] Updating classad values for 108.0:
03/02/16 15:07:51 [7746]    EnteredCurrentStatus = 1456949266
03/02/16 15:07:51 [7746]    HoldReason = "Attempts to submit failed: "
03/02/16 15:07:51 [7746]    HoldReasonCode = 0
03/02/16 15:07:51 [7746]    HoldReasonSubCode = 0
03/02/16 15:07:51 [7746]    JobStatus = 5
03/02/16 15:07:51 [7746]    LastReleaseReason = "Data files spooled"
03/02/16 15:07:51 [7746]    Managed = "Schedd"
03/02/16 15:07:51 [7746]    NumSystemHolds = 1
03/02/16 15:07:51 [7746]    ReleaseReason = undefined
03/02/16 15:07:52 [7746] No jobs left, shutting down
03/02/16 15:07:52 [7746] leaving doContactSchedd()
03/02/16 15:07:52 [7746] Got SIGTERM. Performing graceful shutdown.
03/02/16 15:07:52 [7746] Started timer to call main_shutdown_fast in 1800 seconds
03/02/16 15:07:52 [7746] **** condor_gridmanager (condor_GRIDMANAGER) pid 7746 EXITING WITH STATUS 0
03/02/16 15:11:17 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/02/16 15:11:17 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/02/16 15:11:17 Enumerating interfaces: lo 127.0.0.1 up
03/02/16 15:11:17 Enumerating interfaces: eth2 10.31.131.202 up
03/02/16 15:11:17 Enumerating interfaces: eth3 140.247.179.131 up
03/02/16 15:11:17 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/02/16 15:11:17 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/02/16 15:11:17 ******************************************************
03/02/16 15:11:17 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/02/16 15:11:17 ** /usr/sbin/condor_gridmanager
03/02/16 15:11:17 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/02/16 15:11:17 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/02/16 15:11:17 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/02/16 15:11:17 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/02/16 15:11:17 ** PID = 21172
03/02/16 15:11:17 ** Log last touched 3/2 15:07:52
03/02/16 15:11:17 ******************************************************
03/02/16 15:11:17 Using config source: /etc/condor-ce/condor_config
03/02/16 15:11:17 Using local config sources: 
03/02/16 15:11:17    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/02/16 15:11:17    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/01-ce-auth.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/01-ce-router.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/01-common-auth.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/02-ce-lsf.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/02-ce-pbs.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/03-managed-fork.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/50-osg-configure.conf
03/02/16 15:11:17    /etc/condor-ce/config.d/99-local.conf
03/02/16 15:11:17    /usr/share/condor-ce/condor_ce_router_defaults|
03/02/16 15:11:17 config Macros = 144, Sorted = 144, StringBytes = 12553, TablesBytes = 5392
03/02/16 15:11:17 CLASSAD_CACHING is ENABLED
03/02/16 15:11:17 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/02/16 15:11:17 SharedPortEndpoint: waiting for connections to named socket 4189_4783_2
03/02/16 15:11:17 DaemonCore: command socket at <140.247.179.131:9620?sock=4189_4783_2>
03/02/16 15:11:17 DaemonCore: private command socket at <140.247.179.131:9620?sock=4189_4783_2>
03/02/16 15:11:17 Setting maximum accepts per cycle 8.
03/02/16 15:11:17 Setting maximum reaps per cycle 8.
03/02/16 15:11:17 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/02/16 15:11:17 [21172] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/02/16 15:11:17 [21172] DaemonCore: No more children processes to reap.
03/02/16 15:11:17 [21172] DaemonCore: in SendAliveToParent()
03/02/16 15:11:17 [21172] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:11:17 [21172] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/02/16 15:11:17 [21172] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/02/16 15:11:17 [21172] IPVERIFY: ip found is 0
03/02/16 15:11:17 [21172] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/02/16 15:11:17 [21172] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/02/16 15:11:17 [21172] Buf::write(): condor_write() failed
03/02/16 15:11:17 [21172] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/02/16 15:11:17 [21172] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:11:17 [21172] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/02/16 15:11:17 [21172] DaemonCore: Leaving SendAliveToParent() - success
03/02/16 15:11:17 [21172] Checking proxies
03/02/16 15:11:19 [21172] Received REMOVE_JOBS signal
03/02/16 15:11:19 [21172] in doContactSchedd()
03/02/16 15:11:19 [21172] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:11:19 [21172] querying for new jobs
03/02/16 15:11:19 [21172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/02/16 15:11:19 [21172] Using job type INFNBatch for job 106.0
03/02/16 15:11:19 [21172] (106.0) SetJobLeaseTimers()
03/02/16 15:11:19 [21172] Failed to get expiration time of proxy /n/atlasgrid/condor/105/0/cluster105.proc0.subproc0/x509up_u556792
03/02/16 15:11:19 [21172] Found job 106.0 --- inserting
03/02/16 15:11:19 [21172] Fetched 1 new job ads from schedd
03/02/16 15:11:19 [21172] querying for removed/held jobs
03/02/16 15:11:19 [21172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/02/16 15:11:19 [21172] Fetched 1 job ads from schedd
03/02/16 15:11:19 [21172] leaving doContactSchedd()
03/02/16 15:11:19 [21172] gahp server not up yet, delaying ping
03/02/16 15:11:19 [21172] *** UpdateLeases called
03/02/16 15:11:19 [21172]     Leases not supported, cancelling timer
03/02/16 15:11:19 [21172] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=4189_4783_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=4182_cc7f_4>"
CurrentTime = time()
MyCurrentTime = 1456949479
IdleJobs = 0
JobLimit = 10000

03/02/16 15:11:19 [21172] Trying to update collector <10.31.131.202:9619>
03/02/16 15:11:19 [21172] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/02/16 15:11:19 [21172] File descriptor limits: max 4096, safe 3277
03/02/16 15:11:19 [21172] (106.0) doEvaluateState called: gmState GM_INIT, remoteState -1
03/02/16 15:11:19 [21172] GAHP server pid = 21292
03/02/16 15:11:19 [21172] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/02/16 15:11:19 [21172] GAHP[21292] <- 'COMMANDS'
03/02/16 15:11:19 [21172] GAHP[21292] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/02/16 15:11:19 [21172] GAHP[21292] <- 'ASYNC_MODE_ON'
03/02/16 15:11:19 [21172] GAHP[21292] -> 'S' 'Async mode on'
03/02/16 15:11:19 [21172] (106.0) gm state change: GM_INIT -> GM_START
03/02/16 15:11:19 [21172] (106.0) gm state change: GM_START -> GM_TRANSFER_INPUT
03/02/16 15:11:19 [21172] (106.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/02/16 15:11:19 [21172] GAHP[21292] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#106.0#1456854398";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/02/16 15:11:19 [21172] GAHP[21292] -> 'S'
03/02/16 15:11:19 [21172] GAHP[21292] <- 'RESULTS'
03/02/16 15:11:19 [21172] GAHP[21292] -> 'R'
03/02/16 15:11:19 [21172] GAHP[21292] -> 'S' '1'
03/02/16 15:11:19 [21172] GAHP[21292] -> '2' '1' 'Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)' 'N/A'
03/02/16 15:11:19 [21172] (106.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
03/02/16 15:11:19 [21172] (106.0) blah_job_submit() failed: Unable to limit the proxy (Unable to create limited proxy file (/n/atlasgrid/condor/105/0/cluster105.proc0.subproc0/x509up_u556792.lmt): errno=2, No such file or directory)
03/02/16 15:11:19 [21172] (106.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/02/16 15:11:19 [21172] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/02/16 15:11:19 [21172] (106.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
03/02/16 15:11:19 [21172] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/02/16 15:11:19 [21172] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/02/16 15:11:19 [21172] IPVERIFY: ip found is 1
03/02/16 15:11:20 [21172] Received ADD_JOBS signal
03/02/16 15:11:22 [21172] Evaluating staleness of remote job statuses.
03/02/16 15:11:24 [21172] resource  is now up
03/02/16 15:11:24 [21172] in doContactSchedd()
03/02/16 15:11:24 [21172] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 4182_cc7f_4
03/02/16 15:11:24 [21172] querying for new jobs
03/02/16 15:11:24 [21172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
03/02/16 15:11:24 [21172] Fetched 0 new job ads from schedd
03/02/16 15:11:24 [21172] querying for removed/held jobs
03/02/16 15:11:24 [21172] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/02/16 15:11:24 [21172] Fetched 1 job ads from schedd
03/02/16 15:11:24 [21172] Updating classad values for 106.0:
03/02/16 15:11:24 [21172]    CurrentStatusUnknown = false
03/02/16 15:11:24 [21172]    GridJobId = undefined
03/02/16 15:11:24 [21172]    LastRemoteStatusUpdate = 0
03/02/16 15:11:24 [21172]    Managed = "ScheddDone"
03/02/16 15:11:24 [21172] Deleting job 106.0 from schedd
03/02/16 15:11:24 [21172] No jobs left, shutting down
03/02/16 15:11:24 [21172] leaving doContactSchedd()
03/02/16 15:11:24 [21172] Got SIGTERM. Performing graceful shutdown.
03/02/16 15:11:24 [21172] Started timer to call main_shutdown_fast in 1800 seconds
03/02/16 15:11:24 [21172] **** condor_gridmanager (condor_GRIDMANAGER) pid 21172 EXITING WITH STATUS 0
03/03/16 14:28:08 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/03/16 14:28:08 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/03/16 14:28:08 Enumerating interfaces: lo 127.0.0.1 up
03/03/16 14:28:08 Enumerating interfaces: eth2 10.31.131.202 up
03/03/16 14:28:08 Enumerating interfaces: eth3 140.247.179.131 up
03/03/16 14:28:08 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/03/16 14:28:08 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/03/16 14:28:08 ******************************************************
03/03/16 14:28:08 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/03/16 14:28:08 ** /usr/sbin/condor_gridmanager
03/03/16 14:28:08 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/03/16 14:28:08 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/03/16 14:28:08 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/03/16 14:28:08 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/03/16 14:28:08 ** PID = 18930
03/03/16 14:28:08 ** Log last touched 3/2 15:11:24
03/03/16 14:28:08 ******************************************************
03/03/16 14:28:08 Using config source: /etc/condor-ce/condor_config
03/03/16 14:28:08 Using local config sources: 
03/03/16 14:28:08    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/03/16 14:28:08    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/01-ce-auth.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/01-ce-router.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/01-common-auth.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/02-ce-lsf.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/02-ce-pbs.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/03-managed-fork.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/50-osg-configure.conf
03/03/16 14:28:08    /etc/condor-ce/config.d/99-local.conf
03/03/16 14:28:08    /usr/share/condor-ce/condor_ce_router_defaults|
03/03/16 14:28:08 config Macros = 144, Sorted = 144, StringBytes = 12554, TablesBytes = 5392
03/03/16 14:28:08 CLASSAD_CACHING is ENABLED
03/03/16 14:28:08 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/03/16 14:28:08 SharedPortEndpoint: waiting for connections to named socket 6821_c2ee_1
03/03/16 14:28:08 DaemonCore: command socket at <140.247.179.131:9620?sock=6821_c2ee_1>
03/03/16 14:28:08 DaemonCore: private command socket at <140.247.179.131:9620?sock=6821_c2ee_1>
03/03/16 14:28:08 Setting maximum accepts per cycle 8.
03/03/16 14:28:08 Setting maximum reaps per cycle 8.
03/03/16 14:28:08 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/03/16 14:28:08 [18930] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/03/16 14:28:08 [18930] DaemonCore: No more children processes to reap.
03/03/16 14:28:08 [18930] DaemonCore: in SendAliveToParent()
03/03/16 14:28:08 [18930] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 14:28:09 [18930] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/03/16 14:28:09 [18930] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/03/16 14:28:09 [18930] IPVERIFY: ip found is 0
03/03/16 14:28:09 [18930] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/03/16 14:28:09 [18930] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/03/16 14:28:09 [18930] Buf::write(): condor_write() failed
03/03/16 14:28:09 [18930] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/03/16 14:28:09 [18930] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 14:28:09 [18930] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/03/16 14:28:09 [18930] DaemonCore: Leaving SendAliveToParent() - success
03/03/16 14:28:09 [18930] Checking proxies
03/03/16 14:28:11 [18930] Received ADD_JOBS signal
03/03/16 14:28:11 [18930] in doContactSchedd()
03/03/16 14:28:11 [18930] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 14:28:11 [18930] querying for new jobs
03/03/16 14:28:11 [18930] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/03/16 14:28:11 [18930] Using job type INFNBatch for job 110.0
03/03/16 14:28:11 [18930] (110.0) SetJobLeaseTimers()
03/03/16 14:28:11 [18930] Found job 110.0 --- inserting
03/03/16 14:28:11 [18930] Fetched 1 new job ads from schedd
03/03/16 14:28:11 [18930] querying for removed/held jobs
03/03/16 14:28:11 [18930] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/03/16 14:28:11 [18930] Fetched 0 job ads from schedd
03/03/16 14:28:12 [18930] leaving doContactSchedd()
03/03/16 14:28:12 [18930] gahp server not up yet, delaying ping
03/03/16 14:28:12 [18930] *** UpdateLeases called
03/03/16 14:28:12 [18930]     Leases not supported, cancelling timer
03/03/16 14:28:12 [18930] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=6821_c2ee_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=5224_8549_6>"
CurrentTime = time()
MyCurrentTime = 1457033292
IdleJobs = 1
JobLimit = 10000

03/03/16 14:28:12 [18930] Trying to update collector <10.31.131.202:9619>
03/03/16 14:28:12 [18930] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/03/16 14:28:12 [18930] File descriptor limits: max 4096, safe 3277
03/03/16 14:28:12 [18930] (110.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/03/16 14:28:12 [18930] GAHP server pid = 19602
03/03/16 14:28:12 [18930] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/03/16 14:28:12 [18930] GAHP[19602] <- 'COMMANDS'
03/03/16 14:28:12 [18930] GAHP[19602] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/03/16 14:28:12 [18930] GAHP[19602] <- 'ASYNC_MODE_ON'
03/03/16 14:28:12 [18930] GAHP[19602] -> 'S' 'Async mode on'
03/03/16 14:28:12 [18930] (110.0) gm state change: GM_INIT -> GM_START
03/03/16 14:28:12 [18930] (110.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/03/16 14:28:12 [18930] (110.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/03/16 14:28:12 [18930] (110.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/03/16 14:28:12 [18930] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/03/16 14:28:12 [18930] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/03/16 14:28:12 [18930] IPVERIFY: ip found is 1
03/03/16 14:28:13 [18930] Evaluating staleness of remote job statuses.
03/03/16 14:28:17 [18930] resource  is now up
03/03/16 14:28:17 [18930] in doContactSchedd()
03/03/16 14:28:17 [18930] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 14:28:17 [18930] querying for removed/held jobs
03/03/16 14:28:17 [18930] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/03/16 14:28:17 [18930] Fetched 0 job ads from schedd
03/03/16 14:28:17 [18930] Updating classad values for 110.0:
03/03/16 14:28:17 [18930]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#110.0#1457033233"
03/03/16 14:28:17 [18930]    LastRemoteStatusUpdate = 1457033292
03/03/16 14:28:17 [18930] leaving doContactSchedd()
03/03/16 14:28:17 [18930] (110.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/03/16 14:28:17 [18930] (110.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/03/16 14:28:17 [18930] (110.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/03/16 14:28:17 [18930] GAHP[19602] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/109/0/cluster109.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/109/0/cluster109.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/109/0/cluster109.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#110.0#1457033233";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/03/16 14:28:17 [18930] GAHP[19602] -> 'S'
03/03/16 14:28:21 [18930] GAHP[19602] <- 'RESULTS'
03/03/16 14:28:21 [18930] GAHP[19602] -> 'R'
03/03/16 14:28:21 [18930] GAHP[19602] -> 'S' '1'
03/03/16 14:28:21 [18930] GAHP[19602] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:)' 'N/A'
03/03/16 14:28:21 [18930] (110.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/03/16 14:28:21 [18930] (110.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:)
03/03/16 14:28:21 [18930] (110.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/03/16 14:28:21 [18930] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/03/16 14:28:50 [18930] (110.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/03/16 14:28:50 [18930] in doContactSchedd()
03/03/16 14:28:50 [18930] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 14:28:50 [18930] querying for removed/held jobs
03/03/16 14:28:50 [18930] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/03/16 14:28:50 [18930] Fetched 0 job ads from schedd
03/03/16 14:28:50 [18930] Updating classad values for 110.0:
03/03/16 14:28:50 [18930]    CurrentStatusUnknown = false
03/03/16 14:28:50 [18930]    GridJobId = undefined
03/03/16 14:28:50 [18930]    LastRemoteStatusUpdate = 0
03/03/16 14:28:50 [18930] leaving doContactSchedd()
03/03/16 14:28:50 [18930] (110.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/03/16 14:28:50 [18930] (110.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/03/16 14:28:50 [18930] (110.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/03/16 14:28:55 [18930] in doContactSchedd()
03/03/16 14:28:55 [18930] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 14:28:55 [18930] querying for removed/held jobs
03/03/16 14:28:55 [18930] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/03/16 14:28:55 [18930] Fetched 0 job ads from schedd
03/03/16 14:28:55 [18930] Updating classad values for 110.0:
03/03/16 14:28:55 [18930]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#110.0#1457033233"
03/03/16 14:28:55 [18930]    LastRemoteStatusUpdate = 1457033330
03/03/16 14:28:56 [18930] leaving doContactSchedd()
03/03/16 14:28:56 [18930] (110.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/03/16 14:28:56 [18930] (110.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/03/16 14:28:56 [18930] (110.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/03/16 14:28:56 [18930] (110.0) gm state change: GM_HOLD -> GM_DELETE
03/03/16 14:29:01 [18930] in doContactSchedd()
03/03/16 14:29:01 [18930] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 14:29:01 [18930] querying for removed/held jobs
03/03/16 14:29:01 [18930] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/03/16 14:29:01 [18930] Fetched 0 job ads from schedd
03/03/16 14:29:01 [18930] Updating classad values for 110.0:
03/03/16 14:29:01 [18930]    EnteredCurrentStatus = 1457033336
03/03/16 14:29:01 [18930]    HoldReason = "Attempts to submit failed: "
03/03/16 14:29:01 [18930]    HoldReasonCode = 0
03/03/16 14:29:01 [18930]    HoldReasonSubCode = 0
03/03/16 14:29:01 [18930]    JobStatus = 5
03/03/16 14:29:01 [18930]    LastReleaseReason = "Data files spooled"
03/03/16 14:29:01 [18930]    Managed = "Schedd"
03/03/16 14:29:01 [18930]    NumSystemHolds = 1
03/03/16 14:29:01 [18930]    ReleaseReason = undefined
03/03/16 14:29:02 [18930] No jobs left, shutting down
03/03/16 14:29:02 [18930] leaving doContactSchedd()
03/03/16 14:29:02 [18930] Got SIGTERM. Performing graceful shutdown.
03/03/16 14:29:02 [18930] Started timer to call main_shutdown_fast in 1800 seconds
03/03/16 14:29:02 [18930] **** condor_gridmanager (condor_GRIDMANAGER) pid 18930 EXITING WITH STATUS 0
03/03/16 15:10:04 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/03/16 15:10:04 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/03/16 15:10:04 Enumerating interfaces: lo 127.0.0.1 up
03/03/16 15:10:04 Enumerating interfaces: eth2 10.31.131.202 up
03/03/16 15:10:04 Enumerating interfaces: eth3 140.247.179.131 up
03/03/16 15:10:04 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/03/16 15:10:04 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/03/16 15:10:04 ******************************************************
03/03/16 15:10:04 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/03/16 15:10:04 ** /usr/sbin/condor_gridmanager
03/03/16 15:10:04 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/03/16 15:10:04 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/03/16 15:10:04 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/03/16 15:10:04 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/03/16 15:10:04 ** PID = 10671
03/03/16 15:10:04 ** Log last touched 3/3 14:29:02
03/03/16 15:10:04 ******************************************************
03/03/16 15:10:04 Using config source: /etc/condor-ce/condor_config
03/03/16 15:10:04 Using local config sources: 
03/03/16 15:10:04    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/03/16 15:10:04    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/01-ce-auth.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/01-ce-router.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/01-common-auth.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/02-ce-lsf.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/02-ce-pbs.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/03-managed-fork.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/50-osg-configure.conf
03/03/16 15:10:04    /etc/condor-ce/config.d/99-local.conf
03/03/16 15:10:04    /usr/share/condor-ce/condor_ce_router_defaults|
03/03/16 15:10:04 config Macros = 144, Sorted = 144, StringBytes = 12554, TablesBytes = 5392
03/03/16 15:10:04 CLASSAD_CACHING is ENABLED
03/03/16 15:10:04 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/03/16 15:10:04 SharedPortEndpoint: waiting for connections to named socket 6821_c2ee_2
03/03/16 15:10:04 DaemonCore: command socket at <140.247.179.131:9620?sock=6821_c2ee_2>
03/03/16 15:10:04 DaemonCore: private command socket at <140.247.179.131:9620?sock=6821_c2ee_2>
03/03/16 15:10:04 Setting maximum accepts per cycle 8.
03/03/16 15:10:04 Setting maximum reaps per cycle 8.
03/03/16 15:10:04 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/03/16 15:10:04 [10671] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/03/16 15:10:04 [10671] DaemonCore: No more children processes to reap.
03/03/16 15:10:04 [10671] DaemonCore: in SendAliveToParent()
03/03/16 15:10:04 [10671] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 15:10:05 [10671] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/03/16 15:10:05 [10671] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/03/16 15:10:05 [10671] IPVERIFY: ip found is 0
03/03/16 15:10:05 [10671] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/03/16 15:10:05 [10671] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/03/16 15:10:05 [10671] Buf::write(): condor_write() failed
03/03/16 15:10:05 [10671] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/03/16 15:10:05 [10671] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 15:10:05 [10671] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/03/16 15:10:05 [10671] DaemonCore: Leaving SendAliveToParent() - success
03/03/16 15:10:05 [10671] Checking proxies
03/03/16 15:10:06 [10671] Received REMOVE_JOBS signal
03/03/16 15:10:06 [10671] in doContactSchedd()
03/03/16 15:10:06 [10671] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 15:10:06 [10671] querying for new jobs
03/03/16 15:10:06 [10671] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/03/16 15:10:06 [10671] Using job type INFNBatch for job 108.0
03/03/16 15:10:06 [10671] (108.0) SetJobLeaseTimers()
03/03/16 15:10:07 [10671] Found job 108.0 --- inserting
03/03/16 15:10:07 [10671] Fetched 1 new job ads from schedd
03/03/16 15:10:07 [10671] querying for removed/held jobs
03/03/16 15:10:07 [10671] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/03/16 15:10:07 [10671] Fetched 1 job ads from schedd
03/03/16 15:10:07 [10671] leaving doContactSchedd()
03/03/16 15:10:07 [10671] gahp server not up yet, delaying ping
03/03/16 15:10:07 [10671] *** UpdateLeases called
03/03/16 15:10:07 [10671]     Leases not supported, cancelling timer
03/03/16 15:10:07 [10671] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=6821_c2ee_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=5224_8549_6>"
CurrentTime = time()
MyCurrentTime = 1457035807
IdleJobs = 0
JobLimit = 10000

03/03/16 15:10:07 [10671] Trying to update collector <10.31.131.202:9619>
03/03/16 15:10:07 [10671] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/03/16 15:10:07 [10671] File descriptor limits: max 4096, safe 3277
03/03/16 15:10:07 [10671] (108.0) doEvaluateState called: gmState GM_INIT, remoteState -1
03/03/16 15:10:07 [10671] GAHP server pid = 10675
03/03/16 15:10:07 [10671] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/03/16 15:10:07 [10671] GAHP[10675] <- 'COMMANDS'
03/03/16 15:10:07 [10671] GAHP[10675] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/03/16 15:10:07 [10671] GAHP[10675] <- 'ASYNC_MODE_ON'
03/03/16 15:10:07 [10671] GAHP[10675] -> 'S' 'Async mode on'
03/03/16 15:10:07 [10671] (108.0) gm state change: GM_INIT -> GM_START
03/03/16 15:10:07 [10671] (108.0) gm state change: GM_START -> GM_TRANSFER_INPUT
03/03/16 15:10:07 [10671] (108.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/03/16 15:10:07 [10671] GAHP[10675] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#108.0#1456949227";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/03/16 15:10:07 [10671] GAHP[10675] -> 'S'
03/03/16 15:10:07 [10671] Received ADD_JOBS signal
03/03/16 15:10:07 [10671] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/03/16 15:10:07 [10671] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/03/16 15:10:07 [10671] IPVERIFY: ip found is 1
03/03/16 15:10:07 [10671] GAHP[10675] <- 'RESULTS'
03/03/16 15:10:07 [10671] GAHP[10675] -> 'R'
03/03/16 15:10:07 [10671] GAHP[10675] -> 'S' '1'
03/03/16 15:10:07 [10671] GAHP[10675] -> '2' '1' 'Unable to limit the proxy (Unable to read proxy file (/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0/x509up_u556792): errno=13, Permission denied)' 'N/A'
03/03/16 15:10:07 [10671] (108.0) doEvaluateState called: gmState GM_SUBMIT, remoteState -1
03/03/16 15:10:07 [10671] (108.0) blah_job_submit() failed: Unable to limit the proxy (Unable to read proxy file (/n/atlasgrid/condor/107/0/cluster107.proc0.subproc0/x509up_u556792): errno=13, Permission denied)
03/03/16 15:10:07 [10671] (108.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/03/16 15:10:07 [10671] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/03/16 15:10:07 [10671] (108.0) gm state change: GM_DELETE_SANDBOX -> GM_DELETE
03/03/16 15:10:09 [10671] Evaluating staleness of remote job statuses.
03/03/16 15:10:12 [10671] resource  is now up
03/03/16 15:10:12 [10671] in doContactSchedd()
03/03/16 15:10:12 [10671] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 15:10:27 [10671] condor_read(): timeout reading 21 bytes from schedd at <140.247.179.131:9620>.
03/03/16 15:10:27 [10671] IO: Failed to read packet header
03/03/16 15:10:27 [10671] SetEffectiveOwner(usatlas1) failed with errno=110: Connection timed out.
03/03/16 15:10:27 [10671] Failed to connect to schedd! Will retry
03/03/16 15:10:27 [10671] (108.0) doEvaluateState called: gmState GM_DELETE, remoteState -1
03/03/16 15:10:32 [10671] in doContactSchedd()
03/03/16 15:10:32 [10671] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 5224_8549_6
03/03/16 15:10:32 [10671] querying for new jobs
03/03/16 15:10:32 [10671] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (Matched =!= FALSE) && (JobStatus != 5) && (Managed =!= "External")
03/03/16 15:10:32 [10671] Fetched 0 new job ads from schedd
03/03/16 15:10:32 [10671] querying for removed/held jobs
03/03/16 15:10:32 [10671] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/03/16 15:10:32 [10671] Fetched 1 job ads from schedd
03/03/16 15:10:32 [10671] Updating classad values for 108.0:
03/03/16 15:10:32 [10671]    CurrentStatusUnknown = false
03/03/16 15:10:32 [10671]    GridJobId = undefined
03/03/16 15:10:32 [10671]    LastRemoteStatusUpdate = 0
03/03/16 15:10:32 [10671]    Managed = "ScheddDone"
03/03/16 15:10:32 [10671] Deleting job 108.0 from schedd
03/03/16 15:10:32 [10671] No jobs left, shutting down
03/03/16 15:10:32 [10671] leaving doContactSchedd()
03/03/16 15:10:32 [10671] Got SIGTERM. Performing graceful shutdown.
03/03/16 15:10:32 [10671] Started timer to call main_shutdown_fast in 1800 seconds
03/03/16 15:10:32 [10671] **** condor_gridmanager (condor_GRIDMANAGER) pid 10671 EXITING WITH STATUS 0
03/04/16 10:55:04 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 10:55:04 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 10:55:04 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 10:55:04 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 10:55:04 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 10:55:04 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 10:55:04 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 10:55:04 ******************************************************
03/04/16 10:55:04 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 10:55:04 ** /usr/sbin/condor_gridmanager
03/04/16 10:55:04 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 10:55:04 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 10:55:04 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 10:55:04 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 10:55:04 ** PID = 8807
03/04/16 10:55:04 ** Log last touched 3/3 15:10:32
03/04/16 10:55:04 ******************************************************
03/04/16 10:55:04 Using config source: /etc/condor-ce/condor_config
03/04/16 10:55:04 Using local config sources: 
03/04/16 10:55:04    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 10:55:04    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 10:55:04    /etc/condor-ce/config.d/99-local.conf
03/04/16 10:55:04    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 10:55:04 config Macros = 144, Sorted = 144, StringBytes = 12556, TablesBytes = 5392
03/04/16 10:55:04 CLASSAD_CACHING is ENABLED
03/04/16 10:55:04 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 10:55:04 SharedPortEndpoint: waiting for connections to named socket 15983_3823_1
03/04/16 10:55:04 DaemonCore: command socket at <140.247.179.131:9620?sock=15983_3823_1>
03/04/16 10:55:04 DaemonCore: private command socket at <140.247.179.131:9620?sock=15983_3823_1>
03/04/16 10:55:04 Setting maximum accepts per cycle 8.
03/04/16 10:55:04 Setting maximum reaps per cycle 8.
03/04/16 10:55:04 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 10:55:04 [8807] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 10:55:04 [8807] DaemonCore: No more children processes to reap.
03/04/16 10:55:04 [8807] DaemonCore: in SendAliveToParent()
03/04/16 10:55:04 [8807] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15969_db4e_4
03/04/16 10:55:04 [8807] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 10:55:04 [8807] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 10:55:04 [8807] IPVERIFY: ip found is 0
03/04/16 10:55:04 [8807] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 10:55:04 [8807] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 10:55:04 [8807] Buf::write(): condor_write() failed
03/04/16 10:55:04 [8807] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 10:55:04 [8807] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 15969_db4e_4
03/04/16 10:55:04 [8807] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 10:55:04 [8807] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 10:55:04 [8807] Checking proxies
03/04/16 10:55:07 [8807] Received ADD_JOBS signal
03/04/16 10:55:07 [8807] in doContactSchedd()
03/04/16 10:55:07 [8807] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15969_db4e_4
03/04/16 10:55:07 [8807] querying for new jobs
03/04/16 10:55:07 [8807] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 10:55:07 [8807] Using job type INFNBatch for job 112.0
03/04/16 10:55:07 [8807] (112.0) SetJobLeaseTimers()
03/04/16 10:55:07 [8807] Found job 112.0 --- inserting
03/04/16 10:55:07 [8807] Fetched 1 new job ads from schedd
03/04/16 10:55:07 [8807] querying for removed/held jobs
03/04/16 10:55:07 [8807] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 10:55:07 [8807] Fetched 0 job ads from schedd
03/04/16 10:55:07 [8807] leaving doContactSchedd()
03/04/16 10:55:07 [8807] gahp server not up yet, delaying ping
03/04/16 10:55:07 [8807] *** UpdateLeases called
03/04/16 10:55:07 [8807]     Leases not supported, cancelling timer
03/04/16 10:55:07 [8807] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=15983_3823_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=15969_db4e_4>"
CurrentTime = time()
MyCurrentTime = 1457106907
IdleJobs = 1
JobLimit = 10000

03/04/16 10:55:07 [8807] Trying to update collector <10.31.131.202:9619>
03/04/16 10:55:07 [8807] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 10:55:07 [8807] File descriptor limits: max 4096, safe 3277
03/04/16 10:55:07 [8807] (112.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 10:55:07 [8807] GAHP server pid = 8836
03/04/16 10:55:07 [8807] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 10:55:07 [8807] GAHP[8836] <- 'COMMANDS'
03/04/16 10:55:07 [8807] GAHP[8836] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 10:55:07 [8807] GAHP[8836] <- 'ASYNC_MODE_ON'
03/04/16 10:55:07 [8807] GAHP[8836] -> 'S' 'Async mode on'
03/04/16 10:55:07 [8807] (112.0) gm state change: GM_INIT -> GM_START
03/04/16 10:55:07 [8807] (112.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 10:55:07 [8807] (112.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 10:55:07 [8807] (112.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 10:55:07 [8807] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 10:55:07 [8807] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 10:55:07 [8807] IPVERIFY: ip found is 1
03/04/16 10:55:09 [8807] Evaluating staleness of remote job statuses.
03/04/16 10:55:12 [8807] resource  is now up
03/04/16 10:55:12 [8807] in doContactSchedd()
03/04/16 10:55:12 [8807] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15969_db4e_4
03/04/16 10:55:12 [8807] querying for removed/held jobs
03/04/16 10:55:12 [8807] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 10:55:12 [8807] Fetched 0 job ads from schedd
03/04/16 10:55:12 [8807] Updating classad values for 112.0:
03/04/16 10:55:12 [8807]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864"
03/04/16 10:55:12 [8807]    LastRemoteStatusUpdate = 1457106907
03/04/16 10:55:12 [8807] leaving doContactSchedd()
03/04/16 10:55:12 [8807] (112.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 10:55:12 [8807] (112.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 10:55:12 [8807] (112.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 10:55:12 [8807] GAHP[8836] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 10:55:12 [8807] GAHP[8836] -> 'S'
03/04/16 10:55:16 [8807] GAHP[8836] <- 'RESULTS'
03/04/16 10:55:16 [8807] GAHP[8836] -> 'R'
03/04/16 10:55:16 [8807] GAHP[8836] -> 'S' '1'
03/04/16 10:55:16 [8807] GAHP[8836] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ logpath=/server_logs-+ '[' '!' -d /server_logs -o '!' -x /server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b43c311d600.15983 -C /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b43c311d600.15983 -C /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b43c311d600.15983-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b43c311d600.15983 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b43c311d600.15983 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b43c311d600.15983 -o '!' -w /tmp/condor_g_scratch.0x2b43c311d600.15983 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' 54 f6 a4 f1 68 9b'-+ bls_tmp_name=bl_54f6a4f1689b-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-++ touch /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-++ chmod 600 /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=9036-++ date +%s-+ uni_time=1457106914-+ uni_ext=34905.9036.1457106914-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_54f6a4f1689b.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt bl_54f6a4f1689b.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_54f6a4f1689b.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_54f6a4f1689b.proxy"'-++ bls_inputcopy_remote_0=bl_54f6a4f1689b.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 ']'-+ cat /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-+ /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_54f6a4f1689b.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_54f6a4f1689b.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_54f6a4f1689b.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_54f6a4f1689b.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_54f6a4f1689b.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_54f6a4f1689b.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_54f6a4f1689b.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_54f6a4f1689b.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_54f6a4f1689b.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_54f6a4f1689b.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_54f6a4f1689b.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_54f6a4f1689b.proxy" 2> /dev/null' ']'-+ echo rm '"bl_54f6a4f1689b.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-+ exit 1-)' 'N/A'
03/04/16 10:55:16 [8807] (112.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 10:55:16 [8807] (112.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ logpath=/server_logs-+ '[' '!' -d /server_logs -o '!' -x /server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b43c311d600.15983 -C /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b43c311d600.15983 -C /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b43c311d600.15983-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b43c311d600.15983 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b43c311d600.15983 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b43c311d600.15983 -o '!' -w /tmp/condor_g_scratch.0x2b43c311d600.15983 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' 54 f6 a4 f1 68 9b'-+ bls_tmp_name=bl_54f6a4f1689b-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-++ touch /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-++ chmod 600 /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=9036-++ date +%s-+ uni_time=1457106914-+ uni_ext=34905.9036.1457106914-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_54f6a4f1689b.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt bl_54f6a4f1689b.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_54f6a4f1689b.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_54f6a4f1689b.proxy"'-++ bls_inputcopy_remote_0=bl_54f6a4f1689b.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407 ']'-+ cat /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-+ /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b43c311d600.15983/ce-req-file-1457106914719407-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_54f6a4f1689b.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_54f6a4f1689b.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_54f6a4f1689b.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_54f6a4f1689b.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_54f6a4f1689b.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_54f6a4f1689b.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_54f6a4f1689b.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_54f6a4f1689b.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_54f6a4f1689b.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_54f6a4f1689b.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_54f6a4f1689b.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_54f6a4f1689b.proxy" 2> /dev/null' ']'-+ echo rm '"bl_54f6a4f1689b.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/111/0/cluster111.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/111/0/cluster111.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b43c311d600.15983/bl_54f6a4f1689b-+ exit 1-)
03/04/16 10:55:16 [8807] (112.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/04/16 10:55:16 [8807] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/04/16 10:55:25 [8807] (112.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/04/16 10:55:25 [8807] in doContactSchedd()
03/04/16 10:55:25 [8807] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15969_db4e_4
03/04/16 10:55:25 [8807] querying for removed/held jobs
03/04/16 10:55:25 [8807] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 10:55:25 [8807] Fetched 0 job ads from schedd
03/04/16 10:55:25 [8807] Updating classad values for 112.0:
03/04/16 10:55:25 [8807]    CurrentStatusUnknown = false
03/04/16 10:55:25 [8807]    GridJobId = undefined
03/04/16 10:55:25 [8807]    LastRemoteStatusUpdate = 0
03/04/16 10:55:25 [8807] leaving doContactSchedd()
03/04/16 10:55:25 [8807] (112.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/04/16 10:55:25 [8807] (112.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 10:55:25 [8807] (112.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 10:55:30 [8807] in doContactSchedd()
03/04/16 10:55:30 [8807] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15969_db4e_4
03/04/16 10:55:30 [8807] querying for removed/held jobs
03/04/16 10:55:30 [8807] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 10:55:30 [8807] Fetched 0 job ads from schedd
03/04/16 10:55:30 [8807] Updating classad values for 112.0:
03/04/16 10:55:30 [8807]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#112.0#1457106864"
03/04/16 10:55:30 [8807]    LastRemoteStatusUpdate = 1457106925
03/04/16 10:55:30 [8807] leaving doContactSchedd()
03/04/16 10:55:30 [8807] (112.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 10:55:30 [8807] (112.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 10:55:30 [8807] (112.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/04/16 10:55:30 [8807] (112.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 10:55:35 [8807] in doContactSchedd()
03/04/16 10:55:35 [8807] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 15969_db4e_4
03/04/16 10:55:35 [8807] querying for removed/held jobs
03/04/16 10:55:35 [8807] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 10:55:35 [8807] Fetched 0 job ads from schedd
03/04/16 10:55:35 [8807] Updating classad values for 112.0:
03/04/16 10:55:35 [8807]    EnteredCurrentStatus = 1457106930
03/04/16 10:55:35 [8807]    HoldReason = "Attempts to submit failed: "
03/04/16 10:55:35 [8807]    HoldReasonCode = 0
03/04/16 10:55:35 [8807]    HoldReasonSubCode = 0
03/04/16 10:55:35 [8807]    JobStatus = 5
03/04/16 10:55:35 [8807]    LastReleaseReason = "Data files spooled"
03/04/16 10:55:35 [8807]    Managed = "Schedd"
03/04/16 10:55:35 [8807]    NumSystemHolds = 1
03/04/16 10:55:35 [8807]    ReleaseReason = undefined
03/04/16 10:55:35 [8807] No jobs left, shutting down
03/04/16 10:55:35 [8807] leaving doContactSchedd()
03/04/16 10:55:35 [8807] Got SIGTERM. Performing graceful shutdown.
03/04/16 10:55:35 [8807] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 10:55:35 [8807] **** condor_gridmanager (condor_GRIDMANAGER) pid 8807 EXITING WITH STATUS 0
03/04/16 11:04:11 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 11:04:11 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 11:04:11 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 11:04:11 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 11:04:11 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 11:04:11 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 11:04:11 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 11:04:11 ******************************************************
03/04/16 11:04:11 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 11:04:11 ** /usr/sbin/condor_gridmanager
03/04/16 11:04:11 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 11:04:11 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 11:04:11 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 11:04:11 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 11:04:11 ** PID = 4197
03/04/16 11:04:11 ** Log last touched 3/4 10:55:35
03/04/16 11:04:11 ******************************************************
03/04/16 11:04:11 Using config source: /etc/condor-ce/condor_config
03/04/16 11:04:11 Using local config sources: 
03/04/16 11:04:11    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 11:04:11    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 11:04:11    /etc/condor-ce/config.d/99-local.conf
03/04/16 11:04:11    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 11:04:11 config Macros = 144, Sorted = 144, StringBytes = 12551, TablesBytes = 5392
03/04/16 11:04:11 CLASSAD_CACHING is ENABLED
03/04/16 11:04:11 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 11:04:11 SharedPortEndpoint: waiting for connections to named socket 3859_c3b6_1
03/04/16 11:04:11 DaemonCore: command socket at <140.247.179.131:9620?sock=3859_c3b6_1>
03/04/16 11:04:11 DaemonCore: private command socket at <140.247.179.131:9620?sock=3859_c3b6_1>
03/04/16 11:04:11 Setting maximum accepts per cycle 8.
03/04/16 11:04:11 Setting maximum reaps per cycle 8.
03/04/16 11:04:11 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 11:04:11 [4197] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 11:04:11 [4197] DaemonCore: No more children processes to reap.
03/04/16 11:04:11 [4197] DaemonCore: in SendAliveToParent()
03/04/16 11:04:11 [4197] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3851_5f93_4
03/04/16 11:04:11 [4197] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 11:04:11 [4197] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 11:04:11 [4197] IPVERIFY: ip found is 0
03/04/16 11:04:11 [4197] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 11:04:11 [4197] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 11:04:11 [4197] Buf::write(): condor_write() failed
03/04/16 11:04:11 [4197] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 11:04:11 [4197] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 3851_5f93_4
03/04/16 11:04:11 [4197] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 11:04:11 [4197] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 11:04:11 [4197] Checking proxies
03/04/16 11:04:14 [4197] Received ADD_JOBS signal
03/04/16 11:04:14 [4197] in doContactSchedd()
03/04/16 11:04:14 [4197] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3851_5f93_4
03/04/16 11:04:14 [4197] querying for new jobs
03/04/16 11:04:14 [4197] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 11:04:14 [4197] Using job type INFNBatch for job 114.0
03/04/16 11:04:14 [4197] (114.0) SetJobLeaseTimers()
03/04/16 11:04:14 [4197] Found job 114.0 --- inserting
03/04/16 11:04:14 [4197] Fetched 1 new job ads from schedd
03/04/16 11:04:14 [4197] querying for removed/held jobs
03/04/16 11:04:14 [4197] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:04:14 [4197] Fetched 0 job ads from schedd
03/04/16 11:04:14 [4197] leaving doContactSchedd()
03/04/16 11:04:14 [4197] gahp server not up yet, delaying ping
03/04/16 11:04:14 [4197] *** UpdateLeases called
03/04/16 11:04:14 [4197]     Leases not supported, cancelling timer
03/04/16 11:04:14 [4197] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=3859_c3b6_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=3851_5f93_4>"
CurrentTime = time()
MyCurrentTime = 1457107454
IdleJobs = 1
JobLimit = 10000

03/04/16 11:04:14 [4197] Trying to update collector <10.31.131.202:9619>
03/04/16 11:04:14 [4197] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 11:04:14 [4197] File descriptor limits: max 4096, safe 3277
03/04/16 11:04:14 [4197] (114.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 11:04:14 [4197] GAHP server pid = 4385
03/04/16 11:04:14 [4197] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 11:04:14 [4197] GAHP[4385] <- 'COMMANDS'
03/04/16 11:04:14 [4197] GAHP[4385] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 11:04:14 [4197] GAHP[4385] <- 'ASYNC_MODE_ON'
03/04/16 11:04:14 [4197] GAHP[4385] -> 'S' 'Async mode on'
03/04/16 11:04:14 [4197] (114.0) gm state change: GM_INIT -> GM_START
03/04/16 11:04:14 [4197] (114.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 11:04:14 [4197] (114.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 11:04:14 [4197] (114.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 11:04:14 [4197] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 11:04:14 [4197] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 11:04:14 [4197] IPVERIFY: ip found is 1
03/04/16 11:04:16 [4197] Evaluating staleness of remote job statuses.
03/04/16 11:04:19 [4197] resource  is now up
03/04/16 11:04:19 [4197] in doContactSchedd()
03/04/16 11:04:19 [4197] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3851_5f93_4
03/04/16 11:04:19 [4197] querying for removed/held jobs
03/04/16 11:04:19 [4197] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:04:19 [4197] Fetched 0 job ads from schedd
03/04/16 11:04:19 [4197] Updating classad values for 114.0:
03/04/16 11:04:19 [4197]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448"
03/04/16 11:04:19 [4197]    LastRemoteStatusUpdate = 1457107454
03/04/16 11:04:19 [4197] leaving doContactSchedd()
03/04/16 11:04:19 [4197] (114.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 11:04:19 [4197] (114.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 11:04:19 [4197] (114.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 11:04:19 [4197] GAHP[4385] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 11:04:19 [4197] GAHP[4385] -> 'S'
03/04/16 11:04:20 [4197] GAHP[4385] <- 'RESULTS'
03/04/16 11:04:20 [4197] GAHP[4385] -> 'R'
03/04/16 11:04:20 [4197] GAHP[4385] -> 'S' '1'
03/04/16 11:04:20 [4197] GAHP[4385] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2acd2fdb0820.3859 -C /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2acd2fdb0820.3859 -C /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2acd2fdb0820.3859-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2acd2fdb0820.3859 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2acd2fdb0820.3859 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2acd2fdb0820.3859 -o '!' -w /tmp/condor_g_scratch.0x2acd2fdb0820.3859 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' dd 42 43 c0 35 f3'-+ bls_tmp_name=bl_dd4243c035f3-+ bls_tmp_file=/tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-++ touch /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-++ chmod 600 /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=4620-++ date +%s-+ uni_time=1457107459-+ uni_ext=34905.4620.1457107459-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_dd4243c035f3.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt bl_dd4243c035f3.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_dd4243c035f3.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_dd4243c035f3.proxy"'-++ bls_inputcopy_remote_0=bl_dd4243c035f3.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 ']'-+ cat /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-+ /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_dd4243c035f3.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_dd4243c035f3.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_dd4243c035f3.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_dd4243c035f3.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_dd4243c035f3.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_dd4243c035f3.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_dd4243c035f3.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_dd4243c035f3.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_dd4243c035f3.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_dd4243c035f3.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_dd4243c035f3.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_dd4243c035f3.proxy" 2> /dev/null' ']'-+ echo rm '"bl_dd4243c035f3.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-+ exit 1-)' 'N/A'
03/04/16 11:04:20 [4197] (114.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 11:04:20 [4197] (114.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2acd2fdb0820.3859 -C /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2acd2fdb0820.3859 -C /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2acd2fdb0820.3859-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2acd2fdb0820.3859 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2acd2fdb0820.3859 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2acd2fdb0820.3859 -o '!' -w /tmp/condor_g_scratch.0x2acd2fdb0820.3859 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' dd 42 43 c0 35 f3'-+ bls_tmp_name=bl_dd4243c035f3-+ bls_tmp_file=/tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-++ touch /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-++ chmod 600 /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=4620-++ date +%s-+ uni_time=1457107459-+ uni_ext=34905.4620.1457107459-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_dd4243c035f3.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt bl_dd4243c035f3.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_dd4243c035f3.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_dd4243c035f3.proxy"'-++ bls_inputcopy_remote_0=bl_dd4243c035f3.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292 ']'-+ cat /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-+ /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2acd2fdb0820.3859/ce-req-file-1457107459664292-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_dd4243c035f3.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_dd4243c035f3.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_dd4243c035f3.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_dd4243c035f3.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_dd4243c035f3.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_dd4243c035f3.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_dd4243c035f3.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_dd4243c035f3.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_dd4243c035f3.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_dd4243c035f3.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_dd4243c035f3.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_dd4243c035f3.proxy" 2> /dev/null' ']'-+ echo rm '"bl_dd4243c035f3.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/113/0/cluster113.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/113/0/cluster113.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2acd2fdb0820.3859/bl_dd4243c035f3-+ exit 1-)
03/04/16 11:04:20 [4197] (114.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/04/16 11:04:20 [4197] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/04/16 11:04:21 [4197] (114.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/04/16 11:04:24 [4197] in doContactSchedd()
03/04/16 11:04:24 [4197] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3851_5f93_4
03/04/16 11:04:24 [4197] querying for removed/held jobs
03/04/16 11:04:24 [4197] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:04:24 [4197] Fetched 0 job ads from schedd
03/04/16 11:04:24 [4197] Updating classad values for 114.0:
03/04/16 11:04:24 [4197]    CurrentStatusUnknown = false
03/04/16 11:04:24 [4197]    GridJobId = undefined
03/04/16 11:04:24 [4197]    LastRemoteStatusUpdate = 0
03/04/16 11:04:24 [4197] leaving doContactSchedd()
03/04/16 11:04:24 [4197] (114.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/04/16 11:04:24 [4197] (114.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 11:04:24 [4197] (114.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 11:04:29 [4197] in doContactSchedd()
03/04/16 11:04:29 [4197] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3851_5f93_4
03/04/16 11:04:29 [4197] querying for removed/held jobs
03/04/16 11:04:29 [4197] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:04:29 [4197] Fetched 0 job ads from schedd
03/04/16 11:04:29 [4197] Updating classad values for 114.0:
03/04/16 11:04:29 [4197]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#114.0#1457107448"
03/04/16 11:04:29 [4197]    LastRemoteStatusUpdate = 1457107464
03/04/16 11:04:29 [4197] leaving doContactSchedd()
03/04/16 11:04:29 [4197] (114.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 11:04:29 [4197] (114.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 11:04:29 [4197] (114.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/04/16 11:04:29 [4197] (114.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 11:04:34 [4197] in doContactSchedd()
03/04/16 11:04:34 [4197] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 3851_5f93_4
03/04/16 11:04:34 [4197] querying for removed/held jobs
03/04/16 11:04:34 [4197] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:04:34 [4197] Fetched 0 job ads from schedd
03/04/16 11:04:34 [4197] Updating classad values for 114.0:
03/04/16 11:04:34 [4197]    EnteredCurrentStatus = 1457107469
03/04/16 11:04:34 [4197]    HoldReason = "Attempts to submit failed: "
03/04/16 11:04:34 [4197]    HoldReasonCode = 0
03/04/16 11:04:34 [4197]    HoldReasonSubCode = 0
03/04/16 11:04:34 [4197]    JobStatus = 5
03/04/16 11:04:34 [4197]    LastReleaseReason = "Data files spooled"
03/04/16 11:04:34 [4197]    Managed = "Schedd"
03/04/16 11:04:34 [4197]    NumSystemHolds = 1
03/04/16 11:04:34 [4197]    ReleaseReason = undefined
03/04/16 11:04:34 [4197] No jobs left, shutting down
03/04/16 11:04:34 [4197] leaving doContactSchedd()
03/04/16 11:04:34 [4197] Got SIGTERM. Performing graceful shutdown.
03/04/16 11:04:34 [4197] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 11:04:34 [4197] **** condor_gridmanager (condor_GRIDMANAGER) pid 4197 EXITING WITH STATUS 0
03/04/16 11:25:11 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 11:25:11 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 11:25:11 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 11:25:11 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 11:25:11 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 11:25:11 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 11:25:11 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 11:25:11 ******************************************************
03/04/16 11:25:11 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 11:25:11 ** /usr/sbin/condor_gridmanager
03/04/16 11:25:11 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 11:25:11 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 11:25:11 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 11:25:11 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 11:25:11 ** PID = 5175
03/04/16 11:25:11 ** Log last touched 3/4 11:04:34
03/04/16 11:25:11 ******************************************************
03/04/16 11:25:11 Using config source: /etc/condor-ce/condor_config
03/04/16 11:25:11 Using local config sources: 
03/04/16 11:25:11    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 11:25:11    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 11:25:11    /etc/condor-ce/config.d/99-local.conf
03/04/16 11:25:11    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 11:25:11 config Macros = 144, Sorted = 144, StringBytes = 12556, TablesBytes = 5392
03/04/16 11:25:11 CLASSAD_CACHING is ENABLED
03/04/16 11:25:11 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 11:25:11 SharedPortEndpoint: waiting for connections to named socket 32379_af3e_1
03/04/16 11:25:11 DaemonCore: command socket at <140.247.179.131:9620?sock=32379_af3e_1>
03/04/16 11:25:11 DaemonCore: private command socket at <140.247.179.131:9620?sock=32379_af3e_1>
03/04/16 11:25:11 Setting maximum accepts per cycle 8.
03/04/16 11:25:11 Setting maximum reaps per cycle 8.
03/04/16 11:25:11 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 11:25:11 [5175] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 11:25:11 [5175] DaemonCore: No more children processes to reap.
03/04/16 11:25:11 [5175] DaemonCore: in SendAliveToParent()
03/04/16 11:25:11 [5175] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:25:11 [5175] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 11:25:11 [5175] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 11:25:11 [5175] IPVERIFY: ip found is 0
03/04/16 11:25:11 [5175] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 11:25:11 [5175] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 11:25:11 [5175] Buf::write(): condor_write() failed
03/04/16 11:25:11 [5175] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 11:25:11 [5175] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:25:11 [5175] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 11:25:11 [5175] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 11:25:11 [5175] Checking proxies
03/04/16 11:25:14 [5175] Received ADD_JOBS signal
03/04/16 11:25:14 [5175] in doContactSchedd()
03/04/16 11:25:14 [5175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:25:14 [5175] querying for new jobs
03/04/16 11:25:14 [5175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 11:25:14 [5175] Using job type INFNBatch for job 116.0
03/04/16 11:25:14 [5175] (116.0) SetJobLeaseTimers()
03/04/16 11:25:14 [5175] Found job 116.0 --- inserting
03/04/16 11:25:14 [5175] Fetched 1 new job ads from schedd
03/04/16 11:25:14 [5175] querying for removed/held jobs
03/04/16 11:25:14 [5175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:25:14 [5175] Fetched 0 job ads from schedd
03/04/16 11:25:14 [5175] leaving doContactSchedd()
03/04/16 11:25:14 [5175] gahp server not up yet, delaying ping
03/04/16 11:25:14 [5175] *** UpdateLeases called
03/04/16 11:25:14 [5175]     Leases not supported, cancelling timer
03/04/16 11:25:14 [5175] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_1>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457108714
IdleJobs = 1
JobLimit = 10000

03/04/16 11:25:14 [5175] Trying to update collector <10.31.131.202:9619>
03/04/16 11:25:14 [5175] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 11:25:14 [5175] File descriptor limits: max 4096, safe 3277
03/04/16 11:25:14 [5175] (116.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 11:25:14 [5175] GAHP server pid = 5374
03/04/16 11:25:14 [5175] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 11:25:14 [5175] GAHP[5374] <- 'COMMANDS'
03/04/16 11:25:14 [5175] GAHP[5374] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 11:25:14 [5175] GAHP[5374] <- 'ASYNC_MODE_ON'
03/04/16 11:25:14 [5175] GAHP[5374] -> 'S' 'Async mode on'
03/04/16 11:25:14 [5175] (116.0) gm state change: GM_INIT -> GM_START
03/04/16 11:25:14 [5175] (116.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 11:25:14 [5175] (116.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 11:25:14 [5175] (116.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 11:25:14 [5175] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 11:25:14 [5175] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 11:25:14 [5175] IPVERIFY: ip found is 1
03/04/16 11:25:16 [5175] Evaluating staleness of remote job statuses.
03/04/16 11:25:19 [5175] resource  is now up
03/04/16 11:25:19 [5175] in doContactSchedd()
03/04/16 11:25:19 [5175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:25:19 [5175] querying for removed/held jobs
03/04/16 11:25:19 [5175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:25:19 [5175] Fetched 0 job ads from schedd
03/04/16 11:25:19 [5175] Updating classad values for 116.0:
03/04/16 11:25:19 [5175]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707"
03/04/16 11:25:19 [5175]    LastRemoteStatusUpdate = 1457108714
03/04/16 11:25:19 [5175] leaving doContactSchedd()
03/04/16 11:25:19 [5175] (116.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 11:25:19 [5175] (116.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 11:25:19 [5175] (116.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 11:25:19 [5175] GAHP[5374] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 11:25:19 [5175] GAHP[5374] -> 'S'
03/04/16 11:25:20 [5175] GAHP[5374] <- 'RESULTS'
03/04/16 11:25:20 [5175] GAHP[5374] -> 'R'
03/04/16 11:25:20 [5175] GAHP[5374] -> 'S' '1'
03/04/16 11:25:20 [5175] GAHP[5374] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 -C /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 -C /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' 51 71 05 ce ad 57'-+ bls_tmp_name=bl_517105cead57-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-++ touch /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=5596-++ date +%s-+ uni_time=1457108719-+ uni_ext=34905.5596.1457108719-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_517105cead57.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt bl_517105cead57.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_517105cead57.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_517105cead57.proxy"'-++ bls_inputcopy_remote_0=bl_517105cead57.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_517105cead57.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_517105cead57.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_517105cead57.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_517105cead57.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_517105cead57.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_517105cead57.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_517105cead57.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_517105cead57.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_517105cead57.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_517105cead57.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_517105cead57.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_517105cead57.proxy" 2> /dev/null' ']'-+ echo rm '"bl_517105cead57.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-+ exit 1-)' 'N/A'
03/04/16 11:25:20 [5175] (116.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 11:25:20 [5175] (116.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 -C /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 -C /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' 51 71 05 ce ad 57'-+ bls_tmp_name=bl_517105cead57-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-++ touch /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=5596-++ date +%s-+ uni_time=1457108719-+ uni_ext=34905.5596.1457108719-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_517105cead57.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt bl_517105cead57.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_517105cead57.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_517105cead57.proxy"'-++ bls_inputcopy_remote_0=bl_517105cead57.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/ce-req-file-1457108719260950-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_517105cead57.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_517105cead57.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_517105cead57.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_517105cead57.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_517105cead57.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_517105cead57.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_517105cead57.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_517105cead57.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_517105cead57.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_517105cead57.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_517105cead57.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_517105cead57.proxy" 2> /dev/null' ']'-+ echo rm '"bl_517105cead57.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/115/0/cluster115.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/115/0/cluster115.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54f9ef0.32379/bl_517105cead57-+ exit 1-)
03/04/16 11:25:20 [5175] (116.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/04/16 11:25:20 [5175] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/04/16 11:25:20 [5175] (116.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/04/16 11:25:24 [5175] in doContactSchedd()
03/04/16 11:25:24 [5175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:25:24 [5175] querying for removed/held jobs
03/04/16 11:25:24 [5175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:25:24 [5175] Fetched 0 job ads from schedd
03/04/16 11:25:24 [5175] Updating classad values for 116.0:
03/04/16 11:25:24 [5175]    CurrentStatusUnknown = false
03/04/16 11:25:24 [5175]    GridJobId = undefined
03/04/16 11:25:24 [5175]    LastRemoteStatusUpdate = 0
03/04/16 11:25:26 [5175] leaving doContactSchedd()
03/04/16 11:25:26 [5175] (116.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/04/16 11:25:26 [5175] (116.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 11:25:26 [5175] (116.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 11:25:31 [5175] in doContactSchedd()
03/04/16 11:25:31 [5175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:25:31 [5175] querying for removed/held jobs
03/04/16 11:25:31 [5175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:25:31 [5175] Fetched 0 job ads from schedd
03/04/16 11:25:31 [5175] Updating classad values for 116.0:
03/04/16 11:25:31 [5175]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#116.0#1457108707"
03/04/16 11:25:31 [5175]    LastRemoteStatusUpdate = 1457108726
03/04/16 11:25:31 [5175] leaving doContactSchedd()
03/04/16 11:25:31 [5175] (116.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 11:25:31 [5175] (116.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 11:25:31 [5175] (116.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/04/16 11:25:31 [5175] (116.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 11:25:36 [5175] in doContactSchedd()
03/04/16 11:25:36 [5175] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:25:36 [5175] querying for removed/held jobs
03/04/16 11:25:36 [5175] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:25:36 [5175] Fetched 0 job ads from schedd
03/04/16 11:25:36 [5175] Updating classad values for 116.0:
03/04/16 11:25:36 [5175]    EnteredCurrentStatus = 1457108731
03/04/16 11:25:36 [5175]    HoldReason = "Attempts to submit failed: "
03/04/16 11:25:36 [5175]    HoldReasonCode = 0
03/04/16 11:25:36 [5175]    HoldReasonSubCode = 0
03/04/16 11:25:36 [5175]    JobStatus = 5
03/04/16 11:25:36 [5175]    LastReleaseReason = "Data files spooled"
03/04/16 11:25:36 [5175]    Managed = "Schedd"
03/04/16 11:25:36 [5175]    NumSystemHolds = 1
03/04/16 11:25:36 [5175]    ReleaseReason = undefined
03/04/16 11:25:37 [5175] No jobs left, shutting down
03/04/16 11:25:37 [5175] leaving doContactSchedd()
03/04/16 11:25:37 [5175] Got SIGTERM. Performing graceful shutdown.
03/04/16 11:25:37 [5175] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 11:25:37 [5175] **** condor_gridmanager (condor_GRIDMANAGER) pid 5175 EXITING WITH STATUS 0
03/04/16 11:56:51 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 11:56:51 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 11:56:51 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 11:56:51 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 11:56:51 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 11:56:51 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 11:56:51 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 11:56:51 ******************************************************
03/04/16 11:56:51 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 11:56:51 ** /usr/sbin/condor_gridmanager
03/04/16 11:56:51 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 11:56:51 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 11:56:51 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 11:56:51 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 11:56:51 ** PID = 10573
03/04/16 11:56:51 ** Log last touched 3/4 11:25:37
03/04/16 11:56:51 ******************************************************
03/04/16 11:56:51 Using config source: /etc/condor-ce/condor_config
03/04/16 11:56:51 Using local config sources: 
03/04/16 11:56:51    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 11:56:51    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 11:56:51    /etc/condor-ce/config.d/99-local.conf
03/04/16 11:56:51    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 11:56:51 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
03/04/16 11:56:51 CLASSAD_CACHING is ENABLED
03/04/16 11:56:51 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 11:56:51 SharedPortEndpoint: waiting for connections to named socket 32379_af3e_2
03/04/16 11:56:51 DaemonCore: command socket at <140.247.179.131:9620?sock=32379_af3e_2>
03/04/16 11:56:51 DaemonCore: private command socket at <140.247.179.131:9620?sock=32379_af3e_2>
03/04/16 11:56:51 Setting maximum accepts per cycle 8.
03/04/16 11:56:51 Setting maximum reaps per cycle 8.
03/04/16 11:56:51 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 11:56:51 [10573] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 11:56:51 [10573] DaemonCore: No more children processes to reap.
03/04/16 11:56:51 [10573] DaemonCore: in SendAliveToParent()
03/04/16 11:56:51 [10573] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:56:51 [10573] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 11:56:51 [10573] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 11:56:51 [10573] IPVERIFY: ip found is 0
03/04/16 11:56:51 [10573] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 11:56:51 [10573] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 11:56:51 [10573] Buf::write(): condor_write() failed
03/04/16 11:56:51 [10573] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 11:56:51 [10573] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:56:51 [10573] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 11:56:51 [10573] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 11:56:51 [10573] Checking proxies
03/04/16 11:56:54 [10573] Received ADD_JOBS signal
03/04/16 11:56:54 [10573] in doContactSchedd()
03/04/16 11:56:54 [10573] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:56:54 [10573] querying for new jobs
03/04/16 11:56:54 [10573] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 11:56:54 [10573] Using job type INFNBatch for job 118.0
03/04/16 11:56:54 [10573] (118.0) SetJobLeaseTimers()
03/04/16 11:56:54 [10573] Found job 118.0 --- inserting
03/04/16 11:56:54 [10573] Fetched 1 new job ads from schedd
03/04/16 11:56:54 [10573] querying for removed/held jobs
03/04/16 11:56:54 [10573] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:56:54 [10573] Fetched 0 job ads from schedd
03/04/16 11:56:54 [10573] leaving doContactSchedd()
03/04/16 11:56:54 [10573] gahp server not up yet, delaying ping
03/04/16 11:56:54 [10573] *** UpdateLeases called
03/04/16 11:56:54 [10573]     Leases not supported, cancelling timer
03/04/16 11:56:54 [10573] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_2>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457110614
IdleJobs = 1
JobLimit = 10000

03/04/16 11:56:54 [10573] Trying to update collector <10.31.131.202:9619>
03/04/16 11:56:54 [10573] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 11:56:54 [10573] File descriptor limits: max 4096, safe 3277
03/04/16 11:56:54 [10573] (118.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 11:56:54 [10573] GAHP server pid = 11039
03/04/16 11:56:54 [10573] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 11:56:54 [10573] GAHP[11039] <- 'COMMANDS'
03/04/16 11:56:54 [10573] GAHP[11039] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 11:56:54 [10573] GAHP[11039] <- 'ASYNC_MODE_ON'
03/04/16 11:56:54 [10573] GAHP[11039] -> 'S' 'Async mode on'
03/04/16 11:56:54 [10573] (118.0) gm state change: GM_INIT -> GM_START
03/04/16 11:56:54 [10573] (118.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 11:56:54 [10573] (118.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 11:56:54 [10573] (118.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 11:56:54 [10573] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 11:56:54 [10573] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 11:56:54 [10573] IPVERIFY: ip found is 1
03/04/16 11:56:56 [10573] Evaluating staleness of remote job statuses.
03/04/16 11:56:59 [10573] resource  is now up
03/04/16 11:56:59 [10573] in doContactSchedd()
03/04/16 11:56:59 [10573] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:56:59 [10573] querying for removed/held jobs
03/04/16 11:56:59 [10573] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:56:59 [10573] Fetched 0 job ads from schedd
03/04/16 11:56:59 [10573] Updating classad values for 118.0:
03/04/16 11:56:59 [10573]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602"
03/04/16 11:56:59 [10573]    LastRemoteStatusUpdate = 1457110614
03/04/16 11:57:00 [10573] leaving doContactSchedd()
03/04/16 11:57:00 [10573] (118.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 11:57:00 [10573] (118.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 11:57:00 [10573] (118.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 11:57:00 [10573] GAHP[11039] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 11:57:00 [10573] GAHP[11039] -> 'S'
03/04/16 11:57:07 [10573] GAHP[11039] <- 'RESULTS'
03/04/16 11:57:07 [10573] GAHP[11039] -> 'R'
03/04/16 11:57:07 [10573] GAHP[11039] -> 'S' '1'
03/04/16 11:57:07 [10573] GAHP[11039] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ pbs_spoolpath=/var/spool/pbs-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa6499410.32379 -C /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa6499410.32379 -C /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa6499410.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa6499410.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa6499410.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa6499410.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa6499410.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' a4 1f c2 eb d6 82'-+ bls_tmp_name=bl_a41fc2ebd682-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-++ touch /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=11357-++ date +%s-+ uni_time=1457110626-+ uni_ext=34905.11357.1457110626-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_a41fc2ebd682.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt bl_a41fc2ebd682.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_a41fc2ebd682.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_a41fc2ebd682.proxy"'-++ bls_inputcopy_remote_0=bl_a41fc2ebd682.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a41fc2ebd682.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a41fc2ebd682.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a41fc2ebd682.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a41fc2ebd682.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a41fc2ebd682.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_a41fc2ebd682.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_a41fc2ebd682.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a41fc2ebd682.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a41fc2ebd682.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a41fc2ebd682.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_a41fc2ebd682.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_a41fc2ebd682.proxy" 2> /dev/null' ']'-+ echo rm '"bl_a41fc2ebd682.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-+ echo /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-+ exit 1-)' 'N/A'
03/04/16 11:57:07 [10573] (118.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 11:57:07 [10573] (118.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ pbs_spoolpath=/var/spool/pbs-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa6499410.32379 -C /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa6499410.32379 -C /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa6499410.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa6499410.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa6499410.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa6499410.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa6499410.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' a4 1f c2 eb d6 82'-+ bls_tmp_name=bl_a41fc2ebd682-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-++ touch /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=11357-++ date +%s-+ uni_time=1457110626-+ uni_ext=34905.11357.1457110626-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_a41fc2ebd682.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt bl_a41fc2ebd682.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_a41fc2ebd682.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_a41fc2ebd682.proxy"'-++ bls_inputcopy_remote_0=bl_a41fc2ebd682.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa6499410.32379/ce-req-file-1457110626610722-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a41fc2ebd682.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a41fc2ebd682.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a41fc2ebd682.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a41fc2ebd682.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a41fc2ebd682.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_a41fc2ebd682.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_a41fc2ebd682.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a41fc2ebd682.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a41fc2ebd682.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a41fc2ebd682.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_a41fc2ebd682.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_a41fc2ebd682.proxy" 2> /dev/null' ']'-+ echo rm '"bl_a41fc2ebd682.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/117/0/cluster117.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/117/0/cluster117.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-+ echo /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa6499410.32379/bl_a41fc2ebd682-+ exit 1-)
03/04/16 11:57:07 [10573] (118.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/04/16 11:57:07 [10573] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/04/16 11:58:48 [10573] (118.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/04/16 11:58:48 [10573] Received CHECK_LEASES signal
03/04/16 11:58:48 [10573] GAHP[11039] <- 'RESULTS'
03/04/16 11:58:48 [10573] GAHP[11039] -> 'S' '0'
03/04/16 11:58:48 [10573] Evaluating staleness of remote job statuses.
03/04/16 11:58:48 [10573] in doContactSchedd()
03/04/16 11:58:48 [10573] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:58:48 [10573] querying for renewed leases
03/04/16 11:58:48 [10573] querying for removed/held jobs
03/04/16 11:58:48 [10573] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:58:48 [10573] Fetched 0 job ads from schedd
03/04/16 11:58:48 [10573] Updating classad values for 118.0:
03/04/16 11:58:48 [10573]    CurrentStatusUnknown = false
03/04/16 11:58:48 [10573]    GridJobId = undefined
03/04/16 11:58:48 [10573]    LastRemoteStatusUpdate = 0
03/04/16 11:58:48 [10573] leaving doContactSchedd()
03/04/16 11:58:48 [10573] (118.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/04/16 11:58:48 [10573] (118.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 11:58:48 [10573] (118.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 11:58:53 [10573] in doContactSchedd()
03/04/16 11:58:53 [10573] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:58:53 [10573] querying for removed/held jobs
03/04/16 11:58:53 [10573] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:58:53 [10573] Fetched 0 job ads from schedd
03/04/16 11:58:53 [10573] Updating classad values for 118.0:
03/04/16 11:58:53 [10573]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#118.0#1457110602"
03/04/16 11:58:53 [10573]    LastRemoteStatusUpdate = 1457110728
03/04/16 11:58:54 [10573] leaving doContactSchedd()
03/04/16 11:58:54 [10573] (118.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 11:58:54 [10573] (118.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 11:58:54 [10573] (118.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/04/16 11:58:54 [10573] (118.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 11:58:59 [10573] in doContactSchedd()
03/04/16 11:58:59 [10573] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 11:58:59 [10573] querying for removed/held jobs
03/04/16 11:58:59 [10573] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 11:58:59 [10573] Fetched 0 job ads from schedd
03/04/16 11:58:59 [10573] Updating classad values for 118.0:
03/04/16 11:58:59 [10573]    EnteredCurrentStatus = 1457110734
03/04/16 11:58:59 [10573]    HoldReason = "Attempts to submit failed: "
03/04/16 11:58:59 [10573]    HoldReasonCode = 0
03/04/16 11:58:59 [10573]    HoldReasonSubCode = 0
03/04/16 11:58:59 [10573]    JobStatus = 5
03/04/16 11:58:59 [10573]    LastReleaseReason = "Data files spooled"
03/04/16 11:58:59 [10573]    Managed = "Schedd"
03/04/16 11:58:59 [10573]    NumSystemHolds = 1
03/04/16 11:58:59 [10573]    ReleaseReason = undefined
03/04/16 11:58:59 [10573] No jobs left, shutting down
03/04/16 11:58:59 [10573] leaving doContactSchedd()
03/04/16 11:58:59 [10573] Got SIGTERM. Performing graceful shutdown.
03/04/16 11:58:59 [10573] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 11:58:59 [10573] **** condor_gridmanager (condor_GRIDMANAGER) pid 10573 EXITING WITH STATUS 0
03/04/16 12:06:51 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 12:06:51 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 12:06:51 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 12:06:51 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 12:06:51 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 12:06:51 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 12:06:51 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 12:06:51 ******************************************************
03/04/16 12:06:51 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 12:06:51 ** /usr/sbin/condor_gridmanager
03/04/16 12:06:51 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 12:06:51 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 12:06:51 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 12:06:51 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 12:06:51 ** PID = 19609
03/04/16 12:06:51 ** Log last touched 3/4 11:58:59
03/04/16 12:06:51 ******************************************************
03/04/16 12:06:51 Using config source: /etc/condor-ce/condor_config
03/04/16 12:06:51 Using local config sources: 
03/04/16 12:06:51    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 12:06:51    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 12:06:51    /etc/condor-ce/config.d/99-local.conf
03/04/16 12:06:51    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 12:06:51 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
03/04/16 12:06:51 CLASSAD_CACHING is ENABLED
03/04/16 12:06:51 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 12:06:51 SharedPortEndpoint: waiting for connections to named socket 32379_af3e_3
03/04/16 12:06:51 DaemonCore: command socket at <140.247.179.131:9620?sock=32379_af3e_3>
03/04/16 12:06:51 DaemonCore: private command socket at <140.247.179.131:9620?sock=32379_af3e_3>
03/04/16 12:06:51 Setting maximum accepts per cycle 8.
03/04/16 12:06:51 Setting maximum reaps per cycle 8.
03/04/16 12:06:51 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 12:06:51 [19609] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 12:06:51 [19609] DaemonCore: No more children processes to reap.
03/04/16 12:06:51 [19609] DaemonCore: in SendAliveToParent()
03/04/16 12:06:51 [19609] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:06:51 [19609] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 12:06:51 [19609] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 12:06:51 [19609] IPVERIFY: ip found is 0
03/04/16 12:06:51 [19609] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 12:06:51 [19609] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 12:06:51 [19609] Buf::write(): condor_write() failed
03/04/16 12:06:51 [19609] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 12:06:51 [19609] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:06:51 [19609] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 12:06:51 [19609] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 12:06:51 [19609] Checking proxies
03/04/16 12:06:54 [19609] Received ADD_JOBS signal
03/04/16 12:06:54 [19609] in doContactSchedd()
03/04/16 12:06:54 [19609] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:06:54 [19609] querying for new jobs
03/04/16 12:06:54 [19609] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 12:06:54 [19609] Using job type INFNBatch for job 120.0
03/04/16 12:06:54 [19609] (120.0) SetJobLeaseTimers()
03/04/16 12:06:54 [19609] Found job 120.0 --- inserting
03/04/16 12:06:54 [19609] Fetched 1 new job ads from schedd
03/04/16 12:06:54 [19609] querying for removed/held jobs
03/04/16 12:06:54 [19609] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:06:54 [19609] Fetched 0 job ads from schedd
03/04/16 12:06:54 [19609] leaving doContactSchedd()
03/04/16 12:06:54 [19609] gahp server not up yet, delaying ping
03/04/16 12:06:54 [19609] *** UpdateLeases called
03/04/16 12:06:54 [19609]     Leases not supported, cancelling timer
03/04/16 12:06:54 [19609] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_3>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457111214
IdleJobs = 1
JobLimit = 10000

03/04/16 12:06:54 [19609] Trying to update collector <10.31.131.202:9619>
03/04/16 12:06:54 [19609] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 12:06:54 [19609] File descriptor limits: max 4096, safe 3277
03/04/16 12:06:54 [19609] (120.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 12:06:54 [19609] GAHP server pid = 20609
03/04/16 12:06:54 [19609] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 12:06:54 [19609] GAHP[20609] <- 'COMMANDS'
03/04/16 12:06:54 [19609] GAHP[20609] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 12:06:54 [19609] GAHP[20609] <- 'ASYNC_MODE_ON'
03/04/16 12:06:54 [19609] GAHP[20609] -> 'S' 'Async mode on'
03/04/16 12:06:54 [19609] (120.0) gm state change: GM_INIT -> GM_START
03/04/16 12:06:54 [19609] (120.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 12:06:54 [19609] (120.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 12:06:54 [19609] (120.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 12:06:54 [19609] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 12:06:54 [19609] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 12:06:54 [19609] IPVERIFY: ip found is 1
03/04/16 12:06:56 [19609] Evaluating staleness of remote job statuses.
03/04/16 12:06:59 [19609] resource  is now up
03/04/16 12:06:59 [19609] in doContactSchedd()
03/04/16 12:06:59 [19609] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:06:59 [19609] querying for removed/held jobs
03/04/16 12:06:59 [19609] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:06:59 [19609] Fetched 0 job ads from schedd
03/04/16 12:06:59 [19609] Updating classad values for 120.0:
03/04/16 12:06:59 [19609]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905"
03/04/16 12:06:59 [19609]    LastRemoteStatusUpdate = 1457111214
03/04/16 12:06:59 [19609] leaving doContactSchedd()
03/04/16 12:06:59 [19609] (120.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 12:06:59 [19609] (120.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 12:06:59 [19609] (120.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 12:06:59 [19609] GAHP[20609] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 12:06:59 [19609] GAHP[20609] -> 'S'
03/04/16 12:07:04 [19609] GAHP[20609] <- 'RESULTS'
03/04/16 12:07:04 [19609] GAHP[20609] -> 'R'
03/04/16 12:07:04 [19609] GAHP[20609] -> 'S' '1'
03/04/16 12:07:04 [19609] GAHP[20609] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ pbs_spoolpath=/var/spool/pbs-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa647ef00.32379 -C /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa647ef00.32379 -C /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa647ef00.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa647ef00.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa647ef00.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa647ef00.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa647ef00.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' a5 94 63 66 ec 5c'-+ bls_tmp_name=bl_a5946366ec5c-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-++ touch /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=21214-++ date +%s-+ uni_time=1457111223-+ uni_ext=34905.21214.1457111223-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_a5946366ec5c.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt bl_a5946366ec5c.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_a5946366ec5c.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_a5946366ec5c.proxy"'-++ bls_inputcopy_remote_0=bl_a5946366ec5c.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a5946366ec5c.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a5946366ec5c.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a5946366ec5c.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a5946366ec5c.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a5946366ec5c.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_a5946366ec5c.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_a5946366ec5c.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a5946366ec5c.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a5946366ec5c.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a5946366ec5c.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_a5946366ec5c.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_a5946366ec5c.proxy" 2> /dev/null' ']'-+ echo rm '"bl_a5946366ec5c.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-+ cat /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-/usr/libexec/blahp/pbs_submit.sh: line 205: /var/spool/pbs/pbs.sub: Permission denied-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-+ exit 1-)' 'N/A'
03/04/16 12:07:04 [19609] (120.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 12:07:04 [19609] (120.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ pbs_spoolpath=/var/spool/pbs-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa647ef00.32379 -C /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa647ef00.32379 -C /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa647ef00.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa647ef00.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa647ef00.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa647ef00.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa647ef00.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' a5 94 63 66 ec 5c'-+ bls_tmp_name=bl_a5946366ec5c-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-++ touch /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=21214-++ date +%s-+ uni_time=1457111223-+ uni_ext=34905.21214.1457111223-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_a5946366ec5c.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt bl_a5946366ec5c.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_a5946366ec5c.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_a5946366ec5c.proxy"'-++ bls_inputcopy_remote_0=bl_a5946366ec5c.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa647ef00.32379/ce-req-file-1457111223692778-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a5946366ec5c.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a5946366ec5c.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a5946366ec5c.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a5946366ec5c.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a5946366ec5c.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_a5946366ec5c.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_a5946366ec5c.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a5946366ec5c.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a5946366ec5c.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a5946366ec5c.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_a5946366ec5c.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_a5946366ec5c.proxy" 2> /dev/null' ']'-+ echo rm '"bl_a5946366ec5c.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/119/0/cluster119.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/119/0/cluster119.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-+ cat /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-/usr/libexec/blahp/pbs_submit.sh: line 205: /var/spool/pbs/pbs.sub: Permission denied-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa647ef00.32379/bl_a5946366ec5c-+ exit 1-)
03/04/16 12:07:04 [19609] (120.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/04/16 12:07:04 [19609] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/04/16 12:07:05 [19609] (120.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/04/16 12:07:05 [19609] in doContactSchedd()
03/04/16 12:07:05 [19609] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:07:05 [19609] querying for removed/held jobs
03/04/16 12:07:05 [19609] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:07:05 [19609] Fetched 0 job ads from schedd
03/04/16 12:07:05 [19609] Updating classad values for 120.0:
03/04/16 12:07:05 [19609]    CurrentStatusUnknown = false
03/04/16 12:07:05 [19609]    GridJobId = undefined
03/04/16 12:07:05 [19609]    LastRemoteStatusUpdate = 0
03/04/16 12:07:05 [19609] leaving doContactSchedd()
03/04/16 12:07:05 [19609] (120.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/04/16 12:07:05 [19609] (120.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 12:07:05 [19609] (120.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 12:07:10 [19609] in doContactSchedd()
03/04/16 12:07:10 [19609] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:07:10 [19609] querying for removed/held jobs
03/04/16 12:07:10 [19609] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:07:10 [19609] Fetched 0 job ads from schedd
03/04/16 12:07:10 [19609] Updating classad values for 120.0:
03/04/16 12:07:10 [19609]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#120.0#1457110905"
03/04/16 12:07:10 [19609]    LastRemoteStatusUpdate = 1457111225
03/04/16 12:07:11 [19609] leaving doContactSchedd()
03/04/16 12:07:11 [19609] (120.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 12:07:11 [19609] (120.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 12:07:11 [19609] (120.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/04/16 12:07:11 [19609] (120.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 12:07:16 [19609] in doContactSchedd()
03/04/16 12:07:16 [19609] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:07:16 [19609] querying for removed/held jobs
03/04/16 12:07:16 [19609] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:07:16 [19609] Fetched 0 job ads from schedd
03/04/16 12:07:16 [19609] Updating classad values for 120.0:
03/04/16 12:07:16 [19609]    EnteredCurrentStatus = 1457111231
03/04/16 12:07:16 [19609]    HoldReason = "Attempts to submit failed: "
03/04/16 12:07:16 [19609]    HoldReasonCode = 0
03/04/16 12:07:16 [19609]    HoldReasonSubCode = 0
03/04/16 12:07:16 [19609]    JobStatus = 5
03/04/16 12:07:16 [19609]    LastReleaseReason = "Data files spooled"
03/04/16 12:07:16 [19609]    Managed = "Schedd"
03/04/16 12:07:16 [19609]    NumSystemHolds = 1
03/04/16 12:07:16 [19609]    ReleaseReason = undefined
03/04/16 12:07:17 [19609] No jobs left, shutting down
03/04/16 12:07:17 [19609] leaving doContactSchedd()
03/04/16 12:07:17 [19609] Got SIGTERM. Performing graceful shutdown.
03/04/16 12:07:17 [19609] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 12:07:17 [19609] **** condor_gridmanager (condor_GRIDMANAGER) pid 19609 EXITING WITH STATUS 0
03/04/16 12:09:34 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 12:09:34 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 12:09:34 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 12:09:34 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 12:09:34 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 12:09:34 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 12:09:34 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 12:09:34 ******************************************************
03/04/16 12:09:34 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 12:09:34 ** /usr/sbin/condor_gridmanager
03/04/16 12:09:34 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 12:09:34 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 12:09:34 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 12:09:34 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 12:09:34 ** PID = 31456
03/04/16 12:09:34 ** Log last touched 3/4 12:07:17
03/04/16 12:09:34 ******************************************************
03/04/16 12:09:34 Using config source: /etc/condor-ce/condor_config
03/04/16 12:09:34 Using local config sources: 
03/04/16 12:09:34    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 12:09:34    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 12:09:34    /etc/condor-ce/config.d/99-local.conf
03/04/16 12:09:34    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 12:09:34 config Macros = 144, Sorted = 144, StringBytes = 12558, TablesBytes = 5392
03/04/16 12:09:34 CLASSAD_CACHING is ENABLED
03/04/16 12:09:34 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 12:09:34 SharedPortEndpoint: waiting for connections to named socket 32379_af3e_4
03/04/16 12:09:34 DaemonCore: command socket at <140.247.179.131:9620?sock=32379_af3e_4>
03/04/16 12:09:34 DaemonCore: private command socket at <140.247.179.131:9620?sock=32379_af3e_4>
03/04/16 12:09:34 Setting maximum accepts per cycle 8.
03/04/16 12:09:34 Setting maximum reaps per cycle 8.
03/04/16 12:09:34 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 12:09:34 [31456] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 12:09:34 [31456] DaemonCore: No more children processes to reap.
03/04/16 12:09:34 [31456] DaemonCore: in SendAliveToParent()
03/04/16 12:09:34 [31456] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:09:35 [31456] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 12:09:35 [31456] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 12:09:35 [31456] IPVERIFY: ip found is 0
03/04/16 12:09:35 [31456] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 12:09:35 [31456] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 12:09:35 [31456] Buf::write(): condor_write() failed
03/04/16 12:09:35 [31456] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 12:09:35 [31456] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:09:35 [31456] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 12:09:35 [31456] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 12:09:35 [31456] Checking proxies
03/04/16 12:09:36 [31456] Received ADD_JOBS signal
03/04/16 12:09:36 [31456] in doContactSchedd()
03/04/16 12:09:36 [31456] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:09:36 [31456] querying for new jobs
03/04/16 12:09:36 [31456] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 12:09:36 [31456] Using job type INFNBatch for job 122.0
03/04/16 12:09:36 [31456] (122.0) SetJobLeaseTimers()
03/04/16 12:09:36 [31456] Found job 122.0 --- inserting
03/04/16 12:09:36 [31456] Fetched 1 new job ads from schedd
03/04/16 12:09:36 [31456] querying for removed/held jobs
03/04/16 12:09:36 [31456] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:09:36 [31456] Fetched 0 job ads from schedd
03/04/16 12:09:37 [31456] leaving doContactSchedd()
03/04/16 12:09:37 [31456] gahp server not up yet, delaying ping
03/04/16 12:09:37 [31456] *** UpdateLeases called
03/04/16 12:09:37 [31456]     Leases not supported, cancelling timer
03/04/16 12:09:37 [31456] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_4>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457111377
IdleJobs = 1
JobLimit = 10000

03/04/16 12:09:37 [31456] Trying to update collector <10.31.131.202:9619>
03/04/16 12:09:37 [31456] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 12:09:37 [31456] File descriptor limits: max 4096, safe 3277
03/04/16 12:09:37 [31456] (122.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 12:09:37 [31456] GAHP server pid = 31521
03/04/16 12:09:37 [31456] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 12:09:37 [31456] GAHP[31521] <- 'COMMANDS'
03/04/16 12:09:37 [31456] GAHP[31521] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 12:09:37 [31456] GAHP[31521] <- 'ASYNC_MODE_ON'
03/04/16 12:09:37 [31456] GAHP[31521] -> 'S' 'Async mode on'
03/04/16 12:09:37 [31456] (122.0) gm state change: GM_INIT -> GM_START
03/04/16 12:09:37 [31456] (122.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 12:09:37 [31456] (122.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 12:09:37 [31456] (122.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 12:09:37 [31456] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 12:09:37 [31456] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 12:09:37 [31456] IPVERIFY: ip found is 1
03/04/16 12:09:39 [31456] Evaluating staleness of remote job statuses.
03/04/16 12:09:42 [31456] resource  is now up
03/04/16 12:09:42 [31456] in doContactSchedd()
03/04/16 12:09:42 [31456] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:09:42 [31456] querying for removed/held jobs
03/04/16 12:09:42 [31456] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:09:42 [31456] Fetched 0 job ads from schedd
03/04/16 12:09:42 [31456] Updating classad values for 122.0:
03/04/16 12:09:42 [31456]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362"
03/04/16 12:09:42 [31456]    LastRemoteStatusUpdate = 1457111377
03/04/16 12:09:43 [31456] leaving doContactSchedd()
03/04/16 12:09:43 [31456] (122.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 12:09:43 [31456] (122.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 12:09:43 [31456] (122.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 12:09:43 [31456] GAHP[31521] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 12:09:43 [31456] GAHP[31521] -> 'S'
03/04/16 12:09:52 [31456] GAHP[31521] <- 'RESULTS'
03/04/16 12:09:52 [31456] GAHP[31521] -> 'R'
03/04/16 12:09:52 [31456] GAHP[31521] -> 'S' '1'
03/04/16 12:09:52 [31456] GAHP[31521] -> '2' '1' 'submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ pbs_spoolpath=/var/spool/pbs-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54fd550.32379 -C /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54fd550.32379 -C /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa54fd550.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa54fd550.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa54fd550.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa54fd550.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa54fd550.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' a6 14 7a 05 4f 38'-+ bls_tmp_name=bl_a6147a054f38-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-++ touch /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=32513-++ date +%s-+ uni_time=1457111391-+ uni_ext=34905.32513.1457111391-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_a6147a054f38.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt bl_a6147a054f38.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_a6147a054f38.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_a6147a054f38.proxy"'-++ bls_inputcopy_remote_0=bl_a6147a054f38.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a6147a054f38.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a6147a054f38.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a6147a054f38.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a6147a054f38.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a6147a054f38.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_a6147a054f38.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_a6147a054f38.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a6147a054f38.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a6147a054f38.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a6147a054f38.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_a6147a054f38.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_a6147a054f38.proxy" 2> /dev/null' ']'-+ echo rm '"bl_a6147a054f38.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-+ cat /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-+ exit 1-)' 'N/A'
03/04/16 12:09:52 [31456] (122.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 12:09:52 [31456] (122.0) blah_job_submit() failed: submission command failed (exit code = 1) (stdout:) (stderr:++ dirname /usr/libexec/blahp/pbs_submit.sh-+ . /usr/libexec/blahp/blah_common_submit_functions.sh-+++ dirname /usr/libexec/blahp/pbs_submit.sh-++ . /usr/libexec/blahp/blah_load_config.sh-+++ '[' x '!=' x -a -d /bin ']'-+++ '[' -d /usr/libexec/condor/glite/bin ']'-+++ blah_bin_directory=/usr/libexec/condor/glite/bin-+++ '[' x '!=' x -a -d /sbin ']'-+++ '[' -d /usr/libexec/condor/glite/sbin ']'-+++ blah_sbin_directory=/usr/sbin-+++ '[' x '!=' x -a -d /libexec ']'-+++ '[' -d /usr/libexec/condor/glite/libexec ']'-+++ blah_libexec_directory=/usr/libexec-+++ '[' -r '' ']'-+++ '[' -r /usr/libexec/condor/glite/etc/blah.config ']'-+++ '[' -r /etc/blah.config ']'-+++ . /etc/blah.config-++++ supported_lrms=pbs,lsf,sge,slurm,condor-++++ BLAHPD_ACCOUNTING_INFO_LOG=/var/log/blahpd-++++ blah_disable_wn_proxy_renewal=yes-++++ blah_delegate_renewed_proxies=no-++++ blah_shared_directories=/-++++ blah_graceful_kill_timeout=-++++ blah_enable_glexec_from_condor=-++++ blah_accounting_log_umask=-++++ blah_children_restart_interval=-++++ blah_require_proxy_on_submit=-++++ pbs_binpath=/usr/bin-++++ pbs_spoolpath=/var/spool/pbs-++++ pbs_nochecksubmission=-++++ pbs_nologaccess=yes-++++ pbs_fallback=no-++++ lsf_binpath=/lsf/7.0/linux2.6-glibc2.3-x86_64/bin-++++ lsf_confpath=/lsf/conf-++++ lsf_nochecksubmission=-++++ lsf_nologaccess=-++++ lsf_fallback=no-++++ pbs_BLParser=-++++ pbs_BLPserver=-++++ pbs_BLPport=-++++ pbs_num_BLParser=-++++ pbs_BLPserver1=-++++ pbs_BLPport1=-++++ pbs_BLPserver2=-++++ pbs_BLPport2=-++++ blah_torque_multiple_staging_directive_bug=no-++++ lsf_BLParser=-++++ lsf_BLPserver=-++++ lsf_BLPport=-++++ lsf_num_BLParser=-++++ lsf_BLPserver1=-++++ lsf_BLPport1=-++++ lsf_BLPserver2=-++++ lsf_BLPport2=-++++ loop_interval=-++++ bupdater_path=-++++ bupdater_pidfile=/var/tmp/cream_tomcat_bupdater.pid-++++ job_registry=-++++ job_registry_use_mmap=no-++++ async_notification_host=-++++ async_notification_port=-++++ bupdater_debug_level=1-++++ bupdater_debug_logfile=/var/tmp/bupdater.log-++++ purge_interval=7200-++++ finalstate_query_interval=30-++++ alldone_interval=3600-++++ batch_command_caching_filter=-++++ bupdater_child_poll_timeout=-++++ job_registry_add_remote=-++++ bupdater_consistency_check_interval=-++++ bhist_logs_to_read=-++++ bhist_finalstate_interval=120-++++ bupdater_bjobs_long_format=yes-++++ lsf_batch_caching_enabled=-++++ bupdater_use_bhist_for_susp=no-++++ bupdater_use_bhist_time_constraint=-++++ bupdater_use_btools=-++++ bupdater_btools_path=-++++ bupdater_use_bhist_for_idle=-++++ bupdater_use_bhist_for_killed=-++++ pbs_batch_caching_enabled=-++++ tracejob_logs_to_read=-++++ tracejob_max_output=-+++++ which condor_submit-+++++ sed 's|/[^/]*$||'-++++ condor_binpath=/usr/bin-++++ condor_batch_caching_enabled=-++++ bupdater_use_condor_history=-++++ sge_binpath=-++++ sge_cellname=-++++ sge_rootpath=-++++ slurm_binpath=/usr/bin-++++ bnotifier_path=-++++ bnotifier_pidfile=/var/tmp/cream_tomcat_bnotifier.pid-++++ bnotifier_debug_level=1-++++ bnotifier_debug_logfile=/var/tmp/bnotifier.log-++++ blah_libexec_directory=/usr/libexec/blahp-++++ blah_debug_save_submit_info=/scratch/blah-+ pbs_spoolpath=/var/spool/pbs-+ logpath=/var/spool/pbs/server_logs-+ '[' '!' -d /var/spool/pbs/server_logs -o '!' -x /var/spool/pbs/server_logs ']'-+ '[' -x /usr/bin/tracejob ']'-+ pbs_spoolpath=/var/lib/torque/spool-+ logpath=/var/lib/torque/server_logs-+ bls_job_id_for_renewal=PBS_JOBID-+ srvfound=-+ original_args='-x /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -u  -r no -c /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54fd550.32379 -C /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362 -m 2000 -V "HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ bls_parse_submit_options -x /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -u '' -r no -c /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env -T /tmp/condor_g_scratch.0x2b2aa54fd550.32379 -C /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 -i /dev/null -o _condor_stdout -e _condor_stderr -w /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0 -n 1 -S 1 -D home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362 -m 2000 -V '"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ usage_string='Usage: /usr/libexec/blahp/pbs_submit.sh -c <command> [-i <stdin>] [-o <stdout>] [-e <stderr>] [-x <x509userproxy>] [-v <environment>] [-s <yes | no>] [-- command_arguments]'-+ bls_opt_stgcmd=yes-+ bls_opt_stgproxy=yes-+ '[' x == x ']'-+ bls_proxyrenewald=/usr/libexec/blahp/BPRserver-+ bls_opt_proxyrenew=yes-+ '[' '!' -r /usr/libexec/blahp/BPRserver ']'-+ bls_opt_proxyrenew=no-+ bls_proxy_dir=/n/atlasgrid/home/usatlas1/.blah_jobproxy_dir-+ bls_opt_workdir=/var/log/condor-ce-+ bls_opt_prnpoll=30-+ bls_opt_prnlifetime=0-+ bls_BLClient=/usr/libexec/blahp/BLClient-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_string=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxy_subject=-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_proxyrenew=no-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_the_command=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_temp_dir=/tmp/condor_g_scratch.0x2b2aa54fd550.32379-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_file=/tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdin=/dev/null-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stdout=_condor_stdout-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_stderr=_condor_stderr-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_workdir=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_mpinodes=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_smpgranularity=1-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_req_mem=2000-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ case "$arg" in-+ bls_opt_environment='"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ getopts a:i:o:e:c:s:v:V:dw:q:n:N:z:h:S:r:p:l:x:u:j:T:I:O:R:C:D:m: arg-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env == x ']'-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt == x ']'-+ bls_opt_proxyrenew_numeric=0-+ '[' xno == xyes ']'-++ expr 31 - 1-+ shift 30-+ bls_arguments=-+ '[' xyes '!=' xyes -a x '!=' xyes ']'-+ bls_setup_all_files-+ '[' -z / ']'-+ bls_test_shared_dir /dev/null-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/dev/null-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes '!=' xyes ']'-+ '[' -z '' ']'-+ blah_wn_inputsandbox=-+ '[' -z '' ']'-+ blah_wn_outputsandbox=-+ local last_char_pos-+ '[' -n '' ']'-+ '[' -n '' ']'-++ pwd-+ curdir=/var/log/condor-ce-+ '[' -z /tmp/condor_g_scratch.0x2b2aa54fd550.32379 ']'-+ '[' '!' -e /tmp/condor_g_scratch.0x2b2aa54fd550.32379 ']'-+ '[' '!' -d /tmp/condor_g_scratch.0x2b2aa54fd550.32379 -o '!' -w /tmp/condor_g_scratch.0x2b2aa54fd550.32379 ']'-+ '[' x '!=' xyes ']'-+ '[' '!' -z '' ']'-++ od -A n -t xC -N 6 /dev/urandom-+ rand=' a6 14 7a 05 4f 38'-+ bls_tmp_name=bl_a6147a054f38-+ bls_tmp_file=/tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-++ touch /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-++ chmod 600 /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-+ '[' 0 -ne 0 ']'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 ']'-+ bls_opt_tmp_req_file=/tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-++ id -u-+ uni_uid=34905-+ uni_pid=32513-++ date +%s-+ uni_time=1457111391-+ uni_ext=34905.32513.1457111391-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_opt_stgcmd=no-+ '[' xno == xyes ']'-+ '[' xno == xyes ']'-+ bls_need_to_reset_proxy=no-+ bls_proxy_remote_file=-+ '[' xyes == xyes ']'-++ basename /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ bls_proxy_local_file=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ '[' -r /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ']'-+ '[' -r /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt -a -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ']'-+ bls_proxy_remote_file=bl_a6147a054f38.proxy-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_fl_add_value inputcopy /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt bl_a6147a054f38.proxy-+ local container_name-+ local local_file_name-+ local remote_file_name-+ container_name=inputcopy-+ local_file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ remote_file_name=bl_a6147a054f38.proxy-+ local last_argument-+ local transfer_file-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=0-+ eval 'bls_inputcopy_local_0="/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt"'-++ bls_inputcopy_local_0=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ eval 'bls_inputcopy_remote_0="bl_a6147a054f38.proxy"'-++ bls_inputcopy_remote_0=bl_a6147a054f38.proxy-+ '[' -n '' ']'-+ eval 'let bls_inputcopy_counter++'-++ let bls_inputcopy_counter++-+ bls_need_to_reset_proxy=yes-+ '[' '!' -z /dev/null ']'-+ '[' / '!=' / ']'-+ '[' -f /dev/null ']'-+ bls_arguments=' < "/dev/null"'-+ '[' '!' -z _condor_stdout ']'-+ '[' _ '!=' / ']'-+ bls_opt_stdout=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout"'-+ '[' '!' -z _condor_stderr ']'-+ '[' _ '!=' / ']'-+ bls_opt_stderr=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr-+ bls_test_shared_dir /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr-+ local file_name-+ local test_dir-+ local shared_dir_list-+ file_name=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr-+ bls_is_in_shared_dir=no-+ '[' / '!=' / ']'-+ shared_dir_list=/-+ test_dir=-+ '[' / == '' ']'-+ test_dir=/-+ shared_dir_list=/-+ '[' / == / ']'-+ bls_is_in_shared_dir=yes-+ break-+ '[' xyes == xyes ']'-+ bls_arguments=' < "/dev/null" > "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr"'-+ local xfile-+ local xfile_base-+ '[' '!' -z '' ']'-+ xfile=-+ local xfileremap-+ '[' '!' -z '' ']'-+ cat-++ basename /usr/libexec/blahp/pbs_submit.sh-++ /bin/date-+ '[' x == x ']'-+ pbs_std_storage=/dev/null-+ '[' x/dev/null '!=' x ']'-+ echo '#PBS -o /dev/null'-+ echo '#PBS -e /dev/null'-+ bls_local_submit_attributes_file=/usr/libexec/blahp/pbs_local_submit_attributes.sh-+ '[' x2000 '!=' x ']'-+ echo '#PBS -l mem=2000mb'-+ echo '#PBS -l pmem=2000mb'-+ echo '#PBS -l pvmem=2000mb'-+ bls_set_up_local_and_extra_args-+ '[' -r /usr/libexec/blahp/pbs_local_submit_attributes.sh ']'-+ echo '#!/bin/sh'-+ '[' '!' -z /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718 ']'-+ cat /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-+ '[' -n 1 ']'-+ echo blah_opt_mpinodes=1-+ echo 'source /usr/libexec/blahp/pbs_local_submit_attributes.sh'-+ chmod +x /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-+ /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-+ '[' -e /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54fd550.32379/ce-req-file-1457111391536718-temp_req_script-+ '[' '!' -z '' ']'-+ '[' -z '' ']'-+ '[' x == xyes ']'-+ [[ ! -z 1 ]]-+ n=1-+ r=0-+ ((  r  ))-+ echo '#PBS -l nodes=1:ppn=1'-+ '[' xno == xyes ']'-+ '[' xno == xmultiline ']'-++ hostname -f-+ bls_fl_subst_and_accumulate inputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=inputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-++ hostname -f-+ bls_fl_subst_and_accumulate outputsand @@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL ,-+ local container_name-+ local subst_template-+ local separator-+ container_name=outputsand-+ subst_template=@@F_REMOTE@net2.rc.fas.harvard.edu:@@F_LOCAL-+ separator=,-+ bls_fl_subst_and_accumulate_result=-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ local l_sepa-+ l_sepa=-+ (( ind=0  ))-+ (( ind < 0  ))-+ '[' -z '' ']'-+ echo '#PBS -m n'-+ bls_add_job_wrapper-+ bls_start_job_wrapper-+ '[' 'x"HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"' '!=' x ']'-+ echo ''-+ echo '# Setting the environment:'-+ eval 'env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")'-++ env_array=("HOME=/n/atlasgrid/home/usatlas1" "CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619" "OSG_GRID=/n/atlasgrid/osg-wn-client" "OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128" "OSG_SITE_READ=None" "OSG_APP=/n/atlasgrid/osg/app" "OSG_GLEXEC_LOCATION=None" "OSG_DATA=/n/atlasgrid/osg/data" "OSG_HOSTNAME=net2.rc.fas.harvard.edu" "OSG_STORAGE_ELEMENT=True" "OSG_SITE_NAME=NET2_HU" "ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local" "GLOBUS_LOCATION=/usr" "OSG_WN_TMP=/scratch" "OSG_SITE_WRITE=None" "OSG_DEFAULT_SE=net2.rc.fas.harvard.edu" "OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs")-+ for env_var in '"${env_array[@]}"'-+ echo export '"HOME=/n/atlasgrid/home/usatlas1"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GRID=/n/atlasgrid/osg-wn-client"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SQUID_LOCATION=net2.rc.fas.harvard.edu:3128"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_READ=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_APP=/n/atlasgrid/osg/app"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_GLEXEC_LOCATION=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DATA=/n/atlasgrid/osg/data"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_HOSTNAME=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_STORAGE_ELEMENT=True"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_NAME=NET2_HU"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"ATLAS_LOCAL_AREA=/n/atlasgrid/osg/app/atlas_app/local"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"GLOBUS_LOCATION=/usr"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_WN_TMP=/scratch"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_SITE_WRITE=None"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_DEFAULT_SE=net2.rc.fas.harvard.edu"'-+ for env_var in '"${env_array[@]}"'-+ echo export '"OSG_JOB_CONTACT=net2.rc.fas.harvard.edu/jobmanager-pbs"'-+ echo 'test -r /var/lib/osg/osg-job-environment.conf       && . /var/lib/osg/osg-job-environment.conf'-+ echo 'test -r /var/lib/osg/osg-local-job-environment.conf && . /var/lib/osg/osg-local-job-environment.conf'-+ echo 'old_home=`pwd`'-+ '[' xhome_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362 '!=' x ']'-+ run_dir=home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362-+ '[' -n '' ']'-+ echo 'new_home=${old_home}/home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362'-+ echo 'mkdir $new_home'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home; exit 255''' 1 2 3 15 24'-+ echo 'trap '''wait $job_pid; cd $old_home; rm -rf $new_home''' 0'-+ echo '# Copy into new home any shared input sandbox file'-+ bls_fl_subst_and_dump inputcopy 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='cp "@@F_LOCAL" "$new_home/@@F_REMOTE" &> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a6147a054f38.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a6147a054f38.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a6147a054f38.proxy ')' ']'-+ temp1_result='cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ temp2_result='cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/@@F_REMOTE" &> /dev/null'-+ bls_fl_subst_result='cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a6147a054f38.proxy" &> /dev/null'-+ '[' '!' -z 'cp "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt" "$new_home/bl_a6147a054f38.proxy" &> /dev/null' ']'-+ echo cp '"/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt"' '"$new_home/bl_a6147a054f38.proxy"' '&>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ echo '# Move into new home any relative input sandbox file'-+ bls_fl_subst_relative_paths_and_dump inputsand 'mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='mv "@@F_REMOTE" "$new_home/@@F_WORKNAME" &> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo 'export HOME=$new_home'-+ echo 'cd $new_home'-+ '[' xyes == xyes ']'-+ echo '# Resetting proxy to local position'-+ echo 'export X509_USER_PROXY=$new_home/bl_a6147a054f38.proxy'-+ echo ''-+ echo '# Command to execute:'-+ '[' xno == xyes ']'-+ echo '/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/env  < "/dev/null" > "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stdout" 2> "/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/_condor_stderr" &'-+ echo 'job_pid=$!'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Wait for the user job to finish'-+ echo 'wait $job_pid'-+ echo 'user_retcode=$?'-+ '[' x '!=' x ']'-+ '[' xno == xyes ']'-+ echo ''-+ echo '# Move all relative outputsand paths out of temp home'-+ echo 'cd $new_home'-+ bls_fl_subst_relative_paths_and_dump outputsand 'mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null' '$old_home'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputsand-+ subst_template='mv "@@F_WORKNAME" "@@F_REMOTE" 2> /dev/null'-+ destination_root='$old_home'-+ local last_argument-+ eval 'last_argument=${bls_outputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo '# Move any remapped outputsand file to shared directories'-+ bls_fl_subst_relative_paths_and_dump outputmove 'mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=outputmove-+ subst_template='mv "@@F_REMOTE" "@@F_LOCAL" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_outputmove_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ echo ''-+ echo '# Remove the staged files, if any'-+ bls_fl_subst_and_dump inputcopy 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local subst_template-+ container_name=inputcopy-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local last_argument-+ eval 'last_argument=${bls_inputcopy_counter:=0}'-++ last_argument=1-+ local ind-+ local f_remote-+ (( ind=0  ))-+ (( ind < 1  ))-+ bls_fl_subst inputcopy 0 'rm "@@F_REMOTE" 2> /dev/null'-+ local container_name-+ local container_index-+ local subst_template-+ container_name=inputcopy-+ container_index=0-+ subst_template='rm "@@F_REMOTE" 2> /dev/null'-+ local f_local-+ local f_remote-+ local temp1_result-+ local temp2_result-+ eval 'f_local="${bls_inputcopy_local_0}"'-++ f_local=/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt-+ eval 'f_remote="${bls_inputcopy_remote_0}"'-++ f_remote=bl_a6147a054f38.proxy-+ eval 'f_workname="${bls_inputcopy_workname_0}"'-++ f_workname=-+ '[' -z '' ']'-+ f_workname=bl_a6147a054f38.proxy-+ bls_fl_subst_result=-+ '[' '(' '!' -z /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt ')' -a '(' '!' -z bl_a6147a054f38.proxy ')' ']'-+ temp1_result='rm "@@F_REMOTE" 2> /dev/null'-+ temp2_result='rm "@@F_REMOTE" 2> /dev/null'-+ bls_fl_subst_result='rm "bl_a6147a054f38.proxy" 2> /dev/null'-+ '[' '!' -z 'rm "bl_a6147a054f38.proxy" 2> /dev/null' ']'-+ echo rm '"bl_a6147a054f38.proxy"' '2>' /dev/null-+ (( ind++  ))-+ (( ind < 1  ))-+ bls_fl_subst_relative_paths_and_dump inputsand 'rm "@@F_WORKNAME" 2> /dev/null'-+ local container_name-+ local subst_template-+ local destination_root-+ container_name=inputsand-+ subst_template='rm "@@F_WORKNAME" 2> /dev/null'-+ destination_root=-+ local last_argument-+ eval 'last_argument=${bls_inputsand_counter:=0}'-++ last_argument=0-+ local ind-+ (( ind=0  ))-+ (( ind < 0  ))-+ bls_finish_job_wrapper-+ echo 'cd $old_home'-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt '!=' x ']'-+ echo 'rm -f /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0/x509up_u556792.lmt'-+ echo ''-+ echo 'exit $user_retcode'-+ '[' x == xyes ']'-+ bls_test_working_dir-+ '[' x/n/atlasgrid/condor/121/0/cluster121.proc0.subproc0 '!=' x ']'-+ cd /n/atlasgrid/condor/121/0/cluster121.proc0.subproc0-+ '[' 0 -ne 0 ']'-+ sleep 1-+ cat /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-++ date +%Y%m%d-+ datenow=20160304-++ /usr/bin/qsub /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-+ jobID='There was an error running the SLURM sbatch command.-The command was:-'''/usr/bin/sbatch /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38  2>&1'''-and the output was:-'''sbatch: error: Batch job submission failed: Unspecified error-''''-+ retcode=1-+ '[' 1 '!=' 0 ']'-+ rm -f /tmp/condor_g_scratch.0x2b2aa54fd550.32379/bl_a6147a054f38-+ exit 1-)
03/04/16 12:09:52 [31456] (122.0) gm state change: GM_SUBMIT -> GM_DELETE_SANDBOX
03/04/16 12:09:52 [31456] Initializing Directory: curr_dir = /n/atlasgrid/home/usatlas1
03/04/16 12:09:53 [31456] (122.0) gm state change: GM_DELETE_SANDBOX -> GM_CLEAR_REQUEST
03/04/16 12:09:53 [31456] in doContactSchedd()
03/04/16 12:09:53 [31456] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:09:53 [31456] querying for removed/held jobs
03/04/16 12:09:53 [31456] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:09:53 [31456] Fetched 0 job ads from schedd
03/04/16 12:09:53 [31456] Updating classad values for 122.0:
03/04/16 12:09:53 [31456]    CurrentStatusUnknown = false
03/04/16 12:09:53 [31456]    GridJobId = undefined
03/04/16 12:09:53 [31456]    LastRemoteStatusUpdate = 0
03/04/16 12:09:53 [31456] leaving doContactSchedd()
03/04/16 12:09:53 [31456] (122.0) doEvaluateState called: gmState GM_CLEAR_REQUEST, remoteState 0
03/04/16 12:09:53 [31456] (122.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 12:09:53 [31456] (122.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 12:09:59 [31456] in doContactSchedd()
03/04/16 12:09:59 [31456] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:09:59 [31456] querying for removed/held jobs
03/04/16 12:09:59 [31456] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:09:59 [31456] Fetched 0 job ads from schedd
03/04/16 12:09:59 [31456] Updating classad values for 122.0:
03/04/16 12:09:59 [31456]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#122.0#1457111362"
03/04/16 12:09:59 [31456]    LastRemoteStatusUpdate = 1457111393
03/04/16 12:09:59 [31456] leaving doContactSchedd()
03/04/16 12:09:59 [31456] (122.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 12:09:59 [31456] (122.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 12:09:59 [31456] (122.0) gm state change: GM_TRANSFER_INPUT -> GM_HOLD
03/04/16 12:09:59 [31456] (122.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 12:10:04 [31456] in doContactSchedd()
03/04/16 12:10:04 [31456] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:10:04 [31456] querying for removed/held jobs
03/04/16 12:10:04 [31456] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:10:04 [31456] Fetched 0 job ads from schedd
03/04/16 12:10:04 [31456] Updating classad values for 122.0:
03/04/16 12:10:04 [31456]    EnteredCurrentStatus = 1457111399
03/04/16 12:10:04 [31456]    HoldReason = "Attempts to submit failed: "
03/04/16 12:10:04 [31456]    HoldReasonCode = 0
03/04/16 12:10:04 [31456]    HoldReasonSubCode = 0
03/04/16 12:10:04 [31456]    JobStatus = 5
03/04/16 12:10:04 [31456]    LastReleaseReason = "Data files spooled"
03/04/16 12:10:04 [31456]    Managed = "Schedd"
03/04/16 12:10:04 [31456]    NumSystemHolds = 1
03/04/16 12:10:04 [31456]    ReleaseReason = undefined
03/04/16 12:10:05 [31456] No jobs left, shutting down
03/04/16 12:10:05 [31456] leaving doContactSchedd()
03/04/16 12:10:05 [31456] Got SIGTERM. Performing graceful shutdown.
03/04/16 12:10:05 [31456] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 12:10:05 [31456] **** condor_gridmanager (condor_GRIDMANAGER) pid 31456 EXITING WITH STATUS 0
03/04/16 12:54:55 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 12:54:55 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 12:54:55 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 12:54:55 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 12:54:55 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 12:54:55 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 12:54:55 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 12:54:55 ******************************************************
03/04/16 12:54:55 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 12:54:55 ** /usr/sbin/condor_gridmanager
03/04/16 12:54:55 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 12:54:55 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 12:54:55 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 12:54:55 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 12:54:55 ** PID = 5511
03/04/16 12:54:55 ** Log last touched 3/4 12:10:05
03/04/16 12:54:55 ******************************************************
03/04/16 12:54:55 Using config source: /etc/condor-ce/condor_config
03/04/16 12:54:55 Using local config sources: 
03/04/16 12:54:55    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 12:54:55    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 12:54:55    /etc/condor-ce/config.d/99-local.conf
03/04/16 12:54:55    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 12:54:55 config Macros = 144, Sorted = 144, StringBytes = 12556, TablesBytes = 5392
03/04/16 12:54:55 CLASSAD_CACHING is ENABLED
03/04/16 12:54:55 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 12:54:55 SharedPortEndpoint: waiting for connections to named socket 32379_af3e_5
03/04/16 12:54:55 DaemonCore: command socket at <140.247.179.131:9620?sock=32379_af3e_5>
03/04/16 12:54:55 DaemonCore: private command socket at <140.247.179.131:9620?sock=32379_af3e_5>
03/04/16 12:54:55 Setting maximum accepts per cycle 8.
03/04/16 12:54:55 Setting maximum reaps per cycle 8.
03/04/16 12:54:55 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 12:54:55 [5511] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 12:54:55 [5511] DaemonCore: No more children processes to reap.
03/04/16 12:54:55 [5511] DaemonCore: in SendAliveToParent()
03/04/16 12:54:55 [5511] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:54:55 [5511] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 12:54:55 [5511] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 12:54:55 [5511] IPVERIFY: ip found is 0
03/04/16 12:54:55 [5511] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 12:54:55 [5511] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 12:54:55 [5511] Buf::write(): condor_write() failed
03/04/16 12:54:55 [5511] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 12:54:55 [5511] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:54:55 [5511] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 12:54:55 [5511] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 12:54:55 [5511] Checking proxies
03/04/16 12:54:57 [5511] Received ADD_JOBS signal
03/04/16 12:54:57 [5511] in doContactSchedd()
03/04/16 12:54:57 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:54:57 [5511] querying for new jobs
03/04/16 12:54:57 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 12:54:57 [5511] Using job type INFNBatch for job 124.0
03/04/16 12:54:57 [5511] (124.0) SetJobLeaseTimers()
03/04/16 12:54:58 [5511] Found job 124.0 --- inserting
03/04/16 12:54:58 [5511] Fetched 1 new job ads from schedd
03/04/16 12:54:58 [5511] querying for removed/held jobs
03/04/16 12:54:58 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:54:58 [5511] Fetched 0 job ads from schedd
03/04/16 12:54:58 [5511] leaving doContactSchedd()
03/04/16 12:54:58 [5511] gahp server not up yet, delaying ping
03/04/16 12:54:58 [5511] *** UpdateLeases called
03/04/16 12:54:58 [5511]     Leases not supported, cancelling timer
03/04/16 12:54:58 [5511] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_5>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457114098
IdleJobs = 1
JobLimit = 10000

03/04/16 12:54:58 [5511] Trying to update collector <10.31.131.202:9619>
03/04/16 12:54:58 [5511] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 12:54:58 [5511] File descriptor limits: max 4096, safe 3277
03/04/16 12:54:58 [5511] (124.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 12:54:58 [5511] GAHP server pid = 5740
03/04/16 12:54:58 [5511] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 12:54:58 [5511] GAHP[5740] <- 'COMMANDS'
03/04/16 12:54:58 [5511] GAHP[5740] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 12:54:58 [5511] GAHP[5740] <- 'ASYNC_MODE_ON'
03/04/16 12:54:58 [5511] GAHP[5740] -> 'S' 'Async mode on'
03/04/16 12:54:58 [5511] (124.0) gm state change: GM_INIT -> GM_START
03/04/16 12:54:58 [5511] (124.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 12:54:58 [5511] (124.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 12:54:58 [5511] (124.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 12:54:58 [5511] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 12:54:58 [5511] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 12:54:58 [5511] IPVERIFY: ip found is 1
03/04/16 12:55:00 [5511] Evaluating staleness of remote job statuses.
03/04/16 12:55:03 [5511] resource  is now up
03/04/16 12:55:03 [5511] in doContactSchedd()
03/04/16 12:55:03 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:55:03 [5511] querying for removed/held jobs
03/04/16 12:55:03 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:55:03 [5511] Fetched 0 job ads from schedd
03/04/16 12:55:03 [5511] Updating classad values for 124.0:
03/04/16 12:55:03 [5511]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#124.0#1457114089"
03/04/16 12:55:03 [5511]    LastRemoteStatusUpdate = 1457114098
03/04/16 12:55:03 [5511] leaving doContactSchedd()
03/04/16 12:55:03 [5511] (124.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 12:55:03 [5511] (124.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 12:55:03 [5511] (124.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 12:55:03 [5511] GAHP[5740] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/123/0/cluster123.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/123/0/cluster123.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/123/0/cluster123.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#124.0#1457114089";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 12:55:03 [5511] GAHP[5740] -> 'S'
03/04/16 12:55:05 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:55:05 [5511] GAHP[5740] -> 'R'
03/04/16 12:55:05 [5511] GAHP[5740] -> 'S' '1'
03/04/16 12:55:05 [5511] GAHP[5740] -> '2' '0' 'No error' 'pbs/20160304/3392916'
03/04/16 12:55:05 [5511] (124.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 12:55:05 [5511] (124.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
03/04/16 12:55:08 [5511] in doContactSchedd()
03/04/16 12:55:08 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:55:08 [5511] querying for removed/held jobs
03/04/16 12:55:08 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:55:08 [5511] Fetched 0 job ads from schedd
03/04/16 12:55:08 [5511] Updating classad values for 124.0:
03/04/16 12:55:08 [5511]    DelegatedProxyExpiration = 1457149842
03/04/16 12:55:08 [5511]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#124.0#1457114089 pbs/20160304/3392916"
03/04/16 12:55:09 [5511] leaving doContactSchedd()
03/04/16 12:55:09 [5511] (124.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
03/04/16 12:55:09 [5511] (124.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
03/04/16 12:55:55 [5511] Received CHECK_LEASES signal
03/04/16 12:55:55 [5511] in doContactSchedd()
03/04/16 12:55:55 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:55:55 [5511] querying for renewed leases
03/04/16 12:55:55 [5511] querying for removed/held jobs
03/04/16 12:55:55 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:55:55 [5511] Fetched 0 job ads from schedd
03/04/16 12:55:55 [5511] leaving doContactSchedd()
03/04/16 12:55:58 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:55:58 [5511] GAHP[5740] -> 'S' '0'
03/04/16 12:56:00 [5511] Evaluating staleness of remote job statuses.
03/04/16 12:56:09 [5511] (124.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 12:56:09 [5511] (124.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 12:56:09 [5511] GAHP[5740] <- 'BLAH_JOB_STATUS 3 pbs/20160304/3392916'
03/04/16 12:56:09 [5511] GAHP[5740] -> 'S'
03/04/16 12:56:10 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:56:10 [5511] GAHP[5740] -> 'R'
03/04/16 12:56:10 [5511] GAHP[5740] -> 'S' '1'
03/04/16 12:56:10 [5511] GAHP[5740] -> '3' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 12:56:10 [5511] (124.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 12:56:10 [5511] (124.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 12:56:56 [5511] Received CHECK_LEASES signal
03/04/16 12:56:56 [5511] in doContactSchedd()
03/04/16 12:56:56 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:56:56 [5511] querying for renewed leases
03/04/16 12:56:56 [5511] querying for removed/held jobs
03/04/16 12:56:56 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:56:56 [5511] Fetched 0 job ads from schedd
03/04/16 12:56:56 [5511] leaving doContactSchedd()
03/04/16 12:56:58 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:56:58 [5511] GAHP[5740] -> 'S' '0'
03/04/16 12:57:00 [5511] Evaluating staleness of remote job statuses.
03/04/16 12:57:10 [5511] (124.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 12:57:10 [5511] (124.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 12:57:10 [5511] GAHP[5740] <- 'BLAH_JOB_STATUS 4 pbs/20160304/3392916'
03/04/16 12:57:10 [5511] GAHP[5740] -> 'S'
03/04/16 12:57:10 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:57:10 [5511] GAHP[5740] -> 'R'
03/04/16 12:57:10 [5511] GAHP[5740] -> 'S' '1'
03/04/16 12:57:10 [5511] GAHP[5740] -> '4' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 12:57:10 [5511] (124.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 12:57:10 [5511] (124.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 12:57:56 [5511] Received CHECK_LEASES signal
03/04/16 12:57:56 [5511] in doContactSchedd()
03/04/16 12:57:56 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:57:56 [5511] querying for renewed leases
03/04/16 12:57:56 [5511] querying for removed/held jobs
03/04/16 12:57:56 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:57:56 [5511] Fetched 0 job ads from schedd
03/04/16 12:57:56 [5511] leaving doContactSchedd()
03/04/16 12:57:58 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:57:58 [5511] GAHP[5740] -> 'S' '0'
03/04/16 12:58:00 [5511] Evaluating staleness of remote job statuses.
03/04/16 12:58:10 [5511] (124.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 12:58:10 [5511] (124.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 12:58:10 [5511] GAHP[5740] <- 'BLAH_JOB_STATUS 5 pbs/20160304/3392916'
03/04/16 12:58:10 [5511] GAHP[5740] -> 'S'
03/04/16 12:58:11 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:58:11 [5511] GAHP[5740] -> 'R'
03/04/16 12:58:11 [5511] GAHP[5740] -> 'S' '1'
03/04/16 12:58:11 [5511] GAHP[5740] -> '5' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 12:58:11 [5511] (124.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 12:58:11 [5511] (124.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 12:58:56 [5511] Received CHECK_LEASES signal
03/04/16 12:58:56 [5511] in doContactSchedd()
03/04/16 12:58:56 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:58:56 [5511] querying for renewed leases
03/04/16 12:58:56 [5511] querying for removed/held jobs
03/04/16 12:58:56 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:58:56 [5511] Fetched 0 job ads from schedd
03/04/16 12:58:56 [5511] leaving doContactSchedd()
03/04/16 12:58:58 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:58:58 [5511] GAHP[5740] -> 'S' '0'
03/04/16 12:59:00 [5511] Evaluating staleness of remote job statuses.
03/04/16 12:59:11 [5511] (124.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 12:59:11 [5511] (124.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 12:59:11 [5511] GAHP[5740] <- 'BLAH_JOB_STATUS 6 pbs/20160304/3392916'
03/04/16 12:59:11 [5511] GAHP[5740] -> 'S'
03/04/16 12:59:12 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:59:12 [5511] GAHP[5740] -> 'R'
03/04/16 12:59:12 [5511] GAHP[5740] -> 'S' '1'
03/04/16 12:59:12 [5511] GAHP[5740] -> '6' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 12:59:12 [5511] (124.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 12:59:12 [5511] (124.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 12:59:55 [5511] Evaluating periodic job policy expressions.
03/04/16 12:59:56 [5511] Received CHECK_LEASES signal
03/04/16 12:59:56 [5511] in doContactSchedd()
03/04/16 12:59:56 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 12:59:58 [5511] querying for renewed leases
03/04/16 12:59:58 [5511] querying for removed/held jobs
03/04/16 12:59:58 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 12:59:58 [5511] Fetched 0 job ads from schedd
03/04/16 12:59:58 [5511] leaving doContactSchedd()
03/04/16 12:59:58 [5511] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 1
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_5>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457114398
IdleJobs = 1
JobLimit = 10000

03/04/16 12:59:58 [5511] Trying to update collector <10.31.131.202:9619>
03/04/16 12:59:58 [5511] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 12:59:58 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 12:59:58 [5511] GAHP[5740] -> 'S' '0'
03/04/16 13:00:00 [5511] Evaluating staleness of remote job statuses.
03/04/16 13:00:12 [5511] (124.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 13:00:12 [5511] (124.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 13:00:12 [5511] GAHP[5740] <- 'BLAH_JOB_STATUS 7 pbs/20160304/3392916'
03/04/16 13:00:12 [5511] GAHP[5740] -> 'S'
03/04/16 13:00:13 [5511] GAHP[5740] <- 'RESULTS'
03/04/16 13:00:13 [5511] GAHP[5740] -> 'R'
03/04/16 13:00:13 [5511] GAHP[5740] -> 'S' '1'
03/04/16 13:00:13 [5511] GAHP[5740] -> '7' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 13:00:13 [5511] (124.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 13:00:13 [5511] (124.0) blah_job_status() failed: Error parsing classad or job not found
03/04/16 13:00:13 [5511] (124.0) gm state change: GM_POLL_ACTIVE -> GM_HOLD
03/04/16 13:00:13 [5511] (124.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 13:00:13 [5511] in doContactSchedd()
03/04/16 13:00:13 [5511] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:00:13 [5511] querying for removed/held jobs
03/04/16 13:00:13 [5511] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:00:13 [5511] Fetched 0 job ads from schedd
03/04/16 13:00:13 [5511] Updating classad values for 124.0:
03/04/16 13:00:13 [5511]    EnteredCurrentStatus = 1457114413
03/04/16 13:00:13 [5511]    HoldReason = "Error parsing classad or job not found"
03/04/16 13:00:13 [5511]    HoldReasonCode = 0
03/04/16 13:00:13 [5511]    HoldReasonSubCode = 0
03/04/16 13:00:13 [5511]    JobStatus = 5
03/04/16 13:00:13 [5511]    LastReleaseReason = "Data files spooled"
03/04/16 13:00:13 [5511]    Managed = "Schedd"
03/04/16 13:00:13 [5511]    NumSystemHolds = 1
03/04/16 13:00:13 [5511]    PeriodicRemove = false
03/04/16 13:00:13 [5511]    ReleaseReason = undefined
03/04/16 13:00:13 [5511]    RemoteWallClockTime = 0.0
03/04/16 13:00:16 [5511] No jobs left, shutting down
03/04/16 13:00:16 [5511] leaving doContactSchedd()
03/04/16 13:00:16 [5511] Got SIGTERM. Performing graceful shutdown.
03/04/16 13:00:16 [5511] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 13:00:16 [5511] **** condor_gridmanager (condor_GRIDMANAGER) pid 5511 EXITING WITH STATUS 0
03/04/16 13:01:26 Result of reading /etc/issue:  CentOS release 6.5 (Final)
 
03/04/16 13:01:26 Using IDs: 16 processors, 8 CPUs, 8 HTs
03/04/16 13:01:26 Enumerating interfaces: lo 127.0.0.1 up
03/04/16 13:01:26 Enumerating interfaces: eth2 10.31.131.202 up
03/04/16 13:01:26 Enumerating interfaces: eth3 140.247.179.131 up
03/04/16 13:01:26 Initializing Directory: curr_dir = /usr/share/condor-ce/config.d
03/04/16 13:01:26 Initializing Directory: curr_dir = /etc/condor-ce/config.d
03/04/16 13:01:26 ******************************************************
03/04/16 13:01:26 ** condor_gridmanager (CONDOR_GRIDMANAGER) STARTING UP
03/04/16 13:01:26 ** /usr/sbin/condor_gridmanager
03/04/16 13:01:26 ** SubsystemInfo: name=GRIDMANAGER type=DAEMON(12) class=DAEMON(1)
03/04/16 13:01:26 ** Configuration: subsystem:GRIDMANAGER local:<NONE> class:DAEMON
03/04/16 13:01:26 ** $CondorVersion: 8.2.8 Apr 08 2015 $
03/04/16 13:01:26 ** $CondorPlatform: X86_64-CentOS_6.6 $
03/04/16 13:01:26 ** PID = 8607
03/04/16 13:01:26 ** Log last touched 3/4 13:00:16
03/04/16 13:01:26 ******************************************************
03/04/16 13:01:26 Using config source: /etc/condor-ce/condor_config
03/04/16 13:01:26 Using local config sources: 
03/04/16 13:01:26    /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/01-ce-info-services-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/01-common-collector-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/02-ce-lsf-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
03/04/16 13:01:26    /usr/share/condor-ce/config.d/03-managed-fork-defaults.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/01-ce-auth.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/01-ce-router.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/01-common-auth.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/02-ce-lsf.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/02-ce-pbs.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/03-ce-shared-port.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/03-managed-fork.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/10-ce-collector-generated.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/10-osg-attributes-generated.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/50-osg-configure.conf
03/04/16 13:01:26    /etc/condor-ce/config.d/99-local.conf
03/04/16 13:01:26    /usr/share/condor-ce/condor_ce_router_defaults|
03/04/16 13:01:26 config Macros = 144, Sorted = 144, StringBytes = 12556, TablesBytes = 5392
03/04/16 13:01:26 CLASSAD_CACHING is ENABLED
03/04/16 13:01:26 Daemon Log is logging: D_FULLDEBUG D_ALWAYS D_ERROR
03/04/16 13:01:26 SharedPortEndpoint: waiting for connections to named socket 32379_af3e_6
03/04/16 13:01:26 DaemonCore: command socket at <140.247.179.131:9620?sock=32379_af3e_6>
03/04/16 13:01:26 DaemonCore: private command socket at <140.247.179.131:9620?sock=32379_af3e_6>
03/04/16 13:01:26 Setting maximum accepts per cycle 8.
03/04/16 13:01:26 Setting maximum reaps per cycle 8.
03/04/16 13:01:26 Will use TCP to update collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 13:01:26 [8607] Welcome to the all-singing, all dancing, "amazing" GridManager!
03/04/16 13:01:26 [8607] DaemonCore: No more children processes to reap.
03/04/16 13:01:26 [8607] DaemonCore: in SendAliveToParent()
03/04/16 13:01:26 [8607] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:01:26 [8607] IPVERIFY: checking net2.rc.fas.harvard.edu against 140.247.179.131
03/04/16 13:01:26 [8607] IPVERIFY: comparing 10.31.131.202 to 140.247.179.131
03/04/16 13:01:26 [8607] IPVERIFY: ip found is 0
03/04/16 13:01:26 [8607] WARNING: forward resolution of net2.rc.fas.harvard.edu doesn't match 140.247.179.131!
03/04/16 13:01:26 [8607] condor_write(): Socket closed when trying to write 53 bytes to daemon at <140.247.179.131:9620>, fd is 7
03/04/16 13:01:26 [8607] Buf::write(): condor_write() failed
03/04/16 13:01:26 [8607] ChildAliveMsg: failed to send DC_CHILDALIVE to parent daemon at <140.247.179.131:9620> (try 1 of 3): CEDAR:6002:failed to send EOM
03/04/16 13:01:26 [8607] SharedPortClient: sent connection request to daemon at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:01:26 [8607] Completed DC_CHILDALIVE to daemon at <140.247.179.131:9620>
03/04/16 13:01:26 [8607] DaemonCore: Leaving SendAliveToParent() - success
03/04/16 13:01:26 [8607] Checking proxies
03/04/16 13:01:29 [8607] Received ADD_JOBS signal
03/04/16 13:01:29 [8607] in doContactSchedd()
03/04/16 13:01:29 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:01:29 [8607] querying for new jobs
03/04/16 13:01:29 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && (Managed =!= "ScheddDone") && (((Matched =!= FALSE) && (JobStatus != 5)) || (Managed =?= "External"))
03/04/16 13:01:29 [8607] Using job type INFNBatch for job 126.0
03/04/16 13:01:29 [8607] (126.0) SetJobLeaseTimers()
03/04/16 13:01:29 [8607] Found job 126.0 --- inserting
03/04/16 13:01:29 [8607] Fetched 1 new job ads from schedd
03/04/16 13:01:29 [8607] querying for removed/held jobs
03/04/16 13:01:29 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:01:29 [8607] Fetched 0 job ads from schedd
03/04/16 13:01:31 [8607] leaving doContactSchedd()
03/04/16 13:01:31 [8607] gahp server not up yet, delaying ping
03/04/16 13:01:31 [8607] *** UpdateLeases called
03/04/16 13:01:31 [8607]     Leases not supported, cancelling timer
03/04/16 13:01:31 [8607] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 0
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_6>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457114491
IdleJobs = 1
JobLimit = 10000

03/04/16 13:01:31 [8607] Trying to update collector <10.31.131.202:9619>
03/04/16 13:01:31 [8607] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 13:01:31 [8607] File descriptor limits: max 4096, safe 3277
03/04/16 13:01:31 [8607] (126.0) doEvaluateState called: gmState GM_INIT, remoteState 0
03/04/16 13:01:31 [8607] GAHP server pid = 9352
03/04/16 13:01:31 [8607] GAHP server version: $GahpVersion: 1.8.0 Mar 31 2008 INFN blahpd (poly,new_esc_format) $
03/04/16 13:01:31 [8607] GAHP[9352] <- 'COMMANDS'
03/04/16 13:01:31 [8607] GAHP[9352] -> 'S' 'ASYNC_MODE_OFF' 'ASYNC_MODE_ON' 'BLAH_GET_HOSTPORT' 'BLAH_JOB_CANCEL' 'BLAH_JOB_HOLD' 'BLAH_JOB_REFRESH_PROXY' 'BLAH_JOB_RESUME' 'BLAH_JOB_SEND_PROXY_TO_WORKER_NODE' 'BLAH_JOB_STATUS' 'BLAH_JOB_SUBMIT' 'BLAH_SET_GLEXEC_DN' 'BLAH_SET_GLEXEC_OFF' 'BLAH_SET_SUDO_ID' 'BLAH_SET_SUDO_OFF' 'COMMANDS' 'QUIT' 'RESULTS' 'VERSION'
03/04/16 13:01:31 [8607] GAHP[9352] <- 'ASYNC_MODE_ON'
03/04/16 13:01:31 [8607] GAHP[9352] -> 'S' 'Async mode on'
03/04/16 13:01:31 [8607] (126.0) gm state change: GM_INIT -> GM_START
03/04/16 13:01:31 [8607] (126.0) gm state change: GM_START -> GM_CLEAR_REQUEST
03/04/16 13:01:31 [8607] (126.0) gm state change: GM_CLEAR_REQUEST -> GM_UNSUBMITTED
03/04/16 13:01:31 [8607] (126.0) gm state change: GM_UNSUBMITTED -> GM_SAVE_SANDBOX_ID
03/04/16 13:01:31 [8607] Evaluating staleness of remote job statuses.
03/04/16 13:01:31 [8607] IPVERIFY: checking net2.rc.fas.harvard.edu against 10.31.131.202
03/04/16 13:01:31 [8607] IPVERIFY: matched 10.31.131.202 to 10.31.131.202
03/04/16 13:01:31 [8607] IPVERIFY: ip found is 1
03/04/16 13:01:36 [8607] resource  is now up
03/04/16 13:01:36 [8607] in doContactSchedd()
03/04/16 13:01:36 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:01:36 [8607] querying for removed/held jobs
03/04/16 13:01:36 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:01:36 [8607] Fetched 0 job ads from schedd
03/04/16 13:01:36 [8607] Updating classad values for 126.0:
03/04/16 13:01:36 [8607]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#126.0#1457114472"
03/04/16 13:01:36 [8607]    LastRemoteStatusUpdate = 1457114491
03/04/16 13:01:38 [8607] leaving doContactSchedd()
03/04/16 13:01:38 [8607] (126.0) doEvaluateState called: gmState GM_SAVE_SANDBOX_ID, remoteState 0
03/04/16 13:01:38 [8607] (126.0) gm state change: GM_SAVE_SANDBOX_ID -> GM_TRANSFER_INPUT
03/04/16 13:01:38 [8607] (126.0) gm state change: GM_TRANSFER_INPUT -> GM_SUBMIT
03/04/16 13:01:38 [8607] GAHP[9352] <- 'BLAH_JOB_SUBMIT 2 [\ RequestMemory\ =\ 2000;\ queue\ =\ "";\ Out\ =\ "_condor_stdout";\ cerequirements\ =\ "CondorCE\ ==\ 1";\ gridtype\ =\ "pbs";\ Environment\ =\ "HOME=/n/atlasgrid/home/usatlas1\ CONDORCE_COLLECTOR_HOST=net2.rc.fas.harvard.edu:9619\ OSG_GRID='/n/atlasgrid/osg-wn-client'\ OSG_SQUID_LOCATION='net2.rc.fas.harvard.edu:3128'\ OSG_SITE_READ='None'\ OSG_APP='/n/atlasgrid/osg/app'\ OSG_GLEXEC_LOCATION='None'\ OSG_DATA='/n/atlasgrid/osg/data'\ OSG_HOSTNAME='net2.rc.fas.harvard.edu'\ OSG_STORAGE_ELEMENT='True'\ OSG_SITE_NAME='NET2_HU'\ ATLAS_LOCAL_AREA='/n/atlasgrid/osg/app/atlas_app/local'\ GLOBUS_LOCATION='/usr'\ OSG_WN_TMP='/scratch'\ OSG_SITE_WRITE='None'\ OSG_DEFAULT_SE='net2.rc.fas.harvard.edu'\ OSG_JOB_CONTACT='net2.rc.fas.harvard.edu/jobmanager-pbs'";\ GridResource\ =\ "batch\ pbs";\ x509userproxy\ =\ "/n/atlasgrid/condor/125/0/cluster125.proc0.subproc0/x509up_u556792";\ Iwd\ =\ "/n/atlasgrid/condor/125/0/cluster125.proc0.subproc0";\ Args\ =\ "";\ JobUniverse\ =\ 5;\ Err\ =\ "_condor_stderr";\ In\ =\ "/dev/null";\ Cmd\ =\ "/n/atlasgrid/condor/125/0/cluster125.proc0.subproc0/env";\ NodeNumber\ =\ 1;\ SMPGranularity\ =\ 1;\ JobDirectory\ =\ "home_bl_net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#126.0#1457114472";\ TransferOutputRemaps\ =\ undefined;\ CurrentTime\ =\ time()\ ]'
03/04/16 13:01:38 [8607] GAHP[9352] -> 'S'
03/04/16 13:01:41 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:01:41 [8607] GAHP[9352] -> 'R'
03/04/16 13:01:41 [8607] GAHP[9352] -> 'S' '1'
03/04/16 13:01:41 [8607] GAHP[9352] -> '2' '0' 'No error' 'pbs/20160304/3392944'
03/04/16 13:01:41 [8607] (126.0) doEvaluateState called: gmState GM_SUBMIT, remoteState 0
03/04/16 13:01:41 [8607] (126.0) gm state change: GM_SUBMIT -> GM_SUBMIT_SAVE
03/04/16 13:01:43 [8607] in doContactSchedd()
03/04/16 13:01:43 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:01:43 [8607] querying for removed/held jobs
03/04/16 13:01:43 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:01:43 [8607] Fetched 0 job ads from schedd
03/04/16 13:01:43 [8607] Updating classad values for 126.0:
03/04/16 13:01:43 [8607]    DelegatedProxyExpiration = 1457149842
03/04/16 13:01:43 [8607]    GridJobId = "batch pbs net2.rc.fas.harvard.edu_9619_net2.rc.fas.harvard.edu#126.0#1457114472 pbs/20160304/3392944"
03/04/16 13:01:43 [8607] leaving doContactSchedd()
03/04/16 13:01:43 [8607] (126.0) doEvaluateState called: gmState GM_SUBMIT_SAVE, remoteState 0
03/04/16 13:01:43 [8607] (126.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED
03/04/16 13:02:26 [8607] Received CHECK_LEASES signal
03/04/16 13:02:26 [8607] in doContactSchedd()
03/04/16 13:02:26 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:02:26 [8607] querying for renewed leases
03/04/16 13:02:26 [8607] querying for removed/held jobs
03/04/16 13:02:26 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:02:26 [8607] Fetched 0 job ads from schedd
03/04/16 13:02:26 [8607] leaving doContactSchedd()
03/04/16 13:02:31 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:02:31 [8607] GAHP[9352] -> 'S' '0'
03/04/16 13:02:31 [8607] Evaluating staleness of remote job statuses.
03/04/16 13:02:43 [8607] (126.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 13:02:43 [8607] (126.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 13:02:43 [8607] GAHP[9352] <- 'BLAH_JOB_STATUS 3 pbs/20160304/3392944'
03/04/16 13:02:43 [8607] GAHP[9352] -> 'S'
03/04/16 13:02:44 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:02:44 [8607] GAHP[9352] -> 'R'
03/04/16 13:02:44 [8607] GAHP[9352] -> 'S' '1'
03/04/16 13:02:44 [8607] GAHP[9352] -> '3' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 13:02:44 [8607] (126.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 13:02:44 [8607] (126.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 13:03:26 [8607] Received CHECK_LEASES signal
03/04/16 13:03:26 [8607] in doContactSchedd()
03/04/16 13:03:26 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:03:26 [8607] querying for renewed leases
03/04/16 13:03:26 [8607] querying for removed/held jobs
03/04/16 13:03:26 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:03:26 [8607] Fetched 0 job ads from schedd
03/04/16 13:03:26 [8607] leaving doContactSchedd()
03/04/16 13:03:31 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:03:31 [8607] GAHP[9352] -> 'S' '0'
03/04/16 13:03:31 [8607] Evaluating staleness of remote job statuses.
03/04/16 13:03:44 [8607] (126.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 13:03:44 [8607] (126.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 13:03:44 [8607] GAHP[9352] <- 'BLAH_JOB_STATUS 4 pbs/20160304/3392944'
03/04/16 13:03:44 [8607] GAHP[9352] -> 'S'
03/04/16 13:03:45 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:03:45 [8607] GAHP[9352] -> 'R'
03/04/16 13:03:45 [8607] GAHP[9352] -> 'S' '1'
03/04/16 13:03:45 [8607] GAHP[9352] -> '4' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 13:03:45 [8607] (126.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 13:03:45 [8607] (126.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 13:04:26 [8607] Received CHECK_LEASES signal
03/04/16 13:04:26 [8607] in doContactSchedd()
03/04/16 13:04:26 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:04:26 [8607] querying for renewed leases
03/04/16 13:04:26 [8607] querying for removed/held jobs
03/04/16 13:04:26 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:04:26 [8607] Fetched 0 job ads from schedd
03/04/16 13:04:26 [8607] leaving doContactSchedd()
03/04/16 13:04:31 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:04:31 [8607] GAHP[9352] -> 'S' '0'
03/04/16 13:04:31 [8607] Evaluating staleness of remote job statuses.
03/04/16 13:04:45 [8607] (126.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 13:04:45 [8607] (126.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 13:04:45 [8607] GAHP[9352] <- 'BLAH_JOB_STATUS 5 pbs/20160304/3392944'
03/04/16 13:04:45 [8607] GAHP[9352] -> 'S'
03/04/16 13:04:46 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:04:46 [8607] GAHP[9352] -> 'R'
03/04/16 13:04:46 [8607] GAHP[9352] -> 'S' '1'
03/04/16 13:04:46 [8607] GAHP[9352] -> '5' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 13:04:46 [8607] (126.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 13:04:46 [8607] (126.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 13:05:26 [8607] Received CHECK_LEASES signal
03/04/16 13:05:26 [8607] in doContactSchedd()
03/04/16 13:05:26 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:05:26 [8607] querying for renewed leases
03/04/16 13:05:26 [8607] querying for removed/held jobs
03/04/16 13:05:26 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:05:26 [8607] Fetched 0 job ads from schedd
03/04/16 13:05:26 [8607] leaving doContactSchedd()
03/04/16 13:05:31 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:05:31 [8607] GAHP[9352] -> 'S' '0'
03/04/16 13:05:31 [8607] Evaluating staleness of remote job statuses.
03/04/16 13:05:46 [8607] (126.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 13:05:46 [8607] (126.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 13:05:46 [8607] GAHP[9352] <- 'BLAH_JOB_STATUS 6 pbs/20160304/3392944'
03/04/16 13:05:46 [8607] GAHP[9352] -> 'S'
03/04/16 13:05:47 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:05:47 [8607] GAHP[9352] -> 'R'
03/04/16 13:05:47 [8607] GAHP[9352] -> 'S' '1'
03/04/16 13:05:47 [8607] GAHP[9352] -> '6' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 13:05:47 [8607] (126.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 13:05:47 [8607] (126.0) gm state change: GM_POLL_ACTIVE -> GM_SUBMITTED
03/04/16 13:06:26 [8607] Evaluating periodic job policy expressions.
03/04/16 13:06:26 [8607] Received CHECK_LEASES signal
03/04/16 13:06:26 [8607] in doContactSchedd()
03/04/16 13:06:26 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:06:26 [8607] querying for renewed leases
03/04/16 13:06:26 [8607] querying for removed/held jobs
03/04/16 13:06:26 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:06:26 [8607] Fetched 0 job ads from schedd
03/04/16 13:06:26 [8607] leaving doContactSchedd()
03/04/16 13:06:31 [8607] BaseResource::UpdateResource: 
NumJobs = 1
HashName = "batch PBS"
Machine = "net2.rc.fas.harvard.edu"
SubmitsAllowed = 1
Name = "batch "
CondorPlatform = "$CondorPlatform: X86_64-CentOS_6.6 $"
RunningJobs = 0
Owner = "usatlas1"
MyType = "Grid"
ScheddName = "net2.rc.fas.harvard.edu"
MyAddress = "<140.247.179.131:9620?sock=32379_af3e_6>"
CondorVersion = "$CondorVersion: 8.2.8 Apr 08 2015 $"
SubmitsWanted = 0
ScheddIpAddr = "<140.247.179.131:9620?sock=32132_5a30_4>"
CurrentTime = time()
MyCurrentTime = 1457114791
IdleJobs = 1
JobLimit = 10000

03/04/16 13:06:31 [8607] Trying to update collector <10.31.131.202:9619>
03/04/16 13:06:31 [8607] Attempting to send update via TCP to collector net2.rc.fas.harvard.edu <10.31.131.202:9619>
03/04/16 13:06:31 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:06:31 [8607] GAHP[9352] -> 'S' '0'
03/04/16 13:06:31 [8607] Evaluating staleness of remote job statuses.
03/04/16 13:06:47 [8607] (126.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 0
03/04/16 13:06:47 [8607] (126.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE
03/04/16 13:06:47 [8607] GAHP[9352] <- 'BLAH_JOB_STATUS 7 pbs/20160304/3392944'
03/04/16 13:06:47 [8607] GAHP[9352] -> 'S'
03/04/16 13:06:48 [8607] GAHP[9352] <- 'RESULTS'
03/04/16 13:06:48 [8607] GAHP[9352] -> 'R'
03/04/16 13:06:48 [8607] GAHP[9352] -> 'S' '1'
03/04/16 13:06:48 [8607] GAHP[9352] -> '7' '1' 'Error parsing classad or job not found' '0' 'N/A'
03/04/16 13:06:48 [8607] (126.0) doEvaluateState called: gmState GM_POLL_ACTIVE, remoteState 0
03/04/16 13:06:48 [8607] (126.0) blah_job_status() failed: Error parsing classad or job not found
03/04/16 13:06:48 [8607] (126.0) gm state change: GM_POLL_ACTIVE -> GM_HOLD
03/04/16 13:06:48 [8607] (126.0) gm state change: GM_HOLD -> GM_DELETE
03/04/16 13:06:48 [8607] in doContactSchedd()
03/04/16 13:06:48 [8607] SharedPortClient: sent connection request to schedd at <140.247.179.131:9620> for shared port id 32132_5a30_4
03/04/16 13:06:48 [8607] querying for removed/held jobs
03/04/16 13:06:48 [8607] Using constraint ((Owner=?="usatlas1"&&JobUniverse==9)) && ((Managed =!= "ScheddDone")) && (JobStatus == 3 || JobStatus == 4 || (JobStatus == 5 && Managed =?= "External"))
03/04/16 13:06:48 [8607] Fetched 0 job ads from schedd
03/04/16 13:06:48 [8607] Updating classad values for 126.0:
03/04/16 13:06:48 [8607]    EnteredCurrentStatus = 1457114808
03/04/16 13:06:48 [8607]    HoldReason = "Error parsing classad or job not found"
03/04/16 13:06:48 [8607]    HoldReasonCode = 0
03/04/16 13:06:48 [8607]    HoldReasonSubCode = 0
03/04/16 13:06:48 [8607]    JobStatus = 5
03/04/16 13:06:48 [8607]    LastReleaseReason = "Data files spooled"
03/04/16 13:06:48 [8607]    Managed = "Schedd"
03/04/16 13:06:48 [8607]    NumSystemHolds = 1
03/04/16 13:06:48 [8607]    PeriodicRemove = false
03/04/16 13:06:48 [8607]    ReleaseReason = undefined
03/04/16 13:06:48 [8607]    RemoteWallClockTime = 0.0
03/04/16 13:06:48 [8607] No jobs left, shutting down
03/04/16 13:06:48 [8607] leaving doContactSchedd()
03/04/16 13:06:48 [8607] Got SIGTERM. Performing graceful shutdown.
03/04/16 13:06:48 [8607] Started timer to call main_shutdown_fast in 1800 seconds
03/04/16 13:06:48 [8607] **** condor_gridmanager (condor_GRIDMANAGER) pid 8607 EXITING WITH STATUS 0
