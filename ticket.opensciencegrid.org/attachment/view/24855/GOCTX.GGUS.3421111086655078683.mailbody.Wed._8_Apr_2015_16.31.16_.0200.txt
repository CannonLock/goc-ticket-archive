The expressions I introduced were the same that we have for the Globus 
gatekeepers:

         periodic_hold = periodic_hold=GlobusResourceUnavailableTime =!= 
UNDEFINED &&(CurrentTime-GlobusResourceUnavailableTime>1200)
         periodic_remove = periodic_remove = (JobStatus == 5 && 
(CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && 
globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400) || 
(JobStatus == 2 && (CurrentTime - EnteredCurrentStatus) > 604800)

until I realized they are composed by a lot of Globus* variables.
I removed them on April 2nd for factory gridui18 (*), and on April 6th 
for factories gridui19/20/21 (**).
Last time a pilot was submitted with those expressions was:

         -- for factory gridui18 (*): on April 1st.
         -- for factories gridui19/20/21 (**): on April 5th.

I stopped pilot submission for LUCILLE_CE via HTCondor-CE.


The 2 examples of failing jobs correspond to pilots ID 6968250.39 and 
6969515.0 (these are the numbers I was asking for).
They were submitted 2 days ago, when there was no periodic_* expression 
in the submit file. So that was not the reason they failed.
Running condor_q -analyze  on them does not give me any information. 
Just an error(?) message:

         $ condor_q -analyze 6969515.0
         Error:  Could not fetch startd ads

so I have no info about what happened to those pilots.

Cheers,
Jose


(*)
[AGLT2_SL6-htcondor]
[AGLT2_MCORE-htcondor]
[ANALY_AGLT2_SL6-htcondor]
[ANALY_AGLT2_TIER3_TEST-htcondor]
[ANALY_MWT2_SL6-uct2-gk-htcondor]
[ANALY_SLAC-htcondor]
[ANALY_SLAC_SHORT_1HR-htcondor]
[LUCILLE_CE-htcondor]
[LUCILLE_MCORE-htcondor]
[MWT2_SL6-uct2-gk-htcondor]
[MWT2_MCORE-uct2-gk-htcondor]
[OU_OSCER_ATLAS-htcondor]
[SLACXRD-htcondor]
[SLACXRD_LMEM-htcondor]
[SLACXRD_MP8-htcondor]

(**)
[AGLT2_SL6-htcondor]
[AGLT2_MCORE-htcondor]
[ANALY_AGLT2_SL6-htcondor]
[ANALY_AGLT2_TIER3_TEST-htcondor]
[ANALY_MWT2_SL6-uct2-gk-htcondor]
[ANALY_SLAC_SHORT_1HR-htcondor]
[MWT2_SL6-uct2-gk-htcondor]
[MWT2_MCORE-uct2-gk-htcondor]
[SLACXRD_MP8-htcondor]






On 04/08/2015 08:44 AM, Horst Severini wrote:
> Hi Wei and Jose,
>
> well, I certainly don't understand what's going on.
>
> But in the mean time, please turn off HTCondor-CE pilot submission
> to Lucille, since it's pointless to have all these failed jobs
> while we're still trying to figure out what's going on.
>
> Thanks,
>
> 	Horst
>
> "Yang, Wei" <yangw@slac.stanford.edu> wrote:
>
>> Hi Jose,
>>
>> Could you let us know what periodic_remove parameters you introduced? I looked into job_lease_duration/JobLeaseDuration and TimerRemove. I also reported the issue to the Condor Team. Not sure any of us are looking at the right direction.
>>
>> regards,
>> Wei
>>
>>
>> ________________________________________
>> From: Horst Severini <hs@nhn.ou.edu>
>> Sent: Tuesday, April 7, 2015 12:29 PM
>> To: Caballero, Jose
>> Cc: Hover, John; <helpdesk@ggus.org>; <atlas-adc-cloud-US@cern.ch>; <snow@nhn.ou.edu>; Horst Severini
>> Subject: Re: GGUS-Ticket-ID: #112850 TEAM TICKET SUBMITTED - Lucille_CE: production tasks failed 95% with error lost heartbeat and Job killed by signal 15
>>
>> Hi Jose,
>>
>> sorry to hear you're sick!
>>
>> Two examples of failed jobs:
>> http://bigpanda.cern.ch/job?pandaid=2441872600
>> http://bigpanda.cern.ch/job?pandaid=2442107290
>>
>> But in the mean time, please turn off HTCondor-CE submission on Lucille,
>> until we can get this figured out.
>>
>> Wei already started a thread about this with the HTCondor-CE experts, so
>> we should probably follow up on this there ...
>>
>> Thanks a lot,
>>
>>          Horst
>>
>> On 04/07/2015 01:59 PM, Caballero, Jose wrote:
>>> Hi Horst,
>>>
>>> sorry for the late reply. John is on vacations, and I am at home sick (again!!).
>>>
>>> Are you able to point to me to the ID of some pilot that was killed?
>>> When did it happen?
>>> I remember I introduced some periodic_remove expression in the factory for all HTCondor-CEs, and then I removed it back once I realized it had a lot of Globus variables.
>>> It is possible that this non completely periodic_remove expression killed some pilots for a couple of days last week...
>>>
>>> I can also have a look into the heartbeat. Again, point to me to pilot ID.
>>>
>>> Cheers,
>>> Jose
>>>
>>>
>>> On Apr 7, 2015, at 8:58 AM, Horst Severini wrote:
>>>
>>>>> Hi all,
>>>>>
>>>>> hmm, this looks very much like the same HTCondor-CE problem that SLAC
>>>>> is having, since I can see no HTCondor-CE job in Lucille's local
>>>>> Condor queue that is running for more than 3h, whereas GRAM jobs
>>>>> are running much longer, up to 17h currently.
>>>>>
>>>>> Jose or John, could you please turn off HTCondor-CE submission for Lucille,
>>>>> until we can figure this out?
>>>>>
>>>>> I also see several lost heartbeat jobs for OU_OSCER_ATLAS, but I'm not
>>>>> totally sure these are related to the same issue, since OSCER is only
>>>>> receiving very few HTCondor-CE pilots so far, so the problem doesn't
>>>>> show up at a large scale.
>>>>>
>>>>> Maybe someone with the expertise could look at OSCER jobs and see
>>>>> whether the lost heartbeat jobs on OSCER were submitted via HTCondor-CE?
>>>>> I tried to look for that myself, but I can't even find any log files
>>>>> for these jobs, so it's hard for me to tell how they were submitted.
>>>>>
>>>>> But on Lucille it is very obvious, from the way they show up in the
>>>>> local Condor queue; here's an excerpt from 'condor_q':
>>>>>
>>>>> ----
>>>>> 80788.0   usatlas1        4/6  22:59   0+08:56:19 R  0   1709.0 data --wrapperlogl
>>>>> 81033.0   usatlas1        4/7  00:06   0+07:49:45 R  0   1709.0 data --wrapperlogl
>>>>> 81948.0   usatlas1        4/7  05:01   0+02:52:36 R  0   1953.1 wrapper-0.9.12.sh
>>>>> 81949.0   usatlas1        4/7  05:01   0+02:52:36 R  0   1709.0 wrapper-0.9.12.sh
>>>>> 81950.0   usatlas1        4/7  05:01   0+02:52:37 R  0   1709.0 wrapper-0.9.12.sh
>>>>> ----
>>>>>
>>>>> So any HTCondor-CE (wrapper) job running longer than 2:52 seems to be
>>>>> killed off by HTCondor-CE, just like at SLAC.
>>>>>
>>>>> So I think we need to stop HTCondor-CE submission globally until
>>>>> we understand this problem.
>>>>>
>>>>> Thanks,
>>>>>
>>>>>     Horst
>>>>>
>>>>> <helpdesk@ggus.org>  wrote:
>>>>>
>>>>>>> Dear group members,
>>>>>>> TEAM ticket #112850 was submitted in GGUS system.
>>>>>>>
>>>>>>> REFERENCE LINK:https://ggus.eu/index.php?mode=ticket_info&ticket_id=112850
>>>>>>> SUBJECT: Lucille_CE: production tasks failed 95% with error lost heartbeat and Job killed by signal 15
>>>>>>>
>>>>>>> -----------------------------------------------------------------------------
>>>>>>> TICKET INFORMATION:
>>>>>>> -----------------------------------------------------------------------------
>>>>>>> RESPONSIBLE UNIT -> OSG(Prod)
>>>>>>> STATUS -> assigned
>>>>>>> NOTIFIED SITE -> LUCILLE
>>>>>>> CONCERNED VO -> atlas
>>>>>>> PRIORITY -> less urgent
>>>>>>> TYPE OF ISSUE -> Other
>>>>>>> SUBMITTER ->
>>>>>>> DETAILS ->
>>>>>>> About 95% and 2.4k production jobs failed on site Lucille_CE with error lost heartbeat or Job killed by signal 15, as shown here:
>>>>>>> http://dashb-atlas-ssb.cern.ch/dashboard/request.py/sitehistory?site=LUCILLE#currentView=Shifter+view
>>>>>>>
>>>>>>> All the failed jobs can be found:
>>>>>>> http://bigpanda.cern.ch/jobs/?computingsite=Lucille_CE&jobstatus=failed&hours=12&jobtype=production&display_limit=100
>>>>>>>
>>>>>>> Two examples of the failed job:
>>>>>>> http://bigpanda.cern.ch/job?pandaid=2441872600
>>>>>>>
>>>>>>> http://bigpanda.cern.ch/job?pandaid=2442107290
>>>>>>>
>>>>>>> Please have a look.
>>>>>>>
>>>>>>> *********************************************************************************
>>>>>>> This is an automated mail. When replying don't change the subject line!
>>>>>>> Type your text above this box and S T R I P  P R E V I O U S  M A I L S please!!
>>>>>>> *********************************************************************************


